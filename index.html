<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://cdn.petpictures.xyz/2016/02/07/so-pretty-dogs-photo-14940252-fanpop.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Shuang</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>A propos</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Homepage</a></li>
				        
							<li><a href="/archives">Posts</a></li>
				        
							<li><a href="/categories">Categories</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="github" target="_blank" href="https://github.com/Shuang0420" title="github">github</a>
					        
								<a class="linkedin" target="_blank" href="https://www.linkedin.com/in/shuang-xu-7008b894?trk=hp-identity-name" title="linkedin">linkedin</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/LDA/" style="font-size: 20px;">LDA</a> <a href="/tags/cluster/" style="font-size: 20px;">cluster</a> <a href="/tags/gensim/" style="font-size: 15px;">gensim</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/topic-modeling/" style="font-size: 20px;">topic modeling</a> <a href="/tags/word2vec/" style="font-size: 10px;">word2vec</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">颜色不一样的烟火…</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">Shuang</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
			
				<img lazy-src="http://cdn.petpictures.xyz/2016/02/07/so-pretty-dogs-photo-14940252-fanpop.jpg" class="js-avatar">
			
			</div>
			<hgroup>
			  <h1 class="header-author">Shuang</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Homepage</a></li>
		        
					<li><a href="/archives">Posts</a></li>
		        
					<li><a href="/categories">Categories</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Shuang0420" title="github">github</a>
			        
						<a class="linkedin" target="_blank" href="https://www.linkedin.com/in/shuang-xu-7008b894?trk=hp-identity-name" title="linkedin">linkedin</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
  
    <article id="post-word2vec详解之四-基于Hierarchical-Softmax-的模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/29/word2vec详解之四-基于Hierarchical-Softmax-的模型/" class="article-date">
  	<time datetime="2016-05-29T03:40:03.000Z" itemprop="datePublished">2016-05-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/29/word2vec详解之四-基于Hierarchical-Softmax-的模型/">word2vec详解之四 -- 基于Hierarchical Softmax 的模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。</p>
<h3 id="基于Hierarchical-Softmax-的模型"><a href="#基于Hierarchical-Softmax-的模型" class="headerlink" title="基于Hierarchical Softmax 的模型"></a>基于Hierarchical Softmax 的模型</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/6.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/7.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/8.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/9.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/10.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/11.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/12.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/1.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/2.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/3.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/4.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/5.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word2vec/">word2vec</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-在Mac-OS-El-Capitan下用pip安装Scrapy-失败" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/26/在Mac-OS-El-Capitan下用pip安装Scrapy-失败/" class="article-date">
  	<time datetime="2016-05-26T07:47:25.000Z" itemprop="datePublished">2016-05-26</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/26/在Mac-OS-El-Capitan下用pip安装Scrapy-失败/">在Mac OS El Capitan下用pip安装Scrapy 失败</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-PHP连接数据库js可视化数据" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/26/PHP连接数据库js可视化数据/" class="article-date">
  	<time datetime="2016-05-26T02:09:56.000Z" itemprop="datePublished">2016-05-26</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/26/PHP连接数据库js可视化数据/">PHP连接数据库js可视化数据</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-短问题归一化-LSI模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/25/短问题归一化-LSI模型/" class="article-date">
  	<time datetime="2016-05-25T12:59:36.000Z" itemprop="datePublished">2016-05-25</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/25/短问题归一化-LSI模型/">短句归一化--LSI模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="LSI-理解"><a href="#LSI-理解" class="headerlink" title="LSI 理解"></a>LSI 理解</h3><p>LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。</p>
<blockquote>
<p>潜在语义索引是一种用奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以可以从一篇文章中提取术语关系，从而建立起主要概念内容。</p>
</blockquote>
<h3 id="降维过程"><a href="#降维过程" class="headerlink" title="降维过程"></a>降维过程</h3><p>将文档库表示成VSM模型的词-文档矩阵Am×n(词-文档矩阵那就是词作为行，文档作为列，这是矩阵先行后列的表示决定的，当然如果表示成文档-词矩阵的话，后面的计算就要用该矩阵的转置了),其中m表示文档库中包含的所有不同的词的个(行数是不同词的个数)，即行向量表示一个词在不同文档出现的次数，n 表示文档库中的文档数(列数是不同文档的个数)，即列向量表示的是不同的文档.A表示为A = [α ij ],在此矩阵中 ,α ij为非负值 , 表示第 i 个词在第j 个文档中出现的频度。显然，A是稀疏矩阵(这是VSM和文档决定的)。</p>
<p>利用奇异值分解SVD(Singular Value Decomposition)求A的只有K个正交因子的降秩矩阵，该过程就是降维的过程。SVD的重要作用是把词和文档映射到同一个语义空间中，将词和文档表示为K个因子的形式。显然，这会丢失信息，但主要的信息却被保留了。为什么该过程可以降维呢？因为该过程解决了同义和多义现象。可以看出，K的取值对整个分类结果的影响很大。因为，K过小，则丢失信息就越多；K过大，信息虽然多，但可能有冗余且计算消耗大。K的选择也是值得研究的，不过一般取值为100-300，不绝对。</p>
<h3 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h3><p>对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。</p>
<ol>
<li><p>特征降维<br>LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。TFIDF是另一个通用的降维方法，通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF。<br>不过必须提醒的是，无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）。</p>
</li>
<li><p>单词相关度计算<br>LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。</p>
</li>
<li><p>聚类<br>直接用 LSI 聚类的情景还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。</p>
</li>
</ol>
<h3 id="实验说明"><a href="#实验说明" class="headerlink" title="实验说明"></a>实验说明</h3><p>用了python的gensim包<br>现有的数据是438条标准问题以及3300条人工问题（可以转化为438条标准问题），现在需要对人工问题做一个归一化。<br>这里采用LSI模型进行建模实验，步骤如下。</p>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><pre>
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import logging
import jieba
import jieba.posseg as pseg
# 防止乱码
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
# 打印log信息
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</pre>


<h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><pre>
# 标准FAQ，一行对应一条问句
f = open('FAQuniq.txt', 'r')
# 对问句进行分词
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in f]

# 抽取一个bag-of-words，将文档的token映射为id
dictionary = corpora.Dictionary(texts)
# 保存词典
dictionary.save('LSI.dict')

# 产生文档向量，将用字符串表示的文档转换为用id和词频表示的文档向量
corpus = [dictionary.doc2bow(text) for text in texts]

# 基于这些“训练文档”计算一个TF-IDF模型
tfidf = models.TfidfModel(corpus)

# 转化文档向量，将用词频表示的文档向量表示为一个用tf-idf值表示的文档向量
corpus_tfidf = tfidf[corpus]

# 训练LSI模型 即将训练文档向量组成的矩阵SVD分解，并做一个秩为2的近似SVD分解
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)

# 保存模型
lsi.save('LSI.pkl')
lsi.print_topics(20)</pre>


<h3 id="初始化验证performance的文件"><a href="#初始化验证performance的文件" class="headerlink" title="初始化验证performance的文件"></a>初始化验证performance的文件</h3><p>checkFile的每行格式为：</p>
<pre>原始问题的docid：对应的标准问题的topicid</pre>

<p>把它存到checkDict这个dictionary中，key是docid，value是topicid。</p>
<pre>
checkDict=dict()
def getCheckId():
    fcheck=open('checkFile.txt')
    for line in fcheck:
        line=line.strip('\n')
        if (len(line)==0):
            continue
        docid=line.split(":")[0]
        topicid=line.split(":")[1]
        checkDict[int(docid)]=int(topicid)
getCheckId()</pre>

<h3 id="归一化／计算文档相似度"><a href="#归一化／计算文档相似度" class="headerlink" title="归一化／计算文档相似度"></a>归一化／计算文档相似度</h3><pre>
# 建索引
index = similarities.MatrixSimilarity(lsi[corpus])

# 初始化分数
score1=0
score2=0
score3=0

# 读取文件，文件的每行格式为一个原始问句
f2=open('ORIFAQ3330.txt','r')
# count的作用是和checkFile的docid，即checkDict的key对应
count=1
for query in f2:
    # 获取该原始问句本应对应的正确标准问句
    if (not checkDict.has_key(count)):
        count+=1
        continue
    checkId=checkDict[count]
    # 将问句向量化
    query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
    # 再用之前训练好的LSI模型将其映射到二维的topic空间：
    query_lsi = lsi[query_bow]
    # 计算其和index中doc的余弦相似度了：
    sims = index[query_lsi]
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    # 找出最相关的三篇文档，计算这三篇文档是否包括标准问句，如果文档就是标准问句，对应的分数加1
    if (checkId==sort_sims[0][0]):
        score1+=1
    elif (checkId==sort_sims[1][0]):
        score2+=1
    elif (checkId==sort_sims[2][0]):
        score3+=1
    count+=1</pre>

<h3 id="打印分数"><a href="#打印分数" class="headerlink" title="打印分数"></a>打印分数</h3><pre>
print "Score1: ".format(score1*1.0/count)
print "Score2: ".format(score2*1.0/count)
print "Score3: ".format(score3*1.0/count)</pre>

<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>其实这里的结果非常差，原因是文档（每一条问句）太短，只有十几个字，另外文档数太少，LSI降维牺牲了准确率，下一个实验LDA的准确率相比会高很多。<br>另外，本次实验所用的样本分布并不均匀，“未收到奖励”类似问题出现的频率比“软件无声音”类似问题出现的频率要高很多。<strong><em>重申：LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。</em></strong></p>
<h3 id="LSI-缺陷"><a href="#LSI-缺陷" class="headerlink" title="LSI 缺陷"></a>LSI 缺陷</h3><p>常用的VSM文本表示模型中有两个主要的缺陷：</p>
<ol>
<li><p>该模型假设所有特征词条之间是相互独立、互不影响的（朴素贝叶斯也是这个思想），即该模型还是基于“词袋”模型（应该说所有利用VSM模型没有进行潜在语义分析的算法都是基于“词袋”假设）。</p>
</li>
<li><p>没有进行特征降维，特征维数可能会很高，向量空间可能很大，对存储和计算资源要求会比较高。</p>
</li>
</ol>
<p>LSI的基本思想是文本中的词与词之间不是孤立的，存在着某种潜在的语义关系，通过对样本数据的统计分析，让机器自动挖掘出这些潜在的语义关系，并把这些关系表示成计算机可以”理解”的模型。它可以消除词匹配过程中的同义和多义现象。它可以将传统的VSM降秩到一个低维的语义空间中，在该语义空间中计算文档的相似度等。总的说来，LSI就是利用词的语义关系对VSM模型进行降维，并提高分类的效果。</p>
<blockquote>
<p>参考链接<br><a href="http://www.zwbk.org/MyLemmaShow.aspx?lid=257113" target="_blank" rel="external">http://www.zwbk.org/MyLemmaShow.aspx?lid=257113</a><br><a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C" target="_blank" rel="external">http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LDA/">LDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/topic-modeling/">topic modeling</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-GibbsLDA-A-C-C-使用心得" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/25/GibbsLDA-A-C-C-使用心得/" class="article-date">
  	<time datetime="2016-05-25T04:20:35.000Z" itemprop="datePublished">2016-05-25</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/25/GibbsLDA-A-C-C-使用心得/">GibbsLDA++: A C/C++ 使用心得</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-AP聚类" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/19/AP聚类/" class="article-date">
  	<time datetime="2016-05-19T13:42:04.000Z" itemprop="datePublished">2016-05-19</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/19/AP聚类/">AP聚类</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。</p>
<p>Affinity Propagation (AP) 聚类是最近在Science杂志上提出的一种新的聚类算法。它根据N个数据点之间的相似度进行聚类,这些相似度可以是对称的,即两个数据点互相之间的相似度一样(如欧氏距离);也可以是不对称的,即两个数据点互相之间的相似度不等。这些相似度组成N×N的相似度矩阵S(其中N为有N个数据点)。AP算法不需要事先指定聚类数目,相反它将所有的数据点都作为潜在的聚类中心,称之为exemplar。以S矩阵的对角线上的数值s(k, k)作为k点能否成为聚类中心的评判标准,这意味着该值越大,这个点成为聚类中心的可能性也就越大,这个值又称作参考度p (preference)。<br>在这里介绍几个文中常出现的名词：<br>exemplar：指的是聚类中心。<br>similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，如－|| ||。文中，所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。<br>preference：数据点i的参考度称为P(i)或S(i,i)。是指点i作为聚类中心的参考度。一般取S相似度值的中值。<br>Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度。<br>Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度。</p>
<p>Script output:<br>Estimated number of clusters: 3<br>Homogeneity: 0.872<br>Completeness: 0.872<br>V-measure: 0.872<br>Adjusted Rand Index: 0.912<br>Adjusted Mutual Information: 0.871<br>Silhouette Coefficient: 0.753</p>
<p>Python source code: plot_affinity_propagation.py<br>print(<strong>doc</strong>)</p>
<p>from sklearn.cluster import AffinityPropagation<br>from sklearn import metrics<br>from sklearn.datasets.samples_generator import make_blobs</p>
<p>##############################################################################</p>
<h1 id="Generate-sample-data"><a href="#Generate-sample-data" class="headerlink" title="Generate sample data"></a>Generate sample data</h1><p>centers = [[1, 1], [-1, -1], [1, -1]]<br>X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,<br>                            random_state=0)</p>
<p>##############################################################################</p>
<h1 id="Compute-Affinity-Propagation"><a href="#Compute-Affinity-Propagation" class="headerlink" title="Compute Affinity Propagation"></a>Compute Affinity Propagation</h1><p>af = AffinityPropagation(preference=-50).fit(X)<br>cluster_centers_indices = af.cluster_centers<em>indices</em><br>labels = af.labels_</p>
<p>n<em>clusters</em> = len(cluster_centers_indices)</p>
<p>print(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))<br>print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))<br>print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))<br>print(“Adjusted Rand Index: %0.3f”<br>      % metrics.adjusted_rand_score(labels_true, labels))<br>print(“Adjusted Mutual Information: %0.3f”<br>      % metrics.adjusted_mutual_info_score(labels_true, labels))<br>print(“Silhouette Coefficient: %0.3f”<br>      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))</p>
<p>##############################################################################</p>
<h1 id="Plot-result"><a href="#Plot-result" class="headerlink" title="Plot result"></a>Plot result</h1><p>import matplotlib.pyplot as plt<br>from itertools import cycle</p>
<p>plt.close(‘all’)<br>plt.figure(1)<br>plt.clf()</p>
<p>colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)<br>for k, col in zip(range(n<em>clusters</em>), colors):<br>    class_members = labels == k<br>    cluster_center = X[cluster_centers_indices[k]]<br>    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)<br>    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,<br>             markeredgecolor=’k’, markersize=14)<br>    for x in X[class_members]:<br>        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)</p>
<p>plt.title(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>plt.show()</p>
<blockquote>
<p>参考链接<br><a href="http://scikit-learn.org/stable/modules/clustering.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/clustering.html</a><br><a href="http://blog.csdn.net/u010695420/article/details/42239465" target="_blank" rel="external">http://blog.csdn.net/u010695420/article/details/42239465</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-LDA验证" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/19/LDA验证/" class="article-date">
  	<time datetime="2016-05-19T13:14:42.000Z" itemprop="datePublished">2016-05-19</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/19/LDA验证/">LDA验证</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      
      

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Gensim-and-LDA-Training-and-Prediction" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/18/Gensim-and-LDA-Training-and-Prediction/" class="article-date">
  	<time datetime="2016-05-18T12:46:16.000Z" itemprop="datePublished">2016-05-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/18/Gensim-and-LDA-Training-and-Prediction/">Gensim and LDA--Training and Prediction</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <pre>
# install the related python packages
>>> pip install numpy
>>> pip install scipy
>>> pip install gensim
>>> pip install jieba

from gensim import corpora, models, similarities
import logging
import jieba

# configuration
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# load data from file
f = open('newfile.txt', 'r')
documents = f.readlines()

＃ tokenize
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]

# load id->word mapping (the dictionary)
dictionary = corpora.Dictionary(texts)

# word must appear >10 times, and no more than 40% documents
dictionary.filter_extremes(no_below=40, no_above=0.1)

# save dictionary
dictionary.save('dict_v1.dict')

# load corpus
corpus = [dictionary.doc2bow(text) for text in texts]

# initialize a model
tfidf = models.TfidfModel(corpus)

# use the model to transform vectors, apply a transformation to a whole corpus
corpus_tfidf = tfidf[corpus]

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)

# save model to files
lda.save('mylda_v1.pkl')

# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.
for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, lda.print_topic(index, 10))

# print the most contributing words for 100 randomly selected topics
lda.print_topics(100)

# load model and dictionary
model = models.LdaModel.load('mylda_v1.pkl')
dictionary = corpora.Dictionary.load('dict_v1.dict')

# predict unseen data
query = "未收到奖励"
query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
for index, score in sorted(model[query_bow], key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, model.print_topic(index, 20))

# if you want to predict many lines of data in a file, do the followings
f = open('newfile.txt', 'r')
documents = f.readlines()
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]
corpus = [dictionary.doc2bow(text) for text in texts]

# only print the topic with the highest score
for c in corpus:
    flag = True
    for index, score in sorted(model[c], key=lambda tup: -1*tup[1]):
        if flag:
            print "Score: {}\t Topic: {}".format(score, model.print_topic(index, 20))</pre>

<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips:"></a>Tips:</h1><p>If you occur encoding problems, you can try the following code</p>
<pre>
add it at the beginning of your python file
# -*- coding: utf-8 -*-

# also, do the followings
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

# the following code may lead to encoding problem when there're Chinese characters
model.show_topics(-1, 5)

# use this instead
model.print_topics(-1, 5)</pre>


<p>You can see step-by-step output by the following references.</p>
<blockquote>
<p>References:<br><a href="https://radimrehurek.com/gensim/tut2.html" target="_blank" rel="external">https://radimrehurek.com/gensim/tut2.html</a> official guide (en)<br><a href="http://blog.csdn.net/questionfish/article/details/46725475" target="_blank" rel="external">http://blog.csdn.net/questionfish/article/details/46725475</a>  official guide (ch)<br><a href="https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation" target="_blank" rel="external">https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LDA/">LDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gensim/">gensim</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/topic-modeling/">topic modeling</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Gensim-用Python做主题模型" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/18/Gensim-用Python做主题模型/" class="article-date">
  	<time datetime="2016-05-18T02:22:31.000Z" itemprop="datePublished">2016-05-18</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/18/Gensim-用Python做主题模型/">Gensim-用Python做主题模型</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。</p>
<h3 id="gensim有着如下特性"><a href="#gensim有着如下特性" class="headerlink" title="gensim有着如下特性"></a>gensim有着如下特性</h3><ul>
<li>内存独立- 对于训练语料来说，没必要在任何时间将整个语料都驻留在RAM中</li>
<li>有效实现了许多流行的向量空间算法－包括tf-idf，分布式LSA, 分布式LDA 以及 RP；并且很容易添加新算法</li>
<li>对流行的数据格式进行了IO封装和转换</li>
<li>在其语义表达中，可以相似查询</li>
<li>gensim的创建的目的是，由于缺乏简单的（java很复杂）实现主题建模的可扩展软件框架.</li>
</ul>
<h3 id="gensim的设计原则"><a href="#gensim的设计原则" class="headerlink" title="gensim的设计原则"></a>gensim的设计原则</h3><ul>
<li>简单的接口，学习曲线低。对于原型实现很方便</li>
<li>根据输入的语料的size来说，内存各自独立；基于流的算法操作，一次访问一个文档.</li>
</ul>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><p>gensim的整个package会涉及三个概念：corpus, vector, model.</p>
<ul>
<li>语库(corpus)<br>文档集合，用于自动推出文档结构，以及它们的主题等，也可称作训练语料。</li>
</ul>
<ul>
<li><p>向量(vector)</p>
<p>在向量空间模型(VSM)中，每个文档被表示成一个特征数组。例如，一个单一特征可以被表示成一个问答对(question-answer pair):</p>
<p>[1].在文档中单词”splonge”出现的次数？ 0个<br>[2].文档中包含了多少句子？ 2个<br>[3].文档中使用了多少字体? 5种<br>这里的问题可以表示成整型id (比如：1,2,3等), 因此，上面的文档可以表示成：(1, 0.0), (2, 2.0), (3, 5.0). 如果我们事先知道所有的问题，我们可以显式地写成这样：(0.0, 2.0, 5.0). 这个answer序列可以认为是一个多维矩阵（3维）. 对于实际目的，只有question对应的answer是一个实数.</p>
<p>对于每个文档来说，answer是类似的. 因而，对于两个向量来说（分别表示两个文档），我们希望可以下类似的结论：“如果两个向量中的实数是相似的，那么，原始的文档也可以认为是相似的”。当然，这样的结论依赖于我们如何去选取我们的question。</p>
</li>
</ul>
<ul>
<li><p>稀疏矩阵(Sparse vector)</p>
<p>通常，大多数answer的值都是0.0. 为了节省空间，我们需要从文档表示中忽略它们，只需要写：(2, 2.0), (3, 5.0) 即可(注意：这里忽略了(1, 0.0)). 由于所有的问题集事先都知道，那么在稀疏矩阵的文档表示中所有缺失的特性可以认为都是0.0.</p>
<p>gensim的特别之处在于，它没有限定任何特定的语料格式；语料可以是任何格式，当迭代时，通过稀疏矩阵来完成即可。例如，集合 ([(2, 2.0), (3, 5.0)], ([0, -1.0], [3, -1.0])) 是一个包含两个文档的语料，每个都有两个非零的 pair。</p>
</li>
</ul>
<ul>
<li><p>模型(model)</p>
<p>对于我们来说，一个模型就是一个变换(transformation)，将一种文档表示转换成另一种。初始和目标表示都是向量－－它们只在question和answer之间有区别。这个变换可以通过训练的语料进行自动学习，无需人工监督，最终的文档表示将更加紧凑和有用；相似的文档具有相似的表示。</p>
</li>
</ul>
<h3 id="演示代码见-http-shuang0420-github-io-2016-05-18-Gensim-and-LDA-Training-and-Prediction"><a href="#演示代码见-http-shuang0420-github-io-2016-05-18-Gensim-and-LDA-Training-and-Prediction" class="headerlink" title="演示代码见 http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/"></a>演示代码见 <a href="http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/" target="_blank" rel="external">http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/</a></h3><blockquote>
<p>参考链接<br><a href="http://d0evi1.github.io/gensim/" target="_blank" rel="external">http://d0evi1.github.io/gensim/</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LDA/">LDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gensim/">gensim</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/topic-modeling/">topic modeling</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-JGibbLDA实战" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/05/16/JGibbLDA实战/" class="article-date">
  	<time datetime="2016-05-16T12:50:23.000Z" itemprop="datePublished">2016-05-16</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/05/16/JGibbLDA实战/">JGibbLDA实战</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。<br>命令：</p>
<pre>java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models/casestudy/newdocs.dat</pre>

<p>错误信息：</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/err.png" alt=""></p>
<p>于是尝试导入eclipse运行手动配置，成功，过程如下。</p>
<ol>
<li><p>下载JGibbLDA的jar包并解压；<br>网址：<a href="http://jgibblda.sourceforge.net/#Griffiths04" target="_blank" rel="external">http://jgibblda.sourceforge.net/#Griffiths04</a></p>
</li>
<li><p>导入eclipse，确保jar包在目录中</p>
</li>
<li><p>找到LDACmdOption.java文件， 修改部分代码</p>
<pre> @Option(name="-dir", usage="Specify directory")
 public String dir = "models/casestudy-en";

 @Option(name="-dfile", usage="Specify data file")
 public String dfile = "models/casestudy-en/newdocs.dat";</pre>

<p>值得注意的是，dfile的格式必须是👇这个样子：</p>
<pre>[M]
[document1]
[document2]
...
[documentM]</pre>

<p>第一行[M]是documents的总数，之后的每一行是一个document，每个document是一个word list，或者说是bag of words。</p>
<pre>[document i] = [word i1] [word i2] ... [word iNi]</pre>

<p>各参数含义：<br><strong>-est </strong>从训练语料中评估出LDA模型<br><strong>-alpha</strong> LDA模型中的alpha数值，默认为50/K(K是主题数目)<br><strong>-beta</strong> LDA模型中的beta数值，默认是0.1<br><strong>-ntopics</strong> 主题数目，默认值是100<br><strong>-niters</strong> GIbbs采样的迭代数目，默认值为2000<br><strong>-savestep</strong> 指定开始保存LDA模型的迭代次数<br><strong>-dir</strong> 训练语料目录<br><strong>-dfile</strong> 训练语料文件名称</p>
</li>
<li><p>修改项目的Run Configurations，在Java Application中选择LDA，点击(x)=Arguments，输入</p>
<pre>-est -alpha 0.2 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 100 -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile "newdocs.dat"</pre>

<p> 若利用已训练的LDA模型预测，输入以下参数：</p>
<pre>-inf -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile "test.txt"</pre>

<p> 注意，进行预测时，当前目录下必须包含已有的LDA训练输出文件，包括model-final.others、model-final.phi、model-final.tassign、model-final.theta、model-final.twords、wordmap.txt文件，如果运行报错，尝试修改LDACmdOption.java的modelName，确保和文件名的modelname部分一致。<br><pre>@Option(name=”-model”, usage=”Specify the model name”)<br> public String modelName = “model-final”;</pre>   </p>
<p> 如果出现java heap limited的问题，在VM arguments下添加</p>
<pre>-Xms1g -Xmx1g -Xmn512m</pre>
</li>
<li><p>Run<br>输出文件主要有：<br><strong><model_name>.others</model_name></strong>  文件存储LDA模型参数，如alpha、beta等。<br><strong><model_name>.phi </model_name></strong> 每个topic内对doc的分布情况。文件存储词语-主题分布，每一行是一个主题，列内容为词语。<br><strong><model_name>.theta </model_name></strong> 每个doc内对应上面的n个topic的分布情况。文件主题文档分布，每一行是一个文档，列内容是主题概率。<br><strong><model_name>.tassign</model_name></strong>  文件是训练预料中单词的主题指定（归属），每一行是一个语料文档。<br><strong><model_name>.twords</model_name></strong>  n个topic，以及每个topic下面包含的具体的字词<br><strong>wordmap.txt</strong>  词-id映射<br>其中<model_name>根据采样迭代次数来指定，如model-00800，最后一次采样名称命名为model-final。</model_name></p>
</li>
</ol>
<blockquote>
<p>参考链接：<br><a href="http://www.ithao123.cn/content-4208214.html" target="_blank" rel="external">http://www.ithao123.cn/content-4208214.html</a><br><a href="http://jgibblda.sourceforge.net/" target="_blank" rel="external">http://jgibblda.sourceforge.net/</a></p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LDA/">LDA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cluster/">cluster</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/topic-modeling/">topic modeling</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
	</div>


      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 Shuang
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>