<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />













  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="NLP,machine translation,机器翻译," />





  <link rel="alternate" href="/atom.xml" title="徐阿衡" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.3" />






<meta name="description" content="持续填坑中– NLP 笔记 - Machine Translation主要讲了机器翻译的传统方法，这一篇介绍基于深度学习的机器翻译方法。">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 笔记 - Neural Machine Translation">
<meta property="og:url" content="http://www.shuang0420.com/2017/07/10/NLP 笔记 - Machine Translation-Neuron models/index.html">
<meta property="og:site_name" content="徐阿衡">
<meta property="og:description" content="持续填坑中– NLP 笔记 - Machine Translation主要讲了机器翻译的传统方法，这一篇介绍基于深度学习的机器翻译方法。">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_base_nmt.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/encodedecode.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/nnlm.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/sgd.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20et%20al.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/NNJM1.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20formula.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/res1.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20improve.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/2014_1.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/cat_ex.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sutskever%20et%20al.%203.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sut%20res.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoder2.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/greedySearch.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamForm.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamSearch.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoderResults.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_based%20SMT.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention_al.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention1.JPG">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention2.JPG">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention3.JPG">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/birnnattention.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/score.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/local_attention1.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/global_local.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res2.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/caption.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/doubly%20attention.png">
<meta property="og:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/ling.png">
<meta property="og:updated_time" content="2020-06-16T13:21:44.173Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 笔记 - Neural Machine Translation">
<meta name="twitter:description" content="持续填坑中– NLP 笔记 - Machine Translation主要讲了机器翻译的传统方法，这一篇介绍基于深度学习的机器翻译方法。">
<meta name="twitter:image" content="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_base_nmt.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6294135991397516000',
      author: '阿衡'
    }
  };
</script>

<script async src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6146435155426457",
    enable_page_level_ads: true
  });
</script>




  <link rel="canonical" href="http://www.shuang0420.com/2017/07/10/NLP 笔记 - Machine Translation-Neuron models/"/>


  <title> NLP 笔记 - Neural Machine Translation | 徐阿衡 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">徐阿衡</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Shuang</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-works">
          <a href="/works" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Works
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/aboutme" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input" placeholder="search my blog...">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
         
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                NLP 笔记 - Neural Machine Translation
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-07-10T09:02:27+08:00" content="2017-07-10">
              2017-07-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  , 
                

              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/CMU-11611/" itemprop="url" rel="index">
                    <span itemprop="name">CMU 11611</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2017/07/10/NLP 笔记 - Machine Translation-Neuron models/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/10/NLP 笔记 - Machine Translation-Neuron models/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>持续填坑中– <a href="http://www.shuang0420.com/2017/05/01/NLP%20笔记%20-%20Machine%20Translation/">NLP 笔记 - Machine Translation</a>主要讲了机器翻译的传统方法，这一篇介绍基于深度学习的机器翻译方法。<br><a id="more"></a></p>
<p>本文涉及的论文原文：</p>
<ul>
<li>Bengio et al. (2003), <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a></li>
<li>Devlin et al. (2014), <a href="http://www.aclweb.org/anthology/P14-1129" target="_blank" rel="external">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</a></li>
<li>Sutskever et al. (2014), <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li>Bahdanau et al. (2014), <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li>Xu et al. (2015) <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a></li>
<li>Luong et al. (2015)<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj3qYKMiuDVAhVHXbwKHadFByAQFgg8MAI&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Femnlp15_attn.pdf&amp;usg=AFQjCNGsQHAEQhC6dQuVyiHWcEmajCM-6w" target="_blank" rel="external">Effective approaches to attention based neural machine translation</a></li>
</ul>
<h1 id="Neural-MT"><a href="#Neural-MT" class="headerlink" title="Neural MT"></a>Neural MT</h1><p><strong>Neural MT(NMT)</strong>的定义：</p>
<blockquote>
<p>Neural Machine Translation is the approach of modeling the <strong>entire</strong> MT process via <strong>one</strong> big artificial neural netowrk.</p>
</blockquote>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_base_nmt.png" class="ful-image" alt="phrase_base_nmt.png">
<p>用一个大的神经网络来给整个机器翻译的过程建模，目前主流的结构是 <strong>Neural encoder-decoder architectures</strong>，最<strong>基本的思想</strong>是 encoder 将输入文本转化为一个向量， decoder 根据这个向量生成目标译文，编码解码开始均用 RNN 实现，由于普通 RNN 存在梯度消失/爆炸的问题，通常会引入 LSTM。本篇会介绍 NMT 发展的一些重要节点。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/encodedecode.png" class="ful-image" alt="encodedecode.png">
<blockquote>
<p><strong>一句话解释 encoder-decoder architectures:</strong> Encoder compresses input series into one vector Decoder uses this vector to generate output</p>
</blockquote>
<h2 id="Big-wins-of-Neural-MT"><a href="#Big-wins-of-Neural-MT" class="headerlink" title="Big wins of Neural MT"></a>Big wins of Neural MT</h2><ul>
<li><strong>End-to-end training</strong><br>所有参数同时被训练优化</li>
<li><strong>Distributed representations share strength</strong><br>分布式表达更好的挖掘了单词／词组之间的相似性</li>
<li><strong>Better exploitation of context</strong><br>可以使用更广泛的上下文，无论是 source 还是 target text</li>
<li><strong>More fluent text generation</strong><br>生成的文本质量更高</li>
</ul>
<p>But…</p>
<h2 id="Concerns"><a href="#Concerns" class="headerlink" title="Concerns"></a>Concerns</h2><ul>
<li><strong>Black box component models for reordering, transliteration, etc.</strong></li>
<li><strong>Explicit use of syntactic or semantic structures</strong><br>没有显性的用到句法／语义结构特征</li>
<li><strong>Explicit use of discourse structure, anaphora, etc.</strong><br>没法显性利用指代消解之类的结果</li>
</ul>
<h1 id="NNLM-Bengio-et-al-2003"><a href="#NNLM-Bengio-et-al-2003" class="headerlink" title="NNLM: Bengio et al. (2003)"></a>NNLM: Bengio et al. (2003)</h1><p><a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a>，这篇论文为之后的 NMT 打下了基础。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/nnlm.png" class="ful-image" alt="nnlm.png">
<p>传统的 Language model 有一定缺陷：</p>
<ul>
<li>不能把 ngram 中 n 以外的词考虑进去<br>n 越大句子连贯性越好，但同时数据也就越稀疏，参数也越多</li>
<li>不能考虑单词之间的相似度<br>不能从 the cat is walking in the bedroom 学习到 a dog was running in a room</li>
</ul>
<p><strong>Neural network language model(NNLM)</strong> 可以解决上面的两个问题。主要理解上图，先来定义一些参数：<br><strong>h:</strong> number of hidden units<br><strong>m:</strong> number of features associated with each word<br><strong>b:</strong> output biases, with |V| elements<br><strong>d:</strong> hidden layer biases, with h elements<br><strong>V:</strong> word dictionary $w_1, …w_T \in V$<br><strong>U:</strong> hidden-to-output weights, |V|*h matrix<br><strong>W:</strong> word features to output weights, |V|*(n-1)m matrix<br><strong>H:</strong> hidden layer weights, h*(n-1)m matrix<br><strong>C:</strong> word features C, |V|*m matrix</p>
<p>再来看一下<strong>目标函数:</strong><br> $$f(w_t,…,w_{t-n+1})=\hat P(w_t|w_1^{t-1})$$</p>
<p>目标函数包含了两个部分</p>
<ul>
<li>Matrix C，用来 <strong>map</strong> word_id 和 word feature vector，维度是 |V|*m</li>
<li>g，用来 <strong>map</strong> 这个上下文输入的 feature vector 和下一个单词 $w_t$ 在 V 上的条件概率的分布，g 用来估计 $\hat P(w_t=i|w_1^{t-1})$<br>$$f(i, w_{t-1},…,w_{t-n+1})=g(i,C(w_{t-1}),…,C(w_{t-n+1}))$$</li>
</ul>
<p>也就是说 f 是两个 mapping C 和 g 的组合，词向量矩阵 C 所有单词共享。模型有两个 hidden layer，第一部分的输入是上下文单词 $w_{t-1},…w_{t-n+1}$ 的 word_id，每个 word_id 在 C 中寻找到对应的 word vector，vector 相加得到第二部分的输入， 与 H 相乘加一个 biases 用 tanh 激活，然后与 U 相乘产生一个得分向量，再进入 softmax 把得分向量转化成概率分布形式。</p>
<p>前向传播的式子就是：<br>$$y=b+Wx+Utanh(d+Hx)$$</p>
<p>其中 x 是 word features layer activation vector，由输入词向量拼接而成<br>$$x(C(w_{t-1}), C(w_{t-1}), …, C(w_{t-n+1}))$$</p>
<p>最后的 softmax 层，保证所有概率加和为 1<br>$$\bar P(w_t|w_{t-1},…,w_{t-n+1})={e^{y_{w_t}} \over \sum_ie^{y_i}}$$</p>
<p>通过在训练集上最大化 penalized log-likelihood 来进行训练，其中 $R(\theta)$ 是正则项，如下<br>$$L={1 \over T}\sum_tlogf(w_t,w_{t-1},…,w_{t-n+1;\theta})+R(\theta)$$</p>
<p>用 SGD 梯度下降来更新 U 和 W<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/sgd.png" class="ful-image" alt="sgd.png"></p>
<p>参数集合$\theta = (b,d,W,U,H,C)$，参数量： |V|(1+mn+h)+h(1+(n-1)m)，dominating factor 是|V|(nm+h)</p>
<p>最后提一下两个改进，一个是加上图中的曲线部分，直接把词向量和输出连接起来(direct connections from the word features to the output)，另一个是和 ngram 相结合的 mixture model。看一下在 Brown Corpus 上的评估结果:</p>
<ul>
<li>n-gram model(Kneser-Ney smoothing): 321</li>
<li>neural network language model: 276</li>
<li>neural network + ngram: 252</li>
</ul>
<h1 id="NNJM-Devlin-et-al-2014"><a href="#NNJM-Devlin-et-al-2014" class="headerlink" title="NNJM: Devlin et al. (2014)"></a>NNJM: Devlin et al. (2014)</h1><p><a href="http://www.aclweb.org/anthology/P14-1129" target="_blank" rel="external">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</a>，把 Bengio et al. (2003) 的 model 转化为一个 translation model，简单来说就是把 n-gram target language model 和 m-word source window 相结合，来创建一个 MT decoding feature，可以简单的融入任何的 SMT decoder。</p>
<h2 id="Neural-Network-Joint-Models-NNJM"><a href="#Neural-Network-Joint-Models-NNJM" class="headerlink" title="Neural Network Joint Models(NNJM)"></a>Neural Network Joint Models(NNJM)</h2><p>条件概率模型，generate the next English word <strong>conditioned on</strong></p>
<ul>
<li>The previous n English words you generated $e_{i-1}, e_{i-2}…$</li>
<li>The aligned source word and its m neighbors $…f_{a_i-1}, f_{a_i}, f_{a_i-2}…$</li>
</ul>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20et%20al.png" class="ful-image" alt="Devlin%20et%20al.png">
<p>$$P(T|S) \approx \prod^{|T|}_{i=1}P(t_i|t_{i-1},…,t_{i-n+1}, S_i)$$</p>
<p>假设 target word $t_i$ 只和一个 source word $a_i$ 对齐，我们会关注 source 句子里以 $a_i$ 为中心的一个 window $S_i$，也就是和 target $t_i$ 最相关的 source part。</p>
<p>$$S_i = s_{a_i-{m-1 \over 2}},…,s_{a_i},…,s_{a_i+{m-1 \over 2}}$$</p>
<p>1) 如果 $e_i$ 只和一个 source word 对齐，那么 $a_i$ 就是对齐的那个单词的 index<br>2) 如果 $e_i$ 和多个 source word 对齐，那么 $a_i$ 就是在中间的那个单词的 index<br>3) 如果 $e_i$ 没有对齐的 source word，那么就继承最邻近(右边)的 aligned word 的 affiliation</p>
<p>affiliation 用先验的基于规则的 word alignment 来推。</p>
<p>模型的训练过程和 NNLM 相似，不过是多了个 corpus 而已。最大化训练数据的 log-likelihood<br>$$L=\sum_ilog(P(x_i))$$</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/NNJM1.png" class="ful-image" alt="NNJM1.png">
<h2 id="Self-normalization"><a href="#Self-normalization" class="headerlink" title="Self-normalization"></a>Self-normalization</h2><p>论文还提出了一种训练神经网络的新方法 – self-normalized 技术。由于训练的 cost 主要来自输出层在整个 target vocabulary 上的一个 softmax 计算，<strong>self-normalization</strong> 用近似的概率替代实际的 softmax 操作，思路很简单，主要改造一下目标函数。先来看一下 softmax log likelihood 的计算：<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20formula.png" class="ful-image" alt="Devlin%20formula.png"><br>想象一下，如果 $log(Z(x))=0$，也就是 $Z(x)=1$，那么我们就只用计算输出层第 r 行的值而不用计算整个 matrix 了，所以改造下目标函数，变成<br>$$<br>\begin{aligned} \<br>L =\sum_i[log(P(x_i))-\alpha(log(Z(x_i))-0)^2] \\<br>&amp; =\sum_i[log(P(x_i))-\alpha log^2(Z(x_i))] \\<br> \end{aligned}<br>$$</p>
<p>output layer bias weights 初始化为 log(1/|V|)，这样初始网络就是 self-normalized 的了，decode 时，只用把 $U_r(x)$ 而不是 $log(P(x))$ 作为 feature score，这大大加快了 decoding 时的 lookup 速度(~15x)。其中 $\alpha$ 是一个 trade-off 参数，来平衡网络的 accuracy 以及 mean self-normalization error</p>
<p>其他优化如 pre-computing the (first) hidden layer 等。</p>
<h2 id="Variations"><a href="#Variations" class="headerlink" title="Variations"></a>Variations</h2><p>MT decoder 有两类，一个是 <strong>decoder</strong>方法，用的是 string-to-dependency hierarchical decoder (Shen et al., 2010)，一个是 <strong>1000-best rescoring</strong> 方法，用的 feature 是 5-gram Kneser-Ney LM，Recurrent neural network language model (RNNLM) (Mikolov et al., 2010)</p>
<p>不同模型的影响<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/res1.png" class="ful-image" alt="res1.png"><br>​<br>网络设置的影响<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20improve.png" class="ful-image" alt="Devlin%20improve.png"></p>
<p>模型还有一些变种比如说<strong>翻译方向(source-to-target S2T, target-to-source T2S)</strong>，<strong>language model 的方向(left-to-right L2R, right-to-left R2L)</strong>，<strong>NNTM(Neural Network Lexical Translation Model)</strong>，对 many-to-one 问题用 NULL 解决，对 one-to-many 问题用 token concatenated 解决，训练和评估与 NNJM 相似。</p>
<h1 id="Sutskever-et-al-2014"><a href="#Sutskever-et-al-2014" class="headerlink" title="Sutskever et al. (2014)"></a>Sutskever et al. (2014)</h1><p><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/2014_1.png" class="ful-image" alt="2014_1.png">
<p>使用了 encoder-decoder 框架，用多层的 LSTM 来把输入序列映射到一个固定维度的向量，然后用另一个深层的 LSTM 来 decode 这个向量，得到 target sequence，第二个 LSTM 实际上就是一个 NNLM 不过 conditioned on input sequences。主要的改变：</p>
<ul>
<li>fully <strong>end-to-end</strong> RNN-based translation model</li>
<li><strong>two</strong> different RNN<br>encode the source sentence using one LSTM<br>generate the target sentence one word at a time using another LSTM</li>
<li><strong>reverse</strong> the order of the words in all source sentence(but not target sentences)</li>
</ul>
<p><strong>Encoder</strong>可以看作是一个 <strong>conditional language model</strong> (Bengio et al. ,2003) ，对原始句子，每个词用相应向量表示，每个词都有一个隐含状态 h1，代表这个词以及这个词之前的所有词包含的信息，当找到句尾标记的时候，对应的隐状态也就是 encoder 层的最后一个隐状态就就代表了整个句子的信息。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/cat_ex.png" class="ful-image" alt="cat_ex.png">
<p>然后 <strong>decoder</strong> 对其进行解码，<strong>encoder</strong> 最后一层(最后一个时刻)作为 decoder 第一层，LSTM 能保持中期的记忆，那么解码层的每个隐状态，都包含了已经翻译好的状态以及隐状态，然后输出每个词。具体过程如下图：<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sutskever%20et%20al.%203.png" class="ful-image" alt="Sutskever%20et%20al.%203.png"></p>
<p>将输入序列倒转喂给 LSTM，能够在 source 和 target 句子间引入许多 short-term dependencies，使得优化问题更加容易。</p>
<p><strong>recurrent activation function</strong> 可以使用：</p>
<ul>
<li>Hyperbolic tangent tanh</li>
<li>Gated recurrent unit [Cho et al., 2014]</li>
<li>Long short-term memory [Sutskever et al., 2014]</li>
<li>Convolutional network [Kalchbrenner &amp; Blunsom, 2013]</li>
</ul>
<p>主要的贡献</p>
<ul>
<li>hard-alignment =&gt; soft-alignment</li>
<li>对长句的泛化能力很好</li>
<li>简单的 decoder</li>
</ul>
<p>关于 <strong>Decoder</strong>，论文 baseline 用了 rescoring 1000-best 的策略，实验用了一个 <strong>left-to-right beam search decoder</strong> 结果有一定提升。<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sut%20res.png" class="ful-image" alt="Sut%20res.png"></p>
<p>还可以提一句的是，另一种做法是把 encoder 的最后一层喂给 decoder 的每一层，这样就不会担心记忆丢失了。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoder2.png" class="ful-image" alt="decoder2.png">
<h1 id="乱入：Decoders"><a href="#乱入：Decoders" class="headerlink" title="乱入：Decoders"></a>乱入：Decoders</h1><p>在模型能计算 P(T|S) 后，问题来了，怎样才能找出最可能的译文 T 呢？<br>$$\bar T = argmax_T(P(T|S))$$</p>
<h2 id="Exhaustive-Search"><a href="#Exhaustive-Search" class="headerlink" title="Exhaustive Search"></a>Exhaustive Search</h2><p>最开始的想法当然是生成所有翻译，然后用 language model 打分，挑概率最大的了。<strong>BUT!! DO NOT EVEN THINK OF TRYING IT OUT!!!</strong>译文数量是词表的指数级函数，这还用想么？！</p>
<h2 id="Ancestral-Sampling"><a href="#Ancestral-Sampling" class="headerlink" title="Ancestral Sampling"></a>Ancestral Sampling</h2><p>$$x ~ P(x_t|x_1,…,x_n)$$<br>多次取 sample。然而实践中会产生高方差的结果，同一个句子每次翻译结果都不一样，不大好吧？</p>
<h2 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h2><p>$$x_t=argmax\hat x_tP(\hat x_t|x_1,…,x_n)$$<br>每次选取当前最可能的那个单词，然而，这显然不能达到全局最优，每一步都会影响到后面的部分。<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/greedySearch.png" class="ful-image" alt="greedySearch.png"></p>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamForm.png" class="ful-image" alt="beamForm.png">
<p>每个时刻记录 k 个最可能的选项，相当于剪枝，然后在这些选项中进行搜索<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamSearch.png" class="ful-image" alt="beamSearch.png"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoderResults.png" class="ful-image" alt="decoderResults.png">
<h1 id="Attention-Bahdanau-et-al-2014"><a href="#Attention-Bahdanau-et-al-2014" class="headerlink" title="Attention: Bahdanau et al. (2014)"></a>Attention: Bahdanau et al. (2014)</h1><p><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a>，之前的 encoder-decoder 模型是将 source sentence 编码成一个固定长度的 vector，然后 decoder 产生 target sentence，这就要求 neural network 要能够把 source sentence 所有必要信息都压缩到一个 fixed-length vector 里，这对长句并不友好。我们希望在产生一个 target word 的时候只关注部分的 source word。这一篇提出了一个自动搜寻这样一个要关注的 source word window 的方法，换句话说，每次产生一个单词时，模型会从 source sentence 中搜索到相关信息所在的位置，基于包含了这些位置特征的 context vector 和 previous generated words，来生成下一个单词。这也就是<strong>注意力模型</strong>在机器翻译中的应用。</p>
<blockquote>
<p>一句话解释： Attention Mechanism predicts the output $y_t$ with a weighted average context vector $c_t$, not just the last state</p>
</blockquote>
<p>再通俗一点理解，attention 的作用可以看作是一个对齐模型，传统 SMT 我们用 EM 算法来求解对齐，这里做一个隐式的对齐，将 alignment model 用一个 feedforward neural network 参数化，和其他部分一起训练，神经网络会同时来学习 <strong>翻译模型(translation)</strong> 和 <strong>对齐模型(alignment)</strong>。<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_based%20SMT.png" class="ful-image" alt="phrase_based%20SMT.png"><br>attention 效果：<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention_al.png" class="ful-image" alt="attention_al.png"></p>
<p><strong>具体过程：</strong><br>用一个感知机公式将 source 和 target 的每个词联系起来，$a(h_{i-1},\bar h_j)=v^T_atanh(W_ah_{i-1}+U_ah_j)$，然后通过 softmax 归一化得到一个概率分布，也就是 attention 矩阵，再进行加权平均得到 context vector(可以看作是 annotation 的期望值)。<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention1.JPG" class="ful-image" alt="attention1.JPG"><br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention2.JPG" class="ful-image" alt="attention2.JPG"><br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention3.JPG" class="ful-image" alt="attention3.JPG"></p>
<p>模型将 input sentence 编码成一系列 vector，然后在 decode 时自适应的选择这些 vector 的一个子集。$a_{ij}$，或者说对应的 $e_{ij}$，反映了 annotation $\bar h_j$ 关于前一个隐状态 $h_{i-1}$ 在决定下一个隐状态 $h_i$ 以及产生 $y_i$ 的重要性。直观的说，这在 decoder 里执行了 attention 机制。decoder 来决定应该对 source sentence 的哪一部分给予关注。通过这个 attention 机制，encoder 不用再将 source 的所有信息压缩到一个定长的 vector 里，信息可以在 annotation 序列中传播，然后由 decoder 来选择性的检索。</p>
<p>Encoder 还可以用双向 RNN 来做，这样每个单词的 annotation 不仅概括了前面单词的信息，还包括了后面单词的信息。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/birnnattention.png" class="ful-image" alt="birnnattention.png">
<h1 id="Attention-优化"><a href="#Attention-优化" class="headerlink" title="Attention 优化"></a>Attention 优化</h1><h2 id="More-Score-Functions"><a href="#More-Score-Functions" class="headerlink" title="More Score Functions"></a>More Score Functions</h2><p>怎么算 alignment score 有下面集中不同的方式，实验表示第二种效果最好。<br><strong>compute alignment weight vector</strong><br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/score.png" class="ful-image" alt="score.png"></p>
<h2 id="Global-vs-Local"><a href="#Global-vs-Local" class="headerlink" title="Global vs. Local"></a>Global vs. Local</h2><p>论文<a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Xu et al.2015 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a> 提到了 attention 可以分为 <strong>hard</strong> 和 <strong>soft</strong> 两种模型，简单理解，<strong>hard attention</strong> 就是从 source sentence 里找到一个能与产生单词 $t^{th}$ 对齐的特定单词，把 $s_{t,i}$ 设为 1，source 里的其他单词硬性认为对齐概率为 0；<strong>soft attention</strong> 就是之前 Bahdanau et al. (2014) 提到的，对 source sentence 每个单词都给出一个对齐概率，得到一个概率分布，context vector 每次是这些概率分布的一个加权和，整个模型其实是平滑的且处处可分的。</p>
<p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj3qYKMiuDVAhVHXbwKHadFByAQFgg8MAI&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Femnlp15_attn.pdf&amp;usg=AFQjCNGsQHAEQhC6dQuVyiHWcEmajCM-6w" target="_blank" rel="external">Effective approaches to attention based neural machine translation</a> 提出了一个新的 attention 机制 <strong>local attention</strong>，在得到 context vector 时，我们不想看所有的 source hidden state，而是每次只看一个 hidden state 的子集(subset)，这样的 attention 其实更集中，也会有更好的结果。</p>
<p><strong>Global attention</strong> 其实就是 <strong>soft attention</strong>， <strong>local model</strong> 实际相当于 hard 和 soft attention 的一个混合或者说折中，主要是用来降低 attention 的花费，简单来说就是每次计算先用预测函数得到 source 相关信息的窗口，先预估得到一个 aligned position $p_t$，然后往左往右扩展得到一个 focused window [$p_t-D$,$p_t+D$] 取一个类似于 soft attention 的概率分布。和 global attention 不同，这里 $a_i$ 的维度是固定的。</p>
<p>那么关于 local attention 有两个问题。第一个问题是<strong>怎么产生 aligned position</strong>，之前在 Devlin et al. (2014) 里是用规则，这里用 sigmoid function $p_t = S•sigmoid(v^T_p tanh(W_ph_t))$，其中 S 是 source sentence。第二个问题是<strong>怎么来学习这些 position parameters</strong>，像 $W_p$、$h_t$ 这些参数和网络结构中其他参数没有任何关联，怎么学习呢？方法是像 global attention 一样先计算对齐分数 $score(h_t, \bar h_s)$，然后 normalize，这里有一个 trick 是将得到的 $a_t$ 与一个 truncated Gaussian distribution 结合，也就是 $a_t(s)=align(h_t, \bar h_s)exp(-{(s-p_t)^2 \over 2 \sigma^2})$，$\sigma={D \over 2}$，这样我们会只有一个 peak，现在可以用 BP 来学习预测 position，这个模型这时候几乎是 <strong>处处可分的(differentiable almost everywhere)</strong>，这种对齐称为 <strong>predictive alignment(local-p)</strong></p>
<p><strong>local attention</strong> 的训练花费更少，且几乎处处可分(differentiable)</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/local_attention1.png" class="ful-image" alt="local_attention1.png">
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/global_local.png" class="ful-image" alt="global_local.png">
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res.png" class="ful-image" alt="effective_res.png">
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res2.png" class="ful-image" alt="effective_res2.png">
<h2 id="Coverage-Doubly-attention"><a href="#Coverage-Doubly-attention" class="headerlink" title="Coverage: Doubly attention"></a>Coverage: Doubly attention</h2><p>论文<a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Xu et al.2015 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a>提到的思路，用到机器翻译里就是同时注意原文和译文。</p>
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/caption.png" class="ful-image" alt="caption.png">
<img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/doubly%20attention.png" class="ful-image" alt="doubly%20attention.png">
<h2 id="Linguistic-ideas"><a href="#Linguistic-ideas" class="headerlink" title="Linguistic ideas"></a>Linguistic ideas</h2><ul>
<li>[Tu, Lu, Liu, Liu, Li, ACL’16]: NMT model with coverage-based attention</li>
<li>[Cohn, Hoang, Vymolova, Yao, Dyer, Haffari, NAACL’16]: More substantive models of attention<br> using: position (IBM2) + Markov (HMM) + fertility<br>  (IBM3-5) + alignment symmetry (BerkeleyAligner)</li>
</ul>
<p>一般一个单词最多翻译为两三个单词，如果生成了五六个单词，那么模型可能在重复生成。<br><img src="http://images.shuang0420.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/ling.png" class="ful-image" alt="ling.png"></p>
<h1 id="Current-Research-Direction-on-Neural-MT"><a href="#Current-Research-Direction-on-Neural-MT" class="headerlink" title="Current Research Direction on Neural MT"></a>Current Research Direction on Neural MT</h1><ul>
<li>Incorporation syntax into Neural MT</li>
<li>Handling of morphologically rich languages</li>
<li>Optimizing translation quality (instead of corpus probability)</li>
<li>Multilingual models</li>
<li>Document-level translation</li>
</ul>
<p>到目前为止，我们都是假设在两种语言 F 和 E 之间训练一个模型。但是，世界上有许多种语言，一些研究已经证明能够利用所有语言的数据去训练一个模型。也可以跨语言执行迁移，先在一个语言对上训练模型，然后将其微调用于其他语言对。</p>

      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/uploads/wechat.jpg" alt="徐阿衡 wechat" style="width: 200px; max-width: 100%;"/>
    <div>欢迎关注：徐阿衡的微信公众号</div>
</div>


      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>客官，打个赏呗~</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="http://7xu83c.com1.z0.glb.clouddn.com/1.pic.jpg" alt="徐阿衡 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
    </div>
  </div>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag">#NLP</a>
          
            <a href="/tags/machine-translation/" rel="tag">#machine translation</a>
          
            <a href="/tags/机器翻译/" rel="tag">#机器翻译</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/04/AWS Lambda + API Gateway + DynamoDB 添加 slash command 待办清单/" rel="next" title="AWS Lambda + API Gateway + DynamoDB 添加 slash command 待办清单">
                <i class="fa fa-chevron-left"></i> AWS Lambda + API Gateway + DynamoDB 添加 slash command 待办清单
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/15/CCF-GAIR 参会笔记/" rel="prev" title="CCF-GAIR 参会笔记">
                CCF-GAIR 参会笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      



    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
     
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://7xu83c.com1.z0.glb.clouddn.com/2.pic.jpg"
               alt="徐阿衡" />
          <p class="site-author-name" itemprop="name">徐阿衡</p>
          <p class="site-description motion-element" itemprop="description">读万卷书，行万里路 @SYSU @CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">166</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">126</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Shuang0420" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.linkedin.com/in/shuang-xu-7008b894?trk=nav_responsive_tab_profile_pic" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://zhuanlan.zhihu.com/c_136690664" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://starllap.space" title="Star" target="_blank">Star</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://liam0205.me" title="Liam Huang" target="_blank">Liam Huang</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.libinx.com" title="Li Bin" target="_blank">Li Bin</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-MT"><span class="nav-number">1.</span> <span class="nav-text">Neural MT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Big-wins-of-Neural-MT"><span class="nav-number">1.1.</span> <span class="nav-text">Big wins of Neural MT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Concerns"><span class="nav-number">1.2.</span> <span class="nav-text">Concerns</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NNLM-Bengio-et-al-2003"><span class="nav-number">2.</span> <span class="nav-text">NNLM: Bengio et al. (2003)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NNJM-Devlin-et-al-2014"><span class="nav-number">3.</span> <span class="nav-text">NNJM: Devlin et al. (2014)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Network-Joint-Models-NNJM"><span class="nav-number">3.1.</span> <span class="nav-text">Neural Network Joint Models(NNJM)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Self-normalization"><span class="nav-number">3.2.</span> <span class="nav-text">Self-normalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Variations"><span class="nav-number">3.3.</span> <span class="nav-text">Variations</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sutskever-et-al-2014"><span class="nav-number">4.</span> <span class="nav-text">Sutskever et al. (2014)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#乱入：Decoders"><span class="nav-number">5.</span> <span class="nav-text">乱入：Decoders</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exhaustive-Search"><span class="nav-number">5.1.</span> <span class="nav-text">Exhaustive Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ancestral-Sampling"><span class="nav-number">5.2.</span> <span class="nav-text">Ancestral Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Greedy-Search"><span class="nav-number">5.3.</span> <span class="nav-text">Greedy Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beam-Search"><span class="nav-number">5.4.</span> <span class="nav-text">Beam Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Results"><span class="nav-number">5.5.</span> <span class="nav-text">Results</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Bahdanau-et-al-2014"><span class="nav-number">6.</span> <span class="nav-text">Attention: Bahdanau et al. (2014)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-优化"><span class="nav-number">7.</span> <span class="nav-text">Attention 优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#More-Score-Functions"><span class="nav-number">7.1.</span> <span class="nav-text">More Score Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Global-vs-Local"><span class="nav-number">7.2.</span> <span class="nav-text">Global vs. Local</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Coverage-Doubly-attention"><span class="nav-number">7.3.</span> <span class="nav-text">Coverage: Doubly attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linguistic-ideas"><span class="nav-number">7.4.</span> <span class="nav-text">Linguistic ideas</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Current-Research-Direction-on-Neural-MT"><span class="nav-number">8.</span> <span class="nav-text">Current Research Direction on Neural MT</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <!-- Other code may be here -->
<div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">徐阿衡</span>
  <a href="http://www.miitbeian.gov.cn/">粤ICP备17129486号</a>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'httpshuang0420githubio';
      var disqus_identifier = '2017/07/10/NLP 笔记 - Machine Translation-Neuron models/';
      var disqus_title = "NLP 笔记 - Neural Machine Translation";
      var disqus_url = 'http://www.shuang0420.com/2017/07/10/NLP 笔记 - Machine Translation-Neuron models/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
