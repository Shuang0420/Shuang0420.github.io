<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>徐阿衡</title>
  <subtitle>Shuang</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-06-12T02:05:48.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>徐阿衡</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>爬虫总结（一）</title>
    <link href="http://yoursite.com/2016/06/11/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2016/06/11/爬虫总结（一）/</id>
    <published>2016-06-11T06:35:48.000Z</published>
    <updated>2016-06-12T02:05:48.000Z</updated>
    
    <content type="html">&lt;p&gt;爬虫在平时也经常用，但一直没有系统的总结过，其实它涉及了许多的知识点。这一系列会理一遍这些知识点，不求详尽，只希望以点带面构建一个爬虫的知识框架。这一篇是概念性解释以及入门级爬虫介绍（以爬取网易新闻为例）。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;爬虫基础&quot;&gt;&lt;a href=&quot;#爬虫基础&quot; class=&quot;headerlink&quot; title=&quot;爬虫基础&quot;&gt;&lt;/a&gt;爬虫基础&lt;/h1&gt;&lt;h2 id=&quot;什么是爬虫&quot;&gt;&lt;a href=&quot;#什么是爬虫&quot; class=&quot;headerlink&quot; title=&quot;什么是爬虫&quot;&gt;&lt;/a&gt;什么是爬虫&lt;/h2&gt;&lt;p&gt;爬虫说白了其实就是获取资源的程序&lt;/p&gt;
&lt;h2 id=&quot;爬虫类型&quot;&gt;&lt;a href=&quot;#爬虫类型&quot; class=&quot;headerlink&quot; title=&quot;爬虫类型&quot;&gt;&lt;/a&gt;爬虫类型&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;网络爬虫&lt;br&gt;网络爬虫，是一种按照一定的规则，&lt;strong&gt;自动的&lt;/strong&gt; 抓取万维网信息的程序或者脚本。网络爬虫是搜索引擎系统中十分重要的组成部分，爬取的网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，其性能的优劣直接影响着搜索引擎的效果。&lt;/li&gt;
&lt;li&gt;传统爬虫&lt;br&gt;从一个或若干初始网页的URL开始，获得初始网页的URL，在抓取网页过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;工作原理&quot;&gt;&lt;a href=&quot;#工作原理&quot; class=&quot;headerlink&quot; title=&quot;工作原理&quot;&gt;&lt;/a&gt;工作原理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;根据一定的网页分析算法过滤与主题无关的链接，保留有用链接并将其放入等待抓取的URL队列&lt;/li&gt;
&lt;li&gt;根据一定的搜索策略从队列中选择下一步要抓取的网页URL，重复上述过程，直到达到指定条件才结束爬取&lt;/li&gt;
&lt;li&gt;对所有抓取的网页进行一定的分析、过滤，并建立索引，以便之后的查询和检索。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;爬取策略&quot;&gt;&lt;a href=&quot;#爬取策略&quot; class=&quot;headerlink&quot; title=&quot;爬取策略&quot;&gt;&lt;/a&gt;爬取策略&lt;/h2&gt;&lt;h3 id=&quot;广度优先&quot;&gt;&lt;a href=&quot;#广度优先&quot; class=&quot;headerlink&quot; title=&quot;广度优先&quot;&gt;&lt;/a&gt;广度优先&lt;/h3&gt;&lt;p&gt;完成当前层次的搜索后才进行下一层次的搜索。一般的使用策略。&lt;/p&gt;
&lt;h3 id=&quot;最佳优先&quot;&gt;&lt;a href=&quot;#最佳优先&quot; class=&quot;headerlink&quot; title=&quot;最佳优先&quot;&gt;&lt;/a&gt;最佳优先&lt;/h3&gt;&lt;p&gt;会有评估算法，凡是被算法评估为有用的网页，先来爬取。&lt;/p&gt;
&lt;h3 id=&quot;深度优先&quot;&gt;&lt;a href=&quot;#深度优先&quot; class=&quot;headerlink&quot; title=&quot;深度优先&quot;&gt;&lt;/a&gt;深度优先&lt;/h3&gt;&lt;p&gt;实际应用很少。可能会导致trapped问题&lt;/p&gt;
&lt;h2 id=&quot;URL（-Uniform-Resource-Locator-统一资源定位符）&quot;&gt;&lt;a href=&quot;#URL（-Uniform-Resource-Locator-统一资源定位符）&quot; class=&quot;headerlink&quot; title=&quot;URL（ Uniform Resource Locator: 统一资源定位符）&quot;&gt;&lt;/a&gt;URL（ Uniform Resource Locator: 统一资源定位符）&lt;/h2&gt;&lt;p&gt;互联网上资源均有其唯一的地址，由三部分组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模式/协议&lt;/li&gt;
&lt;li&gt;文件所在IP地址及端口号&lt;/li&gt;
&lt;li&gt;主机上的资源位置&lt;/li&gt;
&lt;li&gt;例子：&lt;a href=&quot;http://www.example.com/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.example.com/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Web-Server／Socket如何建立连接和传输数据的&quot;&gt;&lt;a href=&quot;#Web-Server／Socket如何建立连接和传输数据的&quot; class=&quot;headerlink&quot; title=&quot;Web Server／Socket如何建立连接和传输数据的&quot;&gt;&lt;/a&gt;Web Server／Socket如何建立连接和传输数据的&lt;/h2&gt;&lt;p&gt;web server 的工作过程其实和打电话的过程差不多（买电话–&amp;gt;注册号码–&amp;gt;监听–&amp;gt;排队接听–&amp;gt;读写–&amp;gt;关闭），经典的三步握手（有人在吗？我在呢，你呢？我也在）在排队接听时进行。下面一张图足以解释一切。&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/webserver.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;HTML-DOM&quot;&gt;&lt;a href=&quot;#HTML-DOM&quot; class=&quot;headerlink&quot; title=&quot;HTML DOM&quot;&gt;&lt;/a&gt;HTML DOM&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;DOM 将 HTML 文档表达为树结构&lt;/li&gt;
&lt;li&gt;定义了访问和操作 HTML 文档的标准&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.35.09.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Cookie&quot;&gt;&lt;a href=&quot;#Cookie&quot; class=&quot;headerlink&quot; title=&quot;Cookie&quot;&gt;&lt;/a&gt;Cookie&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;由服务器端生成，发送给 User-Agent(一般是浏览器)，浏览器会将 Cookie 的 key/value 保存到某个目录下的文本文件哪，下次访问同一网站时就发送该 Cookie 给服务器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;HTTP&quot;&gt;&lt;a href=&quot;#HTTP&quot; class=&quot;headerlink&quot; title=&quot;HTTP&quot;&gt;&lt;/a&gt;HTTP&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;GET 直接以链接形式访问，链接中包含了所有的参数&lt;/li&gt;
&lt;li&gt;PUT 把提交的数据放到 HTTP 包的包体中&lt;pre&gt;
eg.
import urllib
import urllib2
url=&#39;http://www.zhihu.com/#signin&#39;
user_agent=&#39;MOZILLA/5.0&#39;
values={&#39;username&#39;:&#39;252618408@qq.com&#39;,&#39;password&#39;:&#39;xxx&#39;}
headers={&#39;User-Agent&#39;:user_agent}
data=urllib.urlencode(values) # urlencode 是 urllib 独有的方法
request=urllib2.Request(url,data,headers) # write a letter
response=urllib2.urlopen(request) # send the letter and get the reply
page=response.read() # read the reply
&lt;/pre&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;urllib 仅可以接受 URL，这意味着你不可以伪装你的 User Agent 字符串等，但 urllib 提供了 urlencode 方法用来GET查询字符串等产生，而 urllib2 没有。&lt;br&gt;因此 urllib, urllib2经常一起使用。&lt;/p&gt;
&lt;h3 id=&quot;Headers-设置&quot;&gt;&lt;a href=&quot;#Headers-设置&quot; class=&quot;headerlink&quot; title=&quot;Headers 设置&quot;&gt;&lt;/a&gt;Headers 设置&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;User-Agent: 部分服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求&lt;/li&gt;
&lt;li&gt;Content-Type: 使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析&lt;/li&gt;
&lt;li&gt;application/xml: 在 XMl RPC, 如 RESTful/SOAP 调用时使用&lt;/li&gt;
&lt;li&gt;application/json: 在 JSON RPC 调用时使用&lt;/li&gt;
&lt;li&gt;application/x-www-form-urlencoded: 浏览器提交 Web 表单时使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;最简单的爬虫&quot;&gt;&lt;a href=&quot;#最简单的爬虫&quot; class=&quot;headerlink&quot; title=&quot;最简单的爬虫&quot;&gt;&lt;/a&gt;最简单的爬虫&lt;/h1&gt;&lt;h2 id=&quot;requests-库&quot;&gt;&lt;a href=&quot;#requests-库&quot; class=&quot;headerlink&quot; title=&quot;requests 库&quot;&gt;&lt;/a&gt;requests 库&lt;/h2&gt;&lt;pre&gt;
import requests
url = &quot;http://shuang0420.github.io/&quot;
r = requests.get(url)
&lt;/pre&gt;

&lt;h2 id=&quot;urllib2-库&quot;&gt;&lt;a href=&quot;#urllib2-库&quot; class=&quot;headerlink&quot; title=&quot;urllib2 库&quot;&gt;&lt;/a&gt;urllib2 库&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;import urllib2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# request source file&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;url = &amp;quot;http://shuang0420.github.io/&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;request = urllib2.Request(url)  # write a letter&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;response = urllib2.urlopen(request)  # send the letter and get the reply&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;page = response.read()  # read the reply&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# save source file&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile = open(&amp;apos;webPage.html&amp;apos;, &amp;apos;wb&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile.write(page)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile.close()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这是一个简单的爬虫，打开 webPage.html 是这样的显示，没有css.&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%881.44.20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;实例：爬取网易新闻&quot;&gt;&lt;a href=&quot;#实例：爬取网易新闻&quot; class=&quot;headerlink&quot; title=&quot;实例：爬取网易新闻&quot;&gt;&lt;/a&gt;实例：爬取网易新闻&lt;/h1&gt;&lt;p&gt;爬取网易新闻 [代码示例]&lt;br&gt;– 使用 urllib2 的 requests包来爬取页面&lt;br&gt;– 使用正则表达式和 bs4 分析一级页面,使用 Xpath 来分析二级页面&lt;br&gt;– 将得到的标题和链接,保存为本地文件&lt;/p&gt;
&lt;h2 id=&quot;分析初始页面&quot;&gt;&lt;a href=&quot;#分析初始页面&quot; class=&quot;headerlink&quot; title=&quot;分析初始页面&quot;&gt;&lt;/a&gt;分析初始页面&lt;/h2&gt;&lt;p&gt;我们的初始页面是 &lt;a href=&quot;http://news.163.com/rank&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://news.163.com/rank&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.08.54.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看源代码&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.10.28.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们想要的是分类标题和URL，需要解析 DOM 文档树,这里使用了 BeautifulSoup 里的方法。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;def Nav_Info(myPage):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # 二级导航的标题和页面&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    pageInfo = re.findall(r&amp;apos;&amp;lt;div class=&amp;quot;subNav&amp;quot;&amp;gt;.*?&amp;lt;div class=&amp;quot;area areabg1&amp;quot;&amp;gt;&amp;apos;, myPage, re.S)[&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        0].replace(&amp;apos;&amp;lt;div class=&amp;quot;subNav&amp;quot;&amp;gt;&amp;apos;, &amp;apos;&amp;apos;).replace(&amp;apos;&amp;lt;div class=&amp;quot;area areabg1&amp;quot;&amp;gt;&amp;apos;, &amp;apos;&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    soup = BeautifulSoup(pageInfo, &amp;quot;lxml&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tags = soup(&amp;apos;a&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    topics = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    for tag in tags:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 只要 科技、财经、体育 的新闻&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # if (tag.string==&amp;apos;科技&amp;apos; or tag.string==&amp;apos;财经&amp;apos; or tag.string==&amp;apos;体育&amp;apos;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        topics.append((tag.string, tag.get(&amp;apos;href&amp;apos;, None)))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    return topics&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;然而，Beautiful Soup对文档的解析速度不会比它所依赖的解析器更快,如果对计算时间要求很高或者计算机的时间比程序员的时间更值钱,那么就应该直接使用 lxml。换句话说,还有提高Beautiful Soup效率的办法,使用lxml作为解析器。Beautiful Soup用lxml做解析器比用html5lib或Python内置解析器速度快很多。bs4 的默认解析器是 html.parser，使用lxml的代码如下：&lt;/p&gt;
&lt;pre&gt;BeautifulSoup(markup, &quot;lxml&quot;)&lt;/pre&gt;

&lt;h2 id=&quot;分析二级页面&quot;&gt;&lt;a href=&quot;#分析二级页面&quot; class=&quot;headerlink&quot; title=&quot;分析二级页面&quot;&gt;&lt;/a&gt;分析二级页面&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.14.06.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看源代码&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.11.23.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们要爬取的是&lt;td&gt;&lt;/td&gt;之间的新闻标题和链接，同样需要解析文档树，可以通过以下代码实现，这里用了 lxml 解析器，效率更高。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;def News_Info(newPage):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # xpath 使用路径表达式来选取文档中的节点或节点集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    dom = etree.HTML(newPage)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    news_titles = dom.xpath(&amp;apos;//tr/td/a/text()&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    news_urls = dom.xpath(&amp;apos;//tr/td/a/@href&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    return zip(news_titles, news_urls)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/Crawler/blob/master/news.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;完整代码&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;潜在问题&quot;&gt;&lt;a href=&quot;#潜在问题&quot; class=&quot;headerlink&quot; title=&quot;潜在问题&quot;&gt;&lt;/a&gt;潜在问题&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;我们的任务是爬取1万个网页，按上面这个程序，耗费时间长，我们可以考虑开启多个线程(池)去一起爬取，或者用分布式架构去并发的爬取网页。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;种子URL和后续解析到的URL都放在一个列表里，我们应该设计一个更合理的数据结构来存放这些待爬取的URL才是，比如队列或者优先队列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对各个网站的url，我们一视同仁，事实上，我们应当区别对待。大站好站优先原则应当予以考虑。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;每次发起请求，我们都是根据url发起请求，而这个过程中会牵涉到DNS解析，将url转换成ip地址。一个网站通常由成千上万的URL，因此，我们可以考虑将这些网站域名的IP地址进行缓存，避免每次都发起DNS请求，费时费力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解析到网页中的urls后，我们没有做任何去重处理，全部放入待爬取的列表中。事实上，可能有很多链接是重复的，我们做了很多重复劳动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;爬虫被封禁问题&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;优化方案&quot;&gt;&lt;a href=&quot;#优化方案&quot; class=&quot;headerlink&quot; title=&quot;优化方案&quot;&gt;&lt;/a&gt;优化方案&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;并行爬取问题&lt;/p&gt;
&lt;p&gt; 关于并行爬取，首先我们想到的是多线程或者线程池方式，一个爬虫程序内部开启多个线程。同一台机器开启多个爬虫程序，这样，我们就有N多爬取线程在同时工作，大大提高了效率。&lt;/p&gt;
&lt;p&gt; 当然，如果我们要爬取的任务特别多，一台机器、一个网点肯定是不够的，我们必须考虑分布式爬虫。分布式架构，考虑的问题有很多，我们需要一个scheduler来分配任务并排序，各个爬虫之间还需要通信合作，共同完成任务，不要重复爬取相同的网页。分配任务时我们还需要考虑负载均衡以做到公平。（可以通过Hash，比如根据网站域名进行hash）&lt;/p&gt;
&lt;p&gt; 负载均衡分派完任务之后，千万不要以为万事大吉了，万一哪台机器挂了呢？原先指派给挂掉的哪台机器的任务指派给谁？又或者哪天要增加几台机器，任务有该如何进行重新分配呢？所以我们还要 task table 来纪录状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;待爬取网页队列&lt;br&gt;如何对待待抓取队列，跟操作系统如何调度进程是类似的场景。&lt;br&gt;不同网站，重要程度不同，因此，可以设计一个优先级队列来存放待爬起的网页链接。如此一来，每次抓取时，我们都优先爬取重要的网页。&lt;br&gt;当然，你也可以效仿操作系统的进程调度策略之多级反馈队列调度算法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DNS缓存&lt;br&gt;为了避免每次都发起DNS查询，我们可以将DNS进行缓存。DNS缓存当然是设计一个hash表来存储已有的域名及其IP。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;网页去重&lt;br&gt;说到网页去重，第一个想到的是垃圾邮件过滤。垃圾邮件过滤一个经典的解决方案是Bloom Filter（布隆过滤器）。布隆过滤器原理简单来说就是：建立一个大的位数组，然后用多个Hash函数对同一个url进行hash得到多个数字，然后将位数组中这些数字对应的位置为1。下次再来一个url时，同样是用多个Hash函数进行hash，得到多个数字，我们只需要判断位数组中这些数字对应的为是全为1，如果全为1，那么说明这个url已经出现过。如此，便完成了url去重的问题。当然，这种方法会有误差，只要误差在我们的容忍范围之类，比如1万个网页，我只爬取到了9999个，剩下那一个网页，who cares！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据存储的问题&lt;br&gt;数据存储同样是个很有技术含量的问题。用关系数据库存取还是用NoSQL，抑或是自己设计特定的文件格式进行存储，都大有文章可做。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程间通信&lt;br&gt;分布式爬虫，就必然离不开进程间的通信。我们可以以规定的数据格式进行数据交互，完成进程间通信。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;反爬虫机制问题&lt;br&gt;针对反爬虫机制，我们可以通过轮换IP地址、轮换Cookie、修改用户代理(User Agent)、限制速度、避免重复性爬行模式等方法解决。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;参考链接:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3NDM1NjUwMQ==&amp;amp;mid=2650486783&amp;amp;idx=2&amp;amp;sn=b022421936afb373f4a00f497396220d&amp;amp;scene=0#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://mp.weixin.qq.com/s?__biz=MzA3NDM1NjUwMQ==&amp;amp;mid=2650486783&amp;amp;idx=2&amp;amp;sn=b022421936afb373f4a00f497396220d&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.chinahadoop.cn/course/596/learn#lesson/11986&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.chinahadoop.cn/course/596/learn#lesson/11986&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;爬虫在平时也经常用，但一直没有系统的总结过，其实它涉及了许多的知识点。这一系列会理一遍这些知识点，不求详尽，只希望以点带面构建一个爬虫的知识框架。这一篇是概念性解释以及入门级爬虫介绍（以爬取网易新闻为例）。&lt;br&gt;
    
    </summary>
    
      <category term="Programming-language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="Crawler" scheme="http://yoursite.com/tags/Crawler/"/>
    
  </entry>
  
  <entry>
    <title>gensim-doc2vec实战</title>
    <link href="http://yoursite.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/06/01/gensim-doc2vec实战/</id>
    <published>2016-06-01T02:22:06.000Z</published>
    <updated>2016-06-09T12:09:08.000Z</updated>
    
    <content type="html">&lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;导入模块&quot;&gt;&lt;a href=&quot;#导入模块&quot; class=&quot;headerlink&quot; title=&quot;导入模块&quot;&gt;&lt;/a&gt;导入模块&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
import os
import jieba

# logging information
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)
&lt;/pre&gt;

&lt;h3 id=&quot;读取文件&quot;&gt;&lt;a href=&quot;#读取文件&quot; class=&quot;headerlink&quot; title=&quot;读取文件&quot;&gt;&lt;/a&gt;读取文件&lt;/h3&gt;&lt;pre&gt;
# get input file, text format
f = open(&#39;trainingdata.txt&#39;,&#39;r&#39;)
input = f.readlines()
count = len(input)
print count
&lt;/pre&gt;

&lt;h3 id=&quot;文件预处理，分词等&quot;&gt;&lt;a href=&quot;#文件预处理，分词等&quot; class=&quot;headerlink&quot; title=&quot;文件预处理，分词等&quot;&gt;&lt;/a&gt;文件预处理，分词等&lt;/h3&gt;&lt;pre&gt;
# read file and separate words
alldocs=[] # for the sake of check, can be removed
count=0 # for the sake of check, can be removed
for line in input:
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count)) # for the sake of check, can be removed
    count+=1 # for the sake of check, can be removed
&lt;/pre&gt;


&lt;h3 id=&quot;模型选择&quot;&gt;&lt;a href=&quot;#模型选择&quot; class=&quot;headerlink&quot; title=&quot;模型选择&quot;&gt;&lt;/a&gt;模型选择&lt;/h3&gt;&lt;p&gt;gensim Doc2Vec 提供了 DM 和 DBOW 两个模型。gensim 的说明文档建议多次训练数据集并调整学习速率或在每次训练中打乱输入信息的顺序以求获得最佳效果。&lt;/p&gt;
&lt;pre&gt;
# PV-DM w/concatenation - window=5 (both sides) approximates paper&#39;s 10-word total window size
Doc2Vec(sentences,dm=1, dm_concat=1, size=100, window=2, hs=0, min_count=2, workers=cores)
# PV-DBOW  
Doc2Vec(sentences,dm=0, size=100, hs=0, min_count=2, workers=cores)
# PV-DM w/average
Doc2Vec(sentences,dm=1, dm_mean=1, size=100, window=2, hs=0, min_count=2, workers=cores)
&lt;/pre&gt;


&lt;h3 id=&quot;训练并保存模型&quot;&gt;&lt;a href=&quot;#训练并保存模型&quot; class=&quot;headerlink&quot; title=&quot;训练并保存模型&quot;&gt;&lt;/a&gt;训练并保存模型&lt;/h3&gt;&lt;pre&gt;
# train and save the model
sentences= gensim.models.doc2vec.TaggedLineDocument(&#39;output.seq&#39;)
model = gensim.models.Doc2Vec(sentences,size=100, window=3)
model.train(sentences)
model.save(&#39;all_model.txt&#39;)
&lt;/pre&gt;

&lt;h3 id=&quot;保存文档向量&quot;&gt;&lt;a href=&quot;#保存文档向量&quot; class=&quot;headerlink&quot; title=&quot;保存文档向量&quot;&gt;&lt;/a&gt;保存文档向量&lt;/h3&gt;&lt;pre&gt;
# save vectors
out=open(&quot;all_vector.txt&quot;,&quot;wb&quot;)
for num in range(0,count):
    docvec =model.docvecs[num]
    out.write(docvec)
    #print num
    #print docvec
out.close()
&lt;/pre&gt;

&lt;h3 id=&quot;检验-计算训练文档中的文档相似度&quot;&gt;&lt;a href=&quot;#检验-计算训练文档中的文档相似度&quot; class=&quot;headerlink&quot; title=&quot;检验 计算训练文档中的文档相似度&quot;&gt;&lt;/a&gt;检验 计算训练文档中的文档相似度&lt;/h3&gt;&lt;pre&gt;
# test, calculate the similarity
# 注意 docid 是从0开始计数的
# 计算与训练集中第一篇文档最相似的文档
sims = model.docvecs.most_similar(0)
print sims
# get similarity between doc1 and doc2 in the training data
sims = model.docvecs.similarity(1,2)
print sims
&lt;/pre&gt;

&lt;h3 id=&quot;infer向量，比较相似度&quot;&gt;&lt;a href=&quot;#infer向量，比较相似度&quot; class=&quot;headerlink&quot; title=&quot;infer向量，比较相似度&quot;&gt;&lt;/a&gt;infer向量，比较相似度&lt;/h3&gt;&lt;p&gt;下面的代码用于检验模型正确性，随机挑一篇trained dataset中的文档，用模型重新infer，再计算与trained dataset中文档相似度，如果模型良好，相似度第一位应该就是挑出的文档。&lt;/p&gt;
&lt;pre&gt;
# check
#############################################################################
# A good check is to re-infer a vector for a document already in the model. #
# if the model is well-trained,                                             #
# the nearest doc should (usually) be the same document.                    #
#############################################################################

print &#39;examing&#39;
doc_id = np.random.randint(model.docvecs.count)  # pick random doc; re-run cell for more examples
print(&#39;for doc %d...&#39; % doc_id)
inferred_docvec = model.infer_vector(alldocs[doc_id].words)
print(&#39;%s:\n %s&#39; % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))
&lt;/pre&gt;

&lt;h3 id=&quot;遇到的问题&quot;&gt;&lt;a href=&quot;#遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;遇到的问题&quot;&gt;&lt;/a&gt;遇到的问题&lt;/h3&gt;&lt;p&gt;👇两个错误还在探索中，根据官方指南是可以运行的，然而我遇到了错误并没能解决。&lt;br&gt;第一段错误代码，关于train the model&lt;/p&gt;
&lt;pre&gt;
alldocs=[]
count=0
for line in input:
    #print line
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    #output.write(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count))
    count+=1

model = Doc2Vec(alldocs,size=100, window=2, min_count=5, workers=4)
model.train(alldocs)
&lt;/pre&gt;

&lt;p&gt;报错信息&lt;/p&gt;
&lt;pre&gt;
Traceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 59, in &lt;module&gt;
    model = Doc2Vec(alldocs[0],size=100, window=2, min_count=5, workers=4)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 596, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/word2vec.py&quot;, line 508, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 639, in scan_vocab
    document_length = len(document.words)
AttributeError: &#39;generator&#39; object has no attribute &#39;words&#39;
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;第二段错误代码，关于infer&lt;/p&gt;
&lt;pre&gt;
doc_words1=[&#39;验证&#39;,&#39;失败&#39;,&#39;验证码&#39;,&#39;未&#39;,&#39;收到&#39;]
doc_words2=[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;]
# get infered vector
invec1 = model.infer_vector(doc_words1, alpha=0.1, min_alpha=0.0001, steps=5)
invec2 = model.infer_vector(doc_words2, alpha=0.1, min_alpha=0.0001, steps=5)
print invec1
print invec2

# get similarity
# the output docid is supposed to be 0
sims = model.docvecs.most_similar([invec1])
print sims

# according to official guide, the following codes are supposed to be fine, but it fails to run
sims= model.docvecs.similarity(invec1,invec2)
print model.similarity([&#39;今天&#39;,&#39;有&#39;,&#39;啥&#39;,&#39;奖励&#39;],[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;])
&lt;/pre&gt;

&lt;p&gt;最后两行代码报错，错误信息&lt;/p&gt;
&lt;pre&gt;
raceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 110, in &lt;module&gt;
    sims= model.docvecs.similarity(invec1,invec2)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 484, in similarity
    return dot(matutils.unitvec(self[d1]), matutils.unitvec(self[d2]))
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
TypeError: &#39;numpy.float32&#39; object is not iterable
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/doc2vec&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/models/doc2vec.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/models/doc2vec.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="doc2vec" scheme="http://yoursite.com/tags/doc2vec/"/>
    
  </entry>
  
  <entry>
    <title>gensim: word2vec实战</title>
    <link href="http://yoursite.com/2016/05/30/gensim-word2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/30/gensim-word2vec实战/</id>
    <published>2016-05-30T03:13:52.000Z</published>
    <updated>2016-06-09T12:11:08.000Z</updated>
    
    <content type="html">&lt;p&gt;介绍如何利用 gensim 库建立简单的 word2vec 模型。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
# -*- coding: utf-8 -*-
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import os
import logging
import jieba
import re
import multiprocessing
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# logging information
logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;)
logging.root.setLevel(level=logging.INFO)

# get input file, text format
inp = sys.argv[1]
input = open(inp, &#39;r&#39;)
output = open(&#39;output.seq&#39;, &#39;w&#39;)

if len(sys.argv) &lt; 2:
    print(globals()[&#39;__doc__&#39;] % locals())
    sys.exit(1)

# read file and separate words
for line in input.readlines():
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)

output.close()
output= open(&#39;output.seq&#39;, &#39;r&#39;)

# initialize the model
# size = the dimensionality of the feature vectors
# window = the maximum distance between the current and predicted word within a sentence
# min_count = ignore all words with total frequency lower than this.
model = Word2Vec(LineSentence(output), size=100, window=3, min_count=5,workers=multiprocessing.cpu_count())

# save model
model.save(&#39;output.model&#39;)
model.save_word2vec_format(&#39;output.vector&#39;, binary=False)

# test
model=gensim.models.Word2Vec.load(&#39;output.model&#39;)
x = model.most_similar([u&#39;奖励&#39;])
for i in x:
    print &quot;Word: {}\t Similarity: {}&quot;.format(i[0], i[1])
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/word2vec_example&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;介绍如何利用 gensim 库建立简单的 word2vec 模型。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之六 -- 若干源码细节</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%85%AD-%E8%8B%A5%E5%B9%B2%E6%BA%90%E7%A0%81%E7%BB%86%E8%8A%82/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之六-若干源码细节/</id>
    <published>2016-05-29T08:28:23.000Z</published>
    <updated>2016-06-09T12:11:43.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;


&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/61.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/62.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/63.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/64.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/65.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/66.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/67.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/68.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之五 -- 基于 Negative Sampling 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%94-%E5%9F%BA%E4%BA%8E-Negative-Sampling-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之五-基于-Negative-Sampling-的模型/</id>
    <published>2016-05-29T07:22:03.000Z</published>
    <updated>2016-06-09T12:11:34.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/51.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/52.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/53.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/54.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/55.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/56.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/57.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/58.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/59.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/510.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/511.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之四 -- 基于Hierarchical Softmax 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8EHierarchical-Softmax-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之四-基于Hierarchical-Softmax-的模型/</id>
    <published>2016-05-29T06:08:03.000Z</published>
    <updated>2016-06-09T12:12:19.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/8.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/10.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/1.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之三 -- 背景知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%89-%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之三-背景知识/</id>
    <published>2016-05-29T06:04:41.000Z</published>
    <updated>2016-06-09T12:11:51.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/31.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/32.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/33.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/34.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/35.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/36.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/37.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/38.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/39.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/310.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/311.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之二 -- 预备知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之二-预备知识/</id>
    <published>2016-05-29T04:21:21.000Z</published>
    <updated>2016-06-09T12:12:13.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/21.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/22.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/23.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/24.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/25.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/26.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/27.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之一 -- 目录和前言</title>
    <link href="http://yoursite.com/2016/05/28/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%80-%E7%9B%AE%E5%BD%95%E5%92%8C%E5%89%8D%E8%A8%80/"/>
    <id>http://yoursite.com/2016/05/28/word2vec详解之一-目录和前言/</id>
    <published>2016-05-28T06:33:10.000Z</published>
    <updated>2016-06-09T12:11:23.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/13.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/14.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/15.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>PHP连接数据库js可视化数据</title>
    <link href="http://yoursite.com/2016/05/26/PHP%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93js%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2016/05/26/PHP连接数据库js可视化数据/</id>
    <published>2016-05-26T02:09:56.000Z</published>
    <updated>2016-05-26T02:11:07.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>短句归一化--LSI模型</title>
    <link href="http://yoursite.com/2016/05/25/%E7%9F%AD%E9%97%AE%E9%A2%98%E5%BD%92%E4%B8%80%E5%8C%96-LSI%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/25/短问题归一化-LSI模型/</id>
    <published>2016-05-25T12:59:36.000Z</published>
    <updated>2016-06-09T12:12:01.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;潜在语义索引是一种用奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以可以从一篇文章中提取术语关系，从而建立起主要概念内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;降维过程&quot;&gt;&lt;a href=&quot;#降维过程&quot; class=&quot;headerlink&quot; title=&quot;降维过程&quot;&gt;&lt;/a&gt;降维过程&lt;/h3&gt;&lt;p&gt;将文档库表示成VSM模型的词-文档矩阵Am×n(词-文档矩阵那就是词作为行，文档作为列，这是矩阵先行后列的表示决定的，当然如果表示成文档-词矩阵的话，后面的计算就要用该矩阵的转置了),其中m表示文档库中包含的所有不同的词的个(行数是不同词的个数)，即行向量表示一个词在不同文档出现的次数，n 表示文档库中的文档数(列数是不同文档的个数)，即列向量表示的是不同的文档.A表示为A = [α ij ],在此矩阵中 ,α ij为非负值 , 表示第 i 个词在第j 个文档中出现的频度。显然，A是稀疏矩阵(这是VSM和文档决定的)。&lt;/p&gt;
&lt;p&gt;利用奇异值分解SVD(Singular Value Decomposition)求A的只有K个正交因子的降秩矩阵，该过程就是降维的过程。SVD的重要作用是把词和文档映射到同一个语义空间中，将词和文档表示为K个因子的形式。显然，这会丢失信息，但主要的信息却被保留了。为什么该过程可以降维呢？因为该过程解决了同义和多义现象。可以看出，K的取值对整个分类结果的影响很大。因为，K过小，则丢失信息就越多；K过大，信息虽然多，但可能有冗余且计算消耗大。K的选择也是值得研究的，不过一般取值为100-300，不绝对。&lt;/p&gt;
&lt;h3 id=&quot;适用性&quot;&gt;&lt;a href=&quot;#适用性&quot; class=&quot;headerlink&quot; title=&quot;适用性&quot;&gt;&lt;/a&gt;适用性&lt;/h3&gt;&lt;p&gt;对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;特征降维&lt;br&gt;LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。TFIDF是另一个通用的降维方法，通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF。&lt;br&gt;不过必须提醒的是，无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;单词相关度计算&lt;br&gt;LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;聚类&lt;br&gt;直接用 LSI 聚类的情景还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;实验说明&quot;&gt;&lt;a href=&quot;#实验说明&quot; class=&quot;headerlink&quot; title=&quot;实验说明&quot;&gt;&lt;/a&gt;实验说明&lt;/h3&gt;&lt;p&gt;用了python的gensim包&lt;br&gt;现有的数据是438条标准问题以及3300条人工问题（可以转化为438条标准问题），现在需要对人工问题做一个归一化。&lt;br&gt;这里采用LSI模型进行建模实验，步骤如下。&lt;/p&gt;
&lt;h3 id=&quot;导入包&quot;&gt;&lt;a href=&quot;#导入包&quot; class=&quot;headerlink&quot; title=&quot;导入包&quot;&gt;&lt;/a&gt;导入包&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import logging
import jieba
import jieba.posseg as pseg
# 防止乱码
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)
# 打印log信息
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)&lt;/pre&gt;


&lt;h3 id=&quot;文本预处理&quot;&gt;&lt;a href=&quot;#文本预处理&quot; class=&quot;headerlink&quot; title=&quot;文本预处理&quot;&gt;&lt;/a&gt;文本预处理&lt;/h3&gt;&lt;pre&gt;
# 标准FAQ，一行对应一条问句
f = open(&#39;FAQuniq.txt&#39;, &#39;r&#39;)
# 对问句进行分词
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in f]

# 抽取一个bag-of-words，将文档的token映射为id
dictionary = corpora.Dictionary(texts)
# 保存词典
dictionary.save(&#39;LSI.dict&#39;)

# 产生文档向量，将用字符串表示的文档转换为用id和词频表示的文档向量
corpus = [dictionary.doc2bow(text) for text in texts]

# 基于这些“训练文档”计算一个TF-IDF模型
tfidf = models.TfidfModel(corpus)

# 转化文档向量，将用词频表示的文档向量表示为一个用tf-idf值表示的文档向量
corpus_tfidf = tfidf[corpus]

# 训练LSI模型 即将训练文档向量组成的矩阵SVD分解，并做一个秩为2的近似SVD分解
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)

# 保存模型
lsi.save(&#39;LSI.pkl&#39;)
lsi.print_topics(20)&lt;/pre&gt;


&lt;h3 id=&quot;初始化验证performance的文件&quot;&gt;&lt;a href=&quot;#初始化验证performance的文件&quot; class=&quot;headerlink&quot; title=&quot;初始化验证performance的文件&quot;&gt;&lt;/a&gt;初始化验证performance的文件&lt;/h3&gt;&lt;p&gt;checkFile的每行格式为：&lt;/p&gt;
&lt;pre&gt;原始问题的docid：对应的标准问题的topicid&lt;/pre&gt;

&lt;p&gt;把它存到checkDict这个dictionary中，key是docid，value是topicid。&lt;/p&gt;
&lt;pre&gt;
checkDict=dict()
def getCheckId():
    fcheck=open(&#39;checkFile.txt&#39;)
    for line in fcheck:
        line=line.strip(&#39;\n&#39;)
        if (len(line)==0):
            continue
        docid=line.split(&quot;:&quot;)[0]
        topicid=line.split(&quot;:&quot;)[1]
        checkDict[int(docid)]=int(topicid)
getCheckId()&lt;/pre&gt;

&lt;h3 id=&quot;归一化／计算文档相似度&quot;&gt;&lt;a href=&quot;#归一化／计算文档相似度&quot; class=&quot;headerlink&quot; title=&quot;归一化／计算文档相似度&quot;&gt;&lt;/a&gt;归一化／计算文档相似度&lt;/h3&gt;&lt;pre&gt;
# 建索引
index = similarities.MatrixSimilarity(lsi[corpus])

# 初始化分数
score1=0
score2=0
score3=0

# 读取文件，文件的每行格式为一个原始问句
f2=open(&#39;ORIFAQ3330.txt&#39;,&#39;r&#39;)
# count的作用是和checkFile的docid，即checkDict的key对应
count=1
for query in f2:
    # 获取该原始问句本应对应的正确标准问句
    if (not checkDict.has_key(count)):
        count+=1
        continue
    checkId=checkDict[count]
    # 将问句向量化
    query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
    # 再用之前训练好的LSI模型将其映射到二维的topic空间：
    query_lsi = lsi[query_bow]
    # 计算其和index中doc的余弦相似度了：
    sims = index[query_lsi]
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    # 找出最相关的三篇文档，计算这三篇文档是否包括标准问句，如果文档就是标准问句，对应的分数加1
    if (checkId==sort_sims[0][0]):
        score1+=1
    elif (checkId==sort_sims[1][0]):
        score2+=1
    elif (checkId==sort_sims[2][0]):
        score3+=1
    count+=1&lt;/pre&gt;

&lt;h3 id=&quot;打印分数&quot;&gt;&lt;a href=&quot;#打印分数&quot; class=&quot;headerlink&quot; title=&quot;打印分数&quot;&gt;&lt;/a&gt;打印分数&lt;/h3&gt;&lt;pre&gt;
print &quot;Score1: &quot;.format(score1*1.0/count)
print &quot;Score2: &quot;.format(score2*1.0/count)
print &quot;Score3: &quot;.format(score3*1.0/count)&lt;/pre&gt;

&lt;h3 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h3&gt;&lt;p&gt;其实这里的结果非常差，原因是文档（每一条问句）太短，只有十几个字，另外文档数太少，LSI降维牺牲了准确率，下一个实验LDA的准确率相比会高很多。&lt;br&gt;另外，本次实验所用的样本分布并不均匀，“未收到奖励”类似问题出现的频率比“软件无声音”类似问题出现的频率要高很多。&lt;strong&gt;&lt;em&gt;重申：LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;LSI-缺陷&quot;&gt;&lt;a href=&quot;#LSI-缺陷&quot; class=&quot;headerlink&quot; title=&quot;LSI 缺陷&quot;&gt;&lt;/a&gt;LSI 缺陷&lt;/h3&gt;&lt;p&gt;常用的VSM文本表示模型中有两个主要的缺陷：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;该模型假设所有特征词条之间是相互独立、互不影响的（朴素贝叶斯也是这个思想），即该模型还是基于“词袋”模型（应该说所有利用VSM模型没有进行潜在语义分析的算法都是基于“词袋”假设）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;没有进行特征降维，特征维数可能会很高，向量空间可能很大，对存储和计算资源要求会比较高。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;LSI的基本思想是文本中的词与词之间不是孤立的，存在着某种潜在的语义关系，通过对样本数据的统计分析，让机器自动挖掘出这些潜在的语义关系，并把这些关系表示成计算机可以”理解”的模型。它可以消除词匹配过程中的同义和多义现象。它可以将传统的VSM降秩到一个低维的语义空间中，在该语义空间中计算文档的相似度等。总的说来，LSI就是利用词的语义关系对VSM模型进行降维，并提高分类的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>GibbsLDA++: A C/C++ 使用心得</title>
    <link href="http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97/"/>
    <id>http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-使用心得/</id>
    <published>2016-05-25T04:20:35.000Z</published>
    <updated>2016-05-25T04:20:35.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>在c里调用python</title>
    <link href="http://yoursite.com/2016/05/22/%E5%9C%A8c%E9%87%8C%E8%B0%83%E7%94%A8python/"/>
    <id>http://yoursite.com/2016/05/22/在c里调用python/</id>
    <published>2016-05-22T08:55:07.000Z</published>
    <updated>2016-06-09T12:08:57.000Z</updated>
    
    <content type="html">&lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;pre&gt;
#include &lt;python2.7 python.h=&quot;&quot;&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
void test1(){
  Py_Initialize();//初始化python
  char *test = &quot;奖励&quot;;
  PyObject * pModule = NULL;
  PyObject * pModule1 = NULL;
  PyObject * pFunc = NULL;
  PyObject * pArg    = NULL;
  PyObject * result;
  pModule = PyImport_ImportModule(&quot;inferSingleDocVec&quot;);//引入模块
  pFunc = PyObject_GetAttrString(pModule, &quot;getDocVec&quot;);//直接获取模块中的函数
  pArg= Py_BuildValue(&quot;(s)&quot;, test);
  result = PyEval_CallObject(pFunc, pArg); //调用直接获得的函数，并传递参数；这里得到的是一个list
  &lt;code&gt;for (int i = 0; i &lt; PyList_Size(result); i++) {&lt;/code&gt;
    printf(&quot;%f\t&quot;, PyFloat_AsDouble(PyList_GetItem(result, (Py_ssize_t)i)));//打印每一个元素
  }
  //下面代码适用于返回值为字符串的情况
  //char* s=NULL;
  //PyArg_Parse(result, &quot;s&quot;, &amp;s);
  //for (int i=0;s[i]!=&#39;\0&#39;;i++){
   // printf(&quot;%c&quot;,s[i]);
 // }
  Py_Finalize(); //释放python
//  return;
}
int main(int argc, char* argv[])
{
    test1();
    return 0;
}
&lt;/stdlib.h&gt;&lt;/stdio.h&gt;&lt;/python2.7&gt;&lt;/pre&gt;

&lt;p&gt;编译运行&lt;/p&gt;
&lt;pre&gt;
$ gcc -I/usr/local/lib/python2.7.11 -o inferDocVec inferDocVec.c -lpython2.7
$ ./inferDocVec
&lt;/pre&gt;

&lt;p&gt;调用的inferSingleDocVec文件&lt;/p&gt;
&lt;pre&gt;
#!/usr/bin/python
# -*- coding: utf-8 -*-
### for infer
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
from gensim.models import Doc2Vec
import os
import jieba
import multiprocessing
import numpy as np
import base64
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

def getDocVec(doc_words):
    docwords=[word for word in jieba.cut(doc_words, cut_all = False)]
    model = Doc2Vec.load(&#39;all_model_v2.txt&#39;)
    invec = model.infer_vector(docwords, alpha=0.1, min_alpha=0.0001, steps=5)
    return (list)(invec)
&lt;/pre&gt;

&lt;p&gt;关于如何将python文件转为模块，详见之前的一篇博文&lt;a href=&quot;https://github.com/Shuang0420/Shuang0420.github.io/wiki/python----%E5%B0%86%E8%87%AA%E5%B7%B1%E5%86%99%E7%9A%84py%E6%96%87%E4%BB%B6%E4%BD%9C%E4%B8%BA%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python 将自己写的py文件作为模块导入&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="c" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>AP聚类</title>
    <link href="http://yoursite.com/2016/05/19/AP%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2016/05/19/AP聚类/</id>
    <published>2016-05-19T13:42:04.000Z</published>
    <updated>2016-06-09T12:12:25.000Z</updated>
    
    <content type="html">&lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Affinity Propagation (AP) 聚类是最近在Science杂志上提出的一种新的聚类算法。它根据N个数据点之间的相似度进行聚类,这些相似度可以是对称的,即两个数据点互相之间的相似度一样(如欧氏距离);也可以是不对称的,即两个数据点互相之间的相似度不等。这些相似度组成N×N的相似度矩阵S(其中N为有N个数据点)。AP算法不需要事先指定聚类数目,相反它将所有的数据点都作为潜在的聚类中心,称之为exemplar。以S矩阵的对角线上的数值s(k, k)作为k点能否成为聚类中心的评判标准,这意味着该值越大,这个点成为聚类中心的可能性也就越大,这个值又称作参考度p (preference)。&lt;br&gt;在这里介绍几个文中常出现的名词：&lt;br&gt;exemplar：指的是聚类中心。&lt;br&gt;similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，如－|| ||。文中，所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。&lt;br&gt;preference：数据点i的参考度称为P(i)或S(i,i)。是指点i作为聚类中心的参考度。一般取S相似度值的中值。&lt;br&gt;Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度。&lt;br&gt;Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度。&lt;/p&gt;
&lt;p&gt;Script output:&lt;br&gt;Estimated number of clusters: 3&lt;br&gt;Homogeneity: 0.872&lt;br&gt;Completeness: 0.872&lt;br&gt;V-measure: 0.872&lt;br&gt;Adjusted Rand Index: 0.912&lt;br&gt;Adjusted Mutual Information: 0.871&lt;br&gt;Silhouette Coefficient: 0.753&lt;/p&gt;
&lt;p&gt;Python source code: plot_affinity_propagation.py&lt;br&gt;print(&lt;strong&gt;doc&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;from sklearn.cluster import AffinityPropagation&lt;br&gt;from sklearn import metrics&lt;br&gt;from sklearn.datasets.samples_generator import make_blobs&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Generate-sample-data&quot;&gt;&lt;a href=&quot;#Generate-sample-data&quot; class=&quot;headerlink&quot; title=&quot;Generate sample data&quot;&gt;&lt;/a&gt;Generate sample data&lt;/h1&gt;&lt;p&gt;centers = [[1, 1], [-1, -1], [1, -1]]&lt;br&gt;X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,&lt;br&gt;                            random_state=0)&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Compute-Affinity-Propagation&quot;&gt;&lt;a href=&quot;#Compute-Affinity-Propagation&quot; class=&quot;headerlink&quot; title=&quot;Compute Affinity Propagation&quot;&gt;&lt;/a&gt;Compute Affinity Propagation&lt;/h1&gt;&lt;p&gt;af = AffinityPropagation(preference=-50).fit(X)&lt;br&gt;cluster_centers_indices = af.cluster_centers&lt;em&gt;indices&lt;/em&gt;&lt;br&gt;labels = af.labels_&lt;/p&gt;
&lt;p&gt;n&lt;em&gt;clusters&lt;/em&gt; = len(cluster_centers_indices)&lt;/p&gt;
&lt;p&gt;print(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))&lt;br&gt;print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))&lt;br&gt;print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))&lt;br&gt;print(“Adjusted Rand Index: %0.3f”&lt;br&gt;      % metrics.adjusted_rand_score(labels_true, labels))&lt;br&gt;print(“Adjusted Mutual Information: %0.3f”&lt;br&gt;      % metrics.adjusted_mutual_info_score(labels_true, labels))&lt;br&gt;print(“Silhouette Coefficient: %0.3f”&lt;br&gt;      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Plot-result&quot;&gt;&lt;a href=&quot;#Plot-result&quot; class=&quot;headerlink&quot; title=&quot;Plot result&quot;&gt;&lt;/a&gt;Plot result&lt;/h1&gt;&lt;p&gt;import matplotlib.pyplot as plt&lt;br&gt;from itertools import cycle&lt;/p&gt;
&lt;p&gt;plt.close(‘all’)&lt;br&gt;plt.figure(1)&lt;br&gt;plt.clf()&lt;/p&gt;
&lt;p&gt;colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)&lt;br&gt;for k, col in zip(range(n&lt;em&gt;clusters&lt;/em&gt;), colors):&lt;br&gt;    class_members = labels == k&lt;br&gt;    cluster_center = X[cluster_centers_indices[k]]&lt;br&gt;    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)&lt;br&gt;    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,&lt;br&gt;             markeredgecolor=’k’, markersize=14)&lt;br&gt;    for x in X[class_members]:&lt;br&gt;        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)&lt;/p&gt;
&lt;p&gt;plt.title(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;plt.show()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://scikit-learn.org/stable/modules/clustering.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://blog.csdn.net/u010695420/article/details/42239465&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/u010695420/article/details/42239465&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LDA验证</title>
    <link href="http://yoursite.com/2016/05/19/LDA%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2016/05/19/LDA验证/</id>
    <published>2016-05-19T13:14:42.000Z</published>
    <updated>2016-05-19T13:14:42.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Gensim and LDA--Training and Prediction</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/</id>
    <published>2016-05-18T12:46:16.000Z</published>
    <updated>2016-06-09T12:19:41.000Z</updated>
    
    <content type="html">&lt;p&gt;用 Gensim 实现 LDA，相比 JGibbLDA 的使用 Gensim 略为麻烦，然而感觉更清晰易懂，也就更灵活。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
# install the related python packages
&gt;&gt;&gt; pip install numpy
&gt;&gt;&gt; pip install scipy
&gt;&gt;&gt; pip install gensim
&gt;&gt;&gt; pip install jieba

from gensim import corpora, models, similarities
import logging
import jieba

# configuration
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# load data from file
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()

＃ tokenize
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]

# load id-&gt;word mapping (the dictionary)
dictionary = corpora.Dictionary(texts)

# word must appear &gt;10 times, and no more than 40% documents
dictionary.filter_extremes(no_below=40, no_above=0.1)

# save dictionary
dictionary.save(&#39;dict_v1.dict&#39;)

# load corpus
corpus = [dictionary.doc2bow(text) for text in texts]

# initialize a model
tfidf = models.TfidfModel(corpus)

# use the model to transform vectors, apply a transformation to a whole corpus
corpus_tfidf = tfidf[corpus]

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)

# save model to files
lda.save(&#39;mylda_v1.pkl&#39;)

# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.
for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, lda.print_topic(index, 10))

# print the most contributing words for 100 randomly selected topics
lda.print_topics(100)

# load model and dictionary
model = models.LdaModel.load(&#39;mylda_v1.pkl&#39;)
dictionary = corpora.Dictionary.load(&#39;dict_v1.dict&#39;)

# predict unseen data
query = &quot;未收到奖励&quot;
query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
for index, score in sorted(model[query_bow], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))

# if you want to predict many lines of data in a file, do the followings
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]
corpus = [dictionary.doc2bow(text) for text in texts]

# only print the topic with the highest score
for c in corpus:
    flag = True
    for index, score in sorted(model[c], key=lambda tup: -1*tup[1]):
        if flag:
            print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))&lt;/pre&gt;

&lt;h1 id=&quot;Tips&quot;&gt;&lt;a href=&quot;#Tips&quot; class=&quot;headerlink&quot; title=&quot;Tips:&quot;&gt;&lt;/a&gt;Tips:&lt;/h1&gt;&lt;p&gt;If you occur encoding problems, you can try the following code&lt;/p&gt;
&lt;pre&gt;
add it at the beginning of your python file
# -*- coding: utf-8 -*-

# also, do the followings
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# the following code may lead to encoding problem when there&#39;re Chinese characters
model.show_topics(-1, 5)

# use this instead
model.print_topics(-1, 5)&lt;/pre&gt;


&lt;p&gt;You can see step-by-step output by the following references.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/tut2.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/tut2.html&lt;/a&gt; official guide (en)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/questionfish/article/details/46725475&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/questionfish/article/details/46725475&lt;/a&gt;  official guide (ch)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;用 Gensim 实现 LDA，相比 JGibbLDA 的使用 Gensim 略为麻烦，然而感觉更清晰易懂，也就更灵活。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>Gensim-用Python做主题模型</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-%E7%94%A8Python%E5%81%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-用Python做主题模型/</id>
    <published>2016-05-18T02:22:31.000Z</published>
    <updated>2016-06-09T12:12:34.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;gensim-特性&quot;&gt;&lt;a href=&quot;#gensim-特性&quot; class=&quot;headerlink&quot; title=&quot;gensim 特性&quot;&gt;&lt;/a&gt;gensim 特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;内存独立- 对于训练语料来说，没必要在任何时间将整个语料都驻留在RAM中&lt;/li&gt;
&lt;li&gt;有效实现了许多流行的向量空间算法－包括tf-idf，分布式LSA, 分布式LDA 以及 RP；并且很容易添加新算法&lt;/li&gt;
&lt;li&gt;对流行的数据格式进行了IO封装和转换&lt;/li&gt;
&lt;li&gt;在其语义表达中，可以相似查询&lt;/li&gt;
&lt;li&gt;gensim的创建的目的是，由于缺乏简单的（java很复杂）实现主题建模的可扩展软件框架.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-设计原则&quot;&gt;&lt;a href=&quot;#gensim-设计原则&quot; class=&quot;headerlink&quot; title=&quot;gensim 设计原则&quot;&gt;&lt;/a&gt;gensim 设计原则&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;简单的接口，学习曲线低。对于原型实现很方便&lt;/li&gt;
&lt;li&gt;根据输入的语料的size来说，内存各自独立；基于流的算法操作，一次访问一个文档.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-核心概念&quot;&gt;&lt;a href=&quot;#gensim-核心概念&quot; class=&quot;headerlink&quot; title=&quot;gensim 核心概念&quot;&gt;&lt;/a&gt;gensim 核心概念&lt;/h3&gt;&lt;p&gt;gensim的整个package会涉及三个概念：corpus, vector, model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语库(corpus)&lt;br&gt;文档集合，用于自动推出文档结构，以及它们的主题等，也可称作训练语料。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;向量(vector)&lt;/p&gt;
&lt;p&gt;在向量空间模型(VSM)中，每个文档被表示成一个特征数组。例如，一个单一特征可以被表示成一个问答对(question-answer pair):&lt;/p&gt;
&lt;p&gt;[1].在文档中单词”splonge”出现的次数？ 0个&lt;br&gt;[2].文档中包含了多少句子？ 2个&lt;br&gt;[3].文档中使用了多少字体? 5种&lt;br&gt;这里的问题可以表示成整型id (比如：1,2,3等), 因此，上面的文档可以表示成：(1, 0.0), (2, 2.0), (3, 5.0). 如果我们事先知道所有的问题，我们可以显式地写成这样：(0.0, 2.0, 5.0). 这个answer序列可以认为是一个多维矩阵（3维）. 对于实际目的，只有question对应的answer是一个实数.&lt;/p&gt;
&lt;p&gt;对于每个文档来说，answer是类似的. 因而，对于两个向量来说（分别表示两个文档），我们希望可以下类似的结论：“如果两个向量中的实数是相似的，那么，原始的文档也可以认为是相似的”。当然，这样的结论依赖于我们如何去选取我们的question。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;稀疏矩阵(Sparse vector)&lt;/p&gt;
&lt;p&gt;通常，大多数answer的值都是0.0. 为了节省空间，我们需要从文档表示中忽略它们，只需要写：(2, 2.0), (3, 5.0) 即可(注意：这里忽略了(1, 0.0)). 由于所有的问题集事先都知道，那么在稀疏矩阵的文档表示中所有缺失的特性可以认为都是0.0.&lt;/p&gt;
&lt;p&gt;gensim的特别之处在于，它没有限定任何特定的语料格式；语料可以是任何格式，当迭代时，通过稀疏矩阵来完成即可。例如，集合 ([(2, 2.0), (3, 5.0)], ([0, -1.0], [3, -1.0])) 是一个包含两个文档的语料，每个都有两个非零的 pair。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;模型(model)&lt;/p&gt;
&lt;p&gt;对于我们来说，一个模型就是一个变换(transformation)，将一种文档表示转换成另一种。初始和目标表示都是向量－－它们只在question和answer之间有区别。这个变换可以通过训练的语料进行自动学习，无需人工监督，最终的文档表示将更加紧凑和有用；相似的文档具有相似的表示。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;演示代码&quot;&gt;&lt;a href=&quot;#演示代码&quot; class=&quot;headerlink&quot; title=&quot;演示代码&quot;&gt;&lt;/a&gt;演示代码&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;演示代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://d0evi1.github.io/gensim/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://d0evi1.github.io/gensim/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>JGibbLDA实战</title>
    <link href="http://yoursite.com/2016/05/16/JGibbLDA%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/16/JGibbLDA实战/</id>
    <published>2016-05-16T12:50:23.000Z</published>
    <updated>2016-06-09T12:09:14.000Z</updated>
    
    <content type="html">&lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;错误&quot;&gt;&lt;a href=&quot;#错误&quot; class=&quot;headerlink&quot; title=&quot;错误&quot;&gt;&lt;/a&gt;错误&lt;/h3&gt;&lt;p&gt;命令：&lt;/p&gt;
&lt;pre&gt;java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models/casestudy/newdocs.dat&lt;/pre&gt;

&lt;p&gt;错误信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/err.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;手动配置&quot;&gt;&lt;a href=&quot;#手动配置&quot; class=&quot;headerlink&quot; title=&quot;手动配置&quot;&gt;&lt;/a&gt;手动配置&lt;/h3&gt;&lt;p&gt;于是尝试导入eclipse运行手动配置，成功，过程如下。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载JGibbLDA的jar包并解压；&lt;br&gt;网址：&lt;a href=&quot;http://jgibblda.sourceforge.net/#Griffiths04&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/#Griffiths04&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;导入eclipse，确保jar包在目录中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;找到LDACmdOption.java文件， 修改部分代码&lt;/p&gt;
&lt;pre&gt; @Option(name=&quot;-dir&quot;, usage=&quot;Specify directory&quot;)
 public String dir = &quot;models/casestudy-en&quot;;

 @Option(name=&quot;-dfile&quot;, usage=&quot;Specify data file&quot;)
 public String dfile = &quot;models/casestudy-en/newdocs.dat&quot;;&lt;/pre&gt;

&lt;p&gt;值得注意的是，dfile的格式必须是👇这个样子：&lt;/p&gt;
&lt;pre&gt;[M]
[document1]
[document2]
...
[documentM]&lt;/pre&gt;

&lt;p&gt;第一行[M]是documents的总数，之后的每一行是一个document，每个document是一个word list，或者说是bag of words。&lt;/p&gt;
&lt;pre&gt;[document i] = [word i1] [word i2] ... [word iNi]&lt;/pre&gt;

&lt;p&gt;各参数含义：&lt;br&gt;&lt;strong&gt;-est &lt;/strong&gt;从训练语料中评估出LDA模型&lt;br&gt;&lt;strong&gt;-alpha&lt;/strong&gt; LDA模型中的alpha数值，默认为50/K(K是主题数目)&lt;br&gt;&lt;strong&gt;-beta&lt;/strong&gt; LDA模型中的beta数值，默认是0.1&lt;br&gt;&lt;strong&gt;-ntopics&lt;/strong&gt; 主题数目，默认值是100&lt;br&gt;&lt;strong&gt;-niters&lt;/strong&gt; GIbbs采样的迭代数目，默认值为2000&lt;br&gt;&lt;strong&gt;-savestep&lt;/strong&gt; 指定开始保存LDA模型的迭代次数&lt;br&gt;&lt;strong&gt;-dir&lt;/strong&gt; 训练语料目录&lt;br&gt;&lt;strong&gt;-dfile&lt;/strong&gt; 训练语料文件名称&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;修改项目的Run Configurations，在Java Application中选择LDA，点击(x)=Arguments，输入&lt;/p&gt;
&lt;pre&gt;-est -alpha 0.2 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 100 -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;newdocs.dat&quot;&lt;/pre&gt;

&lt;p&gt; 若利用已训练的LDA模型预测，输入以下参数：&lt;/p&gt;
&lt;pre&gt;-inf -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;test.txt&quot;&lt;/pre&gt;

&lt;p&gt; 注意，进行预测时，当前目录下必须包含已有的LDA训练输出文件，包括model-final.others、model-final.phi、model-final.tassign、model-final.theta、model-final.twords、wordmap.txt文件，如果运行报错，尝试修改LDACmdOption.java的modelName，确保和文件名的modelname部分一致。&lt;br&gt;&lt;pre&gt;@Option(name=”-model”, usage=”Specify the model name”)&lt;br&gt; public String modelName = “model-final”;&lt;/pre&gt;   &lt;/p&gt;
&lt;p&gt; 如果出现java heap limited的问题，在VM arguments下添加&lt;/p&gt;
&lt;pre&gt;-Xms1g -Xmx1g -Xmn512m&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run&lt;br&gt;输出文件主要有：&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.others&lt;/model_name&gt;&lt;/strong&gt;  文件存储LDA模型参数，如alpha、beta等。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.phi &lt;/model_name&gt;&lt;/strong&gt; 每个topic内对doc的分布情况。文件存储词语-主题分布，每一行是一个主题，列内容为词语。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.theta &lt;/model_name&gt;&lt;/strong&gt; 每个doc内对应上面的n个topic的分布情况。文件主题文档分布，每一行是一个文档，列内容是主题概率。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.tassign&lt;/model_name&gt;&lt;/strong&gt;  文件是训练预料中单词的主题指定（归属），每一行是一个语料文档。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.twords&lt;/model_name&gt;&lt;/strong&gt;  n个topic，以及每个topic下面包含的具体的字词&lt;br&gt;&lt;strong&gt;wordmap.txt&lt;/strong&gt;  词-id映射&lt;br&gt;其中&lt;model_name&gt;根据采样迭代次数来指定，如model-00800，最后一次采样名称命名为model-final。&lt;/model_name&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;br&gt;&lt;a href=&quot;http://www.ithao123.cn/content-4208214.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.ithao123.cn/content-4208214.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://jgibblda.sourceforge.net/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>gollum/-github上搭建个人wiki</title>
    <link href="http://yoursite.com/2016/05/13/gollum:-github%E4%B8%8A%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAwiki/"/>
    <id>http://yoursite.com/2016/05/13/gollum:-github上搭建个人wiki/</id>
    <published>2016-05-13T08:55:07.000Z</published>
    <updated>2016-06-09T12:08:26.000Z</updated>
    
    <content type="html">&lt;p&gt;博客凸显创作，维基则是整理的好工具，很多入门级别、复用别人的操作，如配置环境等，更适合发布在个人维基上，本文就以gollum+github搭建个人wiki做个示范。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;开通Wiki&quot;&gt;&lt;a href=&quot;#开通Wiki&quot; class=&quot;headerlink&quot; title=&quot;开通Wiki&quot;&gt;&lt;/a&gt;开通Wiki&lt;/h3&gt;&lt;p&gt;登陆Github，找到你所开通的Github项目的Settings栏目，开通Wikis，如果只希望别人可读不可写，勾选：Restrict edits to Collaborators only。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%884.56.12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;git-clone-相应维基的git地址&quot;&gt;&lt;a href=&quot;#git-clone-相应维基的git地址&quot; class=&quot;headerlink&quot; title=&quot;git clone 相应维基的git地址&quot;&gt;&lt;/a&gt;git clone 相应维基的git地址&lt;/h3&gt;&lt;pre&gt; git clone git@github.com:Shuang0420/Shuang0420.github.io.wiki.git wiki&lt;/pre&gt;

&lt;p&gt;如果你之前没有设置git密钥，可以参照以下步骤先做配置，如果已经设置，请忽略。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看是否已经有了ssh密钥：&lt;br&gt;&lt;pre&gt;cd ~/.ssh&lt;/pre&gt;&lt;br&gt;如果没有密钥则不会有此文件夹，有则备份删除&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;生成密钥，得到两个文件：id_rsa 和 id_rsa.pub&lt;/p&gt;
&lt;pre&gt;ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;添加密钥到ssh：ssh-add id_rsa&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在github上settings中添加ssh密钥，即“id_rsa.pub”里的公钥。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;测试：ssh git@github.com&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;配置个人wiki&quot;&gt;&lt;a href=&quot;#配置个人wiki&quot; class=&quot;headerlink&quot; title=&quot;配置个人wiki&quot;&gt;&lt;/a&gt;配置个人wiki&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在wiki目录下，安装bundler&lt;/p&gt;
&lt;pre&gt;gem install bundler&lt;/pre&gt;

&lt;p&gt;   如果安装没有问题，可以跳过以下错误解决。&lt;/p&gt;
   &lt;pre&gt;
   ERROR:  Could not find a valid gem &#39;bundler&#39; (&gt;= 0), here is why:
             Unable to download data from https://rubygems.org/ - Errno::EPIPE: Broken pipe - SSL_connect (https://rubygems.org/latest_specs.4.8.gz)&lt;/pre&gt;

&lt;p&gt;   解决：&lt;/p&gt;
   &lt;pre&gt;gem source -a http://rubygems.org/
   gem install bundler&lt;/pre&gt;

&lt;p&gt;   然而还是有错误：&lt;/p&gt;
   &lt;pre&gt;Fetching: bundler-1.12.5.gem (100%)^[[A
   ERROR:  While executing gem ... (Gem::FilePermissionError)
       You don&#39;t have write permissions for the /Library/Ruby/Gems/2.0.0 directory.&lt;/pre&gt;

&lt;p&gt;   因为没有sudo：&lt;/p&gt;
   &lt;pre&gt;sudo gem install bundler&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;新建Gemfile文件，内容如下：&lt;/p&gt;
   &lt;pre&gt;
   source &quot;http://rubygems.org&quot;
   gem &#39;redcarpet&#39;
   gem &quot;grit&quot;, &#39;~&gt; 2.5.0&#39;, git: &#39;https://github.com/gitlabhq/grit.git&#39;, ref: &#39;42297cdcee16284d2e4eff23d41377f52fc28b9d&#39;
   gem &#39;gollum&#39;, git: &#39;https://github.com/gollum/gollum.git&#39;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;运行：&lt;/p&gt;
   &lt;pre&gt;
   # 安装项目依赖的所有gem包;此命令会尝试更新系统中已存在的gem包
   bundle install&lt;/pre&gt;

&lt;p&gt;   时间有点久，耐心等待。&lt;br&gt;   然而最后出现error,&lt;/p&gt;
   &lt;pre&gt;An error occurred while installing charlock_holmes (0.7.3), and Bundler cannot continue.
   Make sure that `gem install charlock_holmes -v &#39;0.7.3&#39;` succeeds before bundling.&lt;/pre&gt;

&lt;p&gt;   好。那就按要求安装。&lt;/p&gt;
   &lt;pre&gt;sudo gem install charlock_holmes -v &#39;0.7.3&#39;&lt;/pre&gt;

&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%885.25.54.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   好。继续按要求安装。&lt;/p&gt;
   &lt;pre&gt;brew install icu4c&lt;/pre&gt;

&lt;p&gt;   再重来&lt;/p&gt;
   &lt;pre&gt;sudo gem install charlock_holmes -v &#39;0.7.3&#39;&lt;/pre&gt;

   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-27%20%E4%B8%8A%E5%8D%889.46.38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   因为没有安装bundle&lt;/p&gt;
   &lt;pre&gt;gem install bundle&lt;/pre&gt;

&lt;p&gt;   安装后再次尝试运行&lt;/p&gt;
   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   error&lt;/p&gt;
   &lt;pre&gt;Could not reach host index.rubygems.org. Check your network connection and try again.&lt;/pre&gt;
   出现这种错误可以尝试把Gemfile里的https改成http（互相转化进行尝试）

   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   终于成功！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;已安装成功gollum等。然后运行：&lt;/p&gt;
   &lt;pre&gt;gollum&lt;/pre&gt;

&lt;p&gt;   走到这一步了本人还是遇到了万恶的失败。&lt;br&gt;   看着已经安装好了&lt;/p&gt;
   &lt;pre&gt;
   Installing nokogiri 1.6.7.2 with native extensions
   Installing rack-protection 1.5.3
   Installing gollum-grit_adapter 1.0.1
   Installing sanitize 2.1.0
   Installing sinatra 1.4.7
   Installing gollum-lib 4.2.0
   Using gollum 4.0.1 from https://github.com/gollum/gollum.git (at master@5a5e56a)
   Bundle complete! 3 Gemfile dependencies, 24 gems now installed.
   Use `bundle show [gemname]` to see where a bundled gem is installed.
   &lt;/pre&gt;

&lt;p&gt;   然而实际并没有&lt;/p&gt;
   &lt;pre&gt;
   $ gollum
   -bash: gollum: command not found&lt;/pre&gt;

&lt;p&gt;   大写的忧伤。最后通过直接安装gollum解决。&lt;/p&gt;
   &lt;pre&gt;sudo gem install gollum&lt;/pre&gt;

   &lt;pre&gt;gollum&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;终于可以在本地启动成功维基。打开网址：&lt;a href=&quot;http://0.0.0.0:4567/，可以直接在浏览器中编辑。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://0.0.0.0:4567/，可以直接在浏览器中编辑。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%886.26.05.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   如果发现不能如下错误，请尝试更新ruby。&lt;/p&gt;
&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%889.31.07.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   更新ruby步骤&lt;br&gt;   安装 RVM&lt;br&gt;   RVM:Ruby Version Manager,Ruby版本管理器，包括Ruby的版本管理和Gem库管理(gemset)&lt;/p&gt;
   &lt;pre&gt;curl -L get.rvm.io | bash -s stable&lt;/pre&gt;

   &lt;pre&gt;source ~/.bashrc  
   source ~/.bash_profile &lt;/pre&gt;

&lt;p&gt;   测试是否安装正常&lt;/p&gt;
   &lt;pre&gt;rvm -v  &lt;/pre&gt;

&lt;p&gt;   用RVM升级Ruby&lt;/p&gt;
   &lt;pre&gt;
   #查看当前ruby版本  
   ruby -v  
   #列出已知的ruby版本  
   rvm list known  
   #安装ruby 2.3.0
   rvm install 2.3.0 &lt;/pre&gt;

&lt;p&gt;   安装完成之后ruby -v查看是否安装成功。&lt;/p&gt;
&lt;p&gt;   重新安装完毕后回到第3步。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;github同步&quot;&gt;&lt;a href=&quot;#github同步&quot; class=&quot;headerlink&quot; title=&quot;github同步&quot;&gt;&lt;/a&gt;github同步&lt;/h3&gt;&lt;p&gt;在wiki目录下面，进行git库操作，提交本地对维基内容的修改。一切将自动保存在你的Github上的个人博客网站的wiki目录下面。&lt;/p&gt;
&lt;pre&gt;
cd ~/wiki
git add .
git commit -am&quot;first commit&quot;
git push&lt;/pre&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;博客凸显创作，维基则是整理的好工具，很多入门级别、复用别人的操作，如配置环境等，更适合发布在个人维基上，本文就以gollum+github搭建个人wiki做个示范。&lt;/p&gt;
    
    </summary>
    
      <category term="Configuration" scheme="http://yoursite.com/categories/Configuration/"/>
    
    
      <category term="wiki" scheme="http://yoursite.com/tags/wiki/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 主题配置</title>
    <link href="http://yoursite.com/2016/05/12/Github-Pages-Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2016/05/12/Github-Pages-Hexo主题配置/</id>
    <published>2016-05-12T12:26:12.000Z</published>
    <updated>2016-06-10T08:14:09.000Z</updated>
    
    <content type="html">&lt;p&gt;终于搭建完自己的博客站点啦，好有成就感✌️分享一些本站使用的 NexT 主题配置技巧。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;添加“标签”页面&quot;&gt;&lt;a href=&quot;#添加“标签”页面&quot; class=&quot;headerlink&quot; title=&quot;添加“标签”页面&quot;&gt;&lt;/a&gt;添加“标签”页面&lt;/h3&gt;&lt;p&gt;在终端窗口下，定位到 Hexo 站点目录下，新建一个页面，命名为 tags ：&lt;/p&gt;
&lt;pre&gt;
$ cd your-hexo-site
$ hexo new page tags
&lt;/pre&gt;

&lt;p&gt;注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如：&lt;/p&gt;
&lt;pre&gt;
title: 标签
date: 2014-12-22 12:39:04
type: &quot;tags&quot;
comments: false
&lt;/pre&gt;

&lt;p&gt;在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下:&lt;/p&gt;
&lt;pre&gt;
menu:
  home: /
  archives: /archives
  tags: /tags
&lt;/pre&gt;

&lt;h3 id=&quot;添加“分类”页面&quot;&gt;&lt;a href=&quot;#添加“分类”页面&quot; class=&quot;headerlink&quot; title=&quot;添加“分类”页面&quot;&gt;&lt;/a&gt;添加“分类”页面&lt;/h3&gt;&lt;p&gt;在终端窗口下，定位到 Hexo 站点目录下，新建一个页面，命名为 categories ：&lt;/p&gt;
&lt;pre&gt;
$ cd your-hexo-site
$ hexo new page categories
&lt;/pre&gt;

&lt;p&gt;注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如：&lt;/p&gt;
&lt;pre&gt;
title: 分类
date: 2014-12-22 12:39:04
type: &quot;categories&quot;
comments: false
&lt;/pre&gt;

&lt;p&gt;在菜单中添加链接。编辑 主题配置文件 ， 添加 categories 到 menu 中，如下:&lt;/p&gt;
&lt;pre&gt;
menu:
  home: /
  archives: /archives
  categories : /categories
&lt;/pre&gt;


&lt;h3 id=&quot;评论系统&quot;&gt;&lt;a href=&quot;#评论系统&quot; class=&quot;headerlink&quot; title=&quot;评论系统&quot;&gt;&lt;/a&gt;评论系统&lt;/h3&gt;&lt;p&gt;感觉 DISUQS 比 多说 的设置简单一些。在 &lt;a href=&quot;https://disqus.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://disqus.com/&lt;/a&gt; 按要求注册，完成后在 admin/settings/general/ 下找到 Shortname。&lt;br&gt;编辑 站点配置文件， 添加 disqus_shortname 字段，设置如下：&lt;/p&gt;
&lt;pre&gt;
disqus_shortname: your-disqus-shortname
&lt;/pre&gt;


&lt;h3 id=&quot;设置-RSS&quot;&gt;&lt;a href=&quot;#设置-RSS&quot; class=&quot;headerlink&quot; title=&quot;设置 RSS&quot;&gt;&lt;/a&gt;设置 RSS&lt;/h3&gt;&lt;p&gt;安装 hexo-generator-feed，在站点的根目录下执行以下命令：&lt;/p&gt;
&lt;pre&gt;$ npm install hexo-generator-feed --save&lt;/pre&gt;

&lt;p&gt;更改 主题配置文件，设定 rss 字段的值，留空表示使用 Hexo 生成的 Feed 链接。&lt;/p&gt;
&lt;h3 id=&quot;访问量统计&quot;&gt;&lt;a href=&quot;#访问量统计&quot; class=&quot;headerlink&quot; title=&quot;访问量统计&quot;&gt;&lt;/a&gt;访问量统计&lt;/h3&gt;&lt;p&gt;编辑 主题配置文件 中的 busuanzi_count 的配置项。&lt;br&gt;当enable: true时，代表开启全局开关。若site_uv、site_pv、page_pv的值均为false时，不蒜子仅作记录而不会在页面上显示。&lt;/p&gt;
&lt;h3 id=&quot;搜索服务&quot;&gt;&lt;a href=&quot;#搜索服务&quot; class=&quot;headerlink&quot; title=&quot;搜索服务&quot;&gt;&lt;/a&gt;搜索服务&lt;/h3&gt;&lt;p&gt;添加百度/谷歌/本地 自定义站点内容搜索&lt;/p&gt;
&lt;p&gt;安装 hexo-generator-search，在站点的根目录下执行以下命令：&lt;/p&gt;
&lt;pre&gt;$ npm install hexo-generator-search --save&lt;/pre&gt;

&lt;p&gt;编辑 站点配置文件，新增以下内容到任意位置：&lt;/p&gt;
&lt;pre&gt;
search:
  path: search.xml
  field: post&lt;/pre&gt;


&lt;h3 id=&quot;开启打赏功能&quot;&gt;&lt;a href=&quot;#开启打赏功能&quot; class=&quot;headerlink&quot; title=&quot;开启打赏功能&quot;&gt;&lt;/a&gt;开启打赏功能&lt;/h3&gt;&lt;p&gt;只需要在 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。&lt;/p&gt;
&lt;pre&gt;
reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！
wechatpay: /path/to/wechat-reward-image
alipay: /path/to/alipay-reward-image
&lt;/pre&gt;

&lt;h3 id=&quot;设置阅读全文&quot;&gt;&lt;a href=&quot;#设置阅读全文&quot; class=&quot;headerlink&quot; title=&quot;设置阅读全文&quot;&gt;&lt;/a&gt;设置阅读全文&lt;/h3&gt;&lt;p&gt;在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法：&lt;/p&gt;
&lt;p&gt;在文章中使用 &lt;!-- more --&gt; 手动进行截断，这是 Hexo 提供的方式，推荐使用。&lt;br&gt;在文章的 front-matter 中添加 description，并提供文章摘录&lt;br&gt;自动形成摘要，在 主题配置文件 中添加：&lt;/p&gt;
&lt;pre&gt;
auto_excerpt:
  enable: true
  length: 150&lt;/pre&gt;

&lt;p&gt;默认截取的长度为 150 字符，可以根据需要自行设定&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;整理自 &lt;a href=&quot;http://theme-next.iissnan.com/getting-started.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NexT 使用文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;终于搭建完自己的博客站点啦，好有成就感✌️分享一些本站使用的 NexT 主题配置技巧。&lt;br&gt;
    
    </summary>
    
      <category term="Configuration" scheme="http://yoursite.com/categories/Configuration/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
</feed>
