<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>徐阿衡</title>
  <subtitle>Shuang</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-06-09T09:29:38.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>徐阿衡</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>gensim-doc2vec实战</title>
    <link href="http://yoursite.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/06/01/gensim-doc2vec实战/</id>
    <published>2016-06-01T02:22:06.000Z</published>
    <updated>2016-06-09T09:29:38.000Z</updated>
    
    <content type="html">&lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
&lt;h3 id=&quot;导入模块&quot;&gt;&lt;a href=&quot;#导入模块&quot; class=&quot;headerlink&quot; title=&quot;导入模块&quot;&gt;&lt;/a&gt;导入模块&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
import os
import jieba

# logging information
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)
&lt;/pre&gt;

&lt;h3 id=&quot;读取文件&quot;&gt;&lt;a href=&quot;#读取文件&quot; class=&quot;headerlink&quot; title=&quot;读取文件&quot;&gt;&lt;/a&gt;读取文件&lt;/h3&gt;&lt;pre&gt;
# get input file, text format
f = open(&#39;trainingdata.txt&#39;,&#39;r&#39;)
input = f.readlines()
count = len(input)
print count
&lt;/pre&gt;

&lt;h3 id=&quot;文件预处理，分词等&quot;&gt;&lt;a href=&quot;#文件预处理，分词等&quot; class=&quot;headerlink&quot; title=&quot;文件预处理，分词等&quot;&gt;&lt;/a&gt;文件预处理，分词等&lt;/h3&gt;&lt;pre&gt;
# read file and separate words
alldocs=[] # for the sake of check, can be removed
count=0 # for the sake of check, can be removed
for line in input:
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count)) # for the sake of check, can be removed
    count+=1 # for the sake of check, can be removed
&lt;/pre&gt;


&lt;h3 id=&quot;模型选择&quot;&gt;&lt;a href=&quot;#模型选择&quot; class=&quot;headerlink&quot; title=&quot;模型选择&quot;&gt;&lt;/a&gt;模型选择&lt;/h3&gt;&lt;p&gt;gensim Doc2Vec 提供了 DM 和 DBOW 两个模型。gensim 的说明文档建议多次训练数据集并调整学习速率或在每次训练中打乱输入信息的顺序以求获得最佳效果。&lt;/p&gt;
&lt;pre&gt;
# PV-DM w/concatenation - window=5 (both sides) approximates paper&#39;s 10-word total window size
Doc2Vec(sentences,dm=1, dm_concat=1, size=100, window=2, hs=0, min_count=2, workers=cores)
# PV-DBOW  
Doc2Vec(sentences,dm=0, size=100, hs=0, min_count=2, workers=cores)
# PV-DM w/average
Doc2Vec(sentences,dm=1, dm_mean=1, size=100, window=2, hs=0, min_count=2, workers=cores)
&lt;/pre&gt;


&lt;h3 id=&quot;训练并保存模型&quot;&gt;&lt;a href=&quot;#训练并保存模型&quot; class=&quot;headerlink&quot; title=&quot;训练并保存模型&quot;&gt;&lt;/a&gt;训练并保存模型&lt;/h3&gt;&lt;pre&gt;
# train and save the model
sentences= gensim.models.doc2vec.TaggedLineDocument(&#39;output.seq&#39;)
model = gensim.models.Doc2Vec(sentences,size=100, window=3)
model.train(sentences)
model.save(&#39;all_model.txt&#39;)
&lt;/pre&gt;

&lt;h3 id=&quot;保存文档向量&quot;&gt;&lt;a href=&quot;#保存文档向量&quot; class=&quot;headerlink&quot; title=&quot;保存文档向量&quot;&gt;&lt;/a&gt;保存文档向量&lt;/h3&gt;&lt;pre&gt;
# save vectors
out=open(&quot;all_vector.txt&quot;,&quot;wb&quot;)
for num in range(0,count):
    docvec =model.docvecs[num]
    out.write(docvec)
    #print num
    #print docvec
out.close()
&lt;/pre&gt;

&lt;h3 id=&quot;检验-计算训练文档中的文档相似度&quot;&gt;&lt;a href=&quot;#检验-计算训练文档中的文档相似度&quot; class=&quot;headerlink&quot; title=&quot;检验 计算训练文档中的文档相似度&quot;&gt;&lt;/a&gt;检验 计算训练文档中的文档相似度&lt;/h3&gt;&lt;pre&gt;
# test, calculate the similarity
# 注意 docid 是从0开始计数的
# 计算与训练集中第一篇文档最相似的文档
sims = model.docvecs.most_similar(0)
print sims
# get similarity between doc1 and doc2 in the training data
sims = model.docvecs.similarity(1,2)
print sims
&lt;/pre&gt;

&lt;h3 id=&quot;infer向量，比较相似度&quot;&gt;&lt;a href=&quot;#infer向量，比较相似度&quot; class=&quot;headerlink&quot; title=&quot;infer向量，比较相似度&quot;&gt;&lt;/a&gt;infer向量，比较相似度&lt;/h3&gt;&lt;p&gt;下面的代码用于检验模型正确性，随机挑一篇trained dataset中的文档，用模型重新infer，再计算与trained dataset中文档相似度，如果模型良好，相似度第一位应该就是挑出的文档。&lt;/p&gt;
&lt;pre&gt;
# check
#############################################################################
# A good check is to re-infer a vector for a document already in the model. #
# if the model is well-trained,                                             #
# the nearest doc should (usually) be the same document.                    #
#############################################################################

print &#39;examing&#39;
doc_id = np.random.randint(model.docvecs.count)  # pick random doc; re-run cell for more examples
print(&#39;for doc %d...&#39; % doc_id)
inferred_docvec = model.infer_vector(alldocs[doc_id].words)
print(&#39;%s:\n %s&#39; % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))
&lt;/pre&gt;

&lt;h3 id=&quot;遇到的问题&quot;&gt;&lt;a href=&quot;#遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;遇到的问题&quot;&gt;&lt;/a&gt;遇到的问题&lt;/h3&gt;&lt;p&gt;👇两个错误还在探索中，根据官方指南是可以运行的，然而我遇到了错误并没能解决。&lt;br&gt;第一段错误代码，关于train the model&lt;/p&gt;
&lt;pre&gt;
alldocs=[]
count=0
for line in input:
    #print line
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    #output.write(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count))
    count+=1

model = Doc2Vec(alldocs,size=100, window=2, min_count=5, workers=4)
model.train(alldocs)
&lt;/pre&gt;

&lt;p&gt;报错信息&lt;/p&gt;
&lt;pre&gt;
Traceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 59, in &lt;module&gt;
    model = Doc2Vec(alldocs[0],size=100, window=2, min_count=5, workers=4)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 596, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/word2vec.py&quot;, line 508, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 639, in scan_vocab
    document_length = len(document.words)
AttributeError: &#39;generator&#39; object has no attribute &#39;words&#39;
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;第二段错误代码，关于infer&lt;/p&gt;
&lt;pre&gt;
doc_words1=[&#39;验证&#39;,&#39;失败&#39;,&#39;验证码&#39;,&#39;未&#39;,&#39;收到&#39;]
doc_words2=[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;]
# get infered vector
invec1 = model.infer_vector(doc_words1, alpha=0.1, min_alpha=0.0001, steps=5)
invec2 = model.infer_vector(doc_words2, alpha=0.1, min_alpha=0.0001, steps=5)
print invec1
print invec2

# get similarity
# the output docid is supposed to be 0
sims = model.docvecs.most_similar([invec1])
print sims

# according to official guide, the following codes are supposed to be fine, but it fails to run
sims= model.docvecs.similarity(invec1,invec2)
print model.similarity([&#39;今天&#39;,&#39;有&#39;,&#39;啥&#39;,&#39;奖励&#39;],[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;])
&lt;/pre&gt;

&lt;p&gt;最后两行代码报错，错误信息&lt;/p&gt;
&lt;pre&gt;
raceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 110, in &lt;module&gt;
    sims= model.docvecs.similarity(invec1,invec2)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 484, in similarity
    return dot(matutils.unitvec(self[d1]), matutils.unitvec(self[d2]))
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
TypeError: &#39;numpy.float32&#39; object is not iterable
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/doc2vec&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/models/doc2vec.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/models/doc2vec.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
&lt;h3 id=&quot;导入模块&quot;&gt;&lt;a href=&quot;#导
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="doc2vec" scheme="http://yoursite.com/tags/doc2vec/"/>
    
  </entry>
  
  <entry>
    <title>gensim: word2vec实战</title>
    <link href="http://yoursite.com/2016/05/30/gensim-word2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/30/gensim-word2vec实战/</id>
    <published>2016-05-30T03:13:52.000Z</published>
    <updated>2016-06-09T09:28:21.000Z</updated>
    
    <content type="html">&lt;pre&gt;

# -*- coding: utf-8 -*-
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import os
import logging
import jieba
import re
import multiprocessing
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# logging information
logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;)
logging.root.setLevel(level=logging.INFO)

# get input file, text format
inp = sys.argv[1]
input = open(inp, &#39;r&#39;)
output = open(&#39;output.seq&#39;, &#39;w&#39;)

if len(sys.argv) &lt; 2:
    print(globals()[&#39;__doc__&#39;] % locals())
    sys.exit(1)

# read file and separate words
for line in input.readlines():
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)

output.close()
output= open(&#39;output.seq&#39;, &#39;r&#39;)

# initialize the model
# size = the dimensionality of the feature vectors
# window = the maximum distance between the current and predicted word within a sentence
# min_count = ignore all words with total frequency lower than this.
model = Word2Vec(LineSentence(output), size=100, window=3, min_count=5,workers=multiprocessing.cpu_count())

# save model
model.save(&#39;output.model&#39;)
model.save_word2vec_format(&#39;output.vector&#39;, binary=False)

# test
model=gensim.models.Word2Vec.load(&#39;output.model&#39;)
x = model.most_similar([u&#39;奖励&#39;])
for i in x:
    print &quot;Word: {}\t Similarity: {}&quot;.format(i[0], i[1])
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/word2vec_example&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;pre&gt;

# -*- coding: utf-8 -*-
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.wor
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之六 -- 若干源码细节</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%85%AD-%E8%8B%A5%E5%B9%B2%E6%BA%90%E7%A0%81%E7%BB%86%E8%8A%82/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之六-若干源码细节/</id>
    <published>2016-05-29T08:28:23.000Z</published>
    <updated>2016-05-29T06:29:52.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;hr&gt;


&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/61.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/62.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/63.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/64.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/65.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/66.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/67.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/68.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之五 -- 基于 Negative Sampling 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%94-%E5%9F%BA%E4%BA%8E-Negative-Sampling-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之五-基于-Negative-Sampling-的模型/</id>
    <published>2016-05-29T07:22:03.000Z</published>
    <updated>2016-05-29T06:30:00.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/51.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/52.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/53.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/54.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/55.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/56.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/57.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/58.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/59.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/510.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/511.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之四 -- 基于Hierarchical Softmax 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8EHierarchical-Softmax-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之四-基于Hierarchical-Softmax-的模型/</id>
    <published>2016-05-29T06:08:03.000Z</published>
    <updated>2016-05-29T06:38:06.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/8.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/10.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/1.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之三 -- 背景知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%89-%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之三-背景知识/</id>
    <published>2016-05-29T06:04:41.000Z</published>
    <updated>2016-05-29T06:38:08.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/31.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/32.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/33.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/34.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/35.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/36.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/37.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/38.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/39.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/310.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/311.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之二 -- 预备知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之二-预备知识/</id>
    <published>2016-05-29T04:21:21.000Z</published>
    <updated>2016-05-29T06:08:19.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/21.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/22.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/23.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/24.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/25.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/26.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/27.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之一 -- 目录和前言</title>
    <link href="http://yoursite.com/2016/05/28/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%80-%E7%9B%AE%E5%BD%95%E5%92%8C%E5%89%8D%E8%A8%80/"/>
    <id>http://yoursite.com/2016/05/28/word2vec详解之一-目录和前言/</id>
    <published>2016-05-28T06:33:10.000Z</published>
    <updated>2016-05-29T06:34:41.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/13.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/14.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/15.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>PHP连接数据库js可视化数据</title>
    <link href="http://yoursite.com/2016/05/26/PHP%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93js%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2016/05/26/PHP连接数据库js可视化数据/</id>
    <published>2016-05-26T02:09:56.000Z</published>
    <updated>2016-05-26T02:11:07.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>短句归一化--LSI模型</title>
    <link href="http://yoursite.com/2016/05/25/%E7%9F%AD%E9%97%AE%E9%A2%98%E5%BD%92%E4%B8%80%E5%8C%96-LSI%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/25/短问题归一化-LSI模型/</id>
    <published>2016-05-25T12:59:36.000Z</published>
    <updated>2016-05-25T13:57:30.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;潜在语义索引是一种用奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以可以从一篇文章中提取术语关系，从而建立起主要概念内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;降维过程&quot;&gt;&lt;a href=&quot;#降维过程&quot; class=&quot;headerlink&quot; title=&quot;降维过程&quot;&gt;&lt;/a&gt;降维过程&lt;/h3&gt;&lt;p&gt;将文档库表示成VSM模型的词-文档矩阵Am×n(词-文档矩阵那就是词作为行，文档作为列，这是矩阵先行后列的表示决定的，当然如果表示成文档-词矩阵的话，后面的计算就要用该矩阵的转置了),其中m表示文档库中包含的所有不同的词的个(行数是不同词的个数)，即行向量表示一个词在不同文档出现的次数，n 表示文档库中的文档数(列数是不同文档的个数)，即列向量表示的是不同的文档.A表示为A = [α ij ],在此矩阵中 ,α ij为非负值 , 表示第 i 个词在第j 个文档中出现的频度。显然，A是稀疏矩阵(这是VSM和文档决定的)。&lt;/p&gt;
&lt;p&gt;利用奇异值分解SVD(Singular Value Decomposition)求A的只有K个正交因子的降秩矩阵，该过程就是降维的过程。SVD的重要作用是把词和文档映射到同一个语义空间中，将词和文档表示为K个因子的形式。显然，这会丢失信息，但主要的信息却被保留了。为什么该过程可以降维呢？因为该过程解决了同义和多义现象。可以看出，K的取值对整个分类结果的影响很大。因为，K过小，则丢失信息就越多；K过大，信息虽然多，但可能有冗余且计算消耗大。K的选择也是值得研究的，不过一般取值为100-300，不绝对。&lt;/p&gt;
&lt;h3 id=&quot;适用性&quot;&gt;&lt;a href=&quot;#适用性&quot; class=&quot;headerlink&quot; title=&quot;适用性&quot;&gt;&lt;/a&gt;适用性&lt;/h3&gt;&lt;p&gt;对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;特征降维&lt;br&gt;LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。TFIDF是另一个通用的降维方法，通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF。&lt;br&gt;不过必须提醒的是，无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;单词相关度计算&lt;br&gt;LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;聚类&lt;br&gt;直接用 LSI 聚类的情景还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;实验说明&quot;&gt;&lt;a href=&quot;#实验说明&quot; class=&quot;headerlink&quot; title=&quot;实验说明&quot;&gt;&lt;/a&gt;实验说明&lt;/h3&gt;&lt;p&gt;用了python的gensim包&lt;br&gt;现有的数据是438条标准问题以及3300条人工问题（可以转化为438条标准问题），现在需要对人工问题做一个归一化。&lt;br&gt;这里采用LSI模型进行建模实验，步骤如下。&lt;/p&gt;
&lt;h3 id=&quot;导入包&quot;&gt;&lt;a href=&quot;#导入包&quot; class=&quot;headerlink&quot; title=&quot;导入包&quot;&gt;&lt;/a&gt;导入包&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import logging
import jieba
import jieba.posseg as pseg
# 防止乱码
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)
# 打印log信息
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)&lt;/pre&gt;


&lt;h3 id=&quot;文本预处理&quot;&gt;&lt;a href=&quot;#文本预处理&quot; class=&quot;headerlink&quot; title=&quot;文本预处理&quot;&gt;&lt;/a&gt;文本预处理&lt;/h3&gt;&lt;pre&gt;
# 标准FAQ，一行对应一条问句
f = open(&#39;FAQuniq.txt&#39;, &#39;r&#39;)
# 对问句进行分词
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in f]

# 抽取一个bag-of-words，将文档的token映射为id
dictionary = corpora.Dictionary(texts)
# 保存词典
dictionary.save(&#39;LSI.dict&#39;)

# 产生文档向量，将用字符串表示的文档转换为用id和词频表示的文档向量
corpus = [dictionary.doc2bow(text) for text in texts]

# 基于这些“训练文档”计算一个TF-IDF模型
tfidf = models.TfidfModel(corpus)

# 转化文档向量，将用词频表示的文档向量表示为一个用tf-idf值表示的文档向量
corpus_tfidf = tfidf[corpus]

# 训练LSI模型 即将训练文档向量组成的矩阵SVD分解，并做一个秩为2的近似SVD分解
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)

# 保存模型
lsi.save(&#39;LSI.pkl&#39;)
lsi.print_topics(20)&lt;/pre&gt;


&lt;h3 id=&quot;初始化验证performance的文件&quot;&gt;&lt;a href=&quot;#初始化验证performance的文件&quot; class=&quot;headerlink&quot; title=&quot;初始化验证performance的文件&quot;&gt;&lt;/a&gt;初始化验证performance的文件&lt;/h3&gt;&lt;p&gt;checkFile的每行格式为：&lt;/p&gt;
&lt;pre&gt;原始问题的docid：对应的标准问题的topicid&lt;/pre&gt;

&lt;p&gt;把它存到checkDict这个dictionary中，key是docid，value是topicid。&lt;/p&gt;
&lt;pre&gt;
checkDict=dict()
def getCheckId():
    fcheck=open(&#39;checkFile.txt&#39;)
    for line in fcheck:
        line=line.strip(&#39;\n&#39;)
        if (len(line)==0):
            continue
        docid=line.split(&quot;:&quot;)[0]
        topicid=line.split(&quot;:&quot;)[1]
        checkDict[int(docid)]=int(topicid)
getCheckId()&lt;/pre&gt;

&lt;h3 id=&quot;归一化／计算文档相似度&quot;&gt;&lt;a href=&quot;#归一化／计算文档相似度&quot; class=&quot;headerlink&quot; title=&quot;归一化／计算文档相似度&quot;&gt;&lt;/a&gt;归一化／计算文档相似度&lt;/h3&gt;&lt;pre&gt;
# 建索引
index = similarities.MatrixSimilarity(lsi[corpus])

# 初始化分数
score1=0
score2=0
score3=0

# 读取文件，文件的每行格式为一个原始问句
f2=open(&#39;ORIFAQ3330.txt&#39;,&#39;r&#39;)
# count的作用是和checkFile的docid，即checkDict的key对应
count=1
for query in f2:
    # 获取该原始问句本应对应的正确标准问句
    if (not checkDict.has_key(count)):
        count+=1
        continue
    checkId=checkDict[count]
    # 将问句向量化
    query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
    # 再用之前训练好的LSI模型将其映射到二维的topic空间：
    query_lsi = lsi[query_bow]
    # 计算其和index中doc的余弦相似度了：
    sims = index[query_lsi]
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    # 找出最相关的三篇文档，计算这三篇文档是否包括标准问句，如果文档就是标准问句，对应的分数加1
    if (checkId==sort_sims[0][0]):
        score1+=1
    elif (checkId==sort_sims[1][0]):
        score2+=1
    elif (checkId==sort_sims[2][0]):
        score3+=1
    count+=1&lt;/pre&gt;

&lt;h3 id=&quot;打印分数&quot;&gt;&lt;a href=&quot;#打印分数&quot; class=&quot;headerlink&quot; title=&quot;打印分数&quot;&gt;&lt;/a&gt;打印分数&lt;/h3&gt;&lt;pre&gt;
print &quot;Score1: &quot;.format(score1*1.0/count)
print &quot;Score2: &quot;.format(score2*1.0/count)
print &quot;Score3: &quot;.format(score3*1.0/count)&lt;/pre&gt;

&lt;h3 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h3&gt;&lt;p&gt;其实这里的结果非常差，原因是文档（每一条问句）太短，只有十几个字，另外文档数太少，LSI降维牺牲了准确率，下一个实验LDA的准确率相比会高很多。&lt;br&gt;另外，本次实验所用的样本分布并不均匀，“未收到奖励”类似问题出现的频率比“软件无声音”类似问题出现的频率要高很多。&lt;strong&gt;&lt;em&gt;重申：LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;LSI-缺陷&quot;&gt;&lt;a href=&quot;#LSI-缺陷&quot; class=&quot;headerlink&quot; title=&quot;LSI 缺陷&quot;&gt;&lt;/a&gt;LSI 缺陷&lt;/h3&gt;&lt;p&gt;常用的VSM文本表示模型中有两个主要的缺陷：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;该模型假设所有特征词条之间是相互独立、互不影响的（朴素贝叶斯也是这个思想），即该模型还是基于“词袋”模型（应该说所有利用VSM模型没有进行潜在语义分析的算法都是基于“词袋”假设）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;没有进行特征降维，特征维数可能会很高，向量空间可能很大，对存储和计算资源要求会比较高。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;LSI的基本思想是文本中的词与词之间不是孤立的，存在着某种潜在的语义关系，通过对样本数据的统计分析，让机器自动挖掘出这些潜在的语义关系，并把这些关系表示成计算机可以”理解”的模型。它可以消除词匹配过程中的同义和多义现象。它可以将传统的VSM降秩到一个低维的语义空间中，在该语义空间中计算文档的相似度等。总的说来，LSI就是利用词的语义关系对VSM模型进行降维，并提高分类的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>GibbsLDA++: A C/C++ 使用心得</title>
    <link href="http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97/"/>
    <id>http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-使用心得/</id>
    <published>2016-05-25T04:20:35.000Z</published>
    <updated>2016-05-25T04:20:35.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>在c里调用python</title>
    <link href="http://yoursite.com/2016/05/22/%E5%9C%A8c%E9%87%8C%E8%B0%83%E7%94%A8python/"/>
    <id>http://yoursite.com/2016/05/22/在c里调用python/</id>
    <published>2016-05-22T08:55:07.000Z</published>
    <updated>2016-06-09T09:09:38.000Z</updated>
    
    <content type="html">&lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
&lt;pre&gt;
#include &lt;python2.7 python.h=&quot;&quot;&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
void test1(){
  Py_Initialize();//初始化python
  char *test = &quot;奖励&quot;;
  PyObject * pModule = NULL;
  PyObject * pModule1 = NULL;
  PyObject * pFunc = NULL;
  PyObject * pArg    = NULL;
  PyObject * result;
  pModule = PyImport_ImportModule(&quot;inferSingleDocVec&quot;);//引入模块
  pFunc = PyObject_GetAttrString(pModule, &quot;getDocVec&quot;);//直接获取模块中的函数
  pArg= Py_BuildValue(&quot;(s)&quot;, test);
  result = PyEval_CallObject(pFunc, pArg); //调用直接获得的函数，并传递参数；这里得到的是一个list
  &lt;code&gt;for (int i = 0; i &lt; PyList_Size(result); i++) {&lt;/code&gt;
    printf(&quot;%f\t&quot;, PyFloat_AsDouble(PyList_GetItem(result, (Py_ssize_t)i)));//打印每一个元素
  }
  //下面代码适用于返回值为字符串的情况
  //char* s=NULL;
  //PyArg_Parse(result, &quot;s&quot;, &amp;s);
  //for (int i=0;s[i]!=&#39;\0&#39;;i++){
   // printf(&quot;%c&quot;,s[i]);
 // }
  Py_Finalize(); //释放python
//  return;
}
int main(int argc, char* argv[])
{
    test1();
    return 0;
}
&lt;/stdlib.h&gt;&lt;/stdio.h&gt;&lt;/python2.7&gt;&lt;/pre&gt;

&lt;p&gt;编译运行&lt;/p&gt;
&lt;pre&gt;
$ gcc -I/usr/local/lib/python2.7.11 -o inferDocVec inferDocVec.c -lpython2.7
$ ./inferDocVec
&lt;/pre&gt;

&lt;p&gt;调用的inferSingleDocVec文件&lt;/p&gt;
&lt;pre&gt;
#!/usr/bin/python
# -*- coding: utf-8 -*-
### for infer
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
from gensim.models import Doc2Vec
import os
import jieba
import multiprocessing
import numpy as np
import base64
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

def getDocVec(doc_words):
    docwords=[word for word in jieba.cut(doc_words, cut_all = False)]
    model = Doc2Vec.load(&#39;all_model_v2.txt&#39;)
    invec = model.infer_vector(docwords, alpha=0.1, min_alpha=0.0001, steps=5)
    return (list)(invec)
&lt;/pre&gt;

&lt;p&gt;关于如何将python文件转为模块，详见之前的一篇博文&lt;a href=&quot;https://github.com/Shuang0420/Shuang0420.github.io/wiki/python----%E5%B0%86%E8%87%AA%E5%B7%B1%E5%86%99%E7%9A%84py%E6%96%87%E4%BB%B6%E4%BD%9C%E4%B8%BA%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python 将自己写的py文件作为模块导入&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
&lt;pre&gt;
#include &lt;python2.7 python.h=&quot;&quot;&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
void test
    
    </summary>
    
      <category term="Programming language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="c" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>AP聚类</title>
    <link href="http://yoursite.com/2016/05/19/AP%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2016/05/19/AP聚类/</id>
    <published>2016-05-19T13:42:04.000Z</published>
    <updated>2016-05-19T13:46:58.000Z</updated>
    
    <content type="html">&lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。&lt;/p&gt;
&lt;p&gt;Affinity Propagation (AP) 聚类是最近在Science杂志上提出的一种新的聚类算法。它根据N个数据点之间的相似度进行聚类,这些相似度可以是对称的,即两个数据点互相之间的相似度一样(如欧氏距离);也可以是不对称的,即两个数据点互相之间的相似度不等。这些相似度组成N×N的相似度矩阵S(其中N为有N个数据点)。AP算法不需要事先指定聚类数目,相反它将所有的数据点都作为潜在的聚类中心,称之为exemplar。以S矩阵的对角线上的数值s(k, k)作为k点能否成为聚类中心的评判标准,这意味着该值越大,这个点成为聚类中心的可能性也就越大,这个值又称作参考度p (preference)。&lt;br&gt;在这里介绍几个文中常出现的名词：&lt;br&gt;exemplar：指的是聚类中心。&lt;br&gt;similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，如－|| ||。文中，所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。&lt;br&gt;preference：数据点i的参考度称为P(i)或S(i,i)。是指点i作为聚类中心的参考度。一般取S相似度值的中值。&lt;br&gt;Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度。&lt;br&gt;Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度。&lt;/p&gt;
&lt;p&gt;Script output:&lt;br&gt;Estimated number of clusters: 3&lt;br&gt;Homogeneity: 0.872&lt;br&gt;Completeness: 0.872&lt;br&gt;V-measure: 0.872&lt;br&gt;Adjusted Rand Index: 0.912&lt;br&gt;Adjusted Mutual Information: 0.871&lt;br&gt;Silhouette Coefficient: 0.753&lt;/p&gt;
&lt;p&gt;Python source code: plot_affinity_propagation.py&lt;br&gt;print(&lt;strong&gt;doc&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;from sklearn.cluster import AffinityPropagation&lt;br&gt;from sklearn import metrics&lt;br&gt;from sklearn.datasets.samples_generator import make_blobs&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Generate-sample-data&quot;&gt;&lt;a href=&quot;#Generate-sample-data&quot; class=&quot;headerlink&quot; title=&quot;Generate sample data&quot;&gt;&lt;/a&gt;Generate sample data&lt;/h1&gt;&lt;p&gt;centers = [[1, 1], [-1, -1], [1, -1]]&lt;br&gt;X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,&lt;br&gt;                            random_state=0)&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Compute-Affinity-Propagation&quot;&gt;&lt;a href=&quot;#Compute-Affinity-Propagation&quot; class=&quot;headerlink&quot; title=&quot;Compute Affinity Propagation&quot;&gt;&lt;/a&gt;Compute Affinity Propagation&lt;/h1&gt;&lt;p&gt;af = AffinityPropagation(preference=-50).fit(X)&lt;br&gt;cluster_centers_indices = af.cluster_centers&lt;em&gt;indices&lt;/em&gt;&lt;br&gt;labels = af.labels_&lt;/p&gt;
&lt;p&gt;n&lt;em&gt;clusters&lt;/em&gt; = len(cluster_centers_indices)&lt;/p&gt;
&lt;p&gt;print(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))&lt;br&gt;print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))&lt;br&gt;print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))&lt;br&gt;print(“Adjusted Rand Index: %0.3f”&lt;br&gt;      % metrics.adjusted_rand_score(labels_true, labels))&lt;br&gt;print(“Adjusted Mutual Information: %0.3f”&lt;br&gt;      % metrics.adjusted_mutual_info_score(labels_true, labels))&lt;br&gt;print(“Silhouette Coefficient: %0.3f”&lt;br&gt;      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Plot-result&quot;&gt;&lt;a href=&quot;#Plot-result&quot; class=&quot;headerlink&quot; title=&quot;Plot result&quot;&gt;&lt;/a&gt;Plot result&lt;/h1&gt;&lt;p&gt;import matplotlib.pyplot as plt&lt;br&gt;from itertools import cycle&lt;/p&gt;
&lt;p&gt;plt.close(‘all’)&lt;br&gt;plt.figure(1)&lt;br&gt;plt.clf()&lt;/p&gt;
&lt;p&gt;colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)&lt;br&gt;for k, col in zip(range(n&lt;em&gt;clusters&lt;/em&gt;), colors):&lt;br&gt;    class_members = labels == k&lt;br&gt;    cluster_center = X[cluster_centers_indices[k]]&lt;br&gt;    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)&lt;br&gt;    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,&lt;br&gt;             markeredgecolor=’k’, markersize=14)&lt;br&gt;    for x in X[class_members]:&lt;br&gt;        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)&lt;/p&gt;
&lt;p&gt;plt.title(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;plt.show()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://scikit-learn.org/stable/modules/clustering.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://blog.csdn.net/u010695420/article/details/42239465&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/u010695420/article/details/42239465&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LDA验证</title>
    <link href="http://yoursite.com/2016/05/19/LDA%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2016/05/19/LDA验证/</id>
    <published>2016-05-19T13:14:42.000Z</published>
    <updated>2016-05-19T13:14:42.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Gensim and LDA--Training and Prediction</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/</id>
    <published>2016-05-18T12:46:16.000Z</published>
    <updated>2016-06-09T09:01:10.000Z</updated>
    
    <content type="html">&lt;pre&gt;
# install the related python packages
&gt;&gt;&gt; pip install numpy
&gt;&gt;&gt; pip install scipy
&gt;&gt;&gt; pip install gensim
&gt;&gt;&gt; pip install jieba

from gensim import corpora, models, similarities
import logging
import jieba

# configuration
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# load data from file
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()

＃ tokenize
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]

# load id-&gt;word mapping (the dictionary)
dictionary = corpora.Dictionary(texts)

# word must appear &gt;10 times, and no more than 40% documents
dictionary.filter_extremes(no_below=40, no_above=0.1)

# save dictionary
dictionary.save(&#39;dict_v1.dict&#39;)

# load corpus
corpus = [dictionary.doc2bow(text) for text in texts]

# initialize a model
tfidf = models.TfidfModel(corpus)

# use the model to transform vectors, apply a transformation to a whole corpus
corpus_tfidf = tfidf[corpus]

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)

# save model to files
lda.save(&#39;mylda_v1.pkl&#39;)

# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.
for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, lda.print_topic(index, 10))

# print the most contributing words for 100 randomly selected topics
lda.print_topics(100)

# load model and dictionary
model = models.LdaModel.load(&#39;mylda_v1.pkl&#39;)
dictionary = corpora.Dictionary.load(&#39;dict_v1.dict&#39;)

# predict unseen data
query = &quot;未收到奖励&quot;
query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
for index, score in sorted(model[query_bow], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))

# if you want to predict many lines of data in a file, do the followings
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]
corpus = [dictionary.doc2bow(text) for text in texts]

# only print the topic with the highest score
for c in corpus:
    flag = True
    for index, score in sorted(model[c], key=lambda tup: -1*tup[1]):
        if flag:
            print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))&lt;/pre&gt;

&lt;h1 id=&quot;Tips&quot;&gt;&lt;a href=&quot;#Tips&quot; class=&quot;headerlink&quot; title=&quot;Tips:&quot;&gt;&lt;/a&gt;Tips:&lt;/h1&gt;&lt;p&gt;If you occur encoding problems, you can try the following code&lt;/p&gt;
&lt;pre&gt;
add it at the beginning of your python file
# -*- coding: utf-8 -*-

# also, do the followings
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# the following code may lead to encoding problem when there&#39;re Chinese characters
model.show_topics(-1, 5)

# use this instead
model.print_topics(-1, 5)&lt;/pre&gt;


&lt;p&gt;You can see step-by-step output by the following references.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/tut2.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/tut2.html&lt;/a&gt; official guide (en)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/questionfish/article/details/46725475&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/questionfish/article/details/46725475&lt;/a&gt;  official guide (ch)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;pre&gt;
# install the related python packages
&gt;&gt;&gt; pip install numpy
&gt;&gt;&gt; pip install scipy
&gt;&gt;&gt; pip install gensim
&gt;&gt;&gt; pip install jieba

from g
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>Gensim-用Python做主题模型</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-%E7%94%A8Python%E5%81%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-用Python做主题模型/</id>
    <published>2016-05-18T02:22:31.000Z</published>
    <updated>2016-06-09T09:00:52.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。&lt;/p&gt;
&lt;h3 id=&quot;gensim-的特性&quot;&gt;&lt;a href=&quot;#gensim-的特性&quot; class=&quot;headerlink&quot; title=&quot;gensim 的特性&quot;&gt;&lt;/a&gt;gensim 的特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;内存独立- 对于训练语料来说，没必要在任何时间将整个语料都驻留在RAM中&lt;/li&gt;
&lt;li&gt;有效实现了许多流行的向量空间算法－包括tf-idf，分布式LSA, 分布式LDA 以及 RP；并且很容易添加新算法&lt;/li&gt;
&lt;li&gt;对流行的数据格式进行了IO封装和转换&lt;/li&gt;
&lt;li&gt;在其语义表达中，可以相似查询&lt;/li&gt;
&lt;li&gt;gensim的创建的目的是，由于缺乏简单的（java很复杂）实现主题建模的可扩展软件框架.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-的设计原则&quot;&gt;&lt;a href=&quot;#gensim-的设计原则&quot; class=&quot;headerlink&quot; title=&quot;gensim 的设计原则&quot;&gt;&lt;/a&gt;gensim 的设计原则&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;简单的接口，学习曲线低。对于原型实现很方便&lt;/li&gt;
&lt;li&gt;根据输入的语料的size来说，内存各自独立；基于流的算法操作，一次访问一个文档.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-的核心概念&quot;&gt;&lt;a href=&quot;#gensim-的核心概念&quot; class=&quot;headerlink&quot; title=&quot;gensim 的核心概念&quot;&gt;&lt;/a&gt;gensim 的核心概念&lt;/h3&gt;&lt;p&gt;gensim的整个package会涉及三个概念：corpus, vector, model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语库(corpus)&lt;br&gt;文档集合，用于自动推出文档结构，以及它们的主题等，也可称作训练语料。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;向量(vector)&lt;/p&gt;
&lt;p&gt;在向量空间模型(VSM)中，每个文档被表示成一个特征数组。例如，一个单一特征可以被表示成一个问答对(question-answer pair):&lt;/p&gt;
&lt;p&gt;[1].在文档中单词”splonge”出现的次数？ 0个&lt;br&gt;[2].文档中包含了多少句子？ 2个&lt;br&gt;[3].文档中使用了多少字体? 5种&lt;br&gt;这里的问题可以表示成整型id (比如：1,2,3等), 因此，上面的文档可以表示成：(1, 0.0), (2, 2.0), (3, 5.0). 如果我们事先知道所有的问题，我们可以显式地写成这样：(0.0, 2.0, 5.0). 这个answer序列可以认为是一个多维矩阵（3维）. 对于实际目的，只有question对应的answer是一个实数.&lt;/p&gt;
&lt;p&gt;对于每个文档来说，answer是类似的. 因而，对于两个向量来说（分别表示两个文档），我们希望可以下类似的结论：“如果两个向量中的实数是相似的，那么，原始的文档也可以认为是相似的”。当然，这样的结论依赖于我们如何去选取我们的question。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;稀疏矩阵(Sparse vector)&lt;/p&gt;
&lt;p&gt;通常，大多数answer的值都是0.0. 为了节省空间，我们需要从文档表示中忽略它们，只需要写：(2, 2.0), (3, 5.0) 即可(注意：这里忽略了(1, 0.0)). 由于所有的问题集事先都知道，那么在稀疏矩阵的文档表示中所有缺失的特性可以认为都是0.0.&lt;/p&gt;
&lt;p&gt;gensim的特别之处在于，它没有限定任何特定的语料格式；语料可以是任何格式，当迭代时，通过稀疏矩阵来完成即可。例如，集合 ([(2, 2.0), (3, 5.0)], ([0, -1.0], [3, -1.0])) 是一个包含两个文档的语料，每个都有两个非零的 pair。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;模型(model)&lt;/p&gt;
&lt;p&gt;对于我们来说，一个模型就是一个变换(transformation)，将一种文档表示转换成另一种。初始和目标表示都是向量－－它们只在question和answer之间有区别。这个变换可以通过训练的语料进行自动学习，无需人工监督，最终的文档表示将更加紧凑和有用；相似的文档具有相似的表示。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;演示代码&quot;&gt;&lt;a href=&quot;#演示代码&quot; class=&quot;headerlink&quot; title=&quot;演示代码&quot;&gt;&lt;/a&gt;演示代码&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;演示代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://d0evi1.github.io/gensim/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://d0evi1.github.io/gensim/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensi
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>JGibbLDA实战</title>
    <link href="http://yoursite.com/2016/05/16/JGibbLDA%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/16/JGibbLDA实战/</id>
    <published>2016-05-16T12:50:23.000Z</published>
    <updated>2016-05-19T02:33:53.000Z</updated>
    
    <content type="html">&lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;br&gt;命令：&lt;/p&gt;
&lt;pre&gt;java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models/casestudy/newdocs.dat&lt;/pre&gt;

&lt;p&gt;错误信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/err.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;于是尝试导入eclipse运行手动配置，成功，过程如下。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载JGibbLDA的jar包并解压；&lt;br&gt;网址：&lt;a href=&quot;http://jgibblda.sourceforge.net/#Griffiths04&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/#Griffiths04&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;导入eclipse，确保jar包在目录中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;找到LDACmdOption.java文件， 修改部分代码&lt;/p&gt;
&lt;pre&gt; @Option(name=&quot;-dir&quot;, usage=&quot;Specify directory&quot;)
 public String dir = &quot;models/casestudy-en&quot;;

 @Option(name=&quot;-dfile&quot;, usage=&quot;Specify data file&quot;)
 public String dfile = &quot;models/casestudy-en/newdocs.dat&quot;;&lt;/pre&gt;

&lt;p&gt;值得注意的是，dfile的格式必须是👇这个样子：&lt;/p&gt;
&lt;pre&gt;[M]
[document1]
[document2]
...
[documentM]&lt;/pre&gt;

&lt;p&gt;第一行[M]是documents的总数，之后的每一行是一个document，每个document是一个word list，或者说是bag of words。&lt;/p&gt;
&lt;pre&gt;[document i] = [word i1] [word i2] ... [word iNi]&lt;/pre&gt;

&lt;p&gt;各参数含义：&lt;br&gt;&lt;strong&gt;-est &lt;/strong&gt;从训练语料中评估出LDA模型&lt;br&gt;&lt;strong&gt;-alpha&lt;/strong&gt; LDA模型中的alpha数值，默认为50/K(K是主题数目)&lt;br&gt;&lt;strong&gt;-beta&lt;/strong&gt; LDA模型中的beta数值，默认是0.1&lt;br&gt;&lt;strong&gt;-ntopics&lt;/strong&gt; 主题数目，默认值是100&lt;br&gt;&lt;strong&gt;-niters&lt;/strong&gt; GIbbs采样的迭代数目，默认值为2000&lt;br&gt;&lt;strong&gt;-savestep&lt;/strong&gt; 指定开始保存LDA模型的迭代次数&lt;br&gt;&lt;strong&gt;-dir&lt;/strong&gt; 训练语料目录&lt;br&gt;&lt;strong&gt;-dfile&lt;/strong&gt; 训练语料文件名称&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;修改项目的Run Configurations，在Java Application中选择LDA，点击(x)=Arguments，输入&lt;/p&gt;
&lt;pre&gt;-est -alpha 0.2 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 100 -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;newdocs.dat&quot;&lt;/pre&gt;

&lt;p&gt; 若利用已训练的LDA模型预测，输入以下参数：&lt;/p&gt;
&lt;pre&gt;-inf -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;test.txt&quot;&lt;/pre&gt;

&lt;p&gt; 注意，进行预测时，当前目录下必须包含已有的LDA训练输出文件，包括model-final.others、model-final.phi、model-final.tassign、model-final.theta、model-final.twords、wordmap.txt文件，如果运行报错，尝试修改LDACmdOption.java的modelName，确保和文件名的modelname部分一致。&lt;br&gt;&lt;pre&gt;@Option(name=”-model”, usage=”Specify the model name”)&lt;br&gt; public String modelName = “model-final”;&lt;/pre&gt;   &lt;/p&gt;
&lt;p&gt; 如果出现java heap limited的问题，在VM arguments下添加&lt;/p&gt;
&lt;pre&gt;-Xms1g -Xmx1g -Xmn512m&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run&lt;br&gt;输出文件主要有：&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.others&lt;/model_name&gt;&lt;/strong&gt;  文件存储LDA模型参数，如alpha、beta等。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.phi &lt;/model_name&gt;&lt;/strong&gt; 每个topic内对doc的分布情况。文件存储词语-主题分布，每一行是一个主题，列内容为词语。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.theta &lt;/model_name&gt;&lt;/strong&gt; 每个doc内对应上面的n个topic的分布情况。文件主题文档分布，每一行是一个文档，列内容是主题概率。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.tassign&lt;/model_name&gt;&lt;/strong&gt;  文件是训练预料中单词的主题指定（归属），每一行是一个语料文档。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.twords&lt;/model_name&gt;&lt;/strong&gt;  n个topic，以及每个topic下面包含的具体的字词&lt;br&gt;&lt;strong&gt;wordmap.txt&lt;/strong&gt;  词-id映射&lt;br&gt;其中&lt;model_name&gt;根据采样迭代次数来指定，如model-00800，最后一次采样名称命名为model-final。&lt;/model_name&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;br&gt;&lt;a href=&quot;http://www.ithao123.cn/content-4208214.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.ithao123.cn/content-4208214.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://jgibblda.sourceforge.net/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;br&gt;命令：&lt;/p&gt;
&lt;pre&gt;java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>gollum/-github上搭建个人wiki</title>
    <link href="http://yoursite.com/2016/05/13/gollum:-github%E4%B8%8A%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAwiki/"/>
    <id>http://yoursite.com/2016/05/13/gollum:-github上搭建个人wiki/</id>
    <published>2016-05-13T08:55:07.000Z</published>
    <updated>2016-06-09T09:12:32.000Z</updated>
    
    <content type="html">&lt;p&gt;博客凸显创作，维基则是整理的好工具，很多入门级别、复用别人的操作，如配置环境等，更适合发布在个人维基上，本文就以gollum+github搭建个人wiki做个示范。&lt;/p&gt;
&lt;h3 id=&quot;开通Wiki&quot;&gt;&lt;a href=&quot;#开通Wiki&quot; class=&quot;headerlink&quot; title=&quot;开通Wiki&quot;&gt;&lt;/a&gt;开通Wiki&lt;/h3&gt;&lt;p&gt;登陆Github，找到你所开通的Github项目的Settings栏目，开通Wikis，如果只希望别人可读不可写，勾选：Restrict edits to Collaborators only。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%884.56.12.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;git-clone下相应维基的git地址&quot;&gt;&lt;a href=&quot;#git-clone下相应维基的git地址&quot; class=&quot;headerlink&quot; title=&quot;git clone下相应维基的git地址&quot;&gt;&lt;/a&gt;git clone下相应维基的git地址&lt;/h3&gt;&lt;pre&gt; git clone git@github.com:Shuang0420/Shuang0420.github.io.wiki.git wiki&lt;/pre&gt;

&lt;p&gt;如果你之前没有设置git密钥，可以参照以下步骤先做配置，如果已经设置，请忽略。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看是否已经有了ssh密钥：&lt;br&gt;&lt;pre&gt;cd ~/.ssh&lt;/pre&gt;&lt;br&gt;如果没有密钥则不会有此文件夹，有则备份删除&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;生成密钥，得到两个文件：id_rsa 和 id_rsa.pub&lt;/p&gt;
&lt;pre&gt;ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;添加密钥到ssh：ssh-add id_rsa&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在github上settings中添加ssh密钥，即“id_rsa.pub”里的公钥。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;测试：ssh git@github.com&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;配置个人wiki&quot;&gt;&lt;a href=&quot;#配置个人wiki&quot; class=&quot;headerlink&quot; title=&quot;配置个人wiki&quot;&gt;&lt;/a&gt;配置个人wiki&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在wiki目录下，安装bundler&lt;/p&gt;
&lt;pre&gt;gem install bundler&lt;/pre&gt;

&lt;p&gt;   如果安装没有问题，可以跳过以下错误解决。&lt;/p&gt;
   &lt;pre&gt;
   ERROR:  Could not find a valid gem &#39;bundler&#39; (&gt;= 0), here is why:
             Unable to download data from https://rubygems.org/ - Errno::EPIPE: Broken pipe - SSL_connect (https://rubygems.org/latest_specs.4.8.gz)&lt;/pre&gt;

&lt;p&gt;   解决：&lt;/p&gt;
   &lt;pre&gt;gem source -a http://rubygems.org/
   gem install bundler&lt;/pre&gt;

&lt;p&gt;   然而还是有错误：&lt;/p&gt;
   &lt;pre&gt;Fetching: bundler-1.12.5.gem (100%)^[[A
   ERROR:  While executing gem ... (Gem::FilePermissionError)
       You don&#39;t have write permissions for the /Library/Ruby/Gems/2.0.0 directory.&lt;/pre&gt;

&lt;p&gt;   因为没有sudo：&lt;/p&gt;
   &lt;pre&gt;sudo gem install bundler&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;新建Gemfile文件，内容如下：&lt;/p&gt;
   &lt;pre&gt;
   source &quot;http://rubygems.org&quot;
   gem &#39;redcarpet&#39;
   gem &quot;grit&quot;, &#39;~&gt; 2.5.0&#39;, git: &#39;https://github.com/gitlabhq/grit.git&#39;, ref: &#39;42297cdcee16284d2e4eff23d41377f52fc28b9d&#39;
   gem &#39;gollum&#39;, git: &#39;https://github.com/gollum/gollum.git&#39;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;运行：&lt;/p&gt;
   &lt;pre&gt;
   # 安装项目依赖的所有gem包;此命令会尝试更新系统中已存在的gem包
   bundle install&lt;/pre&gt;

&lt;p&gt;   时间有点久，耐心等待。&lt;br&gt;   然而最后出现error,&lt;/p&gt;
   &lt;pre&gt;An error occurred while installing charlock_holmes (0.7.3), and Bundler cannot continue.
   Make sure that `gem install charlock_holmes -v &#39;0.7.3&#39;` succeeds before bundling.&lt;/pre&gt;

&lt;p&gt;   好。那就按要求安装。&lt;/p&gt;
   &lt;pre&gt;sudo gem install charlock_holmes -v &#39;0.7.3&#39;&lt;/pre&gt;

&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%885.25.54.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   好。继续按要求安装。&lt;/p&gt;
   &lt;pre&gt;brew install icu4c&lt;/pre&gt;

&lt;p&gt;   再重来&lt;/p&gt;
   &lt;pre&gt;sudo gem install charlock_holmes -v &#39;0.7.3&#39;&lt;/pre&gt;

   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-27%20%E4%B8%8A%E5%8D%889.46.38.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   因为没有安装bundle&lt;/p&gt;
   &lt;pre&gt;gem install bundle&lt;/pre&gt;

&lt;p&gt;   安装后再次尝试运行&lt;/p&gt;
   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   error&lt;/p&gt;
   &lt;pre&gt;Could not reach host index.rubygems.org. Check your network connection and try again.&lt;/pre&gt;
   出现这种错误可以尝试把Gemfile里的https改成http（互相转化进行尝试）

   &lt;pre&gt;bundle install&lt;/pre&gt;

&lt;p&gt;   终于成功！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;已安装成功gollum等。然后运行：&lt;/p&gt;
   &lt;pre&gt;gollum&lt;/pre&gt;

&lt;p&gt;   走到这一步了本人还是遇到了万恶的失败。&lt;br&gt;   看着已经安装好了&lt;/p&gt;
   &lt;pre&gt;
   Installing nokogiri 1.6.7.2 with native extensions
   Installing rack-protection 1.5.3
   Installing gollum-grit_adapter 1.0.1
   Installing sanitize 2.1.0
   Installing sinatra 1.4.7
   Installing gollum-lib 4.2.0
   Using gollum 4.0.1 from https://github.com/gollum/gollum.git (at master@5a5e56a)
   Bundle complete! 3 Gemfile dependencies, 24 gems now installed.
   Use `bundle show [gemname]` to see where a bundled gem is installed.
   &lt;/pre&gt;

&lt;p&gt;   然而实际并没有&lt;/p&gt;
   &lt;pre&gt;
   $ gollum
   -bash: gollum: command not found&lt;/pre&gt;

&lt;p&gt;   大写的忧伤。最后通过直接安装gollum解决。&lt;/p&gt;
   &lt;pre&gt;sudo gem install gollum&lt;/pre&gt;

   &lt;pre&gt;gollum&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;终于可以在本地启动成功维基。打开网址：&lt;a href=&quot;http://0.0.0.0:4567/，可以直接在浏览器中编辑。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://0.0.0.0:4567/，可以直接在浏览器中编辑。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%886.26.05.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   如果发现不能如下错误，请尝试更新ruby。&lt;/p&gt;
&lt;p&gt;   &lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%889.31.07.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;   更新ruby步骤&lt;br&gt;   安装 RVM&lt;br&gt;   RVM:Ruby Version Manager,Ruby版本管理器，包括Ruby的版本管理和Gem库管理(gemset)&lt;/p&gt;
   &lt;pre&gt;curl -L get.rvm.io | bash -s stable&lt;/pre&gt;

   &lt;pre&gt;source ~/.bashrc  
   source ~/.bash_profile &lt;/pre&gt;

&lt;p&gt;   测试是否安装正常&lt;/p&gt;
   &lt;pre&gt;rvm -v  &lt;/pre&gt;

&lt;p&gt;   用RVM升级Ruby&lt;/p&gt;
   &lt;pre&gt;
   #查看当前ruby版本  
   ruby -v  
   #列出已知的ruby版本  
   rvm list known  
   #安装ruby 2.3.0
   rvm install 2.3.0 &lt;/pre&gt;

&lt;p&gt;   安装完成之后ruby -v查看是否安装成功。&lt;/p&gt;
&lt;p&gt;   重新安装完毕后回到第3步。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;github同步&quot;&gt;&lt;a href=&quot;#github同步&quot; class=&quot;headerlink&quot; title=&quot;github同步&quot;&gt;&lt;/a&gt;github同步&lt;/h3&gt;&lt;p&gt;在wiki目录下面，进行git库操作，提交本地对维基内容的修改。一切将自动保存在你的Github上的个人博客网站的wiki目录下面。&lt;/p&gt;
&lt;pre&gt;
cd ~/wiki
git add .
git commit -am&quot;first commit&quot;
git push&lt;/pre&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;博客凸显创作，维基则是整理的好工具，很多入门级别、复用别人的操作，如配置环境等，更适合发布在个人维基上，本文就以gollum+github搭建个人wiki做个示范。&lt;/p&gt;
&lt;h3 id=&quot;开通Wiki&quot;&gt;&lt;a href=&quot;#开通Wiki&quot; class=&quot;headerlin
    
    </summary>
    
      <category term="Configuration" scheme="http://yoursite.com/categories/Configuration/"/>
    
    
      <category term="wiki" scheme="http://yoursite.com/tags/wiki/"/>
    
  </entry>
  
  <entry>
    <title>Github Pages+Hexo搭建个人博客</title>
    <link href="http://yoursite.com/2016/05/12/Github-Pages-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2016/05/12/Github-Pages-Hexo搭建个人博客/</id>
    <published>2016-05-12T08:55:07.000Z</published>
    <updated>2016-06-09T08:58:59.000Z</updated>
    
    <content type="html">&lt;h1 id=&quot;为什么用Github-Pages-Hexo&quot;&gt;&lt;a href=&quot;#为什么用Github-Pages-Hexo&quot; class=&quot;headerlink&quot; title=&quot;为什么用Github Pages + Hexo&quot;&gt;&lt;/a&gt;为什么用Github Pages + Hexo&lt;/h1&gt;&lt;h2 id=&quot;Github-Page优点&quot;&gt;&lt;a href=&quot;#Github-Page优点&quot; class=&quot;headerlink&quot; title=&quot;Github Page优点&quot;&gt;&lt;/a&gt;Github Page优点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;轻量级的博客系统，没有麻烦的配置&lt;/li&gt;
&lt;li&gt;使用标记语言，比如&lt;a href=&quot;http://markdown.tw/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;无需自己搭建服务器&lt;/li&gt;
&lt;li&gt;根据Github的限制，对应的每个站有300MB空间&lt;/li&gt;
&lt;li&gt;可以绑定自己的域名&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Github Page有两种page模式，User/Organization Pages（个人或公司站点）和Project Pages（项目站点）。这里我们用的是user/Organization Pages，要求使用自己的用户名，每个用户名下面只能建立一个，资源命名必须符合这样的规则username/username.github.com，主干上内容被用来构建和发布页面&lt;/p&gt;
&lt;h2 id=&quot;Hexo优点&quot;&gt;&lt;a href=&quot;#Hexo优点&quot; class=&quot;headerlink&quot; title=&quot;Hexo优点&quot;&gt;&lt;/a&gt;Hexo优点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;用于搭建博客网站框架，可以简单实现优美的博客网站;&lt;/li&gt;
&lt;li&gt;在本地端搭建，就可脱机查阅;&lt;/li&gt;
&lt;li&gt;架构不依托于其他门户网站，不再担心门户网站倒闭，不担心博文丢失或难以导出;&lt;/li&gt;
&lt;li&gt;博文为markdown格式，通用，容易上手，便于快速书写;&lt;/li&gt;
&lt;li&gt;可部署在github上；&lt;/li&gt;
&lt;li&gt;创造者来自中国台湾，所以几乎所有模板都关注到了中文的兼容性，很适合使用汉语的码农。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;搭建步骤&quot;&gt;&lt;a href=&quot;#搭建步骤&quot; class=&quot;headerlink&quot; title=&quot;搭建步骤&quot;&gt;&lt;/a&gt;搭建步骤&lt;/h1&gt;&lt;h2 id=&quot;新建github-repository&quot;&gt;&lt;a href=&quot;#新建github-repository&quot; class=&quot;headerlink&quot; title=&quot;新建github repository&quot;&gt;&lt;/a&gt;新建github repository&lt;/h2&gt;&lt;p&gt;在github上新建repository，name为username.github.io。&lt;/p&gt;
&lt;h2 id=&quot;Hexo安装&quot;&gt;&lt;a href=&quot;#Hexo安装&quot; class=&quot;headerlink&quot; title=&quot;Hexo安装&quot;&gt;&lt;/a&gt;Hexo安装&lt;/h2&gt;&lt;p&gt;先安装git和node.js&lt;/p&gt;
&lt;pre&gt;brew install git
brew install node&lt;/pre&gt;
验证是否安装成功
&lt;pre&gt;node -v
npm -v&lt;/pre&gt;
安装Hexo
&lt;pre&gt;npm install -g hexo #-g表示全局安装, npm默认为当前项目安装&lt;/pre&gt;

&lt;h2 id=&quot;Hexo部署&quot;&gt;&lt;a href=&quot;#Hexo部署&quot; class=&quot;headerlink&quot; title=&quot;Hexo部署&quot;&gt;&lt;/a&gt;Hexo部署&lt;/h2&gt;&lt;p&gt;新建文件夹并打开，在文件夹内操作。&lt;/p&gt;
&lt;pre&gt; hexo init #新建博客目录
 hexo g #根据当前目录下文件生成静态网页
 hexo s #启动服务器&lt;/pre&gt;

&lt;p&gt;现在就可以到浏览器输入localhost:4000查看啦。&lt;/p&gt;
&lt;p&gt;简单介绍一下文件目录&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;public：执行hexo generate命令，输出的静态网页内容目录&lt;/li&gt;
&lt;li&gt;scaffolds：layout模板文件目录，其中的md文件可以添加编辑&lt;/li&gt;
&lt;li&gt;scripts：扩展脚本目录，这里可以自定义一些javascript脚本&lt;/li&gt;
&lt;li&gt;source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。&lt;/li&gt;
&lt;li&gt;drafts：草稿文章&lt;/li&gt;
&lt;li&gt;posts：发布文章themes：主题文件目录&lt;/li&gt;
&lt;li&gt;config.yml：全局配置文件，大多数的设置都在这里&lt;/li&gt;
&lt;li&gt;package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的 关于 按钮&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Hexo复制主题&quot;&gt;&lt;a href=&quot;#Hexo复制主题&quot; class=&quot;headerlink&quot; title=&quot;Hexo复制主题&quot;&gt;&lt;/a&gt;Hexo复制主题&lt;/h2&gt;&lt;pre&gt; hexo clean
 hexo g
 hexo s
 git clone https://github.com/cnfeat/cnfeat.git themes/jacman&lt;/pre&gt;

&lt;h2 id=&quot;启用主题&quot;&gt;&lt;a href=&quot;#启用主题&quot; class=&quot;headerlink&quot; title=&quot;启用主题&quot;&gt;&lt;/a&gt;启用主题&lt;/h2&gt;&lt;p&gt;修改Hexo目录下的config.yml配置文件中的theme属性，将其设置为jacman。&lt;/p&gt;
&lt;pre&gt;theme: jacman #或你的主题名，注意冒号后有一个空格&lt;/pre&gt;

&lt;p&gt;注意：Hexo有两个config.yml文件，一个在根目录，一个在theme下，此时修改的是在根目录下的。&lt;/p&gt;
&lt;h2 id=&quot;更新主题&quot;&gt;&lt;a href=&quot;#更新主题&quot; class=&quot;headerlink&quot; title=&quot;更新主题&quot;&gt;&lt;/a&gt;更新主题&lt;/h2&gt;&lt;pre&gt; cd themes/jacman
 git pull&lt;/pre&gt;

&lt;p&gt;注意：为避免出错，请先备份你的_config.yml 文件后再升级&lt;/p&gt;
&lt;h2 id=&quot;Hexo本地调试&quot;&gt;&lt;a href=&quot;#Hexo本地调试&quot; class=&quot;headerlink&quot; title=&quot;Hexo本地调试&quot;&gt;&lt;/a&gt;Hexo本地调试&lt;/h2&gt;&lt;pre&gt; hexo g #生成
 hexo s #启动本地服务，进行文章预览调试
 hexo d -g #或者直接作用组合命令&lt;/pre&gt;

&lt;p&gt;浏览器输入localhost:4000，即可查看搭建效果。每次变更config.yml 文件或者上传文件都可以先用此命令调试。&lt;/p&gt;
&lt;h2 id=&quot;Hexo部署到github&quot;&gt;&lt;a href=&quot;#Hexo部署到github&quot; class=&quot;headerlink&quot; title=&quot;Hexo部署到github&quot;&gt;&lt;/a&gt;Hexo部署到github&lt;/h2&gt;&lt;pre&gt;npm install hexo-deployer-git --save&lt;/pre&gt;
在 Hexo 文件夹下找到 config.yml 文件, 找到其中的 deploy 标签，改成下图所示形式，并保存。注意：冒号后面要加上一个空格，否则会报错
&lt;pre&gt;
deploy:
  type: git
  repo: https://github.com/Shuang0420/Shuang0420.github.io.git&lt;/pre&gt;

&lt;p&gt;运行如下命令：&lt;/p&gt;
&lt;pre&gt;hexo clean
hexo generate
hexo deploy&lt;/pre&gt;


&lt;h2 id=&quot;发博文&quot;&gt;&lt;a href=&quot;#发博文&quot; class=&quot;headerlink&quot; title=&quot;发博文&quot;&gt;&lt;/a&gt;发博文&lt;/h2&gt;&lt;pre&gt;
 hexo new &quot;postname&quot; #然后在posts目录下的postname.md文件中编辑博客
 hexo clean
 hexo generate
 # (若要本地预览就先执行 hexo server)
 hexo deploy&lt;/pre&gt;

&lt;h2 id=&quot;快捷命令：&quot;&gt;&lt;a href=&quot;#快捷命令：&quot; class=&quot;headerlink&quot; title=&quot;快捷命令：&quot;&gt;&lt;/a&gt;快捷命令：&lt;/h2&gt;&lt;pre&gt; hexo g == hexo generate
 hexo d == hexo deploy
 hexo s == hexo server
 hexo n == hexo new
# 还能组合使用，如：
hexo d -g&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://mozhenhau.com/2015/03/05/%E5%9C%A8Mac%E9%80%9A%E8%BF%87Hexo%E5%9C%A8github%E4%B8%8A%E5%BB%BA%E7%AB%8B%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://mozhenhau.com/2015/03/05/%E5%9C%A8Mac%E9%80%9A%E8%BF%87Hexo%E5%9C%A8github%E4%B8%8A%E5%BB%BA%E7%AB%8B%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://evakasch.github.io/2016/05/04/hexo-setup/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://evakasch.github.io/2016/05/04/hexo-setup/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;为什么用Github-Pages-Hexo&quot;&gt;&lt;a href=&quot;#为什么用Github-Pages-Hexo&quot; class=&quot;headerlink&quot; title=&quot;为什么用Github Pages + Hexo&quot;&gt;&lt;/a&gt;为什么用Github Pages + He
    
    </summary>
    
      <category term="Configuration" scheme="http://yoursite.com/categories/Configuration/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>让进程在后台可靠运行的几种方法</title>
    <link href="http://yoursite.com/2016/04/22/%E8%AE%A9%E8%BF%9B%E7%A8%8B%E5%9C%A8%E5%90%8E%E5%8F%B0%E5%8F%AF%E9%9D%A0%E8%BF%90%E8%A1%8C%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2016/04/22/让进程在后台可靠运行的几种方法/</id>
    <published>2016-04-22T08:55:07.000Z</published>
    <updated>2016-06-09T09:09:27.000Z</updated>
    
    <content type="html">&lt;p&gt;当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。&lt;/p&gt;
&lt;h3 id=&quot;nohup&quot;&gt;&lt;a href=&quot;#nohup&quot; class=&quot;headerlink&quot; title=&quot;nohup&quot;&gt;&lt;/a&gt;nohup&lt;/h3&gt;&lt;p&gt;只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。同时可在结尾加上”&amp;amp;”来将命令同时放入后台运行，也可用”&amp;gt;filename 2&amp;gt;&amp;amp;1”来更改缺省的重定向文件名。&lt;/p&gt;
&lt;pre&gt;
$ nohup ping www.ibm.com &amp;
[1] 6982
$ nohup: appending output to `nohup.out&#39;
$ ps -ef |grep www.ibm.com
  501  6982  5823   0  4:23下午 ttys000    0:00.03 ping www.ibm.com
  501  7120  5823   0  4:26下午 ttys000    0:00.01 grep www.ibm.com
&lt;/pre&gt;

&lt;h3 id=&quot;setsid&quot;&gt;&lt;a href=&quot;#setsid&quot; class=&quot;headerlink&quot; title=&quot;setsid&quot;&gt;&lt;/a&gt;setsid&lt;/h3&gt;&lt;p&gt;nohup 能通过忽略 HUP 信号来使我们的进程避免中途被中断，换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。&lt;/p&gt;
&lt;pre&gt;
$ setsid ping www.ibm.com
$ ps -ef |grep www.ibm.com
root     31094     1  0 07:28 ?        00:00:00 ping www.ibm.com
root     31102 29217  0 07:29 pts/4    00:00:00 grep www.ibm.com
&lt;/pre&gt;

&lt;p&gt;  值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。&lt;/p&gt;
&lt;h3 id=&quot;disown&quot;&gt;&lt;a href=&quot;#disown&quot; class=&quot;headerlink&quot; title=&quot;disown&quot;&gt;&lt;/a&gt;disown&lt;/h3&gt;&lt;p&gt;如果未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用disown -h jobspec来使某个作业忽略HUP信号。&lt;/li&gt;
&lt;li&gt;用disown -ah 来使所有的作业都忽略HUP信号。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用disown -rh 来使正在运行的作业忽略HUP信号。&lt;/p&gt;
&lt;p&gt;需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。&lt;br&gt;但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了”&amp;amp;”来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用 CTRL-z（按住Ctrl键的同时按住z键）了！&lt;br&gt;CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用bg jobspec来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。&lt;/p&gt;
&lt;p&gt;disown 示例1（如果提交命令时已经用“&amp;amp;”将命令放入后台运行，则可以直接使用“disown”）&lt;/p&gt;
&lt;pre&gt;$ cp -r testLargeFile largeFile &amp;
[1] 4825
$ jobs
[1]+  Running                 cp -i -r testLargeFile largeFile &amp;
$ disown -h %1
$ ps -ef |grep largeFile
root      4825   968  1 09:46 pts/4    00:00:00 cp -i -r testLargeFile largeFile
root      4853   968  0 09:46 pts/4    00:00:00 grep largeFile
$ logout&lt;/pre&gt;

&lt;p&gt;disown 示例2（如果提交命令时未使用“&amp;amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）&lt;/p&gt;
&lt;pre&gt;$ cp -r testLargeFile largeFile2
[1]+  Stopped                 cp -i -r testLargeFile largeFile2
$ bg %1
[1]+ cp -i -r testLargeFile largeFile2 &amp;
$ jobs
[1]+  Running                 cp -i -r testLargeFile largeFile2 &amp;
$ disown -h %1
$ ps -ef |grep largeFile2
root      5790  5577  1 10:04 pts/3    00:00:00 cp -i -r testLargeFile largeFile2
root      5824  5577  0 10:05 pts/3    00:00:00 grep largeFile2&lt;/pre&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;screen&quot;&gt;&lt;a href=&quot;#screen&quot; class=&quot;headerlink&quot; title=&quot;screen&quot;&gt;&lt;/a&gt;screen&lt;/h3&gt;&lt;p&gt;如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？&lt;br&gt;此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen 的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP 信号的影响。&lt;br&gt;使用 screen 很方便，有以下几个常用选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用screen -dmS session name来建立一个处于断开模式下的会话（并指定其会话名）。&lt;/li&gt;
&lt;li&gt;用screen -list 来列出所有会话。&lt;/li&gt;
&lt;li&gt;用screen -r session name来重新连接指定会话。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;用快捷键CTRL-a d 来暂时断开当前会话。&lt;/p&gt;
&lt;p&gt;screen 示例&lt;/p&gt;
&lt;pre&gt;$ screen -dmS Urumchi
$ screen -list
There is a screen on:
      12842.Urumchi   (Detached)
1 Socket in /tmp/screens/S-root.
$ screen -r Urumchi&lt;/pre&gt;

&lt;p&gt;当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;未使用 screen 时新进程的进程树&lt;/p&gt;
&lt;pre&gt;$ ping www.google.com &amp;
[1] 9499
$ pstree -H 9499
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─2*[sendmail]
  ├─sshd─┬─sshd───bash───pstree
  │       └─sshd───bash───ping&lt;/pre&gt;

&lt;p&gt;我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP 信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用了 screen 后新进程的进程树&lt;/p&gt;
&lt;pre&gt;$screen -r Urumchi
$ ping www.ibm.com &amp;
[1] 9488
$ pstree -H 9488
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─screen───bash───ping
  ├─2*[sendmail]&lt;/pre&gt;

&lt;p&gt;而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是 init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。&lt;/p&gt;
&lt;h3 id=&quot;nohup&quot;&gt;&lt;a hre
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="c" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
</feed>
