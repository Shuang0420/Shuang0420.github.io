<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>徐阿衡</title>
  <subtitle>Shuang</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-06-17T03:57:28.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>徐阿衡</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>讲座笔记 -- 腾讯应用宝</title>
    <link href="http://yoursite.com/2016/06/15/%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0%20--%20%E8%85%BE%E8%AE%AF%E5%BA%94%E7%94%A8%E5%AE%9D/"/>
    <id>http://yoursite.com/2016/06/15/讲座笔记 -- 腾讯应用宝/</id>
    <published>2016-06-15T13:35:48.000Z</published>
    <updated>2016-06-17T03:57:28.000Z</updated>
    
    <content type="html">&lt;p&gt;卓居超，2013年加入腾讯内部搜索部门，现负责腾讯应用宝搜索项目。近年来从事的科研工作集中在垂直领域的搜索、推荐技术研究。2015年代表腾讯公司在 WSDM 会议上做题为 “Semantic Matching in APP Search” 的主题报告，介绍腾讯应用宝语义搜索的技术实现。&lt;br&gt;今天他在公司做了一场关于腾讯应用宝的分享，这是一篇讲座笔记。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;应用宝-–-腾讯的安卓应用市场&quot;&gt;&lt;a href=&quot;#应用宝-–-腾讯的安卓应用市场&quot; class=&quot;headerlink&quot; title=&quot;应用宝 – 腾讯的安卓应用市场&quot;&gt;&lt;/a&gt;应用宝 – 腾讯的安卓应用市场&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;搜索是重要入口（新应用的分发）&lt;/li&gt;
&lt;li&gt;app 快速的增长 一年增长几百万&lt;/li&gt;
&lt;li&gt;二八原则 长尾大 0.1%的应用 80%的分发&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;指标&quot;&gt;&lt;a href=&quot;#指标&quot; class=&quot;headerlink&quot; title=&quot;指标&quot;&gt;&lt;/a&gt;指标&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Downloads&lt;/li&gt;
&lt;li&gt;QV&lt;/li&gt;
&lt;li&gt;UV&lt;/li&gt;
&lt;li&gt;CTR (Click-Through-Rate)&lt;/li&gt;
&lt;li&gt;ROP (Rate-Of-Penetration)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;CTR(Click-Through-Rate): 网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的点击量（严格的来说，可以是到达目标页面的数量）除以广告的浏览量（PV- Page View）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;语义计算策略&quot;&gt;&lt;a href=&quot;#语义计算策略&quot; class=&quot;headerlink&quot; title=&quot;语义计算策略&quot;&gt;&lt;/a&gt;语义计算策略&lt;/h2&gt;&lt;h3 id=&quot;数据特征&quot;&gt;&lt;a href=&quot;#数据特征&quot; class=&quot;headerlink&quot; title=&quot;数据特征&quot;&gt;&lt;/a&gt;数据特征&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;数据量少 审核通过的应用数量只有数十万&lt;/li&gt;
&lt;li&gt;文本信息少 附带文本信息少&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这就意味着能建索引的量少 –&amp;gt; 所以要将信息泛化&lt;/p&gt;
&lt;h3 id=&quot;应对策略&quot;&gt;&lt;a href=&quot;#应对策略&quot; class=&quot;headerlink&quot; title=&quot;应对策略&quot;&gt;&lt;/a&gt;应对策略&lt;/h3&gt;&lt;p&gt;搜索＋推荐&lt;br&gt;用 &lt;strong&gt;词、主题、标签&lt;/strong&gt; 来描述语义 (query –&amp;gt; term + topic + tag –&amp;gt; app)&lt;/p&gt;
&lt;h3 id=&quot;数据补充&quot;&gt;&lt;a href=&quot;#数据补充&quot; class=&quot;headerlink&quot; title=&quot;数据补充&quot;&gt;&lt;/a&gt;数据补充&lt;/h3&gt;&lt;p&gt;爬取全网资源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;游戏站点、用户评价&lt;/li&gt;
&lt;li&gt;知识库：百度百科、百度知道&lt;/li&gt;
&lt;li&gt;其他应用商店&lt;/li&gt;
&lt;li&gt;搜索引擎 解析&lt;/li&gt;
&lt;li&gt;应用宝用户行为&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;容易出现的问题是噪音会很大，所以需要机器学习的方法进一步的过滤&lt;/p&gt;
&lt;p&gt;过程就是 &lt;strong&gt;页面抓取 –&amp;gt; 内容抓取 –&amp;gt; 知识挖掘 –&amp;gt; 标签 + 句法模板 + 标签集合 + 标签关联 –&amp;gt; 标签关联净化 –&amp;gt; 标签索引&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;数据挖掘&quot;&gt;&lt;a href=&quot;#数据挖掘&quot; class=&quot;headerlink&quot; title=&quot;数据挖掘&quot;&gt;&lt;/a&gt;数据挖掘&lt;/h3&gt;&lt;p&gt;利用用户行为来指导排序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击下载因子。赋予大的权重（增强鲁棒性）&lt;/li&gt;
&lt;li&gt;entropy因子。entropy可以表示用户query的集中程度，，点击散，entropy高，区分精准query和模糊query&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;主题模型&quot;&gt;&lt;a href=&quot;#主题模型&quot; class=&quot;headerlink&quot; title=&quot;主题模型&quot;&gt;&lt;/a&gt;主题模型&lt;/h3&gt;&lt;p&gt;LDA 聚类，对 topic 进行人工标注，把 app 映射到 topic&lt;br&gt;LDA 在业界用法比较多。然而它最大的特点是需要大量的语料，语料少效果就不好。所以需要补充大量文本数据。&lt;/p&gt;
&lt;h3 id=&quot;标签挖掘&quot;&gt;&lt;a href=&quot;#标签挖掘&quot; class=&quot;headerlink&quot; title=&quot;标签挖掘&quot;&gt;&lt;/a&gt;标签挖掘&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;元搜方式挖掘 tag （通过搜app）&lt;/li&gt;
&lt;li&gt;根据用户行为、画像挖掘 tag&lt;br&gt;对用户进行分群 地区／年龄／职业／性别等 生成代表用户属性的标签给app&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;元搜，上大学的时候还学过来着，居然听讲座的时候没想起来😳&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;元搜索引擎又称多搜索引擎，通过一个统一的用户界面帮助用户在多个搜索引擎中选择和利用合适的（甚至是同时利用若干个）搜索引擎来实现检索操作，是对分布于网络的多种检索工具的全局控制机制。（搜索引擎分类：全文搜索引擎、目录索引、元搜索引擎）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;数据清洗&quot;&gt;&lt;a href=&quot;#数据清洗&quot; class=&quot;headerlink&quot; title=&quot;数据清洗&quot;&gt;&lt;/a&gt;数据清洗&lt;/h3&gt;&lt;p&gt;方法：机器学习模型计算 confidence level&lt;br&gt;human editor + web data + qa (lda) user group tags  –&amp;gt; &lt;a href=&quot;http://blog.csdn.net/w28971023/article/details/8240756&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GBDT mode&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;app-语义画像&quot;&gt;&lt;a href=&quot;#app-语义画像&quot; class=&quot;headerlink&quot; title=&quot;app 语义画像&quot;&gt;&lt;/a&gt;app 语义画像&lt;/h3&gt;&lt;p&gt;语义描述体系&lt;br&gt;分多个维度&lt;/p&gt;
&lt;h3 id=&quot;机器学习（LTR）&quot;&gt;&lt;a href=&quot;#机器学习（LTR）&quot; class=&quot;headerlink&quot; title=&quot;机器学习（LTR）&quot;&gt;&lt;/a&gt;机器学习（LTR）&lt;/h3&gt;&lt;p&gt;挑战：多来源检索结果不可比（类别／tag）&lt;br&gt;利器：lambdaMART 排序模型（GBRT的变种）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;LTR - Learning to rank：学习排序&lt;br&gt;用机器学习的方法进行排序，可用于相关性排序、推荐引擎等系统中。Learning to rank or machine-learned ranking (MLR) is a type of supervised or semi-supervised machine learning problem in which the goal is to automatically construct a ranking model from training data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;应用宝搜索商业化&quot;&gt;&lt;a href=&quot;#应用宝搜索商业化&quot; class=&quot;headerlink&quot; title=&quot;应用宝搜索商业化&quot;&gt;&lt;/a&gt;应用宝搜索商业化&lt;/h2&gt;&lt;h3 id=&quot;分发升级&quot;&gt;&lt;a href=&quot;#分发升级&quot; class=&quot;headerlink&quot; title=&quot;分发升级&quot;&gt;&lt;/a&gt;分发升级&lt;/h3&gt;&lt;p&gt;应用+&lt;br&gt;应用分发 –&amp;gt; 内容服务分发&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;意图识别优化：什么时候出应用，什么时候出音乐，热度&lt;/li&gt;
&lt;li&gt;多来源混排：机器学习＋运营系统优化异构排序，促进分发效率（应用、音乐等怎么混排）&lt;/li&gt;
&lt;li&gt;多场景引导：在热词、直达区（搜索补充呈现）、联想词等场景引导用户，培养内容搜索习惯&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;应用搜索广告&quot;&gt;&lt;a href=&quot;#应用搜索广告&quot; class=&quot;headerlink&quot; title=&quot;应用搜索广告&quot;&gt;&lt;/a&gt;应用搜索广告&lt;/h3&gt;&lt;p&gt;技术核心&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;app 画像&lt;br&gt;基于标签、主题、类别的 app 细粒度商业词&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;动态混排&lt;br&gt;根据 query 动态选择广告槽位&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用相似应用打tag（confidence level –&amp;gt; filter）&lt;/p&gt;
&lt;h1 id=&quot;机器学习的本质&quot;&gt;&lt;a href=&quot;#机器学习的本质&quot; class=&quot;headerlink&quot; title=&quot;机器学习的本质&quot;&gt;&lt;/a&gt;机器学习的本质&lt;/h1&gt;&lt;p&gt;已知数据 先验知识（专家系统） 未知数据的特征 –&amp;gt; 求未知数据的优化分布&lt;br&gt;通用技术难点：空间搜索 函数泛化&lt;/p&gt;
&lt;p&gt;实际工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型10% 其他90%&lt;/li&gt;
&lt;li&gt;数据从哪来&lt;/li&gt;
&lt;li&gt;特征如何抽取&lt;/li&gt;
&lt;li&gt;领域先验知识&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大量噪音？维数灾难？&lt;br&gt;&lt;a href=&quot;http://blog.csdn.net/vividonly/article/details/50723852&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;L0,L1,L2正则化&lt;/a&gt; 剪枝&lt;br&gt;琐碎的准备工作很重要&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L0正则化的值是模型参数中非零参数的个数。&lt;br&gt;L1正则化表示各个参数绝对值之和。&lt;br&gt;L2正则化标识各个参数的平方的和的开方值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;大公司-vs-小公司&quot;&gt;&lt;a href=&quot;#大公司-vs-小公司&quot; class=&quot;headerlink&quot; title=&quot;大公司 vs 小公司&quot;&gt;&lt;/a&gt;大公司 vs 小公司&lt;/h1&gt;&lt;h2 id=&quot;大公司&quot;&gt;&lt;a href=&quot;#大公司&quot; class=&quot;headerlink&quot; title=&quot;大公司&quot;&gt;&lt;/a&gt;大公司&lt;/h2&gt;&lt;p&gt;搜索 推荐 广告 都能接触到，可以和牛人接触&lt;br&gt;流程化 冗余 很多团队想做一件事&lt;/p&gt;
&lt;h2 id=&quot;小公司&quot;&gt;&lt;a href=&quot;#小公司&quot; class=&quot;headerlink&quot; title=&quot;小公司&quot;&gt;&lt;/a&gt;小公司&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;方向更对&lt;br&gt;不被商业价值束缚 不被同伴利益束缚 不被自己经验束缚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;跑的更快&lt;br&gt;不被用户束缚 不被流程束缚 不被一般道德束缚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;复利效应&quot;&gt;&lt;a href=&quot;#复利效应&quot; class=&quot;headerlink&quot; title=&quot;复利效应&quot;&gt;&lt;/a&gt;复利效应&lt;/h1&gt;&lt;p&gt;应用宝光是去噪就做了一年。&lt;br&gt;每天积累一点 –&amp;gt; 复利效应 –&amp;gt; 无法超越&lt;br&gt;腾讯去做搜索，做不过百度，为什么？技术团队不强？no！因为百度做了几十年的搜索，每天进步一点，复利效应无法超越。&lt;br&gt;我们要找到可以产生复利效应的点。算法是数学专家的事，我们可以做的是应用方面的复利效应，比如说聊天机器人。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;卓居超，2013年加入腾讯内部搜索部门，现负责腾讯应用宝搜索项目。近年来从事的科研工作集中在垂直领域的搜索、推荐技术研究。2015年代表腾讯公司在 WSDM 会议上做题为 “Semantic Matching in APP Search” 的主题报告，介绍腾讯应用宝语义搜索的技术实现。&lt;br&gt;今天他在公司做了一场关于腾讯应用宝的分享，这是一篇讲座笔记。&lt;br&gt;
    
    </summary>
    
      <category term="他山之石" scheme="http://yoursite.com/categories/%E4%BB%96%E5%B1%B1%E4%B9%8B%E7%9F%B3/"/>
    
    
      <category term="腾讯" scheme="http://yoursite.com/tags/%E8%85%BE%E8%AE%AF/"/>
    
  </entry>
  
  <entry>
    <title>爬虫总结(二)-- scrapy</title>
    <link href="http://yoursite.com/2016/06/12/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%BA%8C-scrapy/"/>
    <id>http://yoursite.com/2016/06/12/爬虫总结-二-scrapy/</id>
    <published>2016-06-12T09:59:51.000Z</published>
    <updated>2016-06-16T13:48:25.000Z</updated>
    
    <content type="html">&lt;p&gt;把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&amp;gt; 定义目标（Items）–&amp;gt; 制作爬虫（Spider）–&amp;gt; 存储结果（Pipeline）&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Scrapy-概述&quot;&gt;&lt;a href=&quot;#Scrapy-概述&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 概述&quot;&gt;&lt;/a&gt;Scrapy 概述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。&lt;br&gt;其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Scrapy-架构&quot;&gt;&lt;a href=&quot;#Scrapy-架构&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 架构&quot;&gt;&lt;/a&gt;Scrapy 架构&lt;/h2&gt;&lt;p&gt;Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/crawler.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Scrapy-组件&quot;&gt;&lt;a href=&quot;#Scrapy-组件&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 组件&quot;&gt;&lt;/a&gt;Scrapy 组件&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)&lt;/li&gt;
&lt;li&gt;调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址&lt;/li&gt;
&lt;li&gt;下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)&lt;/li&gt;
&lt;li&gt;爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面&lt;/li&gt;
&lt;li&gt;项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。&lt;/li&gt;
&lt;li&gt;下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。&lt;/li&gt;
&lt;li&gt;爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。&lt;/li&gt;
&lt;li&gt;调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Scrapy-运行流程&quot;&gt;&lt;a href=&quot;#Scrapy-运行流程&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 运行流程&quot;&gt;&lt;/a&gt;Scrapy 运行流程&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;引擎从调度器中取出一个链接(URL)用于接下来的抓取&lt;/li&gt;
&lt;li&gt;引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)&lt;/li&gt;
&lt;li&gt;爬虫解析Response&lt;/li&gt;
&lt;li&gt;若是解析出实体（Item）,则交给实体管道进行进一步的处理;若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;Scrapy-实例&quot;&gt;&lt;a href=&quot;#Scrapy-实例&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 实例&quot;&gt;&lt;/a&gt;Scrapy 实例&lt;/h1&gt;&lt;h2 id=&quot;新建项目-Project&quot;&gt;&lt;a href=&quot;#新建项目-Project&quot; class=&quot;headerlink&quot; title=&quot;新建项目 (Project)&quot;&gt;&lt;/a&gt;新建项目 (Project)&lt;/h2&gt;&lt;pre&gt;
scrapy startproject news_scrapy
&lt;/pre&gt;

&lt;p&gt;输入以上命令之后，就会看见命令行运行的目录下多了一个名为 news_scrapy 的目录，目录的结构如下：&lt;/p&gt;
&lt;pre&gt;
|---- news_scrapy
| |---- news_scrapy
|   |---- __init__.py
|   |---- items.py        #用来存储爬下来的数据结构（字典形式）
|    |---- pipelines.py    #用来对爬出来的item进行后续处理，如存入数据库等
|    |---- settings.py    #爬虫配置文件
|    |---- spiders        #此目录用来存放创建的新爬虫文件（爬虫主体）
|     |---- __init__.py
| |---- scrapy.cfg        #项目配置文件
&lt;/pre&gt;

&lt;h2 id=&quot;定义目标（Items）&quot;&gt;&lt;a href=&quot;#定义目标（Items）&quot; class=&quot;headerlink&quot; title=&quot;定义目标（Items）&quot;&gt;&lt;/a&gt;定义目标（Items）&lt;/h2&gt;&lt;p&gt;Items是装载抓取的数据的容器，工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误&lt;br&gt;通过创建scrapy.Item类, 并且定义类型为 scrapy.Field 的类属性来声明一个Item，通过将需要的item模型化，来控制站点数据。&lt;br&gt;编辑 items.py&lt;/p&gt;
&lt;pre&gt;
# -*- coding: utf-8 -*-
import scrapy
class NewsScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    category = scrapy.Field()
    url = scrapy.Field()
    secondary_title = scrapy.Field()
    secondary_url = scrapy.Field()
    #text = Field()
&lt;/pre&gt;


&lt;h2 id=&quot;制作爬虫（Spider）&quot;&gt;&lt;a href=&quot;#制作爬虫（Spider）&quot; class=&quot;headerlink&quot; title=&quot;制作爬虫（Spider）&quot;&gt;&lt;/a&gt;制作爬虫（Spider）&lt;/h2&gt;&lt;p&gt;Spider 定义了用于下载的URL列表、跟踪链接的方案、解析网页内容的方式，以此来提取items。&lt;br&gt;要建立一个Spider，你必须用scrapy.spider.BaseSpider创建一个子类，并确定三个强制的属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。&lt;/li&gt;
&lt;li&gt;start_urls：爬取的URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。&lt;/li&gt;
&lt;li&gt;parse()：解析的方法，调用的时候传入从每一个URL传回的Response对象作为唯一参数，负责解析并匹配抓取的数据(解析为item)，跟踪更多的URL。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 spiders 目录下新建Wynews.py，代码如下。利用 yield Request(url=item[‘url’],meta={‘item_1’: item},callback=self.second_parse) 来进行第二层爬取。&lt;/p&gt;
&lt;pre&gt;
class WynewsSpider(BaseSpider):
    name = &quot;Wynews&quot;
    start_urls = [&#39;http://news.163.com/rank/&#39;]

    def parse(self,response):
        html = HtmlXPathSelector(response)
        page = html.xpath(&#39;//div[@class=&quot;subNav&quot;]/a&#39;)
        for i in page:
            item = dict()
            item[&#39;category&#39;] = i.xpath(&#39;text()&#39;).extract_first()
            item[&#39;url&#39;] = i.xpath(&#39;@href&#39;).extract_first()
            print item[&#39;category&#39;],item[&#39;url&#39;]
            yield Request(url=item[&#39;url&#39;],meta={&#39;item_1&#39;: item},callback=self.second_parse)

    def second_parse(self,response):
        item_1= response.meta[&#39;item_1&#39;]
        html = HtmlXPathSelector(response)
        #print &#39;response &#39;,response
        page = html.xpath(&#39;//tr/td/a&#39;)
        #print &#39;page &#39;,page
        items = []
        for i in page:
            item = DidiScrapyItem()
            item[&#39;category&#39;] = item_1[&#39;category&#39;].encode(&#39;utf8&#39;)
            item[&#39;url&#39;] = item_1[&#39;url&#39;].encode(&#39;utf8&#39;)
            item[&#39;secondary_title&#39;] = i.xpath(&#39;text()&#39;).extract_first().encode(&#39;utf8&#39;)
            item[&#39;secondary_url&#39;] = i.xpath(&#39;@href&#39;).extract_first().encode(&#39;utf8&#39;)
            #print i.xpath(&#39;text()&#39;).extract(),i.xpath(&#39;@href&#39;).extract()
            items.append(item)
        return items
&lt;/pre&gt;

&lt;h2 id=&quot;存储结果（Pipeline）&quot;&gt;&lt;a href=&quot;#存储结果（Pipeline）&quot; class=&quot;headerlink&quot; title=&quot;存储结果（Pipeline）&quot;&gt;&lt;/a&gt;存储结果（Pipeline）&lt;/h2&gt;&lt;p&gt;Item pipeline 的主要责任是负责处理 spider 抽取的 Item，主要任务是清理、验证和存储数据。当页面被 spider 解析后，将被发送到 pipeline，每个 pipeline 的组件都是由一个简单的方法组成的Python类。pipeline 获取Item，执行相应的方法，并确定是否需要在 pipeline中继续执行下一步或是直接丢弃掉不处理。&lt;/p&gt;
&lt;h3 id=&quot;执行过程&quot;&gt;&lt;a href=&quot;#执行过程&quot; class=&quot;headerlink&quot; title=&quot;执行过程&quot;&gt;&lt;/a&gt;执行过程&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;清理HTML数据&lt;/li&gt;
&lt;li&gt;验证解析到的数据（检查Item是否包含必要的字段）&lt;/li&gt;
&lt;li&gt;检查是否是重复数据（如果重复就删除）&lt;/li&gt;
&lt;li&gt;将解析到的数据存储到 数据库/文件 中&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;主要方法&quot;&gt;&lt;a href=&quot;#主要方法&quot; class=&quot;headerlink&quot; title=&quot;主要方法&quot;&gt;&lt;/a&gt;主要方法&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;process_item(item, spider)&lt;br&gt;每一个item管道组件都会调用该方法，并且必须返回一个item对象实例或raise DropItem异常。&lt;br&gt;被丢掉的item将不会在管道组件进行执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;open_spider(spider)&lt;br&gt;当spider执行的时候将调用该方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;close_spider(spider)&lt;br&gt;当spider关闭的时候将调用该方法&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;编写自己的-Pipeline&quot;&gt;&lt;a href=&quot;#编写自己的-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;编写自己的 Pipeline&quot;&gt;&lt;/a&gt;编写自己的 Pipeline&lt;/h3&gt;&lt;p&gt;编辑 pipelines.py。把抓取的 items 保存到 json 文件中。&lt;/p&gt;
&lt;pre&gt;
import json
class NewsScrapyPipeline(object):
    def __init__(self):
        self.file = open(&#39;items.json&#39;, &#39;w&#39;)
    def process_item(self, item, spider):
        line = json.dumps(dict(item),ensure_ascii=False) + &quot;\n&quot;
        self.file.write(line)
        return item
&lt;/pre&gt;

&lt;h3 id=&quot;激活Item-Pipeline组件&quot;&gt;&lt;a href=&quot;#激活Item-Pipeline组件&quot; class=&quot;headerlink&quot; title=&quot;激活Item Pipeline组件&quot;&gt;&lt;/a&gt;激活Item Pipeline组件&lt;/h3&gt;&lt;p&gt;在settings.py文件中，往ITEM_PIPELINES中添加项目管道的类名，激活项目管道组件&lt;/p&gt;
&lt;pre&gt;
ITEM_PIPELINES = {
    &#39;news_scrapy.pipelines.NewsScrapyPipeline&#39;: 300,
}
&lt;/pre&gt;

&lt;h2 id=&quot;开启爬虫-Crawl&quot;&gt;&lt;a href=&quot;#开启爬虫-Crawl&quot; class=&quot;headerlink&quot; title=&quot;开启爬虫 (Crawl)&quot;&gt;&lt;/a&gt;开启爬虫 (Crawl)&lt;/h2&gt;&lt;pre&gt;scrapy crawl Wynews&lt;/pre&gt;

&lt;h2 id=&quot;可能出现的问题-Problem&quot;&gt;&lt;a href=&quot;#可能出现的问题-Problem&quot; class=&quot;headerlink&quot; title=&quot;可能出现的问题 (Problem)&quot;&gt;&lt;/a&gt;可能出现的问题 (Problem)&lt;/h2&gt;&lt;p&gt;打开 items.json 文件，中文可能会出现文件乱码问题&lt;/p&gt;
&lt;pre&gt;
[{&quot;category&quot;: &quot;\u93c2\u4f34\u6908&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_news.html&quot;, &quot;secondary_title&quot;: &quot;\u934b\u950b\u9422\u5cf0\u30b3\u95c3\u8e6d\u7b09\u9473\u6ec8\u69fb\u951b\u5c7e\u5d0f\u6fc2\u7a3f\u5dfb\u9359\u53c9\u7c2e\u6769\u6ec4\u7966\u95c0&quot;, &quot;secondary_url&quot;: &quot;http://caozhi.news.163.com/16/0615/09/BPJG6SB60001544E.html&quot;},
&lt;/pre&gt;

&lt;p&gt;这一行代码就能解决。&lt;/p&gt;
&lt;pre&gt;
line = json.dumps(dict(item),ensure_ascii=False) + &quot;\n&quot;
&lt;/pre&gt;

&lt;p&gt;结果&lt;/p&gt;
&lt;pre&gt;
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;A股闯关MSCI再度失败 索罗斯们押注对冲胜出&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/06/BPJ4T69300253B0H.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;湖北副省长担心房价下跌：泡沫若破裂后果很严重&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/08/BPJBM36U00252G50.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;马云:假货质量超过正品 打假很复杂&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/08/BPJAIOVI00253G87.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;A股闯关未成功 纳入MSCI新兴市场指数被延迟&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/07/BPJ7260D00252G50.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;马云称许多假货比真品好 网友:怪不得淘宝假货多&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/08/BPJC437N002526O3.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;贪官示意家人低价买地 拆迁后获赔近亿元&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/08/BPJAT58400252G50.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;又是毒胶囊:浙江查获1亿多粒毒胶囊 6人被捕&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/07/BPJ8NMRG00253B0H.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;还不起了？委内瑞拉寻求宽限1年偿还中国贷款&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/07/BPJ9IH3400252C1E.html&quot;}
{&quot;category&quot;: &quot;财经&quot;, &quot;url&quot;: &quot;http://money.163.com/special/002526BH/rank.html&quot;, &quot;secondary_title&quot;: &quot;A股频现清仓式减持 上半年十大减持王曝光&quot;, &quot;secondary_url&quot;: &quot;http://money.163.com/16/0615/07/BPJ7Q9BC00254IU4.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;《装X购车指南》 30-50万都能买到啥车？&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0615/07/BPJ6U1J900084TUP.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;看挡杆还以为是A8L 新款哈弗H9内饰曝光&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0615/00/BPIGTP4B00084TUO.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;前脸/尾灯有变 新款捷达搭1.5L油耗更低&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0615/00/BPIGMEHE00084TUO.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;主打车型不超10万良心价 远景SUV将8月上市&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0615/00/BPIHR2A500084TUO.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;Macan并不是我真姓 众泰SR8搭2.0T/D&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0613/00/BPDBPB0J00084TUO.html&quot;}
{&quot;category&quot;: &quot;汽车&quot;, &quot;url&quot;: &quot;http://news.163.com/special/0001386F/rank_auto.html&quot;, &quot;secondary_title&quot;: &quot;上海福特翼搏优惠1.5万元&quot;, &quot;secondary_url&quot;: &quot;http://auto.163.com/16/0615/00/BPIHH8FF000857M6.html&quot;}
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/Crawler/tree/master/news_scrapy&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;完整代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/php_fly/article/details/19571121&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/php_fly/article/details/19571121&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.jianshu.com/p/078ad2067419&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.jianshu.com/p/078ad2067419&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&amp;gt; 定义目标（Items）–&amp;gt; 制作爬虫（Spider）–&amp;gt; 存储结果（Pipeline）&lt;br&gt;
    
    </summary>
    
      <category term="Programming language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="Crawler" scheme="http://yoursite.com/tags/Crawler/"/>
    
  </entry>
  
  <entry>
    <title>爬虫总结（一）</title>
    <link href="http://yoursite.com/2016/06/11/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2016/06/11/爬虫总结（一）/</id>
    <published>2016-06-11T06:35:48.000Z</published>
    <updated>2016-06-16T13:47:45.000Z</updated>
    
    <content type="html">&lt;p&gt;爬虫在平时也经常用，但一直没有系统的总结过，其实它涉及了许多的知识点。这一系列会理一遍这些知识点，不求详尽，只希望以点带面构建一个爬虫的知识框架。这一篇是概念性解释以及入门级爬虫介绍（以爬取网易新闻为例）。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;爬虫基础&quot;&gt;&lt;a href=&quot;#爬虫基础&quot; class=&quot;headerlink&quot; title=&quot;爬虫基础&quot;&gt;&lt;/a&gt;爬虫基础&lt;/h1&gt;&lt;h2 id=&quot;什么是爬虫&quot;&gt;&lt;a href=&quot;#什么是爬虫&quot; class=&quot;headerlink&quot; title=&quot;什么是爬虫&quot;&gt;&lt;/a&gt;什么是爬虫&lt;/h2&gt;&lt;p&gt;爬虫说白了其实就是获取资源的程序。制作爬虫的总体分三步：爬－取－存。首先要获取整个网页的所有内容，然后再取出其中对你有用的部分，最后再保存🏊的部分。&lt;/p&gt;
&lt;h2 id=&quot;爬虫类型&quot;&gt;&lt;a href=&quot;#爬虫类型&quot; class=&quot;headerlink&quot; title=&quot;爬虫类型&quot;&gt;&lt;/a&gt;爬虫类型&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;网络爬虫&lt;br&gt;网络爬虫，是一种按照一定的规则，&lt;strong&gt;自动的&lt;/strong&gt; 抓取万维网信息的程序或者脚本。网络爬虫是搜索引擎系统中十分重要的组成部分，爬取的网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，其性能的优劣直接影响着搜索引擎的效果。&lt;/li&gt;
&lt;li&gt;传统爬虫&lt;br&gt;从一个或若干初始网页的URL开始，获得初始网页的URL，在抓取网页过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;工作原理&quot;&gt;&lt;a href=&quot;#工作原理&quot; class=&quot;headerlink&quot; title=&quot;工作原理&quot;&gt;&lt;/a&gt;工作原理&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;根据一定的网页分析算法过滤与主题无关的链接，保留有用链接并将其放入等待抓取的URL队列&lt;/li&gt;
&lt;li&gt;根据一定的搜索策略从队列中选择下一步要抓取的网页URL，重复上述过程，直到达到指定条件才结束爬取&lt;/li&gt;
&lt;li&gt;对所有抓取的网页进行一定的分析、过滤，并建立索引，以便之后的查询和检索。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;爬取策略&quot;&gt;&lt;a href=&quot;#爬取策略&quot; class=&quot;headerlink&quot; title=&quot;爬取策略&quot;&gt;&lt;/a&gt;爬取策略&lt;/h2&gt;&lt;h3 id=&quot;广度优先&quot;&gt;&lt;a href=&quot;#广度优先&quot; class=&quot;headerlink&quot; title=&quot;广度优先&quot;&gt;&lt;/a&gt;广度优先&lt;/h3&gt;&lt;p&gt;完成当前层次的搜索后才进行下一层次的搜索。一般的使用策略，一般通过队列来实现。&lt;/p&gt;
&lt;h3 id=&quot;最佳优先&quot;&gt;&lt;a href=&quot;#最佳优先&quot; class=&quot;headerlink&quot; title=&quot;最佳优先&quot;&gt;&lt;/a&gt;最佳优先&lt;/h3&gt;&lt;p&gt;会有评估算法，凡是被算法评估为有用的网页，先来爬取。&lt;/p&gt;
&lt;h3 id=&quot;深度优先&quot;&gt;&lt;a href=&quot;#深度优先&quot; class=&quot;headerlink&quot; title=&quot;深度优先&quot;&gt;&lt;/a&gt;深度优先&lt;/h3&gt;&lt;p&gt;实际应用很少。可能会导致trapped问题。通过栈来实现。&lt;/p&gt;
&lt;h2 id=&quot;URL（-Uniform-Resource-Locator-统一资源定位符）&quot;&gt;&lt;a href=&quot;#URL（-Uniform-Resource-Locator-统一资源定位符）&quot; class=&quot;headerlink&quot; title=&quot;URL（ Uniform Resource Locator: 统一资源定位符）&quot;&gt;&lt;/a&gt;URL（ Uniform Resource Locator: 统一资源定位符）&lt;/h2&gt;&lt;p&gt;互联网上资源均有其唯一的地址，由三部分组成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模式/协议&lt;/li&gt;
&lt;li&gt;文件所在IP地址及端口号&lt;/li&gt;
&lt;li&gt;主机上的资源位置&lt;/li&gt;
&lt;li&gt;例子：&lt;a href=&quot;http://www.example.com/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.example.com/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Web-Server／Socket如何建立连接和传输数据的&quot;&gt;&lt;a href=&quot;#Web-Server／Socket如何建立连接和传输数据的&quot; class=&quot;headerlink&quot; title=&quot;Web Server／Socket如何建立连接和传输数据的&quot;&gt;&lt;/a&gt;Web Server／Socket如何建立连接和传输数据的&lt;/h2&gt;&lt;p&gt;web server 的工作过程其实和打电话的过程差不多（买电话–&amp;gt;注册号码–&amp;gt;监听–&amp;gt;排队接听–&amp;gt;读写–&amp;gt;关闭），经典的三步握手（有人在吗？我在呢，你呢？我也在）在排队接听时进行。下面一张图足以解释一切。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/webserver.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Crawler端需要一个socket接口，向服务器端发起connect请求，完成连接后就可以和服务器交流了，操作完毕会关闭socket接口。服务器端更复杂一点，也需要一个socket接口，并且这个socket接口需要绑定一个地址（bind()），这就相当于有一个固定的电话号码，这样其他人拨打这个号码就可以找到这个服务器。绑定之后服务器的socket就开始监听（listen()）有没有用户请求，如果有，就接收请求（accept()），和用户建立连接，然后就可以交流。&lt;/p&gt;
&lt;h2 id=&quot;HTML-DOM&quot;&gt;&lt;a href=&quot;#HTML-DOM&quot; class=&quot;headerlink&quot; title=&quot;HTML DOM&quot;&gt;&lt;/a&gt;HTML DOM&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;DOM 将 HTML 文档表达为树结构&lt;/li&gt;
&lt;li&gt;定义了访问和操作 HTML 文档的标准&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.35.09.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Cookie&quot;&gt;&lt;a href=&quot;#Cookie&quot; class=&quot;headerlink&quot; title=&quot;Cookie&quot;&gt;&lt;/a&gt;Cookie&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;由服务器端生成，发送给 User-Agent(一般是浏览器)，浏览器会将 Cookie 的 key/value 保存到某个目录下的文本文件哪，下次访问同一网站时就发送该 Cookie 给服务器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;HTTP&quot;&gt;&lt;a href=&quot;#HTTP&quot; class=&quot;headerlink&quot; title=&quot;HTTP&quot;&gt;&lt;/a&gt;HTTP&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;GET 直接以链接形式访问，链接中包含了所有的参数&lt;/li&gt;
&lt;li&gt;PUT 把提交的数据放到 HTTP 包的包体中&lt;pre&gt;
eg.
import urllib
import urllib2
url=&#39;http://www.zhihu.com/#signin&#39;
user_agent=&#39;MOZILLA/5.0&#39;
values={&#39;username&#39;:&#39;252618408@qq.com&#39;,&#39;password&#39;:&#39;xxx&#39;}
headers={&#39;User-Agent&#39;:user_agent}
data=urllib.urlencode(values) # urlencode 是 urllib 独有的方法
request=urllib2.Request(url,data,headers) # write a letter
response=urllib2.urlopen(request) # send the letter and get the reply
page=response.read() # read the reply
&lt;/pre&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;urllib 仅可以接受 URL，这意味着你不可以伪装你的 User Agent 字符串等，但 urllib 提供了 urlencode 方法用来GET查询字符串等产生，而 urllib2 没有。&lt;br&gt;因此 urllib, urllib2经常一起使用。&lt;/p&gt;
&lt;h3 id=&quot;Headers-设置&quot;&gt;&lt;a href=&quot;#Headers-设置&quot; class=&quot;headerlink&quot; title=&quot;Headers 设置&quot;&gt;&lt;/a&gt;Headers 设置&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;User-Agent: 部分服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求&lt;/li&gt;
&lt;li&gt;Content-Type: 使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析&lt;/li&gt;
&lt;li&gt;application/xml: 在 XMl RPC, 如 RESTful/SOAP 调用时使用&lt;/li&gt;
&lt;li&gt;application/json: 在 JSON RPC 调用时使用&lt;/li&gt;
&lt;li&gt;application/x-www-form-urlencoded: 浏览器提交 Web 表单时使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;最简单的爬虫&quot;&gt;&lt;a href=&quot;#最简单的爬虫&quot; class=&quot;headerlink&quot; title=&quot;最简单的爬虫&quot;&gt;&lt;/a&gt;最简单的爬虫&lt;/h1&gt;&lt;h2 id=&quot;requests-库&quot;&gt;&lt;a href=&quot;#requests-库&quot; class=&quot;headerlink&quot; title=&quot;requests 库&quot;&gt;&lt;/a&gt;requests 库&lt;/h2&gt;&lt;pre&gt;
import requests
url = &quot;http://shuang0420.github.io/&quot;
r = requests.get(url)
&lt;/pre&gt;

&lt;h2 id=&quot;urllib2-库&quot;&gt;&lt;a href=&quot;#urllib2-库&quot; class=&quot;headerlink&quot; title=&quot;urllib2 库&quot;&gt;&lt;/a&gt;urllib2 库&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;import urllib2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# request source file&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;url = &amp;quot;http://shuang0420.github.io/&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;request = urllib2.Request(url)  # write a letter&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;response = urllib2.urlopen(request)  # send the letter and get the reply&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;page = response.read()  # read the reply&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# save source file&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile = open(&amp;apos;webPage.html&amp;apos;, &amp;apos;wb&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile.write(page)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;webFile.close()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这是一个简单的爬虫，打开 webPage.html 是这样的显示，没有css.&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%881.44.20.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;实例：爬取网易新闻&quot;&gt;&lt;a href=&quot;#实例：爬取网易新闻&quot; class=&quot;headerlink&quot; title=&quot;实例：爬取网易新闻&quot;&gt;&lt;/a&gt;实例：爬取网易新闻&lt;/h1&gt;&lt;p&gt;爬取网易新闻 [代码示例]&lt;br&gt;– 使用 urllib2 的 requests包来爬取页面&lt;br&gt;– 使用正则表达式和 bs4 分析一级页面,使用 Xpath 来分析二级页面&lt;br&gt;– 将得到的标题和链接,保存为本地文件&lt;/p&gt;
&lt;h2 id=&quot;分析初始页面&quot;&gt;&lt;a href=&quot;#分析初始页面&quot; class=&quot;headerlink&quot; title=&quot;分析初始页面&quot;&gt;&lt;/a&gt;分析初始页面&lt;/h2&gt;&lt;p&gt;我们的初始页面是 &lt;a href=&quot;http://news.163.com/rank&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://news.163.com/rank&lt;/a&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.08.54.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看源代码&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.10.28.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们想要的是分类标题和URL，需要解析 DOM 文档树,这里使用了 BeautifulSoup 里的方法。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;def Nav_Info(myPage):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # 二级导航的标题和页面&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    pageInfo = re.findall(r&amp;apos;&amp;lt;div class=&amp;quot;subNav&amp;quot;&amp;gt;.*?&amp;lt;div class=&amp;quot;area areabg1&amp;quot;&amp;gt;&amp;apos;, myPage, re.S)[&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        0].replace(&amp;apos;&amp;lt;div class=&amp;quot;subNav&amp;quot;&amp;gt;&amp;apos;, &amp;apos;&amp;apos;).replace(&amp;apos;&amp;lt;div class=&amp;quot;area areabg1&amp;quot;&amp;gt;&amp;apos;, &amp;apos;&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    soup = BeautifulSoup(pageInfo, &amp;quot;lxml&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tags = soup(&amp;apos;a&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    topics = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    for tag in tags:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 只要 科技、财经、体育 的新闻&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # if (tag.string==&amp;apos;科技&amp;apos; or tag.string==&amp;apos;财经&amp;apos; or tag.string==&amp;apos;体育&amp;apos;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        topics.append((tag.string, tag.get(&amp;apos;href&amp;apos;, None)))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    return topics&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;然而，Beautiful Soup对文档的解析速度不会比它所依赖的解析器更快,如果对计算时间要求很高或者计算机的时间比程序员的时间更值钱,那么就应该直接使用 lxml。换句话说,还有提高Beautiful Soup效率的办法,使用lxml作为解析器。Beautiful Soup用lxml做解析器比用html5lib或Python内置解析器速度快很多。bs4 的默认解析器是 html.parser，使用lxml的代码如下：&lt;/p&gt;
&lt;pre&gt;BeautifulSoup(markup, &quot;lxml&quot;)&lt;/pre&gt;

&lt;h2 id=&quot;分析二级页面&quot;&gt;&lt;a href=&quot;#分析二级页面&quot; class=&quot;headerlink&quot; title=&quot;分析二级页面&quot;&gt;&lt;/a&gt;分析二级页面&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.14.06.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;查看源代码&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.11.23.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们要爬取的是&lt;td&gt;&lt;/td&gt;之间的新闻标题和链接，同样需要解析文档树，可以通过以下代码实现，这里用了 lxml 解析器，效率更高。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;def News_Info(newPage):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # xpath 使用路径表达式来选取文档中的节点或节点集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    dom = etree.HTML(newPage)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    news_titles = dom.xpath(&amp;apos;//tr/td/a/text()&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    news_urls = dom.xpath(&amp;apos;//tr/td/a/@href&amp;apos;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    return zip(news_titles, news_urls)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/Crawler/blob/master/news.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;完整代码&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;潜在问题&quot;&gt;&lt;a href=&quot;#潜在问题&quot; class=&quot;headerlink&quot; title=&quot;潜在问题&quot;&gt;&lt;/a&gt;潜在问题&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;我们的任务是爬取1万个网页，按上面这个程序，耗费时间长，我们可以考虑开启多个线程(池)去一起爬取，或者用分布式架构去并发的爬取网页。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;种子URL和后续解析到的URL都放在一个列表里，我们应该设计一个更合理的数据结构来存放这些待爬取的URL才是，比如队列或者优先队列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对各个网站的url，我们一视同仁，事实上，我们应当区别对待。大站好站优先原则应当予以考虑。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;每次发起请求，我们都是根据url发起请求，而这个过程中会牵涉到DNS解析，将url转换成ip地址。一个网站通常由成千上万的URL，因此，我们可以考虑将这些网站域名的IP地址进行缓存，避免每次都发起DNS请求，费时费力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解析到网页中的urls后，我们没有做任何去重处理，全部放入待爬取的列表中。事实上，可能有很多链接是重复的，我们做了很多重复劳动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;爬虫被封禁问题&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;优化方案&quot;&gt;&lt;a href=&quot;#优化方案&quot; class=&quot;headerlink&quot; title=&quot;优化方案&quot;&gt;&lt;/a&gt;优化方案&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;并行爬取问题&lt;/p&gt;
&lt;p&gt; 关于并行爬取，首先我们想到的是多线程或者线程池方式，一个爬虫程序内部开启多个线程。同一台机器开启多个爬虫程序，这样，我们就有N多爬取线程在同时工作，大大提高了效率。&lt;/p&gt;
&lt;p&gt; 当然，如果我们要爬取的任务特别多，一台机器、一个网点肯定是不够的，我们必须考虑分布式爬虫。分布式架构，考虑的问题有很多，我们需要一个scheduler来分配任务并排序，各个爬虫之间还需要通信合作，共同完成任务，不要重复爬取相同的网页。分配任务时我们还需要考虑负载均衡以做到公平。（可以通过Hash，比如根据网站域名进行hash）&lt;/p&gt;
&lt;p&gt; 负载均衡分派完任务之后，千万不要以为万事大吉了，万一哪台机器挂了呢？原先指派给挂掉的哪台机器的任务指派给谁？又或者哪天要增加几台机器，任务有该如何进行重新分配呢？所以我们还要 task table 来纪录状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;待爬取网页队列&lt;br&gt;如何对待待抓取队列，跟操作系统如何调度进程是类似的场景。&lt;br&gt;不同网站，重要程度不同，因此，可以设计一个优先级队列来存放待爬起的网页链接。如此一来，每次抓取时，我们都优先爬取重要的网页。&lt;br&gt;当然，你也可以效仿操作系统的进程调度策略之多级反馈队列调度算法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DNS缓存&lt;br&gt;为了避免每次都发起DNS查询，我们可以将DNS进行缓存。DNS缓存当然是设计一个hash表来存储已有的域名及其IP。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;网页去重&lt;br&gt;说到网页去重，第一个想到的是垃圾邮件过滤。垃圾邮件过滤一个经典的解决方案是Bloom Filter（布隆过滤器）。布隆过滤器原理简单来说就是：建立一个大的位数组，然后用多个Hash函数对同一个url进行hash得到多个数字，然后将位数组中这些数字对应的位置为1。下次再来一个url时，同样是用多个Hash函数进行hash，得到多个数字，我们只需要判断位数组中这些数字对应的为是全为1，如果全为1，那么说明这个url已经出现过。如此，便完成了url去重的问题。当然，这种方法会有误差，只要误差在我们的容忍范围之类，比如1万个网页，我只爬取到了9999个，并不会有太大的实际影响。&lt;a href=&quot;http://itindex.net/detail/39767-url-%E7%9B%B8%E4%BC%BC-%E8%AE%A1%E7%AE%97&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;url相似度计算&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据存储的问题&lt;br&gt;数据存储同样是个很有技术含量的问题。用关系数据库存取还是用NoSQL，抑或是自己设计特定的文件格式进行存储，都大有文章可做。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程间通信&lt;br&gt;分布式爬虫，就必然离不开进程间的通信。我们可以以规定的数据格式进行数据交互，完成进程间通信。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;反爬虫机制问题&lt;br&gt;针对反爬虫机制，我们可以通过轮换IP地址、轮换Cookie、修改用户代理(User Agent)、限制速度、避免重复性爬行模式等方法解决。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;参考链接:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3NDM1NjUwMQ==&amp;amp;mid=2650486783&amp;amp;idx=2&amp;amp;sn=b022421936afb373f4a00f497396220d&amp;amp;scene=0#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://mp.weixin.qq.com/s?__biz=MzA3NDM1NjUwMQ==&amp;amp;mid=2650486783&amp;amp;idx=2&amp;amp;sn=b022421936afb373f4a00f497396220d&amp;amp;scene=0#wechat_redirect&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.chinahadoop.cn/course/596/learn#lesson/11986&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.chinahadoop.cn/course/596/learn#lesson/11986&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;爬虫在平时也经常用，但一直没有系统的总结过，其实它涉及了许多的知识点。这一系列会理一遍这些知识点，不求详尽，只希望以点带面构建一个爬虫的知识框架。这一篇是概念性解释以及入门级爬虫介绍（以爬取网易新闻为例）。&lt;br&gt;
    
    </summary>
    
      <category term="Programming language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="Crawler" scheme="http://yoursite.com/tags/Crawler/"/>
    
  </entry>
  
  <entry>
    <title>gensim-doc2vec实战</title>
    <link href="http://yoursite.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/06/01/gensim-doc2vec实战/</id>
    <published>2016-06-01T02:22:06.000Z</published>
    <updated>2016-06-09T12:09:08.000Z</updated>
    
    <content type="html">&lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;导入模块&quot;&gt;&lt;a href=&quot;#导入模块&quot; class=&quot;headerlink&quot; title=&quot;导入模块&quot;&gt;&lt;/a&gt;导入模块&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
import os
import jieba

# logging information
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)
&lt;/pre&gt;

&lt;h3 id=&quot;读取文件&quot;&gt;&lt;a href=&quot;#读取文件&quot; class=&quot;headerlink&quot; title=&quot;读取文件&quot;&gt;&lt;/a&gt;读取文件&lt;/h3&gt;&lt;pre&gt;
# get input file, text format
f = open(&#39;trainingdata.txt&#39;,&#39;r&#39;)
input = f.readlines()
count = len(input)
print count
&lt;/pre&gt;

&lt;h3 id=&quot;文件预处理，分词等&quot;&gt;&lt;a href=&quot;#文件预处理，分词等&quot; class=&quot;headerlink&quot; title=&quot;文件预处理，分词等&quot;&gt;&lt;/a&gt;文件预处理，分词等&lt;/h3&gt;&lt;pre&gt;
# read file and separate words
alldocs=[] # for the sake of check, can be removed
count=0 # for the sake of check, can be removed
for line in input:
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count)) # for the sake of check, can be removed
    count+=1 # for the sake of check, can be removed
&lt;/pre&gt;


&lt;h3 id=&quot;模型选择&quot;&gt;&lt;a href=&quot;#模型选择&quot; class=&quot;headerlink&quot; title=&quot;模型选择&quot;&gt;&lt;/a&gt;模型选择&lt;/h3&gt;&lt;p&gt;gensim Doc2Vec 提供了 DM 和 DBOW 两个模型。gensim 的说明文档建议多次训练数据集并调整学习速率或在每次训练中打乱输入信息的顺序以求获得最佳效果。&lt;/p&gt;
&lt;pre&gt;
# PV-DM w/concatenation - window=5 (both sides) approximates paper&#39;s 10-word total window size
Doc2Vec(sentences,dm=1, dm_concat=1, size=100, window=2, hs=0, min_count=2, workers=cores)
# PV-DBOW  
Doc2Vec(sentences,dm=0, size=100, hs=0, min_count=2, workers=cores)
# PV-DM w/average
Doc2Vec(sentences,dm=1, dm_mean=1, size=100, window=2, hs=0, min_count=2, workers=cores)
&lt;/pre&gt;


&lt;h3 id=&quot;训练并保存模型&quot;&gt;&lt;a href=&quot;#训练并保存模型&quot; class=&quot;headerlink&quot; title=&quot;训练并保存模型&quot;&gt;&lt;/a&gt;训练并保存模型&lt;/h3&gt;&lt;pre&gt;
# train and save the model
sentences= gensim.models.doc2vec.TaggedLineDocument(&#39;output.seq&#39;)
model = gensim.models.Doc2Vec(sentences,size=100, window=3)
model.train(sentences)
model.save(&#39;all_model.txt&#39;)
&lt;/pre&gt;

&lt;h3 id=&quot;保存文档向量&quot;&gt;&lt;a href=&quot;#保存文档向量&quot; class=&quot;headerlink&quot; title=&quot;保存文档向量&quot;&gt;&lt;/a&gt;保存文档向量&lt;/h3&gt;&lt;pre&gt;
# save vectors
out=open(&quot;all_vector.txt&quot;,&quot;wb&quot;)
for num in range(0,count):
    docvec =model.docvecs[num]
    out.write(docvec)
    #print num
    #print docvec
out.close()
&lt;/pre&gt;

&lt;h3 id=&quot;检验-计算训练文档中的文档相似度&quot;&gt;&lt;a href=&quot;#检验-计算训练文档中的文档相似度&quot; class=&quot;headerlink&quot; title=&quot;检验 计算训练文档中的文档相似度&quot;&gt;&lt;/a&gt;检验 计算训练文档中的文档相似度&lt;/h3&gt;&lt;pre&gt;
# test, calculate the similarity
# 注意 docid 是从0开始计数的
# 计算与训练集中第一篇文档最相似的文档
sims = model.docvecs.most_similar(0)
print sims
# get similarity between doc1 and doc2 in the training data
sims = model.docvecs.similarity(1,2)
print sims
&lt;/pre&gt;

&lt;h3 id=&quot;infer向量，比较相似度&quot;&gt;&lt;a href=&quot;#infer向量，比较相似度&quot; class=&quot;headerlink&quot; title=&quot;infer向量，比较相似度&quot;&gt;&lt;/a&gt;infer向量，比较相似度&lt;/h3&gt;&lt;p&gt;下面的代码用于检验模型正确性，随机挑一篇trained dataset中的文档，用模型重新infer，再计算与trained dataset中文档相似度，如果模型良好，相似度第一位应该就是挑出的文档。&lt;/p&gt;
&lt;pre&gt;
# check
#############################################################################
# A good check is to re-infer a vector for a document already in the model. #
# if the model is well-trained,                                             #
# the nearest doc should (usually) be the same document.                    #
#############################################################################

print &#39;examing&#39;
doc_id = np.random.randint(model.docvecs.count)  # pick random doc; re-run cell for more examples
print(&#39;for doc %d...&#39; % doc_id)
inferred_docvec = model.infer_vector(alldocs[doc_id].words)
print(&#39;%s:\n %s&#39; % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))
&lt;/pre&gt;

&lt;h3 id=&quot;遇到的问题&quot;&gt;&lt;a href=&quot;#遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;遇到的问题&quot;&gt;&lt;/a&gt;遇到的问题&lt;/h3&gt;&lt;p&gt;👇两个错误还在探索中，根据官方指南是可以运行的，然而我遇到了错误并没能解决。&lt;br&gt;第一段错误代码，关于train the model&lt;/p&gt;
&lt;pre&gt;
alldocs=[]
count=0
for line in input:
    #print line
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    #output.write(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count))
    count+=1

model = Doc2Vec(alldocs,size=100, window=2, min_count=5, workers=4)
model.train(alldocs)
&lt;/pre&gt;

&lt;p&gt;报错信息&lt;/p&gt;
&lt;pre&gt;
Traceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 59, in &lt;module&gt;
    model = Doc2Vec(alldocs[0],size=100, window=2, min_count=5, workers=4)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 596, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/word2vec.py&quot;, line 508, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 639, in scan_vocab
    document_length = len(document.words)
AttributeError: &#39;generator&#39; object has no attribute &#39;words&#39;
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;第二段错误代码，关于infer&lt;/p&gt;
&lt;pre&gt;
doc_words1=[&#39;验证&#39;,&#39;失败&#39;,&#39;验证码&#39;,&#39;未&#39;,&#39;收到&#39;]
doc_words2=[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;]
# get infered vector
invec1 = model.infer_vector(doc_words1, alpha=0.1, min_alpha=0.0001, steps=5)
invec2 = model.infer_vector(doc_words2, alpha=0.1, min_alpha=0.0001, steps=5)
print invec1
print invec2

# get similarity
# the output docid is supposed to be 0
sims = model.docvecs.most_similar([invec1])
print sims

# according to official guide, the following codes are supposed to be fine, but it fails to run
sims= model.docvecs.similarity(invec1,invec2)
print model.similarity([&#39;今天&#39;,&#39;有&#39;,&#39;啥&#39;,&#39;奖励&#39;],[&#39;今天&#39;,&#39;奖励&#39;,&#39;有&#39;,&#39;哪些&#39;,&#39;呢&#39;])
&lt;/pre&gt;

&lt;p&gt;最后两行代码报错，错误信息&lt;/p&gt;
&lt;pre&gt;
raceback (most recent call last):
  File &quot;d2vTestv5.py&quot;, line 110, in &lt;module&gt;
    sims= model.docvecs.similarity(invec1,invec2)
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 484, in similarity
    return dot(matutils.unitvec(self[d1]), matutils.unitvec(self[d2]))
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
  File &quot;/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py&quot;, line 341, in __getitem__
    return vstack([self[i] for i in index])
TypeError: &#39;numpy.float32&#39; object is not iterable
&lt;/module&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/doc2vec&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/models/doc2vec.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/models/doc2vec.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="doc2vec" scheme="http://yoursite.com/tags/doc2vec/"/>
    
  </entry>
  
  <entry>
    <title>gensim: word2vec实战</title>
    <link href="http://yoursite.com/2016/05/30/gensim-word2vec%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/30/gensim-word2vec实战/</id>
    <published>2016-05-30T03:13:52.000Z</published>
    <updated>2016-06-09T12:11:08.000Z</updated>
    
    <content type="html">&lt;p&gt;介绍如何利用 gensim 库建立简单的 word2vec 模型。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
# -*- coding: utf-8 -*-
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import os
import logging
import jieba
import re
import multiprocessing
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# logging information
logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;)
logging.root.setLevel(level=logging.INFO)

# get input file, text format
inp = sys.argv[1]
input = open(inp, &#39;r&#39;)
output = open(&#39;output.seq&#39;, &#39;w&#39;)

if len(sys.argv) &lt; 2:
    print(globals()[&#39;__doc__&#39;] % locals())
    sys.exit(1)

# read file and separate words
for line in input.readlines():
    line=line.strip(&#39;\n&#39;)
    seg_list = jieba.cut(line)
    output.write(&#39; &#39;.join(seg_list) + &#39;\n&#39;)

output.close()
output= open(&#39;output.seq&#39;, &#39;r&#39;)

# initialize the model
# size = the dimensionality of the feature vectors
# window = the maximum distance between the current and predicted word within a sentence
# min_count = ignore all words with total frequency lower than this.
model = Word2Vec(LineSentence(output), size=100, window=3, min_count=5,workers=multiprocessing.cpu_count())

# save model
model.save(&#39;output.model&#39;)
model.save_word2vec_format(&#39;output.vector&#39;, binary=False)

# test
model=gensim.models.Word2Vec.load(&#39;output.model&#39;)
x = model.most_similar([u&#39;奖励&#39;])
for i in x:
    print &quot;Word: {}\t Similarity: {}&quot;.format(i[0], i[1])
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shuang0420/word2vec_example&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;更多代码&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;介绍如何利用 gensim 库建立简单的 word2vec 模型。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之六 -- 若干源码细节</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%85%AD-%E8%8B%A5%E5%B9%B2%E6%BA%90%E7%A0%81%E7%BB%86%E8%8A%82/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之六-若干源码细节/</id>
    <published>2016-05-29T08:28:23.000Z</published>
    <updated>2016-06-09T12:11:43.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;


&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/61.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/62.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/63.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/64.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/65.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/66.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/67.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/68.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之五 -- 基于 Negative Sampling 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%94-%E5%9F%BA%E4%BA%8E-Negative-Sampling-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之五-基于-Negative-Sampling-的模型/</id>
    <published>2016-05-29T07:22:03.000Z</published>
    <updated>2016-06-09T12:11:34.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/51.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/52.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/53.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/54.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/55.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/56.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/57.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/58.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/59.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/510.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/511.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之四 -- 基于Hierarchical Softmax 的模型</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8EHierarchical-Softmax-%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之四-基于Hierarchical-Softmax-的模型/</id>
    <published>2016-05-29T06:08:03.000Z</published>
    <updated>2016-06-09T12:12:19.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/6.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/7.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/8.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/10.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/1.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/2.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/3.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之三 -- 背景知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%89-%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之三-背景知识/</id>
    <published>2016-05-29T06:04:41.000Z</published>
    <updated>2016-06-09T12:11:51.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/31.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/32.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/33.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/34.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/35.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/36.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/37.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/38.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/39.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/310.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/311.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之二 -- 预备知识</title>
    <link href="http://yoursite.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2016/05/29/word2vec详解之二-预备知识/</id>
    <published>2016-05-29T04:21:21.000Z</published>
    <updated>2016-06-09T12:12:13.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/21.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/22.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/23.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/24.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/25.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/26.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/27.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>word2vec详解之一 -- 目录和前言</title>
    <link href="http://yoursite.com/2016/05/28/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%80-%E7%9B%AE%E5%BD%95%E5%92%8C%E5%89%8D%E8%A8%80/"/>
    <id>http://yoursite.com/2016/05/28/word2vec详解之一-目录和前言/</id>
    <published>2016-05-28T06:33:10.000Z</published>
    <updated>2016-06-09T12:11:23.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;hr&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/11.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/12.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/13.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/14.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/15.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;作者: peghoty&lt;br&gt;出处: &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/37969979&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/itplus/article/details/37969979&lt;/a&gt;&lt;br&gt;欢迎转载/分享, 但请务必声明文章出处.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>PHP连接数据库js可视化数据</title>
    <link href="http://yoursite.com/2016/05/26/PHP%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93js%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2016/05/26/PHP连接数据库js可视化数据/</id>
    <published>2016-05-26T02:09:56.000Z</published>
    <updated>2016-05-26T02:11:07.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>短句归一化--LSI模型</title>
    <link href="http://yoursite.com/2016/05/25/%E7%9F%AD%E9%97%AE%E9%A2%98%E5%BD%92%E4%B8%80%E5%8C%96-LSI%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/25/短问题归一化-LSI模型/</id>
    <published>2016-05-25T12:59:36.000Z</published>
    <updated>2016-06-09T12:12:01.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;潜在语义索引是一种用奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以可以从一篇文章中提取术语关系，从而建立起主要概念内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;降维过程&quot;&gt;&lt;a href=&quot;#降维过程&quot; class=&quot;headerlink&quot; title=&quot;降维过程&quot;&gt;&lt;/a&gt;降维过程&lt;/h3&gt;&lt;p&gt;将文档库表示成VSM模型的词-文档矩阵Am×n(词-文档矩阵那就是词作为行，文档作为列，这是矩阵先行后列的表示决定的，当然如果表示成文档-词矩阵的话，后面的计算就要用该矩阵的转置了),其中m表示文档库中包含的所有不同的词的个(行数是不同词的个数)，即行向量表示一个词在不同文档出现的次数，n 表示文档库中的文档数(列数是不同文档的个数)，即列向量表示的是不同的文档.A表示为A = [α ij ],在此矩阵中 ,α ij为非负值 , 表示第 i 个词在第j 个文档中出现的频度。显然，A是稀疏矩阵(这是VSM和文档决定的)。&lt;/p&gt;
&lt;p&gt;利用奇异值分解SVD(Singular Value Decomposition)求A的只有K个正交因子的降秩矩阵，该过程就是降维的过程。SVD的重要作用是把词和文档映射到同一个语义空间中，将词和文档表示为K个因子的形式。显然，这会丢失信息，但主要的信息却被保留了。为什么该过程可以降维呢？因为该过程解决了同义和多义现象。可以看出，K的取值对整个分类结果的影响很大。因为，K过小，则丢失信息就越多；K过大，信息虽然多，但可能有冗余且计算消耗大。K的选择也是值得研究的，不过一般取值为100-300，不绝对。&lt;/p&gt;
&lt;h3 id=&quot;适用性&quot;&gt;&lt;a href=&quot;#适用性&quot; class=&quot;headerlink&quot; title=&quot;适用性&quot;&gt;&lt;/a&gt;适用性&lt;/h3&gt;&lt;p&gt;对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;特征降维&lt;br&gt;LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。TFIDF是另一个通用的降维方法，通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF。&lt;br&gt;不过必须提醒的是，无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;单词相关度计算&lt;br&gt;LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;聚类&lt;br&gt;直接用 LSI 聚类的情景还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;实验说明&quot;&gt;&lt;a href=&quot;#实验说明&quot; class=&quot;headerlink&quot; title=&quot;实验说明&quot;&gt;&lt;/a&gt;实验说明&lt;/h3&gt;&lt;p&gt;用了python的gensim包&lt;br&gt;现有的数据是438条标准问题以及3300条人工问题（可以转化为438条标准问题），现在需要对人工问题做一个归一化。&lt;br&gt;这里采用LSI模型进行建模实验，步骤如下。&lt;/p&gt;
&lt;h3 id=&quot;导入包&quot;&gt;&lt;a href=&quot;#导入包&quot; class=&quot;headerlink&quot; title=&quot;导入包&quot;&gt;&lt;/a&gt;导入包&lt;/h3&gt;&lt;pre&gt;
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import logging
import jieba
import jieba.posseg as pseg
# 防止乱码
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)
# 打印log信息
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)&lt;/pre&gt;


&lt;h3 id=&quot;文本预处理&quot;&gt;&lt;a href=&quot;#文本预处理&quot; class=&quot;headerlink&quot; title=&quot;文本预处理&quot;&gt;&lt;/a&gt;文本预处理&lt;/h3&gt;&lt;pre&gt;
# 标准FAQ，一行对应一条问句
f = open(&#39;FAQuniq.txt&#39;, &#39;r&#39;)
# 对问句进行分词
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in f]

# 抽取一个bag-of-words，将文档的token映射为id
dictionary = corpora.Dictionary(texts)
# 保存词典
dictionary.save(&#39;LSI.dict&#39;)

# 产生文档向量，将用字符串表示的文档转换为用id和词频表示的文档向量
corpus = [dictionary.doc2bow(text) for text in texts]

# 基于这些“训练文档”计算一个TF-IDF模型
tfidf = models.TfidfModel(corpus)

# 转化文档向量，将用词频表示的文档向量表示为一个用tf-idf值表示的文档向量
corpus_tfidf = tfidf[corpus]

# 训练LSI模型 即将训练文档向量组成的矩阵SVD分解，并做一个秩为2的近似SVD分解
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)

# 保存模型
lsi.save(&#39;LSI.pkl&#39;)
lsi.print_topics(20)&lt;/pre&gt;


&lt;h3 id=&quot;初始化验证performance的文件&quot;&gt;&lt;a href=&quot;#初始化验证performance的文件&quot; class=&quot;headerlink&quot; title=&quot;初始化验证performance的文件&quot;&gt;&lt;/a&gt;初始化验证performance的文件&lt;/h3&gt;&lt;p&gt;checkFile的每行格式为：&lt;/p&gt;
&lt;pre&gt;原始问题的docid：对应的标准问题的topicid&lt;/pre&gt;

&lt;p&gt;把它存到checkDict这个dictionary中，key是docid，value是topicid。&lt;/p&gt;
&lt;pre&gt;
checkDict=dict()
def getCheckId():
    fcheck=open(&#39;checkFile.txt&#39;)
    for line in fcheck:
        line=line.strip(&#39;\n&#39;)
        if (len(line)==0):
            continue
        docid=line.split(&quot;:&quot;)[0]
        topicid=line.split(&quot;:&quot;)[1]
        checkDict[int(docid)]=int(topicid)
getCheckId()&lt;/pre&gt;

&lt;h3 id=&quot;归一化／计算文档相似度&quot;&gt;&lt;a href=&quot;#归一化／计算文档相似度&quot; class=&quot;headerlink&quot; title=&quot;归一化／计算文档相似度&quot;&gt;&lt;/a&gt;归一化／计算文档相似度&lt;/h3&gt;&lt;pre&gt;
# 建索引
index = similarities.MatrixSimilarity(lsi[corpus])

# 初始化分数
score1=0
score2=0
score3=0

# 读取文件，文件的每行格式为一个原始问句
f2=open(&#39;ORIFAQ3330.txt&#39;,&#39;r&#39;)
# count的作用是和checkFile的docid，即checkDict的key对应
count=1
for query in f2:
    # 获取该原始问句本应对应的正确标准问句
    if (not checkDict.has_key(count)):
        count+=1
        continue
    checkId=checkDict[count]
    # 将问句向量化
    query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
    # 再用之前训练好的LSI模型将其映射到二维的topic空间：
    query_lsi = lsi[query_bow]
    # 计算其和index中doc的余弦相似度了：
    sims = index[query_lsi]
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    # 找出最相关的三篇文档，计算这三篇文档是否包括标准问句，如果文档就是标准问句，对应的分数加1
    if (checkId==sort_sims[0][0]):
        score1+=1
    elif (checkId==sort_sims[1][0]):
        score2+=1
    elif (checkId==sort_sims[2][0]):
        score3+=1
    count+=1&lt;/pre&gt;

&lt;h3 id=&quot;打印分数&quot;&gt;&lt;a href=&quot;#打印分数&quot; class=&quot;headerlink&quot; title=&quot;打印分数&quot;&gt;&lt;/a&gt;打印分数&lt;/h3&gt;&lt;pre&gt;
print &quot;Score1: &quot;.format(score1*1.0/count)
print &quot;Score2: &quot;.format(score2*1.0/count)
print &quot;Score3: &quot;.format(score3*1.0/count)&lt;/pre&gt;

&lt;h3 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h3&gt;&lt;p&gt;其实这里的结果非常差，原因是文档（每一条问句）太短，只有十几个字，另外文档数太少，LSI降维牺牲了准确率，下一个实验LDA的准确率相比会高很多。&lt;br&gt;另外，本次实验所用的样本分布并不均匀，“未收到奖励”类似问题出现的频率比“软件无声音”类似问题出现的频率要高很多。&lt;strong&gt;&lt;em&gt;重申：LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;LSI-缺陷&quot;&gt;&lt;a href=&quot;#LSI-缺陷&quot; class=&quot;headerlink&quot; title=&quot;LSI 缺陷&quot;&gt;&lt;/a&gt;LSI 缺陷&lt;/h3&gt;&lt;p&gt;常用的VSM文本表示模型中有两个主要的缺陷：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;该模型假设所有特征词条之间是相互独立、互不影响的（朴素贝叶斯也是这个思想），即该模型还是基于“词袋”模型（应该说所有利用VSM模型没有进行潜在语义分析的算法都是基于“词袋”假设）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;没有进行特征降维，特征维数可能会很高，向量空间可能很大，对存储和计算资源要求会比较高。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;LSI的基本思想是文本中的词与词之间不是孤立的，存在着某种潜在的语义关系，通过对样本数据的统计分析，让机器自动挖掘出这些潜在的语义关系，并把这些关系表示成计算机可以”理解”的模型。它可以消除词匹配过程中的同义和多义现象。它可以将传统的VSM降秩到一个低维的语义空间中，在该语义空间中计算文档的相似度等。总的说来，LSI就是利用词的语义关系对VSM模型进行降维，并提高分类的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.zwbk.org/MyLemmaShow.aspx?lid=257113&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;LSI-理解&quot;&gt;&lt;a href=&quot;#LSI-理解&quot; class=&quot;headerlink&quot; title=&quot;LSI 理解&quot;&gt;&lt;/a&gt;LSI 理解&lt;/h3&gt;&lt;p&gt;LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>GibbsLDA++: A C/C++ 使用心得</title>
    <link href="http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97/"/>
    <id>http://yoursite.com/2016/05/25/GibbsLDA-A-C-C-使用心得/</id>
    <published>2016-05-25T04:20:35.000Z</published>
    <updated>2016-05-25T04:20:35.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>在c里调用python</title>
    <link href="http://yoursite.com/2016/05/22/%E5%9C%A8c%E9%87%8C%E8%B0%83%E7%94%A8python/"/>
    <id>http://yoursite.com/2016/05/22/在c里调用python/</id>
    <published>2016-05-22T08:55:07.000Z</published>
    <updated>2016-06-09T12:08:57.000Z</updated>
    
    <content type="html">&lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;pre&gt;
#include &lt;python2.7 python.h=&quot;&quot;&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
void test1(){
  Py_Initialize();//初始化python
  char *test = &quot;奖励&quot;;
  PyObject * pModule = NULL;
  PyObject * pModule1 = NULL;
  PyObject * pFunc = NULL;
  PyObject * pArg    = NULL;
  PyObject * result;
  pModule = PyImport_ImportModule(&quot;inferSingleDocVec&quot;);//引入模块
  pFunc = PyObject_GetAttrString(pModule, &quot;getDocVec&quot;);//直接获取模块中的函数
  pArg= Py_BuildValue(&quot;(s)&quot;, test);
  result = PyEval_CallObject(pFunc, pArg); //调用直接获得的函数，并传递参数；这里得到的是一个list
  &lt;code&gt;for (int i = 0; i &lt; PyList_Size(result); i++) {&lt;/code&gt;
    printf(&quot;%f\t&quot;, PyFloat_AsDouble(PyList_GetItem(result, (Py_ssize_t)i)));//打印每一个元素
  }
  //下面代码适用于返回值为字符串的情况
  //char* s=NULL;
  //PyArg_Parse(result, &quot;s&quot;, &amp;s);
  //for (int i=0;s[i]!=&#39;\0&#39;;i++){
   // printf(&quot;%c&quot;,s[i]);
 // }
  Py_Finalize(); //释放python
//  return;
}
int main(int argc, char* argv[])
{
    test1();
    return 0;
}
&lt;/stdlib.h&gt;&lt;/stdio.h&gt;&lt;/python2.7&gt;&lt;/pre&gt;

&lt;p&gt;编译运行&lt;/p&gt;
&lt;pre&gt;
$ gcc -I/usr/local/lib/python2.7.11 -o inferDocVec inferDocVec.c -lpython2.7
$ ./inferDocVec
&lt;/pre&gt;

&lt;p&gt;调用的inferSingleDocVec文件&lt;/p&gt;
&lt;pre&gt;
#!/usr/bin/python
# -*- coding: utf-8 -*-
### for infer
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
import gensim, logging
from gensim.models import Doc2Vec
import os
import jieba
import multiprocessing
import numpy as np
import base64
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

def getDocVec(doc_words):
    docwords=[word for word in jieba.cut(doc_words, cut_all = False)]
    model = Doc2Vec.load(&#39;all_model_v2.txt&#39;)
    invec = model.infer_vector(docwords, alpha=0.1, min_alpha=0.0001, steps=5)
    return (list)(invec)
&lt;/pre&gt;

&lt;p&gt;关于如何将python文件转为模块，详见之前的一篇博文&lt;a href=&quot;https://github.com/Shuang0420/Shuang0420.github.io/wiki/python----%E5%B0%86%E8%87%AA%E5%B7%B1%E5%86%99%E7%9A%84py%E6%96%87%E4%BB%B6%E4%BD%9C%E4%B8%BA%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python 将自己写的py文件作为模块导入&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。&lt;/p&gt;
    
    </summary>
    
      <category term="Programming language" scheme="http://yoursite.com/categories/Programming-language/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="c" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>AP聚类</title>
    <link href="http://yoursite.com/2016/05/19/AP%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2016/05/19/AP聚类/</id>
    <published>2016-05-19T13:42:04.000Z</published>
    <updated>2016-06-09T12:12:25.000Z</updated>
    
    <content type="html">&lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Affinity Propagation (AP) 聚类是最近在Science杂志上提出的一种新的聚类算法。它根据N个数据点之间的相似度进行聚类,这些相似度可以是对称的,即两个数据点互相之间的相似度一样(如欧氏距离);也可以是不对称的,即两个数据点互相之间的相似度不等。这些相似度组成N×N的相似度矩阵S(其中N为有N个数据点)。AP算法不需要事先指定聚类数目,相反它将所有的数据点都作为潜在的聚类中心,称之为exemplar。以S矩阵的对角线上的数值s(k, k)作为k点能否成为聚类中心的评判标准,这意味着该值越大,这个点成为聚类中心的可能性也就越大,这个值又称作参考度p (preference)。&lt;br&gt;在这里介绍几个文中常出现的名词：&lt;br&gt;exemplar：指的是聚类中心。&lt;br&gt;similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，如－|| ||。文中，所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。&lt;br&gt;preference：数据点i的参考度称为P(i)或S(i,i)。是指点i作为聚类中心的参考度。一般取S相似度值的中值。&lt;br&gt;Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度。&lt;br&gt;Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度。&lt;/p&gt;
&lt;p&gt;Script output:&lt;br&gt;Estimated number of clusters: 3&lt;br&gt;Homogeneity: 0.872&lt;br&gt;Completeness: 0.872&lt;br&gt;V-measure: 0.872&lt;br&gt;Adjusted Rand Index: 0.912&lt;br&gt;Adjusted Mutual Information: 0.871&lt;br&gt;Silhouette Coefficient: 0.753&lt;/p&gt;
&lt;p&gt;Python source code: plot_affinity_propagation.py&lt;br&gt;print(&lt;strong&gt;doc&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;from sklearn.cluster import AffinityPropagation&lt;br&gt;from sklearn import metrics&lt;br&gt;from sklearn.datasets.samples_generator import make_blobs&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Generate-sample-data&quot;&gt;&lt;a href=&quot;#Generate-sample-data&quot; class=&quot;headerlink&quot; title=&quot;Generate sample data&quot;&gt;&lt;/a&gt;Generate sample data&lt;/h1&gt;&lt;p&gt;centers = [[1, 1], [-1, -1], [1, -1]]&lt;br&gt;X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,&lt;br&gt;                            random_state=0)&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Compute-Affinity-Propagation&quot;&gt;&lt;a href=&quot;#Compute-Affinity-Propagation&quot; class=&quot;headerlink&quot; title=&quot;Compute Affinity Propagation&quot;&gt;&lt;/a&gt;Compute Affinity Propagation&lt;/h1&gt;&lt;p&gt;af = AffinityPropagation(preference=-50).fit(X)&lt;br&gt;cluster_centers_indices = af.cluster_centers&lt;em&gt;indices&lt;/em&gt;&lt;br&gt;labels = af.labels_&lt;/p&gt;
&lt;p&gt;n&lt;em&gt;clusters&lt;/em&gt; = len(cluster_centers_indices)&lt;/p&gt;
&lt;p&gt;print(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))&lt;br&gt;print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))&lt;br&gt;print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))&lt;br&gt;print(“Adjusted Rand Index: %0.3f”&lt;br&gt;      % metrics.adjusted_rand_score(labels_true, labels))&lt;br&gt;print(“Adjusted Mutual Information: %0.3f”&lt;br&gt;      % metrics.adjusted_mutual_info_score(labels_true, labels))&lt;br&gt;print(“Silhouette Coefficient: %0.3f”&lt;br&gt;      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))&lt;/p&gt;
&lt;p&gt;##############################################################################&lt;/p&gt;
&lt;h1 id=&quot;Plot-result&quot;&gt;&lt;a href=&quot;#Plot-result&quot; class=&quot;headerlink&quot; title=&quot;Plot result&quot;&gt;&lt;/a&gt;Plot result&lt;/h1&gt;&lt;p&gt;import matplotlib.pyplot as plt&lt;br&gt;from itertools import cycle&lt;/p&gt;
&lt;p&gt;plt.close(‘all’)&lt;br&gt;plt.figure(1)&lt;br&gt;plt.clf()&lt;/p&gt;
&lt;p&gt;colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)&lt;br&gt;for k, col in zip(range(n&lt;em&gt;clusters&lt;/em&gt;), colors):&lt;br&gt;    class_members = labels == k&lt;br&gt;    cluster_center = X[cluster_centers_indices[k]]&lt;br&gt;    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)&lt;br&gt;    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,&lt;br&gt;             markeredgecolor=’k’, markersize=14)&lt;br&gt;    for x in X[class_members]:&lt;br&gt;        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)&lt;/p&gt;
&lt;p&gt;plt.title(‘Estimated number of clusters: %d’ % n&lt;em&gt;clusters&lt;/em&gt;)&lt;br&gt;plt.show()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;br&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/clustering.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://scikit-learn.org/stable/modules/clustering.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://blog.csdn.net/u010695420/article/details/42239465&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/u010695420/article/details/42239465&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LDA验证</title>
    <link href="http://yoursite.com/2016/05/19/LDA%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2016/05/19/LDA验证/</id>
    <published>2016-05-19T13:14:42.000Z</published>
    <updated>2016-05-19T13:14:42.000Z</updated>
    
    <content type="html"></content>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Gensim and LDA--Training and Prediction</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/</id>
    <published>2016-05-18T12:46:16.000Z</published>
    <updated>2016-06-09T12:19:41.000Z</updated>
    
    <content type="html">&lt;p&gt;用 Gensim 实现 LDA，相比 JGibbLDA 的使用 Gensim 略为麻烦，然而感觉更清晰易懂，也就更灵活。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
# install the related python packages
&gt;&gt;&gt; pip install numpy
&gt;&gt;&gt; pip install scipy
&gt;&gt;&gt; pip install gensim
&gt;&gt;&gt; pip install jieba

from gensim import corpora, models, similarities
import logging
import jieba

# configuration
logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# load data from file
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()

＃ tokenize
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]

# load id-&gt;word mapping (the dictionary)
dictionary = corpora.Dictionary(texts)

# word must appear &gt;10 times, and no more than 40% documents
dictionary.filter_extremes(no_below=40, no_above=0.1)

# save dictionary
dictionary.save(&#39;dict_v1.dict&#39;)

# load corpus
corpus = [dictionary.doc2bow(text) for text in texts]

# initialize a model
tfidf = models.TfidfModel(corpus)

# use the model to transform vectors, apply a transformation to a whole corpus
corpus_tfidf = tfidf[corpus]

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)

# save model to files
lda.save(&#39;mylda_v1.pkl&#39;)

# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.
for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, lda.print_topic(index, 10))

# print the most contributing words for 100 randomly selected topics
lda.print_topics(100)

# load model and dictionary
model = models.LdaModel.load(&#39;mylda_v1.pkl&#39;)
dictionary = corpora.Dictionary.load(&#39;dict_v1.dict&#39;)

# predict unseen data
query = &quot;未收到奖励&quot;
query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
for index, score in sorted(model[query_bow], key=lambda tup: -1*tup[1]):
    print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))

# if you want to predict many lines of data in a file, do the followings
f = open(&#39;newfile.txt&#39;, &#39;r&#39;)
documents = f.readlines()
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]
corpus = [dictionary.doc2bow(text) for text in texts]

# only print the topic with the highest score
for c in corpus:
    flag = True
    for index, score in sorted(model[c], key=lambda tup: -1*tup[1]):
        if flag:
            print &quot;Score: {}\t Topic: {}&quot;.format(score, model.print_topic(index, 20))&lt;/pre&gt;

&lt;h1 id=&quot;Tips&quot;&gt;&lt;a href=&quot;#Tips&quot; class=&quot;headerlink&quot; title=&quot;Tips:&quot;&gt;&lt;/a&gt;Tips:&lt;/h1&gt;&lt;p&gt;If you occur encoding problems, you can try the following code&lt;/p&gt;
&lt;pre&gt;
add it at the beginning of your python file
# -*- coding: utf-8 -*-

# also, do the followings
import sys
reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

# the following code may lead to encoding problem when there&#39;re Chinese characters
model.show_topics(-1, 5)

# use this instead
model.print_topics(-1, 5)&lt;/pre&gt;


&lt;p&gt;You can see step-by-step output by the following references.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/tut2.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/tut2.html&lt;/a&gt; official guide (en)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/questionfish/article/details/46725475&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/questionfish/article/details/46725475&lt;/a&gt;  official guide (ch)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;用 Gensim 实现 LDA，相比 JGibbLDA 的使用 Gensim 略为麻烦，然而感觉更清晰易懂，也就更灵活。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>Gensim-用Python做主题模型</title>
    <link href="http://yoursite.com/2016/05/18/Gensim-%E7%94%A8Python%E5%81%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2016/05/18/Gensim-用Python做主题模型/</id>
    <published>2016-05-18T02:22:31.000Z</published>
    <updated>2016-06-09T12:12:34.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;gensim-特性&quot;&gt;&lt;a href=&quot;#gensim-特性&quot; class=&quot;headerlink&quot; title=&quot;gensim 特性&quot;&gt;&lt;/a&gt;gensim 特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;内存独立- 对于训练语料来说，没必要在任何时间将整个语料都驻留在RAM中&lt;/li&gt;
&lt;li&gt;有效实现了许多流行的向量空间算法－包括tf-idf，分布式LSA, 分布式LDA 以及 RP；并且很容易添加新算法&lt;/li&gt;
&lt;li&gt;对流行的数据格式进行了IO封装和转换&lt;/li&gt;
&lt;li&gt;在其语义表达中，可以相似查询&lt;/li&gt;
&lt;li&gt;gensim的创建的目的是，由于缺乏简单的（java很复杂）实现主题建模的可扩展软件框架.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-设计原则&quot;&gt;&lt;a href=&quot;#gensim-设计原则&quot; class=&quot;headerlink&quot; title=&quot;gensim 设计原则&quot;&gt;&lt;/a&gt;gensim 设计原则&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;简单的接口，学习曲线低。对于原型实现很方便&lt;/li&gt;
&lt;li&gt;根据输入的语料的size来说，内存各自独立；基于流的算法操作，一次访问一个文档.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;gensim-核心概念&quot;&gt;&lt;a href=&quot;#gensim-核心概念&quot; class=&quot;headerlink&quot; title=&quot;gensim 核心概念&quot;&gt;&lt;/a&gt;gensim 核心概念&lt;/h3&gt;&lt;p&gt;gensim的整个package会涉及三个概念：corpus, vector, model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;语库(corpus)&lt;br&gt;文档集合，用于自动推出文档结构，以及它们的主题等，也可称作训练语料。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;向量(vector)&lt;/p&gt;
&lt;p&gt;在向量空间模型(VSM)中，每个文档被表示成一个特征数组。例如，一个单一特征可以被表示成一个问答对(question-answer pair):&lt;/p&gt;
&lt;p&gt;[1].在文档中单词”splonge”出现的次数？ 0个&lt;br&gt;[2].文档中包含了多少句子？ 2个&lt;br&gt;[3].文档中使用了多少字体? 5种&lt;br&gt;这里的问题可以表示成整型id (比如：1,2,3等), 因此，上面的文档可以表示成：(1, 0.0), (2, 2.0), (3, 5.0). 如果我们事先知道所有的问题，我们可以显式地写成这样：(0.0, 2.0, 5.0). 这个answer序列可以认为是一个多维矩阵（3维）. 对于实际目的，只有question对应的answer是一个实数.&lt;/p&gt;
&lt;p&gt;对于每个文档来说，answer是类似的. 因而，对于两个向量来说（分别表示两个文档），我们希望可以下类似的结论：“如果两个向量中的实数是相似的，那么，原始的文档也可以认为是相似的”。当然，这样的结论依赖于我们如何去选取我们的question。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;稀疏矩阵(Sparse vector)&lt;/p&gt;
&lt;p&gt;通常，大多数answer的值都是0.0. 为了节省空间，我们需要从文档表示中忽略它们，只需要写：(2, 2.0), (3, 5.0) 即可(注意：这里忽略了(1, 0.0)). 由于所有的问题集事先都知道，那么在稀疏矩阵的文档表示中所有缺失的特性可以认为都是0.0.&lt;/p&gt;
&lt;p&gt;gensim的特别之处在于，它没有限定任何特定的语料格式；语料可以是任何格式，当迭代时，通过稀疏矩阵来完成即可。例如，集合 ([(2, 2.0), (3, 5.0)], ([0, -1.0], [3, -1.0])) 是一个包含两个文档的语料，每个都有两个非零的 pair。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;模型(model)&lt;/p&gt;
&lt;p&gt;对于我们来说，一个模型就是一个变换(transformation)，将一种文档表示转换成另一种。初始和目标表示都是向量－－它们只在question和answer之间有区别。这个变换可以通过训练的语料进行自动学习，无需人工监督，最终的文档表示将更加紧凑和有用；相似的文档具有相似的表示。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;演示代码&quot;&gt;&lt;a href=&quot;#演示代码&quot; class=&quot;headerlink&quot; title=&quot;演示代码&quot;&gt;&lt;/a&gt;演示代码&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;演示代码&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://d0evi1.github.io/gensim/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://d0evi1.github.io/gensim/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;gensim-介绍&quot;&gt;&lt;a href=&quot;#gensim-介绍&quot; class=&quot;headerlink&quot; title=&quot;gensim 介绍&quot;&gt;&lt;/a&gt;gensim 介绍&lt;/h3&gt;&lt;p&gt;gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。&lt;br&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="gensim" scheme="http://yoursite.com/tags/gensim/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
  <entry>
    <title>JGibbLDA实战</title>
    <link href="http://yoursite.com/2016/05/16/JGibbLDA%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2016/05/16/JGibbLDA实战/</id>
    <published>2016-05-16T12:50:23.000Z</published>
    <updated>2016-06-09T12:09:14.000Z</updated>
    
    <content type="html">&lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;错误&quot;&gt;&lt;a href=&quot;#错误&quot; class=&quot;headerlink&quot; title=&quot;错误&quot;&gt;&lt;/a&gt;错误&lt;/h3&gt;&lt;p&gt;命令：&lt;/p&gt;
&lt;pre&gt;java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models/casestudy/newdocs.dat&lt;/pre&gt;

&lt;p&gt;错误信息：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xu83c.com1.z0.glb.clouddn.com/err.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;手动配置&quot;&gt;&lt;a href=&quot;#手动配置&quot; class=&quot;headerlink&quot; title=&quot;手动配置&quot;&gt;&lt;/a&gt;手动配置&lt;/h3&gt;&lt;p&gt;于是尝试导入eclipse运行手动配置，成功，过程如下。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载JGibbLDA的jar包并解压；&lt;br&gt;网址：&lt;a href=&quot;http://jgibblda.sourceforge.net/#Griffiths04&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/#Griffiths04&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;导入eclipse，确保jar包在目录中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;找到LDACmdOption.java文件， 修改部分代码&lt;/p&gt;
&lt;pre&gt; @Option(name=&quot;-dir&quot;, usage=&quot;Specify directory&quot;)
 public String dir = &quot;models/casestudy-en&quot;;

 @Option(name=&quot;-dfile&quot;, usage=&quot;Specify data file&quot;)
 public String dfile = &quot;models/casestudy-en/newdocs.dat&quot;;&lt;/pre&gt;

&lt;p&gt;值得注意的是，dfile的格式必须是👇这个样子：&lt;/p&gt;
&lt;pre&gt;[M]
[document1]
[document2]
...
[documentM]&lt;/pre&gt;

&lt;p&gt;第一行[M]是documents的总数，之后的每一行是一个document，每个document是一个word list，或者说是bag of words。&lt;/p&gt;
&lt;pre&gt;[document i] = [word i1] [word i2] ... [word iNi]&lt;/pre&gt;

&lt;p&gt;各参数含义：&lt;br&gt;&lt;strong&gt;-est &lt;/strong&gt;从训练语料中评估出LDA模型&lt;br&gt;&lt;strong&gt;-alpha&lt;/strong&gt; LDA模型中的alpha数值，默认为50/K(K是主题数目)&lt;br&gt;&lt;strong&gt;-beta&lt;/strong&gt; LDA模型中的beta数值，默认是0.1&lt;br&gt;&lt;strong&gt;-ntopics&lt;/strong&gt; 主题数目，默认值是100&lt;br&gt;&lt;strong&gt;-niters&lt;/strong&gt; GIbbs采样的迭代数目，默认值为2000&lt;br&gt;&lt;strong&gt;-savestep&lt;/strong&gt; 指定开始保存LDA模型的迭代次数&lt;br&gt;&lt;strong&gt;-dir&lt;/strong&gt; 训练语料目录&lt;br&gt;&lt;strong&gt;-dfile&lt;/strong&gt; 训练语料文件名称&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;修改项目的Run Configurations，在Java Application中选择LDA，点击(x)=Arguments，输入&lt;/p&gt;
&lt;pre&gt;-est -alpha 0.2 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 100 -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;newdocs.dat&quot;&lt;/pre&gt;

&lt;p&gt; 若利用已训练的LDA模型预测，输入以下参数：&lt;/p&gt;
&lt;pre&gt;-inf -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile &quot;test.txt&quot;&lt;/pre&gt;

&lt;p&gt; 注意，进行预测时，当前目录下必须包含已有的LDA训练输出文件，包括model-final.others、model-final.phi、model-final.tassign、model-final.theta、model-final.twords、wordmap.txt文件，如果运行报错，尝试修改LDACmdOption.java的modelName，确保和文件名的modelname部分一致。&lt;br&gt;&lt;pre&gt;@Option(name=”-model”, usage=”Specify the model name”)&lt;br&gt; public String modelName = “model-final”;&lt;/pre&gt;   &lt;/p&gt;
&lt;p&gt; 如果出现java heap limited的问题，在VM arguments下添加&lt;/p&gt;
&lt;pre&gt;-Xms1g -Xmx1g -Xmn512m&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run&lt;br&gt;输出文件主要有：&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.others&lt;/model_name&gt;&lt;/strong&gt;  文件存储LDA模型参数，如alpha、beta等。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.phi &lt;/model_name&gt;&lt;/strong&gt; 每个topic内对doc的分布情况。文件存储词语-主题分布，每一行是一个主题，列内容为词语。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.theta &lt;/model_name&gt;&lt;/strong&gt; 每个doc内对应上面的n个topic的分布情况。文件主题文档分布，每一行是一个文档，列内容是主题概率。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.tassign&lt;/model_name&gt;&lt;/strong&gt;  文件是训练预料中单词的主题指定（归属），每一行是一个语料文档。&lt;br&gt;&lt;strong&gt;&lt;model_name&gt;.twords&lt;/model_name&gt;&lt;/strong&gt;  n个topic，以及每个topic下面包含的具体的字词&lt;br&gt;&lt;strong&gt;wordmap.txt&lt;/strong&gt;  词-id映射&lt;br&gt;其中&lt;model_name&gt;根据采样迭代次数来指定，如model-00800，最后一次采样名称命名为model-final。&lt;/model_name&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;参考链接：&lt;br&gt;&lt;a href=&quot;http://www.ithao123.cn/content-4208214.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.ithao123.cn/content-4208214.html&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://jgibblda.sourceforge.net/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jgibblda.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="LDA" scheme="http://yoursite.com/tags/LDA/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="topic modeling" scheme="http://yoursite.com/tags/topic-modeling/"/>
    
      <category term="cluster" scheme="http://yoursite.com/tags/cluster/"/>
    
  </entry>
  
</feed>
