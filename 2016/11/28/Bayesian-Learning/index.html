<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />













  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="NLP,machine learning," />





  <link rel="alternate" href="/atom.xml" title="徐阿衡" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.3" />






<meta name="description" content="CMU 10601 的课程笔记。朴素贝叶斯思想即对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。">
<meta property="og:type" content="article">
<meta property="og:title" content="Bayesian Learning">
<meta property="og:url" content="http://www.shuang0420.com/2016/11/28/Bayesian-Learning/index.html">
<meta property="og:site_name" content="徐阿衡">
<meta property="og:description" content="CMU 10601 的课程笔记。朴素贝叶斯思想即对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/bayesg.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb1.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb6.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb7.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb9.jpg">
<meta property="og:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/algorithm.jpg">
<meta property="og:updated_time" content="2018-09-17T11:14:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian Learning">
<meta name="twitter:description" content="CMU 10601 的课程笔记。朴素贝叶斯思想即对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。">
<meta name="twitter:image" content="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/bayesg.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6294135991397516000',
      author: '阿衡'
    }
  };
</script>




  <link rel="canonical" href="http://www.shuang0420.com/2016/11/28/Bayesian-Learning/"/>


  <title> Bayesian Learning | 徐阿衡 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">徐阿衡</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Shuang</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-works">
          <a href="/works" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Works
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/aboutme" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input" placeholder="search my blog...">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
         
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Bayesian Learning
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-11-28T13:35:18+08:00" content="2016-11-28">
              2016-11-28
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/11/28/Bayesian-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/11/28/Bayesian-Learning/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>CMU 10601 的课程笔记。朴素贝叶斯思想即对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<br><a id="more"></a></p>
<h1 id="Generative-and-discriminative-algorithms"><a href="#Generative-and-discriminative-algorithms" class="headerlink" title="Generative and discriminative algorithms"></a>Generative and discriminative algorithms</h1><ul>
<li><p>Generative<br>try to model data, how data comes out, which disease cause with symptom with what probability - provide a profile for each disease<br>给定输入 x，生成模型可以给出输入和输出的联合分布 P(x,y)，生成模型的目标是求联合分布 P(x,y)，然后求出条件概率分布 P(Y|X) 作为预测的模型</p>
<ul>
<li>以朴素贝叶斯为例，生成模型的求解思路是：<strong>联合分布——-&gt;求解类别先验概率和类别条件概率</strong></li>
<li>联合分布能提供更多的信息，需要更多的样本和更多计算</li>
<li>收敛速度比较快，当样本数量较多时，生成模型能更快地收敛于真实模型。</li>
<li>生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。</li>
<li>e.g. Naive Bayes, HMM</li>
</ul>
</li>
<li><p>Discriminative<br>try to find mapping symptom -&gt; disease reverse order</p>
<ul>
<li>判别模型的求解思路是：<strong>条件分布——&gt;模型参数后验概率最大——-&gt;（似然函数参数先验）最大——-&gt;最大似然</strong></li>
<li>直接学习 P(Y|X)，节省计算资源，另外，需要的样本数量也少于生成模型</li>
<li>准确率往往较生成模型高</li>
<li>e.g. Logistic Regression</li>
<li>实践中多数情况下判别模型效果更好</li>
</ul>
</li>
</ul>
<h1 id="MLE-and-MAP"><a href="#MLE-and-MAP" class="headerlink" title="MLE and MAP"></a>MLE and MAP</h1><p><strong>MLE(maximum likelihood estimate):</strong> choose $\theta$ that maximize D probability of observed data.<br>$$\theta = argmax \ P(D|\theta)$$</p>
<p><strong>MAP(maximum a posterior estimate):</strong> choose $\theta$ that is most probable given prior probability and the data.<br>$$\theta = argmax \ P(\theta|D)=argmax \ {P(D|\theta)P(\theta) \over P(D)}$$</p>
<h1 id="Bayesian-statistics"><a href="#Bayesian-statistics" class="headerlink" title="Bayesian statistics"></a>Bayesian statistics</h1><h2 id="主旨"><a href="#主旨" class="headerlink" title="主旨"></a>主旨</h2><ul>
<li>Combine prior knowledge (prior probabilities) with observed data</li>
<li>Provides “gold standard” for evaluating other learning algorithms</li>
<li>Additional insight into Occam’s razor</li>
</ul>
<h2 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/bayesg.jpg" class="ful-image" alt="bayesg.jpg">
<p>要记住的是: today’s posterior is tomorrow’s prior</p>
<h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>$$P(h|D)= {P(D|h)P(h) \over P(D)}$$<br>P(h) = prior probability of hypothesis h<br>P(D) = prior probability of training data D<br>P(h|D) = probability of h given D<br>P(D|h) = probability of D given h</p>
<p>推导非常简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">P(h,D) = P(h)P(D|h)</div><div class="line">       = P(D)P(h|D)</div></pre></td></tr></table></figure></p>
<p>一般来说，prior 不能是 0，否则 posterior 也会是 0 了。</p>
<h2 id="Base-rate-neglect"><a href="#Base-rate-neglect" class="headerlink" title="Base rate neglect"></a>Base rate neglect</h2><p>Ignore prior in reasoning</p>
<blockquote>
<p>If presented with related base rate information (i.e. generic, general information) and specific information (information only pertaining to a certain case), the mind tends to ignore the former and focus on the latter.</p>
</blockquote>
<h2 id="MAP-Maximum-a-posteriori"><a href="#MAP-Maximum-a-posteriori" class="headerlink" title="MAP (Maximum a posteriori)"></a>MAP (Maximum a posteriori)</h2><p>判断 how well a hypothesis matches a data<br>$h \in H$<br>$$h_{MAP} = argmax P(h|D) = argmax {P(D|h)P(h) \over P(D)} = argmax P(D|h)P(h)$$</p>
<h2 id="Example-1-Cancer"><a href="#Example-1-Cancer" class="headerlink" title="Example 1. Cancer"></a>Example 1. Cancer</h2><p>通过例子来说明<br>问题是：这个患者是否有癌症<br>条件是：</p>
<ol>
<li>一个患者接受了一个检验，显示是阳性的(positive)。</li>
<li>这个检验返回的 correct positive 概率是 98%，correct negative 的概率是 97%。</li>
<li>人群中 0.008 的人可能会患这个癌症。</li>
</ol>
<p>首先用熟悉符号表示这个问题<br>Hypothesis: $H = {cancer, \lnot {cancer}}$<br>Prior: $\pi(cancer) = 0.008$, $\pi (\lnot cancer) = 0.992$<br>L(D|h)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">h   | pos  | neg</div><div class="line">c   | 0.98 | 0.02</div><div class="line">nc  | 0.03 | 0.97</div></pre></td></tr></table></figure>
<p>右上角的是 false negative (negative result which is false)，又叫 miss<br>左下角的是 false positive (positive result which is false)，又叫 alarm<br>miss 和 alarm 两者是一个 tradeoff，如果永远都说是 cancer，那么没有任何 miss，却有很多的 alarm</p>
<p>计算：<br>$$<br>  \begin{aligned}<br>  P(c|D=pos) &amp; = {\pi(c)L(pos|c) \over P(pos)} \\<br>  &amp; =  {\pi(c)L(pos|c) \over \sum_i P(pos|h_i)P(h_i)} \\<br>   &amp; = {0.008*0.98 \over 0.008*0.98+0.992*0.03} \\<br>   &amp; = 0.208<br>  \end{aligned}<br>$$</p>
<h2 id="Example-2-Cancer-cont"><a href="#Example-2-Cancer-cont" class="headerlink" title="Example 2. Cancer cont."></a>Example 2. Cancer cont.</h2><p>如果第二次检验的结果还是 positive，那么该患者患癌症的几率有多大？</p>
<h3 id="Method-1-Sequential-apply-of-Baye’s-Rules"><a href="#Method-1-Sequential-apply-of-Baye’s-Rules" class="headerlink" title="Method 1: Sequential apply of Baye’s Rules"></a>Method 1: Sequential apply of Baye’s Rules</h3><p>前提是 $D1 \bot D2 |h$<br>此时的 Posterior:<br>$\pi(cancer) = 0.208$, $\pi (\lnot cancer) = 0.792$<br>H, L(D|h) = SAME<br>D2 = pos<br>$$<br>  \begin{aligned}<br>  P(c|D2=pos) &amp; = {\pi(c)L(pos|c) \over P(pos)} \\<br>   &amp; = {0.208*0.98 \over 0.208*0.98+0.792*0.03} \\<br>   &amp; = 0.895<br>  \end{aligned}<br>$$</p>
<p>每多做一次结果为 positive 的 test，最后的概率都会越大。</p>
<h3 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a>Method 2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">h   |     ++    |     +-    |     -+    |    --     |</div><div class="line">c   | 0.98*0.98 | 0.98*0.02 | 0.02*0.98 | 0.02*0.02 |</div><div class="line">nc  | 0.03*0.03 | 0.03*0.97 | 0.97*0.03 | 0.97*0.97 |</div></pre></td></tr></table></figure>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Hypothesis H: a family of distributions indexed by $\ge $ 1 parameters</p>
<ul>
<li>subjective because of prior</li>
<li>returns a whole distribution<br>$\theta_{map} = argmax P(\theta|D)$</li>
</ul>
<p>Try to minimize MSE, $\theta_{mean}$<br>Try to minimize MAE, $\theta_{median}$</p>
<p><strong>Hard bias:</strong> choices of H family<br><strong>Soft bias:</strong> priors</p>
<h1 id="Frequentist-statistics"><a href="#Frequentist-statistics" class="headerlink" title="Frequentist statistics"></a>Frequentist statistics</h1><ul>
<li>no notion of prior</li>
<li>focus on likelihood L(D|h)</li>
</ul>
<h2 id="MLP-Maximum-likelihood-principle"><a href="#MLP-Maximum-likelihood-principle" class="headerlink" title="MLP (Maximum likelihood principle)"></a>MLP (Maximum likelihood principle)</h2><p>Given L() function family &amp; data, choose $$\theta = argmax \ L(D|\theta)$$</p>
<h2 id="Example-1-Binomial-distribution-X-B-n-p"><a href="#Example-1-Binomial-distribution-X-B-n-p" class="headerlink" title="Example 1. Binomial distribution X~B(n,p)"></a>Example 1. Binomial distribution X~B(n,p)</h2><p>X = {0,…n}<br>$$P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$$<br>Given n, and result of n flips -&gt; k heads, how to estimate P?</p>
<p><strong>简化</strong><br>$$<br>  \begin{aligned}<br>  P_{ML}  &amp; = argmax  \ L(k  \ heads  \ out  \ of  \ n  \ flips | P) \\<br>   &amp; = argmax \binom {n}{k}P^k(1-P)^{n-k}\\<br>   &amp; = argmax \  klogP + (n-k)log(1-P)<br>  \end{aligned}<br>$$</p>
<p><strong>求导</strong><br>=&gt; $\int {dll \over dp} =  k {1 \over p} - (n-k) {1 \over 1-p} = 0$<br>=&gt; ${k \over p} = {n-k \over 1-p}$<br>=&gt; $P_{ML} = {k \over n}$</p>
<h2 id="Example-2-Gaussian"><a href="#Example-2-Gaussian" class="headerlink" title="Example 2.Gaussian"></a>Example 2.Gaussian</h2><p>Fixed stdev, unkown mean X~N$(\mu \sigma^2)$<br>$$P(X=x)={1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu)^2 \over 2 \sigma^2}$$<br>Given $\sigma^2$ &amp; data x1,…xn $ \in$ IR, drawn i,i,d (independently, identical distributed)</p>
<p><strong>简化</strong><br>$$<br>\begin{aligned}<br>\hat \mu_{ML}  &amp; = argmax \ \prod_i^n {1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu)^2 \over 2 \sigma^2} \\<br> &amp; = argmax \sum_i^n {-(x_i - \mu)^2 \over 2 \sigma^2} \\<br> &amp; = argmin \sum_i^n (x_i-\mu)^2<br>\end{aligned}<br>$$</p>
<p><strong>求导</strong><br>=&gt; $\int {dll \over dp} =  \sum_{i=1}^n 2(x_i-\mu)= 0$<br>=&gt; $\sum_{i=1}^n x_i-n\mu$<br>=&gt; $\mu_{ML} = {\sum_{i=1}^n \ x_i \over n} = \overline x_i$<br>=&gt; sample mean = max likelihood gaussian true mean</p>
<h2 id="Example-3-Linear-regression"><a href="#Example-3-Linear-regression" class="headerlink" title="Example 3. Linear regression"></a>Example 3. Linear regression</h2><p>$$y = f(x_1,..x_n) + \epsilon$$</p>
<p>$$f(\overline x)=\sum_{j=1}^n \beta_jx_j$$<br>ASSUME</p>
<ul>
<li>$\epsilon$~N $(\mu, \sigma^2)$</li>
<li>$\epsilon = y - f(x_1,…x_n)$ ~ N $(\mu, \sigma^2)$</li>
</ul>
<p><strong>简化</strong><br>$$<br>\begin{aligned}<br>L(y_1, ….,y_n|x_1,…,x_n)  &amp; = \prod_i^n L(y_i|\overline x_i) \\<br> &amp; = \prod {1 \over 2 \pi \sigma^2} e^{-(y_i-f(\overline x_i)^2) \over 2 \sigma^2}<br>\end{aligned}<br>$$</p>
<p><strong>求导</strong><br>$\hat \beta = argmax \sum_{i=1}^n {-(y_i - \overline x_i)^2 \over 2 \sigma^2}$<br>  $= argmin \ \sum_{i=1}^n (y_i-f(\overline x_i)^2)$<br>  $= MSE(mean \ squared \ error)$</p>
<p>=&gt; MSE &lt;=&gt; Gaussian noise</p>
<h3 id="Special-case-in-Bayesian"><a href="#Special-case-in-Bayesian" class="headerlink" title="Special case in Bayesian"></a>Special case in Bayesian</h3><p>$\pi(\theta) = constant$  (uniform prior)<br>=&gt; $\theta_{MAP} = argmax \ \pi(\theta)L(D|\theta) = \theta_{ML}$</p>
<h3 id="How-to-estimate-P"><a href="#How-to-estimate-P" class="headerlink" title="How to estimate P?"></a>How to estimate P?</h3><p>我们希望 Estimator</p>
<ol>
<li>with enough data, should converge to the true value (asymptotical consistency)</li>
<li>fast converge to the right answer (efficiency)</li>
<li>low bias (usually no bias)<br>我们希望 $E[\hat \theta(D)]=\theta$。$bias(\hat \theta)=E[\hat \theta(D)]-\theta$ 这里我们想让它为 0，但是如果这是 ML inductive bias，我们不希望它为 0.</li>
<li>low variance (but can have high variants especially with little data)</li>
</ol>
<h1 id="MDL-minimum-length-description-principle"><a href="#MDL-minimum-length-description-principle" class="headerlink" title="MDL (minimum length description principle)"></a>MDL (minimum length description principle)</h1><p>Encode 数据。比如说我们要传递 1-100 万之间的质数，那我们只用告诉对方这是质数，而不用把所有的质数列出来，如果我们要传递的是除了 2 和 3 的质数，那么我们要告诉对方这是质数，而且里面没有  2 和 3。<br>放到 machine learning 里，假设双方都有一堆数据 X1..Xn(X有若干属性(x11,x22…xnn)，我们要通讯的是 Y 也就是 X 的标签，我们可以按顺序传送，这需要很多的 bits，或者可以建立一个 decision tree，这个 tree 可以 classify 大多数的数据，把 tree + exceptions 发送给对方就好，可以节省很多 bits。<br>$h_{MDL}$ = argmin (bits to describe h + bits to describe exceptions)</p>
<h1 id="Naive-Bayes-Classification"><a href="#Naive-Bayes-Classification" class="headerlink" title="Naive Bayes Classification"></a>Naive Bayes Classification</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>f: X -&gt; Y    &lt;=&gt;    P(Y |X )<br>X = ⟨X1,X2 …,Xn⟩   n 个属性<br>Y = boolean value    true or false</p>
<p>根据 Bayes rule, 来算 P(Y = yi|X)<br>$$P(Y=y_i|X=x_k)={P(X=x_k|Y=y_i)P(Y=y_i) \over \sum_j P(X=x_k|Y=y_j)P(Y=y_j)}$$</p>
<p>我们的目的是学习 P(Y|X)，来估计 P(X|Y) 和 P(Y)，用这些估计值，加上上面的 Bayes rule，就可以对新的 instance 分类。</p>
<h3 id="Unbiased-Learning-of-Bayes-Classifiers-is-Impractical"><a href="#Unbiased-Learning-of-Bayes-Classifiers-is-Impractical" class="headerlink" title="Unbiased Learning of Bayes Classifiers is Impractical"></a>Unbiased Learning of Bayes Classifiers is Impractical</h3><p>Unbiased Learning of Bayes Classifiers 是不实际的，因为它要估计的参数太多，计算量太大。<br>当 Y is boolean and X is a vector of n boolean attributes 的情况下，我们为了得到 P(X|Y) 需要的估计的参数有：</p>
<p>$$θ_{ij} ≡P(X =x_i|Y =y_j)$$</p>
<p>i takes on $2^n$ possible values (one for each of the possible vector values of X )<br>j takes on 2 possible values (boolean)<br> =&gt; $2^{n+1}$ parameters</p>
<p>For any fixed j, the sum over i of $θ_{ij}$ must be one.<br>i takes on $2^n-1$ possible values (one for each of the possible vector values of X )<br>j takes on 2 possible values (boolean)<br>=&gt; $2(2^n-1)$ parameters</p>
<p>To obtain reliable estimates of each of these parameters, we will need to observe each of these distinct instances multiple times!</p>
<p><strong>This is clearly unrealistic in most practical learning domains.</strong></p>
<p>For example, if X is a vector containing 30 boolean features, then we will need to estimate more than 3 billion parameters.</p>
<h3 id="Naive-Bayes-Algorithm"><a href="#Naive-Bayes-Algorithm" class="headerlink" title="Naive Bayes Algorithm"></a>Naive Bayes Algorithm</h3><p>Naive Bayes 的优势在于它能让我们对 P(X|Y) 建模时用更少的参数，$2(2^n-1)$ -&gt; 2n</p>
<h2 id="Naive-bayes-assumption-Independence-assumption"><a href="#Naive-bayes-assumption-Independence-assumption" class="headerlink" title="Naive bayes assumption (Independence assumption)"></a>Naive bayes assumption (Independence assumption)</h2><blockquote>
<p>Each $X_i$ is conditionally independent of each of the other $X_ks$ given Y, and also independent of each subset of the other $X_k$’s given Y.</p>
</blockquote>
<p>在这个假设下，我们举个例子，X = ⟨X1,X2⟩ 时，可以得到</p>
<p>$$<br>\begin{aligned}<br>P(X|Y)  &amp; = P(X1,X2|Y) \\<br> &amp; = P(X1|X2,Y)P(X2|Y) \\<br> &amp; = P(X1|Y)P(X2|Y)<br>\end{aligned}<br>$$</p>
<p>更加 general 的形式 <strong>Factor L(|Y)</strong><br>$$P(X1,X2…Xn|Y)=\prod_{i=1}^nP(X_i|Y)$$</p>
<p>同样的，Y and the Xi are boolean variables，这时候，我们只需要 2n 个参数来定义 $P(X_i = x_{ik}|Y = y_j)$!</p>
<p>首先我们产生 label，然后从 label 我们产生每一个属性。我们可以用下面的图来表示这种关系。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb.jpg" class="ful-image" alt="nb.jpg"></p>
<p>这个假设经常会被打破，然而即使是 horrible assumption，Naive Bayes 的效果也非常好。<br>tend to give extreme result</p>
<h2 id="Derivation-of-Naive-Bayes-Algorithm"><a href="#Derivation-of-Naive-Bayes-Algorithm" class="headerlink" title="Derivation of Naive Bayes Algorithm"></a>Derivation of Naive Bayes Algorithm</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb1.jpg" class="ful-image" alt="nb1.jpg">
<p>在 conditional independent assumption 下，可以继续得到<br>$$P(Y=y_k|X1…Xn)={P(Y=y_k)\prod_i P(X_i|Y=y_k) \over \sum_j P(Y=y_j)\prod_iP(Xi|Y=y_j)} \ (2)$$</p>
<p>$$Y \ &lt;= \ argmax \ {P(Y=y_k)\prod_i P(X_i|Y=y_k) \over \sum_j P(Y=y_j)\prod_iP(Xi|Y=y_j)}$$</p>
<p>简化下就是<br>$$Y \ &lt;= \ argmax \ P(Y=y_k)\prod_i P(X_i|Y=y_i=k) \ (3)$$</p>
<h2 id="Naive-Bayes-for-Discrete-Valued-Inputs"><a href="#Naive-Bayes-for-Discrete-Valued-Inputs" class="headerlink" title="Naive Bayes for Discrete-Valued Inputs"></a>Naive Bayes for Discrete-Valued Inputs</h2><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>n input attributes Xi each take on J possible discrete values<br>Y: a discrete variable taking on K possible values</p>
<p>Learning task is to estimate two sets of parameters.<br>$$\theta_{ijk}≡P(X_i=x_{ij}|Y=y_k)$$</p>
<p>这需要 nJK 个参数，对每个 j,k value 都满足  $1 = \sum_j \theta_{ijk}$，也就是 n(J-1)K 个参数。</p>
<p>另外我们要定义 prior probability over Y<br>$$\pi_k=P(Y=y_k)$$<br>需要 K-1 个参数。</p>
<h3 id="Estimate-parameters"><a href="#Estimate-parameters" class="headerlink" title="Estimate parameters"></a>Estimate parameters</h3><p>有两种估计参数的方法。Maximum likelihood 和 Bayesian MAP。</p>
<p><strong>MLE:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb6.jpg" class="ful-image" alt="nb6.jpg"></p>
<p>这种情况下有一个危险，当没有 example 符合条件的情况下，$\theta$ 可能为 0，所以我们需要 smooth。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb7.jpg" class="ful-image" alt="nb7.jpg"></p>
<p>J: the number of distinct values Xi can take on<br>l: determines the strength of this smoothing</p>
<p>同样的，估计 $\pi_k$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb9.jpg" class="ful-image" alt="nb9.jpg"></p>
<p>K: the number of distinct values Y can take on<br>l: determines the strength of the prior assumptions relative to the observed data D.</p>
<p><strong>MAP classifier:</strong><br>$Vmap = argmax P(v|a_1=x_1…a_n=x_n)=argmax P(v)L(a_1=x_1…a_n=x_n|v)$</p>
<p>为了估计 prior，我们需要 O(k) 个 parameter，这一般不是问题。<br>估计 L 的概率，我么需要 EXP(n) 个 parameter。</p>
<p>通过 Factor analysis 来减少 parameters</p>
<h2 id="Naive-Bayes-for-Continuous-Inputs"><a href="#Naive-Bayes-for-Continuous-Inputs" class="headerlink" title="Naive Bayes for Continuous Inputs"></a>Naive Bayes for Continuous Inputs</h2><p>当输入是连续的情况下，我们同样可以用(2)(3)来设计 Naive Bayes classifier，然而，由于 Xi 是连续的，我们必须用不同的方法来表示 P(Xi|Y) 的分布。通常的方法是假设 Xi 服从 Gaussian 分布，于是我们要定义 mean 和 standard deviation。</p>
<p>$$\mu_{ik}=E[X_i|Y=y_k]$$</p>
<p>$$\sigma^2_{ik}=E[(X_i-\mu_{ik})^2|Y=y_k]$$<br>这样我们需要估计 2nK 个参数。</p>
<p>prior:<br>$$\pi_k=P(Y=y_k)$$</p>
<h2 id="Squashing"><a href="#Squashing" class="headerlink" title="Squashing"></a>Squashing</h2><p>因为 independency 的假设，我们会得到非常极端的数值，如真实概率是 0.7, NB 出来的结果可能是 0.99999，真实概率是 0.2，NB 出来的结果可能是 0.0000001。<br>$$P^{\alpha}(y) \over \sum_i^n P^{\alpha}(y)$$</p>
<h2 id="Naive-Bayes-Classifier-vs-Logistic-Regression"><a href="#Naive-Bayes-Classifier-vs-Logistic-Regression" class="headerlink" title="Naive Bayes Classifier vs Logistic Regression"></a>Naive Bayes Classifier vs Logistic Regression</h2><ul>
<li>LR is discriminative classifier as LR directly estimates the parameters of P(Y|X). We can view the distribution P(Y|X) as directly discriminating the value of the target value Y for any given instance X.</li>
<li>NB is generative classifier as NB directly estimates parameters for P(Y) and P(X|Y). We can view the distribution P(X|Y) as describing how to generat random instances X conditioned on the target attribute Y.</li>
<li>NB is with greater bias but lower variance than LR.</li>
</ul>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Naive_Bayes_Learn(examples)<br> For each target value vj<br>  $\hat P(vj) &lt;- estimate P(vj)$<br>    For each attribute value ai of each attribute a<br>      $\hat P(ai|vj) &lt;- estimate P(ai|vj)$</p>
<p><strong>Classifier_New_Instance(x)</strong><br>  $Vnb = argmax \hat P(vj)\prod \hat P(ai|vj)$</p>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>new instance</p>
<outlk =="" sun,="" temp="cool," humid="high," wind="st">

<p>$$Vnb = argmax \hat P(vj)\prod \hat P(ai|vj)$$</p>
<p>P(y)P(sun|y)P(cool|y)P(high|y)P(strong|y)=0.005<br>P(n)P(sun|n)P(cool|n)P(high|n)P(strong|n)=0.021</p>
<h2 id="Learn-Naive-Bayes-Text"><a href="#Learn-Naive-Bayes-Text" class="headerlink" title="Learn Naive Bayes Text"></a>Learn Naive Bayes Text</h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/algorithm.jpg" class="ful-image" alt="algorithm.jpg">
<h3 id="python-代码"><a href="#python-代码" class="headerlink" title="python 代码"></a>python 代码</h3><p>运行命令</p>
<pre>
python nb.py split.train split.test</pre>

<p>split.train / split.test 每行是一个 training file 文件名，有两个类别，con 和 lib。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">con10.txt</div><div class="line">con11.txt</div><div class="line">con1.txt</div><div class="line">con46.txt</div><div class="line">con48.txt</div><div class="line">con6.txt</div><div class="line">lib16.txt</div><div class="line">lib25.txt</div><div class="line">lib28.txt</div><div class="line">lib29.txt</div><div class="line">lib2.txt</div><div class="line">lib30.txt</div><div class="line">lib31.txt</div><div class="line">con25.txt</div><div class="line">con39.txt</div></pre></td></tr></table></figure></p>
<p>training file 每行是代表这个文件名类别的单词<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Random</div><div class="line">Jottings</div><div class="line">October</div><div class="line">30</div><div class="line">2008</div><div class="line">And</div><div class="line">you</div></pre></td></tr></table></figure></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">import math</div><div class="line">import sys</div><div class="line">from collections import Counter</div><div class="line">from sets import Set</div><div class="line"></div><div class="line"># collect all distinct words and other tokens in examples</div><div class="line"></div><div class="line"></div><div class="line">def initiate(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter</div><div class="line">    conCounter, libCounter = Counter(), Counter()</div><div class="line">    conDoc, libDoc = 0, 0</div><div class="line">    f = open(file, &apos;r&apos;)</div><div class="line">    files = f.readlines()</div><div class="line">    for file in files:</div><div class="line">        assert file.startswith(&apos;con&apos;) or file.startswith(&apos;lib&apos;)</div><div class="line">        if file.startswith(&apos;con&apos;):</div><div class="line">            conDoc += 1</div><div class="line">            conCounter = readTrain(file.strip(), conCounter, libCounter)</div><div class="line">        else:</div><div class="line">            libDoc += 1</div><div class="line">            libCounter = readTrain(file.strip(), libCounter, conCounter)</div><div class="line"></div><div class="line"></div><div class="line">def readTrain(trainFile, tokenDict, secondDict):</div><div class="line">    f = open(trainFile, &apos;r&apos;)</div><div class="line">    tokens = [line.strip().lower() for line in f]</div><div class="line">    for token in tokens:</div><div class="line">        tokenDict[token] += 1</div><div class="line">        if token not in secondDict:</div><div class="line">            secondDict[token] = 0</div><div class="line">    return tokenDict</div><div class="line"></div><div class="line"># calculate the requeired P(vj) and P(wk|vj)probability terms</div><div class="line"></div><div class="line"></div><div class="line">def train(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter, priorCon, priorLib</div><div class="line">    initiate(file)</div><div class="line">    # P(vj) = docsj/examples</div><div class="line">    priorCon = math.log(conDoc / float(conDoc + libDoc))</div><div class="line">    priorLib = math.log(libDoc / float(conDoc + libDoc))</div><div class="line">    # total number of words in Vocabulary</div><div class="line">    vocLen = len(Set(conCounter.keys()).union(Set(libCounter.keys())))</div><div class="line">    # P(wk|vj) = (nk+1)/(n+vocLen)</div><div class="line">    conN = sum(conCounter.values()) + vocLen</div><div class="line">    libN = sum(libCounter.values()) + vocLen</div><div class="line">    conDefault = math.log(1.0 / conN)</div><div class="line">    libDefault = math.log(1.0 / libN)</div><div class="line">    for key in conCounter:</div><div class="line">        conCounter[key] = math.log(</div><div class="line">            float(conCounter[key] + 1) / conN)</div><div class="line">    for key in libCounter:</div><div class="line">        libCounter[key] = math.log(</div><div class="line">            float(libCounter[key] + 1) / libN)</div><div class="line"></div><div class="line"></div><div class="line">def test(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter, priorCon, priorLib</div><div class="line">    testFile = open(file)</div><div class="line">    testNum, correct = 0, 0</div><div class="line">    for line in testFile:</div><div class="line">        testNum += 1</div><div class="line">        libProb = priorLib</div><div class="line">        conProb = priorCon</div><div class="line">        conProb, libProb = readTest(line.strip(), conProb, libProb)</div><div class="line">        # print conProb,libProb</div><div class="line">        if conProb &gt; libProb:</div><div class="line">            print &apos;C&apos;</div><div class="line">            if line.startswith(&apos;con&apos;):</div><div class="line">                correct += 1</div><div class="line">        else:</div><div class="line">            print &apos;L&apos;</div><div class="line">            if line.startswith(&apos;lib&apos;):</div><div class="line">                correct += 1</div><div class="line">    accuracy = correct / float(testNum)</div><div class="line">    print &quot;Accuracy: %.04f&quot; % accuracy</div><div class="line"></div><div class="line"></div><div class="line">def readTest(file, conProb, libProb):</div><div class="line">    global conCounter, libCounter</div><div class="line">    f = open(file, &apos;r&apos;)</div><div class="line">    tokens = [line.strip().lower() for line in f]</div><div class="line">    for token in tokens:</div><div class="line">        if token in conCounter:</div><div class="line">            conProb += conCounter[token]</div><div class="line">        if token in libCounter:</div><div class="line">            libProb += libCounter[token]</div><div class="line">    return conProb, libProb</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    trainFile = sys.argv[1]</div><div class="line">    testFile = sys.argv[2]</div><div class="line">    train(trainFile)</div><div class="line">    test(testFile)</div></pre></td></tr></table></figure></p>
<h1 id="Error-and-test-of-hypothesis"><a href="#Error-and-test-of-hypothesis" class="headerlink" title="Error and test of hypothesis"></a>Error and test of hypothesis</h1><h2 id="True-error-of-hypothesis"><a href="#True-error-of-hypothesis" class="headerlink" title="True error of hypothesis"></a>True error of hypothesis</h2><p>sample error 和 true error 是不一样的。从 X 中抽取的样本 S，hypothesis h 关于 S 的 sample error 是 h 的实例在 S 中所在的比例，true error 也就是 true error 指的是对按某个分布随机抽取的实例，h 对它错误分类的概率。</p>
<p>h was wrong on 3/20 TEST ~ D<br>True error rate of $P(h(x)) \neq c(x)$   x ~ D</p>
<p>可能样本里只有一个 instance 错误，但是真实中会有很多这样的 input。</p>
<p>如果 $\hat \theta_{error} = 0.15$</p>
<h2 id="Compare-two-hypothesis"><a href="#Compare-two-hypothesis" class="headerlink" title="Compare two hypothesis"></a>Compare two hypothesis</h2><p>计算 sample error 之间的差异，通过统计学理论计算。<br>$Err(h_A) &gt;&lt;= Err(h_B)$ =&gt; Err(h_B) - Err(h_A) &gt; 0?<br>Depend on sample</p>
<h2 id="Compare-learning-algorithms"><a href="#Compare-learning-algorithms" class="headerlink" title="Compare learning algorithms"></a>Compare learning algorithms</h2><p>La: training set -&gt; $h_A$<br>Lb: training set -&gt; $h_b$</p>
<p>Compare h_A(test set y)  h_B(test set y)</p>
<p>better on this training set<br>avg performance of La is better than Lb</p>
<p>True error, E[Err_{test~D}[$L_A$(train ~ D)]]  this is hypothesis</p>
</outlk>
      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/uploads/wechat.jpg" alt="徐阿衡 wechat" style="width: 200px; max-width: 100%;"/>
    <div>欢迎关注：徐阿衡的微信公众号</div>
</div>


      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>客官，打个赏呗~</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="http://7xu83c.com1.z0.glb.clouddn.com/1.pic.jpg" alt="徐阿衡 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
    </div>
  </div>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag">#NLP</a>
          
            <a href="/tags/machine-learning/" rel="tag">#machine learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/11/26/Hidden-Markov-Models/" rel="next" title="Hidden-Markov-Models">
                <i class="fa fa-chevron-left"></i> Hidden-Markov-Models
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/11/29/Search Engines笔记 - ReDDE Algorithm for Resource Selection /" rel="prev" title="论文笔记 - ReDDE Algorithm for Resource Selection">
                论文笔记 - ReDDE Algorithm for Resource Selection <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      



    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
     
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://7xu83c.com1.z0.glb.clouddn.com/2.pic.jpg"
               alt="徐阿衡" />
          <p class="site-author-name" itemprop="name">徐阿衡</p>
          <p class="site-description motion-element" itemprop="description">读万卷书，行万里路 @SYSU @CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">160</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">123</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Shuang0420" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.linkedin.com/in/shuang-xu-7008b894?trk=nav_responsive_tab_profile_pic" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://zhuanlan.zhihu.com/c_136690664" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://starllap.space" title="Star" target="_blank">Star</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://liam0205.me" title="Liam Huang" target="_blank">Liam Huang</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.libinx.com" title="Li Bin" target="_blank">Li Bin</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Generative-and-discriminative-algorithms"><span class="nav-number">1.</span> <span class="nav-text">Generative and discriminative algorithms</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MLE-and-MAP"><span class="nav-number">2.</span> <span class="nav-text">MLE and MAP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bayesian-statistics"><span class="nav-number">3.</span> <span class="nav-text">Bayesian statistics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#主旨"><span class="nav-number">3.1.</span> <span class="nav-text">主旨</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图示"><span class="nav-number">3.2.</span> <span class="nav-text">图示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#公式"><span class="nav-number">3.3.</span> <span class="nav-text">公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Base-rate-neglect"><span class="nav-number">3.4.</span> <span class="nav-text">Base rate neglect</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MAP-Maximum-a-posteriori"><span class="nav-number">3.5.</span> <span class="nav-text">MAP (Maximum a posteriori)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-1-Cancer"><span class="nav-number">3.6.</span> <span class="nav-text">Example 1. Cancer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-2-Cancer-cont"><span class="nav-number">3.7.</span> <span class="nav-text">Example 2. Cancer cont.</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Method-1-Sequential-apply-of-Baye’s-Rules"><span class="nav-number">3.7.1.</span> <span class="nav-text">Method 1: Sequential apply of Baye’s Rules</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Method-2"><span class="nav-number">3.7.2.</span> <span class="nav-text">Method 2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">3.8.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Frequentist-statistics"><span class="nav-number">4.</span> <span class="nav-text">Frequentist statistics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP-Maximum-likelihood-principle"><span class="nav-number">4.1.</span> <span class="nav-text">MLP (Maximum likelihood principle)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-1-Binomial-distribution-X-B-n-p"><span class="nav-number">4.2.</span> <span class="nav-text">Example 1. Binomial distribution X~B(n,p)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-2-Gaussian"><span class="nav-number">4.3.</span> <span class="nav-text">Example 2.Gaussian</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-3-Linear-regression"><span class="nav-number">4.4.</span> <span class="nav-text">Example 3. Linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Special-case-in-Bayesian"><span class="nav-number">4.4.1.</span> <span class="nav-text">Special case in Bayesian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-estimate-P"><span class="nav-number">4.4.2.</span> <span class="nav-text">How to estimate P?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MDL-minimum-length-description-principle"><span class="nav-number">5.</span> <span class="nav-text">MDL (minimum length description principle)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Naive-Bayes-Classification"><span class="nav-number">6.</span> <span class="nav-text">Naive Bayes Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Goal"><span class="nav-number">6.1.</span> <span class="nav-text">Goal</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unbiased-Learning-of-Bayes-Classifiers-is-Impractical"><span class="nav-number">6.1.1.</span> <span class="nav-text">Unbiased Learning of Bayes Classifiers is Impractical</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-Algorithm"><span class="nav-number">6.1.2.</span> <span class="nav-text">Naive Bayes Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naive-bayes-assumption-Independence-assumption"><span class="nav-number">6.2.</span> <span class="nav-text">Naive bayes assumption (Independence assumption)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Derivation-of-Naive-Bayes-Algorithm"><span class="nav-number">6.3.</span> <span class="nav-text">Derivation of Naive Bayes Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naive-Bayes-for-Discrete-Valued-Inputs"><span class="nav-number">6.4.</span> <span class="nav-text">Naive Bayes for Discrete-Valued Inputs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parameters"><span class="nav-number">6.4.1.</span> <span class="nav-text">Parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimate-parameters"><span class="nav-number">6.4.2.</span> <span class="nav-text">Estimate parameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naive-Bayes-for-Continuous-Inputs"><span class="nav-number">6.5.</span> <span class="nav-text">Naive Bayes for Continuous Inputs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Squashing"><span class="nav-number">6.6.</span> <span class="nav-text">Squashing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naive-Bayes-Classifier-vs-Logistic-Regression"><span class="nav-number">6.7.</span> <span class="nav-text">Naive Bayes Classifier vs Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm"><span class="nav-number">6.8.</span> <span class="nav-text">Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example"><span class="nav-number">6.9.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Learn-Naive-Bayes-Text"><span class="nav-number">6.10.</span> <span class="nav-text">Learn Naive Bayes Text</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#算法"><span class="nav-number">6.10.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python-代码"><span class="nav-number">6.10.2.</span> <span class="nav-text">python 代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Error-and-test-of-hypothesis"><span class="nav-number">7.</span> <span class="nav-text">Error and test of hypothesis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#True-error-of-hypothesis"><span class="nav-number">7.1.</span> <span class="nav-text">True error of hypothesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Compare-two-hypothesis"><span class="nav-number">7.2.</span> <span class="nav-text">Compare two hypothesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Compare-learning-algorithms"><span class="nav-number">7.3.</span> <span class="nav-text">Compare learning algorithms</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <!-- Other code may be here -->
<div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">徐阿衡</span>
  <a href="http://www.miitbeian.gov.cn/">粤ICP备17129486号</a>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'httpshuang0420githubio';
      var disqus_identifier = '2016/11/28/Bayesian-Learning/';
      var disqus_title = "Bayesian Learning";
      var disqus_url = 'http://www.shuang0420.com/2016/11/28/Bayesian-Learning/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
