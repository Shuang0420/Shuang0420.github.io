<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />













  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Crawler," />





  <link rel="alternate" href="/atom.xml" title="徐阿衡" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.3" />






<meta name="description" content="用现成的框架的好处就是不用担心 cookie、retry、频率限制、多线程的事。这一篇把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&amp;gt; 定义目标（Items）–&amp;gt; 制作爬虫（Spider）–&amp;gt; 存储结果（Pipeline）">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫总结(二)-- scrapy">
<meta property="og:url" content="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/index.html">
<meta property="og:site_name" content="徐阿衡">
<meta property="og:description" content="用现成的框架的好处就是不用担心 cookie、retry、频率限制、多线程的事。这一篇把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&amp;gt; 定义目标（Items）–&amp;gt; 制作爬虫（Spider）–&amp;gt; 存储结果（Pipeline）">
<meta property="og:image" content="http://7xu83c.com1.z0.glb.clouddn.com/crawler.png">
<meta property="og:updated_time" content="2018-11-25T08:24:15.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫总结(二)-- scrapy">
<meta name="twitter:description" content="用现成的框架的好处就是不用担心 cookie、retry、频率限制、多线程的事。这一篇把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&amp;gt; 定义目标（Items）–&amp;gt; 制作爬虫（Spider）–&amp;gt; 存储结果（Pipeline）">
<meta name="twitter:image" content="http://7xu83c.com1.z0.glb.clouddn.com/crawler.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6294135991397516000',
      author: '阿衡'
    }
  };
</script>

<script async src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6146435155426457",
    enable_page_level_ads: true
  });
</script>




  <link rel="canonical" href="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/"/>


  <title> 爬虫总结(二)-- scrapy | 徐阿衡 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">徐阿衡</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Shuang</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-works">
          <a href="/works" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Works
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/aboutme" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input" placeholder="search my blog...">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
         
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                爬虫总结(二)-- scrapy
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-06-12T17:59:51+08:00" content="2016-06-12">
              2016-06-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  , 
                

              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/Crawler/" itemprop="url" rel="index">
                    <span itemprop="name">Crawler</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/12/爬虫总结-二-scrapy/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/06/12/爬虫总结-二-scrapy/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
              &nbsp; | &nbsp;
              <span class="page-pv"><i class="fa fa-file-o"></i>
              <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
              </span>
          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>用现成的框架的好处就是不用担心 cookie、retry、频率限制、多线程的事。这一篇把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&gt; 定义目标（Items）–&gt; 制作爬虫（Spider）–&gt; 存储结果（Pipeline）<br><a id="more"></a></p>
<h1 id="Scrapy-概述"><a href="#Scrapy-概述" class="headerlink" title="Scrapy 概述"></a>Scrapy 概述</h1><blockquote>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试</p>
</blockquote>
<h2 id="Scrapy-架构"><a href="#Scrapy-架构" class="headerlink" title="Scrapy 架构"></a>Scrapy 架构</h2><p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/crawler.png" alt=""></p>
<p>绿线是数据流向，首先从初始 URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，Spider 分析出来的结果有两种：一种是需要进一步抓取的链接，例如之前分析的“下一页”的链接，这些东西会被传回 Scheduler ；另一种是需要保存的数据，它们则被送到 Item Pipeline 那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。</p>
<h2 id="Scrapy-组件"><a href="#Scrapy-组件" class="headerlink" title="Scrapy 组件"></a>Scrapy 组件</h2><ul>
<li>引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li>调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li>下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li>爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li>项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li>下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li>爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li>调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<h2 id="Scrapy-运行流程"><a href="#Scrapy-运行流程" class="headerlink" title="Scrapy 运行流程"></a>Scrapy 运行流程</h2><ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>若是解析出实体（Item）,则交给实体管道进行进一步的处理;若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取</li>
</ol>
<p>默认情况下，Scrapy使用 LIFO 队列来存储等待的请求。简单的说，就是 <strong>深度优先顺序</strong> 。如果想要 <strong>广度优先顺序</strong> 进行爬取，需要进行设定。</p>
<h2 id="Scrapy-存在的问题"><a href="#Scrapy-存在的问题" class="headerlink" title="Scrapy 存在的问题"></a>Scrapy 存在的问题</h2><p>爬虫是一个很依赖于网络io的应用，单机的处理能力有限，很快就变成瓶颈。而scrapy并不是一个分布式的设计，在需要大规模爬取的情况下就很成问题。当然可以通过修改Request队列来实现分布式爬取，而且工作量也不算特别大。</p>
<ul>
<li>scrapy的并行度不高。力图在爬虫里做一些计算性的操作就会影响抓取的速率。这主要是python里的线程机制造成的，因为Python使用了GIL(和Ruby一样)，多线程并不会带来太多速度上的提升(除非用Python的C扩展实现自己的模块，这样绕过了GIL)。Summary:Use Python threads if you need to run IO operations in parallel. Do not if you need to run computations in parallel.</li>
<li>scrapy的内存消耗很快。可能是出于性能方面的考虑，pending requests并不是序列化存储在硬盘中，而是放在内存中的(毕竟IO很费时)，而且所有Request都放在内存中。你抓取到 百万网页的时候，考虑到单个网页时产生很多链接的，pending request很可能就近千万了，加上脚本语言里的对象本来就有额外成本，再考虑到GC不会立即释放内存，内存占用就相当可观了。<br>归根到底，这两个问题是根植于语言之中的。</li>
</ul>
<h1 id="Scrapy-实例"><a href="#Scrapy-实例" class="headerlink" title="Scrapy 实例"></a>Scrapy 实例</h1><h2 id="新建项目-Project"><a href="#新建项目-Project" class="headerlink" title="新建项目 (Project)"></a>新建项目 (Project)</h2><pre>
scrapy startproject news_scrapy
</pre>

<p>输入以上命令之后，就会看见命令行运行的目录下多了一个名为 news_scrapy 的目录，目录的结构如下：</p>
<pre>
|---- news_scrapy
| |---- news_scrapy
|   |---- __init__.py
|   |---- items.py        #用来存储爬下来的数据结构（字典形式）
|    |---- pipelines.py    #用来对爬出来的item进行后续处理，如存入数据库等
|    |---- settings.py    #爬虫配置文件
|    |---- spiders        #此目录用来存放创建的新爬虫文件（爬虫主体）
|     |---- __init__.py
| |---- scrapy.cfg        #项目配置文件
</pre>

<h2 id="定义目标（Items）"><a href="#定义目标（Items）" class="headerlink" title="定义目标（Items）"></a>定义目标（Items）</h2><p>Items是装载抓取的数据的容器，工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误<br>通过创建scrapy.Item类, 并且定义类型为 scrapy.Field 的类属性来声明一个Item，通过将需要的item模型化，来控制站点数据。<br>编辑 items.py</p>
<pre>
# -*- coding: utf-8 -*-
import scrapy
class NewsScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    category = scrapy.Field()
    url = scrapy.Field()
    secondary_title = scrapy.Field()
    secondary_url = scrapy.Field()
    #text = Field()
</pre>


<h2 id="制作爬虫（Spider）"><a href="#制作爬虫（Spider）" class="headerlink" title="制作爬虫（Spider）"></a>制作爬虫（Spider）</h2><p>Spider 定义了用于下载的URL列表、跟踪链接的方案、解析网页内容的方式，以此来提取items。<br>要建立一个Spider，你必须用scrapy.spider.BaseSpider创建一个子类，并确定三个强制的属性：</p>
<ul>
<li>name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。</li>
<li>start_urls：爬取的URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li>
<li>parse()：解析的方法，调用的时候传入从每一个URL传回的Response对象作为唯一参数，负责解析并匹配抓取的数据(解析为item)，跟踪更多的URL。</li>
</ul>
<p>在 spiders 目录下新建 Wynews.py，代码如下。利用 yield Request(url=item[‘url’],meta={‘item_1’: item},callback=self.second_parse) 来进行第二层爬取。</p>
<pre>
class WynewsSpider(BaseSpider):
    name = "Wynews"
    start_urls = ['http://news.163.com/rank/']

    def parse(self,response):
        html = HtmlXPathSelector(response)
        page = html.xpath('//div[@class="subNav"]/a')
        for i in page:
            item = dict()
            item['category'] = i.xpath('text()').extract_first()
            item['url'] = i.xpath('@href').extract_first()
            print item['category'],item['url']
            yield Request(url=item['url'],meta={'item_1': item},callback=self.second_parse)

    def second_parse(self,response):
        item_1= response.meta['item_1']
        html = HtmlXPathSelector(response)
        #print 'response ',response
        page = html.xpath('//tr/td/a')
        #print 'page ',page
        items = []
        for i in page:
            item = DidiScrapyItem()
            item['category'] = item_1['category'].encode('utf8')
            item['url'] = item_1['url'].encode('utf8')
            item['secondary_title'] = i.xpath('text()').extract_first().encode('utf8')
            item['secondary_url'] = i.xpath('@href').extract_first().encode('utf8')
            #print i.xpath('text()').extract(),i.xpath('@href').extract()
            items.append(item)
        return items
</pre>

<h2 id="存储结果（Pipeline）"><a href="#存储结果（Pipeline）" class="headerlink" title="存储结果（Pipeline）"></a>存储结果（Pipeline）</h2><p>Item pipeline 的主要责任是负责处理 spider 抽取的 Item，主要任务是清理、验证和存储数据。当页面被 spider 解析后，将被发送到 pipeline，每个 pipeline 的组件都是由一个简单的方法组成的Python类。pipeline 获取Item，执行相应的方法，并确定是否需要在 pipeline中继续执行下一步或是直接丢弃掉不处理。</p>
<h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><ul>
<li>清理HTML数据</li>
<li>验证解析到的数据（检查Item是否包含必要的字段）</li>
<li>检查是否是重复数据（如果重复就删除）</li>
<li>将解析到的数据存储到 数据库/文件 中</li>
</ul>
<h3 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h3><ul>
<li><p>process_item(item, spider)<br>每一个item管道组件都会调用该方法，并且必须返回一个item对象实例或raise DropItem异常。<br>被丢掉的item将不会在管道组件进行执行</p>
</li>
<li><p>open_spider(spider)<br>当spider执行的时候将调用该方法</p>
</li>
<li><p>close_spider(spider)<br>当spider关闭的时候将调用该方法</p>
</li>
</ul>
<h3 id="编写自己的-Pipeline"><a href="#编写自己的-Pipeline" class="headerlink" title="编写自己的 Pipeline"></a>编写自己的 Pipeline</h3><p>编辑 pipelines.py。把抓取的 items 保存到 json 文件中。</p>
<pre>
import json
class NewsScrapyPipeline(object):
    def __init__(self):
        self.file = open('items.json', 'w')
    def process_item(self, item, spider):
        line = json.dumps(dict(item),ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
</pre>

<p>另外，如果不考虑编码（没有中文），可以在运行爬虫的时候直接通过下面的命令导出结果。</p>
<p>dump到JSON文件:</p>
<pre>scrapy crawl myspider -o items.json</pre>

<p>dump到CSV文件:</p>
<pre>scrapy crawl myspider -o items.csv</pre>

<p>dump到XML文件:</p>
<pre>scrapy crawl myspider -o items.xml</pre>


<h3 id="激活Item-Pipeline组件"><a href="#激活Item-Pipeline组件" class="headerlink" title="激活Item Pipeline组件"></a>激活Item Pipeline组件</h3><p>在settings.py文件中，往ITEM_PIPELINES中添加项目管道的类名，激活项目管道组件</p>
<pre>
ITEM_PIPELINES = {
    'news_scrapy.pipelines.NewsScrapyPipeline': 300,
}
</pre>

<h2 id="开启爬虫-Crawl"><a href="#开启爬虫-Crawl" class="headerlink" title="开启爬虫 (Crawl)"></a>开启爬虫 (Crawl)</h2><pre>scrapy crawl Wynews</pre>

<p><a href="https://github.com/Shuang0420/Crawler/tree/master/news_scrapy" target="_blank" rel="external">完整代码</a></p>
<h2 id="可能出现的问题-Problem"><a href="#可能出现的问题-Problem" class="headerlink" title="可能出现的问题 (Problem)"></a>可能出现的问题 (Problem)</h2><p>打开 items.json 文件，中文可能会出现文件乱码问题</p>
<pre>
[{"category": "\u93c2\u4f34\u6908", "url": "http://news.163.com/special/0001386F/rank_news.html", "secondary_title": "\u934b\u950b\u9422\u5cf0\u30b3\u95c3\u8e6d\u7b09\u9473\u6ec8\u69fb\u951b\u5c7e\u5d0f\u6fc2\u7a3f\u5dfb\u9359\u53c9\u7c2e\u6769\u6ec4\u7966\u95c0", "secondary_url": "http://caozhi.news.163.com/16/0615/09/BPJG6SB60001544E.html"},
</pre>

<p>这一行代码就能解决。</p>
<pre>
line = json.dumps(dict(item),ensure_ascii=False) + "\n"
</pre>

<p>结果</p>
<pre>
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股闯关MSCI再度失败 索罗斯们押注对冲胜出", "secondary_url": "http://money.163.com/16/0615/06/BPJ4T69300253B0H.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "湖北副省长担心房价下跌：泡沫若破裂后果很严重", "secondary_url": "http://money.163.com/16/0615/08/BPJBM36U00252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "马云:假货质量超过正品 打假很复杂", "secondary_url": "http://money.163.com/16/0615/08/BPJAIOVI00253G87.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股闯关未成功 纳入MSCI新兴市场指数被延迟", "secondary_url": "http://money.163.com/16/0615/07/BPJ7260D00252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "马云称许多假货比真品好 网友:怪不得淘宝假货多", "secondary_url": "http://money.163.com/16/0615/08/BPJC437N002526O3.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "贪官示意家人低价买地 拆迁后获赔近亿元", "secondary_url": "http://money.163.com/16/0615/08/BPJAT58400252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "又是毒胶囊:浙江查获1亿多粒毒胶囊 6人被捕", "secondary_url": "http://money.163.com/16/0615/07/BPJ8NMRG00253B0H.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "还不起了？委内瑞拉寻求宽限1年偿还中国贷款", "secondary_url": "http://money.163.com/16/0615/07/BPJ9IH3400252C1E.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股频现清仓式减持 上半年十大减持王曝光", "secondary_url": "http://money.163.com/16/0615/07/BPJ7Q9BC00254IU4.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "《装X购车指南》 30-50万都能买到啥车？", "secondary_url": "http://auto.163.com/16/0615/07/BPJ6U1J900084TUP.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "看挡杆还以为是A8L 新款哈弗H9内饰曝光", "secondary_url": "http://auto.163.com/16/0615/00/BPIGTP4B00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "前脸/尾灯有变 新款捷达搭1.5L油耗更低", "secondary_url": "http://auto.163.com/16/0615/00/BPIGMEHE00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "主打车型不超10万良心价 远景SUV将8月上市", "secondary_url": "http://auto.163.com/16/0615/00/BPIHR2A500084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "Macan并不是我真姓 众泰SR8搭2.0T/D", "secondary_url": "http://auto.163.com/16/0613/00/BPDBPB0J00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "上海福特翼搏优惠1.5万元", "secondary_url": "http://auto.163.com/16/0615/00/BPIHH8FF000857M6.html"}
</pre>

<h2 id="添加命令行参数"><a href="#添加命令行参数" class="headerlink" title="添加命令行参数"></a>添加命令行参数</h2><p>第一种方法，在命令行用crawl控制spider爬取的时候，加上-a选项，如</p>
<pre>scrapy crawl WangyiSpider -a category=打车</pre>

<p>然后在 spider 的构造函数里加上带入的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class WangyiSpider(BaseSpider):</div><div class="line">    name = &quot;Wangyi&quot;</div><div class="line"></div><div class="line">    def __init__(self, category=None, *args, **kwargs):</div><div class="line">        super(WangyiSpider, self).__init__(*args, **kwargs)</div><div class="line">        self.base_url = &apos;http://news.yodao.com/&apos;</div><div class="line">        self.start_urls = [&apos;http://news.yodao.com/search?q=&apos; +</div><div class="line">                           category]</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/Shuang0420/Crawler/blob/master/wangyi/wangyi/spiders/WangyiSpider.py" target="_blank" rel="external">代码</a><br><a href="https://github.com/Shuang0420/Crawler/tree/master/wangyi" target="_blank" rel="external">通过关键词爬取网易新闻－代码</a></p>
<h2 id="运行多个爬虫"><a href="#运行多个爬虫" class="headerlink" title="运行多个爬虫"></a>运行多个爬虫</h2><p>默认情况当你每次执行scrapy crawl命令时会创建一个新的进程。但我们可以使用核心API在同一个进程中同时运行多个spider，如下，在 settings.py 的同级目录下编辑 run.py，导入编写的 spider 类如 JingdongSpider, SuningSpider。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">from twisted.internet import reactor</div><div class="line">from scrapy.crawler import CrawlerRunner</div><div class="line">from scrapy.utils.log import configure_logging</div><div class="line">from scrapy.spiders import Spider</div><div class="line">from scrapy.selector import HtmlXPathSelector</div><div class="line">from items import FaqscrapyItem</div><div class="line">from scrapy.http import Request</div><div class="line">from scrapy.selector import Selector</div><div class="line">from scrapy.utils.project import get_project_settings</div><div class="line">from spiders.FAQ_jingdong import JingdongSpider</div><div class="line">from spiders.FAQ_suning import SuningSpider</div><div class="line">import re</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    settings = get_project_settings()</div><div class="line">    configure_logging(settings)</div><div class="line">    runner = CrawlerRunner(settings)</div><div class="line"></div><div class="line">    runner.crawl(JingdongSpider)</div><div class="line">    runner.crawl(SuningSpider)</div><div class="line"></div><div class="line">    d = runner.join()</div><div class="line">    d.addBoth(lambda _: reactor.stop())</div><div class="line"></div><div class="line">    # blocks process so always keep as the last statement</div><div class="line">    reactor.run()</div></pre></td></tr></table></figure></p>
<p>然而不幸的是同一进程内运行多个 spider 可能会出现数据丢失问题，影响进一步的数据使用。如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;url&quot;: &quot;http://help.jd.com/user/issue/231-213.html&quot;, &quot;text&quot;: &quot;订单已提交成功，如何付款？付款方式分为以下几种：（注：先款订单请您在订单提交后24小时内完成支付，否则订单会自动取消）1.货到付款：选择货到付款，在订单送达时您可选择现金、POS机刷卡、支票方式支付货款或通过京东APP手机客户端【扫一扫】功能扫描包裹单上的订单条形码方式用手机来完成订单的支付（扫码支付）；在订单未妥投之前您还可以进入“我的订单”在线支付货款。注意：货到付款的订单，如果一个ID帐号在一个月内有过1次以上或一年内有过3次以上，无理由不接收我司配送的商品，我司将在相应的ID帐户里按每单扣除500个京豆做为运费；时间计算方法为：成功提交订单后向前推算30天为一个月，成功提交订单后向前推算365天为一年，不以自然月和自然年计算。2.在线支付：选择在线支付，请您进入“我的订单”，点击“付款”，按提示进行操作；目前在线支付支持京东白条、余额、银行卡、网银+、微信、银联在线、网银钱包、信用卡等方式进行支付，可根据您的使用喜好进行选择。3.分期付款：目前不支持信用卡分期付款。4.公司转账：提交订单后选择线下公司转账会生成15位汇款识别码，请您按照提示到银行操作转账，然后进入“我的订单”填写付款确认；5.邮局汇款：订单提交成功后，请您按照提示到邮局操作汇款，然后进入“我的订单”填写付款确认。&quot;, &quot;question&quot;: &quot;订单已提交成功，如何付款？&quot;, &quot;title&quot;: &quot;支付流程&quot;&#125;</div><div class="line">�系统停机维护期间。（二） 电信设备出现故障不能进行数据传输的。（三） 由于黑客攻击、网络供应商技术调整或故障、网站升级、银行方面的问题等原因而造成的易付宝服务中断或延迟。（四） 因台风、地震、海啸、洪水、停电、战争、恐怖袭击等不可抗力之因素，造成易付宝系统障碍不能执行业务的。 第十三条  关于本协议条款和其他协议、告示或其他有关您使用本服务的通知，易付宝将以电子形式或纸张形式通知您，包括但不限于依据您向易付宝提供的电子邮件地址发送电子邮件的方式、依据投资者提供的联系地址寄送挂号信的方式、易付宝或合作伙伴网站公告、或发送手机短信、系统内通知和电话通知等方式。 第十四条  易付宝有权根据需要不时地修改本协议或制定、修改各类规则，但是，对于减少您权益或加重您义务的新增、变更或修改，易付宝将在生效日前提前至少7个日历日进行公示，如您不同意相关新增、变更或修改，您可以选择在公示期内终止本协议并停止使用本服务。如果相关新增、变更或修改生效后，您继续使用本服务则表示您接受修订后的权利义务条款。 第十五条 因本协议引起的或与本协议有关的争议，均适用中华人民共和国法律。 第十六条  因本协议引起的或与本协议有关的争议，易付宝与用户协商解决。协商不成的，任何一方均有权向被告住所地人民法院提起诉讼。 第十七条   本协议作为《易付宝余额理财服务协议》的有效补充，本协议未约定的内容，双方需按照《易付宝余额理财服务协议》相关约定。 &quot;, &quot;question&quot;: &quot;零钱宝定期转出服务协议&quot;, &quot;title&quot;: &quot;苏宁理财&quot;&#125;</div><div class="line">l&quot;: &quot;http://help.suning.com/page/id-536.htm&quot;, &quot;text&quot;: &quot;</div></pre></td></tr></table></figure></p>
<p><a href="">代码</a></p>
<h1 id="Scrapy-调优"><a href="#Scrapy-调优" class="headerlink" title="Scrapy 调优"></a>Scrapy 调优</h1><h2 id="提高并发能力"><a href="#提高并发能力" class="headerlink" title="提高并发能力"></a>提高并发能力</h2><h3 id="增加并发"><a href="#增加并发" class="headerlink" title="增加并发"></a>增加并发</h3><p>并发是指同时处理的request的数量。其有全局限制和局部(每个网站)的限制。Scrapy 默认的全局并发限制(16)对同时爬取大量网站的情况并不适用，因此需要增加这个值。 增加多少取决于爬虫能占用多少CPU。 一般开始可以设置为 100 。不过最好的方式是做一些测试，获得 Scrapy 进程占取CPU与并发数的关系。选择一个能使CPU占用率在80%-90%的并发数比较恰当。</p>
<pre>
# 增加全局并发数
CONCURRENT_REQUESTS = 100</pre>

<p>mac 下调试，运行程序后通过 top 监控，p 按 cpu 排序。观察发现，在 CONCURRENT_REQUESTS = 32 时，cpu 占用最多到 50% 左右，调整到 CONCURRENT_REQUESTS = 100，cpu 占用 90% 上下。</p>
<p>查看本机 cpu 信息，用 sysctl machdep.cpu 命令，如下，可以看到我的机子是双核、4线程的。</p>
<pre>
# cpu 信息
$ sysctl machdep.cpu
..........
machdep.cpu.core_count: 2
machdep.cpu.thread_count: 4
machdep.cpu.tsc_ccc.numerator: 0
machdep.cpu.tsc_ccc.denominator: 0
</pre>

<h3 id="降低log级别"><a href="#降低log级别" class="headerlink" title="降低log级别"></a>降低log级别</h3><p>为了减少CPU使用率(及记录log存储的要求), 当调试程序完毕后，可以不使用 DEBUG log级别。</p>
<pre>
# 设置Log级别:
LOG_LEVEL = 'INFO'</pre>


<h3 id="禁止cookies"><a href="#禁止cookies" class="headerlink" title="禁止cookies"></a>禁止cookies</h3><p>禁止cookies能减少CPU使用率及Scrapy爬虫在内存中记录的踪迹，提高性能。</p>
<pre># 禁止cookies:
COOKIES_ENABLED = False</pre>


<h3 id="禁止重试"><a href="#禁止重试" class="headerlink" title="禁止重试"></a>禁止重试</h3><p>对失败的HTTP请求进行重试会减慢爬取的效率，尤其是当站点响应很慢(甚至失败)时， 访问这样的站点会造成超时并重试多次。这是不必要的，同时也占用了爬虫爬取其他站点的能力。</p>
<pre>
# 禁止重试:
RETRY_ENABLED = False</pre>

<h3 id="减小下载超时"><a href="#减小下载超时" class="headerlink" title="减小下载超时"></a>减小下载超时</h3><p>对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。</p>
<pre>
# 减小下载超时:
DOWNLOAD_TIMEOUT = 15

# 可能会引发的错误
TimeoutError: User timeout caused connection failure: Getting http://homea.people.com.cn/n1/2016/0628/c69176-28504657.html took longer than 15.0 seconds..</pre>

<p>通过如上配置，我的爬虫每分钟响应的request是之前的4倍，然而值得注意的是，这些设置并不是在所有场景都适用，需要通过具体场景试验，具体问题具体分析。</p>
<h2 id="避免被禁止-ban"><a href="#避免被禁止-ban" class="headerlink" title="避免被禁止(ban)"></a>避免被禁止(ban)</h2><p>有些网站实现了特定的机制，以一定规则来避免被爬虫爬取。下面是些处理这些站点的建议(tips):</p>
<ul>
<li>使用user agent池，轮流选择之一来作为user agent。池中包含常见的浏览器的user agent(google一下一大堆)</li>
<li>禁止cookies(参考 COOKIES_ENABLED)，有些站点会使用cookies来发现爬虫的轨迹。</li>
<li>设置下载延迟(2或更高)。参考 DOWNLOAD_DELAY 设置。</li>
<li>如果可行，使用 Google cache 来爬取数据，而不是直接访问站点。</li>
<li>使用IP池。例如免费的 Tor项目 或付费服务(ProxyMesh)。</li>
<li>使用高度分布式的下载器(downloader)来绕过禁止(ban)，就只需要专注分析处理页面。这样的例子有: Crawlera</li>
</ul>
<p>如果仍然无法避免被ban，考虑商业支持.</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><ol>
<li><p>首先要有的是 <strong>user agent池</strong> 和 <strong>IP池</strong>。user agent池如下，添加在 settings.py 中。</p>
<pre>
USER_AGENTS = [
 "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
 "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
 "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
 "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
 "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
 "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
 "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
 "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
 "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
 "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
 "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
 "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
]
</pre>
</li>
<li><p>IP池 获取方式有多种，这里抓取的是<a href="http://www.xicidaili.com/" target="_blank" rel="external">西刺免费代理IP</a>的 IP，注意实时更新问题，否则很容易失败。将抓取的 IP 以 <a href="http://host1:port" target="_blank" rel="external">http://host1:port</a> 的格式存储于 list.txt 文本中。在 settings.py 里添加 PROXY_LIST = ‘/path/to/proxy/list.txt’。</p>
</li>
<li><p>有了 <strong>user agent池</strong> 和 <strong>IP池</strong>，接下来需要编写中间件，如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line">import random</div><div class="line">import base64</div><div class="line">from scrapy import log</div><div class="line"></div><div class="line">class RandomProxy(object):</div><div class="line">    def __init__(self, settings):</div><div class="line">        self.proxy_list = settings.get(&apos;PROXY_LIST&apos;)</div><div class="line">        fin = open(self.proxy_list)</div><div class="line"></div><div class="line">        self.proxies = &#123;&#125;</div><div class="line">        for line in fin.readlines():</div><div class="line">            parts = re.match(&apos;(\w+://)(\w+:\w+@)?(.+)&apos;, line)</div><div class="line">            if not parts:</div><div class="line">                continue</div><div class="line"></div><div class="line">            # Cut trailing @</div><div class="line">            if parts.group(2):</div><div class="line">                user_pass = parts.group(2)[:-1]</div><div class="line">            else:</div><div class="line">                user_pass = &apos;&apos;</div><div class="line"></div><div class="line">            self.proxies[parts.group(1) + parts.group(3)] = user_pass</div><div class="line"></div><div class="line">        fin.close()</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def from_crawler(cls, crawler):</div><div class="line">        return cls(crawler.settings)</div><div class="line"></div><div class="line">    def process_request(self, request, spider):</div><div class="line">        # Don&apos;t overwrite with a random one (server-side state for IP)</div><div class="line">        if &apos;proxy&apos; in request.meta:</div><div class="line">            return</div><div class="line"></div><div class="line">        proxy_address = random.choice(self.proxies.keys())</div><div class="line">        proxy_user_pass = self.proxies[proxy_address]</div><div class="line"></div><div class="line">        request.meta[&apos;proxy&apos;] = proxy_address</div><div class="line">        if proxy_user_pass:</div><div class="line">            basic_auth = &apos;Basic &apos; + base64.encodestring(proxy_user_pass)</div><div class="line">            request.headers[&apos;Proxy-Authorization&apos;] = basic_auth</div><div class="line">            print &quot;**************ProxyMiddleware have pass************&quot; + proxy[&apos;ip_port&apos;]</div><div class="line"></div><div class="line">    def process_exception(self, request, exception, spider):</div><div class="line">        proxy = request.meta[&apos;proxy&apos;]</div><div class="line">        log.msg(&apos;Removing failed proxy &lt;%s&gt;, %d proxies left&apos; % (</div><div class="line">                    proxy, len(self.proxies)))</div><div class="line">        try:</div><div class="line">            del self.proxies[proxy]</div><div class="line">        except ValueError:</div><div class="line">            pass</div><div class="line"></div><div class="line"></div><div class="line">class RandomUserAgent(object):</div><div class="line">    &quot;&quot;&quot;Randomly rotate user agents based on a list of predefined ones&quot;&quot;&quot;</div><div class="line"></div><div class="line">    def __init__(self, agents):</div><div class="line">        self.agents = agents</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def from_crawler(cls, crawler):</div><div class="line">        return cls(crawler.settings.getlist(&apos;USER_AGENTS&apos;))</div><div class="line"></div><div class="line">    def process_request(self, request, spider):</div><div class="line">        print &quot;**************************&quot; + random.choice(self.agents)</div><div class="line">        request.headers.setdefault(&apos;User-Agent&apos;, random.choice(self.agents))</div></pre></td></tr></table></figure>
</li>
<li><p>改写 spider，check 某个元素，确保 proxy 能够返回 target page。</p>
<pre>
if not pageUrls:
 yield Request(url=response.url, dont_filter=True)
</pre>
</li>
<li><p>配置 settings.py</p>
<pre>
# Retry many times since proxies often fail
RETRY_TIMES = 10
# Retry on most error codes since proxies fail for different reasons
RETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]
# Configure a delay for requests for the same website (default: 0)
DOWNLOAD_DELAY=3
# Disable cookies (enabled by default)
COOKIES_ENABLED=False
# Enable downloader middlewares
DOWNLOADER_MIDDLEWARES = {
 'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 90,
 # Fix path to this module
 'blogCrawler.middlewares.RandomProxy': 100,
 'blogCrawler.middlewares.RandomUserAgent': 1,
 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,
}
</pre>


</li>
</ol>
<p><a href="https://github.com/Shuang0420/Crawler/tree/master/blogCrawler" target="_blank" rel="external">这是一份简单的测试代码</a></p>
<h3 id="其他调优"><a href="#其他调优" class="headerlink" title="其他调优"></a>其他调优</h3><p>来自 <a href="http://ju.outofmemory.cn/entry/18981" target="_blank" rel="external">使用scrapy进行大规模抓取</a></p>
<ol>
<li><p>如果想要爬取的质量更高，尽量使用宽度优先的策略，在配置里设置 SCHEDULER_ORDER = ‘BFO’</p>
</li>
<li><p>修改单爬虫的最大并行请求数 CONCURRENT_REQUESTS_PER_SPIDER</p>
</li>
<li><p>修改twisted的线程池大小，默认值是10。参考<a href="http://twistedmatrix.com/documents/10.1.0/core/howto/threading.html" target="_blank" rel="external">Using Threads in Twisted</a>，在scrapy/core/manage.py爬虫启动前加上</p>
</li>
</ol>
<pre>reactor.suggestThreadPoolSize(poolsize)</pre>

<ol>
<li><p>可以开启dns cache来提高性能。在配置里面加上</p>
<pre>EXTENSIONS={’scrapy.contrib.resolver.CachingResolver’: 0,}</pre>
</li>
<li><p>如果自己实现duplicate filter的话注意要保证它是一直可用的，dupfilter里的异常是不会出现在日志文件中的，好像外面做了try-expect处理</p>
</li>
</ol>
<h2 id="去重与增量抓取"><a href="#去重与增量抓取" class="headerlink" title="去重与增量抓取"></a>去重与增量抓取</h2><h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h3><p>Scrapy支持通过RFPDupeFilter来完成页面的去重（防止重复抓取）。RFPDupeFilter实际是根据request_fingerprint实现过滤的，实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def request_fingerprint(request, include_headers=None):</div><div class="line">if include_headers:</div><div class="line">  include_headers = tuple([h.lower() for h in sorted(include_headers)])</div><div class="line">cache = _fingerprint_cache.setdefault(request, &#123;&#125;)</div><div class="line">if include_headers not in cache:</div><div class="line">  fp = hashlib.sha1()</div><div class="line">  fp.update(request.method)</div><div class="line">  fp.update(canonicalize_url(request.url))</div><div class="line">  fp.update(request.body or &apos;&apos;)</div><div class="line">  if include_headers:</div><div class="line">    for hdr in include_headers:</div><div class="line">      if hdr in request.headers:</div><div class="line">        fp.update(hdr)</div><div class="line">        for v in request.headers.getlist(hdr):</div><div class="line">          fp.update(v)</div><div class="line">  cache[include_headers] = fp.hexdigest()</div><div class="line">return cache[include_headers]</div></pre></td></tr></table></figure></p>
<p>我们可以看到，去重指纹是sha1(method + url + body + header)，所以，实际能够去掉重复的比例并不大。</p>
<p>如果我们需要自己提取去重的finger，需要自己实现Filter，并配置上它。</p>
<p>例如下面这个Filter只根据url去重：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">from scrapy.dupefilter import RFPDupeFilter</div><div class="line">class SeenURLFilter(RFPDupeFilter):</div><div class="line">  &quot;&quot;&quot;A dupe filter that considers the URL&quot;&quot;&quot;</div><div class="line">  def __init__(self, path=None):</div><div class="line">    self.urls_seen = set()</div><div class="line">    RFPDupeFilter.__init__(self, path)</div><div class="line">  def request_seen(self, request):</div><div class="line">    if request.url in self.urls_seen:</div><div class="line">      return True</div><div class="line">    else:</div><div class="line">      self.urls_seen.add(request.url)</div></pre></td></tr></table></figure></p>
<p>要在 settings 添加配置。<br>DUPEFILTER_CLASS =’scraper.custom_filters.SeenURLFilter’</p>
<h3 id="增量爬取"><a href="#增量爬取" class="headerlink" title="增量爬取"></a>增量爬取</h3><p>可以看这篇<a href="http://www.ihowandwhy.com/z/%E5%9F%BA%E4%BA%8Epython%E7%9A%84scrapy%E7%88%AC%E8%99%AB%EF%BC%8C%E5%85%B3%E4%BA%8E%E5%A2%9E%E9%87%8F%E7%88%AC%E5%8F%96%E6%98%AF%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%9F" target="_blank" rel="external">汇总贴</a></p>
<p>其实如果根据 url 判断的话有很多种方案，如下面这种（比起上面汇总贴的其他方案来说算是复杂的）。</p>
<blockquote>
<p>增量抓取。一个针对多个网站的爬虫很难一次性把所有网页爬取下来，并且网页也处于不断更新的状态中，爬取是一个动态的过程，爬虫支持增量的抓取是很必要的。大概的流程就是关闭爬虫时保存duplicate filter的数据，保存当前的request队列，爬虫启动时导入duplicate filter，并且用上次request队列的数据作为start url。这里还涉及scrapy一个称得上bug的问题，一旦抓取队列里url过多，关闭scrapy需要很久，有时候要花费几天的时间。我们hack了scrapy的代码，在接收到关闭命令后，保存duplicate filter数据和当前的request队列和已抓取的url列表，然后调用twisted的reactor.stop()强制退出。当前的request队列可以通过scrapy.core.scheduler的pending_requests成员得到。</p>
</blockquote>
<p>然而，如果使所有网站的动态过滤，比如是不是多了一个新回复，在url上的变化并不能体现出来，搜索引擎采用的是一系列的算法，判断某一个页面的更新时机。这个时候只能尝试用网页在进入下一级页面的时候都类似于最后更新时间、最后活动时间的参数进行判断了。</p>
<p>有机会会去尝试。</p>
<blockquote>
<p>参考资料<br><a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/faq.html" target="_blank" rel="external">scrapy 文档</a><br><a href="http://m.blog.csdn.net/article/details?id=50748700" target="_blank" rel="external">向scrapy中的spider传递参数的几种方法</a><br><a href="http://ju.outofmemory.cn/entry/18981" target="_blank" rel="external">使用scrapy进行大规模抓取</a><br><a href="http://www.pycoding.com/2016/04/10/scrapy-10.html" target="_blank" rel="external">Scrapy笔记（10）- 动态配置爬虫</a></p>
</blockquote>

      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/uploads/wechat.jpg" alt="徐阿衡 wechat" style="width: 200px; max-width: 100%;"/>
    <div>欢迎关注：徐阿衡的微信公众号</div>
</div>


      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>客官，打个赏呗~</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="http://7xu83c.com1.z0.glb.clouddn.com/1.pic.jpg" alt="徐阿衡 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
    </div>
  </div>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Crawler/" rel="tag">#Crawler</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/06/11/爬虫总结（一）/" rel="next" title="爬虫总结(一)-- 爬虫基础 & python实现">
                <i class="fa fa-chevron-left"></i> 爬虫总结(一)-- 爬虫基础 & python实现
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/15/爬虫总结-三-scrapinghub/" rel="prev" title="爬虫总结(三)-- cloud scrapy">
                爬虫总结(三)-- cloud scrapy <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      



    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
     
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://7xu83c.com1.z0.glb.clouddn.com/2.pic.jpg"
               alt="徐阿衡" />
          <p class="site-author-name" itemprop="name">徐阿衡</p>
          <p class="site-description motion-element" itemprop="description">读万卷书，行万里路 @SYSU @CMU</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">161</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">124</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Shuang0420" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.linkedin.com/in/shuang-xu-7008b894?trk=nav_responsive_tab_profile_pic" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://zhuanlan.zhihu.com/c_136690664" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://starllap.space" title="Star" target="_blank">Star</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://liam0205.me" title="Liam Huang" target="_blank">Liam Huang</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.libinx.com" title="Li Bin" target="_blank">Li Bin</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-概述"><span class="nav-number">1.</span> <span class="nav-text">Scrapy 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-架构"><span class="nav-number">1.1.</span> <span class="nav-text">Scrapy 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-组件"><span class="nav-number">1.2.</span> <span class="nav-text">Scrapy 组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-运行流程"><span class="nav-number">1.3.</span> <span class="nav-text">Scrapy 运行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scrapy-存在的问题"><span class="nav-number">1.4.</span> <span class="nav-text">Scrapy 存在的问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-实例"><span class="nav-number">2.</span> <span class="nav-text">Scrapy 实例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#新建项目-Project"><span class="nav-number">2.1.</span> <span class="nav-text">新建项目 (Project)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定义目标（Items）"><span class="nav-number">2.2.</span> <span class="nav-text">定义目标（Items）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#制作爬虫（Spider）"><span class="nav-number">2.3.</span> <span class="nav-text">制作爬虫（Spider）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储结果（Pipeline）"><span class="nav-number">2.4.</span> <span class="nav-text">存储结果（Pipeline）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#执行过程"><span class="nav-number">2.4.1.</span> <span class="nav-text">执行过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#主要方法"><span class="nav-number">2.4.2.</span> <span class="nav-text">主要方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写自己的-Pipeline"><span class="nav-number">2.4.3.</span> <span class="nav-text">编写自己的 Pipeline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活Item-Pipeline组件"><span class="nav-number">2.4.4.</span> <span class="nav-text">激活Item Pipeline组件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开启爬虫-Crawl"><span class="nav-number">2.5.</span> <span class="nav-text">开启爬虫 (Crawl)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可能出现的问题-Problem"><span class="nav-number">2.6.</span> <span class="nav-text">可能出现的问题 (Problem)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#添加命令行参数"><span class="nav-number">2.7.</span> <span class="nav-text">添加命令行参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运行多个爬虫"><span class="nav-number">2.8.</span> <span class="nav-text">运行多个爬虫</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-调优"><span class="nav-number">3.</span> <span class="nav-text">Scrapy 调优</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#提高并发能力"><span class="nav-number">3.1.</span> <span class="nav-text">提高并发能力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#增加并发"><span class="nav-number">3.1.1.</span> <span class="nav-text">增加并发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#降低log级别"><span class="nav-number">3.1.2.</span> <span class="nav-text">降低log级别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#禁止cookies"><span class="nav-number">3.1.3.</span> <span class="nav-text">禁止cookies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#禁止重试"><span class="nav-number">3.1.4.</span> <span class="nav-text">禁止重试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#减小下载超时"><span class="nav-number">3.1.5.</span> <span class="nav-text">减小下载超时</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#避免被禁止-ban"><span class="nav-number">3.2.</span> <span class="nav-text">避免被禁止(ban)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实例"><span class="nav-number">3.2.1.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他调优"><span class="nav-number">3.2.2.</span> <span class="nav-text">其他调优</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#去重与增量抓取"><span class="nav-number">3.3.</span> <span class="nav-text">去重与增量抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#去重"><span class="nav-number">3.3.1.</span> <span class="nav-text">去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增量爬取"><span class="nav-number">3.3.2.</span> <span class="nav-text">增量爬取</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <!-- Other code may be here -->
<div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">徐阿衡</span>
  <a href="http://www.miitbeian.gov.cn/">粤ICP备17129486号</a>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'httpshuang0420githubio';
      var disqus_identifier = '2016/06/12/爬虫总结-二-scrapy/';
      var disqus_title = "爬虫总结(二)-- scrapy";
      var disqus_url = 'http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = false;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title >= 0 || index_content >= 0 ){
                                isMatch = true;
								if (i == 0) {
                                    first_occur = index_content;
                                }
                            } 
							
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>
