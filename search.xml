<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[知识抽取-实体及关系抽取]]></title>
      <url>http://www.shuang0420.com/2018/09/15/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/</url>
      <content type="html"><![CDATA[<p>这一篇是关于知识抽取，整理并补充了上学时的两篇笔记 <a href="http://www.shuang0420.com/2017/03/18/NLP%20笔记%20-Information%20Extraction/">NLP笔记 - Information Extraction</a> 和 <a href="http://www.shuang0420.com/2017/04/10/NLP笔记%20-%20Relation%20Extraction/">NLP笔记 - Relation Extraction</a>。</p>
<p>知识抽取涉及的“知识”通常是 <strong>清楚的、事实性的信息</strong>，这些信息来自不同的来源和结构，而对不同数据源进行的知识抽取的方法各有不同，从结构化数据中获取知识用 D2R，其难点在于复杂表数据的处理，包括嵌套表、多列、外键关联等，从链接数据中获取知识用图映射，难点在于数据对齐，从半结构化数据中获取知识用包装器，难点在于 wrapper 的自动生成、更新和维护，这一篇主要讲从文本中获取知识，也就是我们广义上说的信息抽取。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/general.png">
<p>信息抽取三个最重要/最受关注的子任务：</p>
<ul>
<li><strong>实体抽取</strong><br>也就是命名实体识别，包括实体的检测（find）和分类（classify）</li>
<li><strong>关系抽取</strong><br>通常我们说的<strong>三元组（triple）</strong> 抽取，一个谓词（predicate）带 2 个形参（argument），如 Founding-location(IBM,New York) </li>
<li><strong>事件抽取</strong><br>相当于一种多元关系的抽取</li>
</ul>
<p>篇幅限制，这一篇主要整理实体抽取和关系抽取，下一篇再上事件抽取。</p>
<h1 id="相关竞赛与数据集"><a href="#相关竞赛与数据集" class="headerlink" title="相关竞赛与数据集"></a>相关竞赛与数据集</h1><p>信息抽取相关的会议/数据集有 <strong>MUC、ACE、KBP、SemEval</strong> 等。其中，<strong>ACE(Automated Content Extraction)</strong> 对 MUC 定义的任务进行了融合、分类和细化，<strong><a href="https://tac.nist.gov/2017/KBP/" target="_blank" rel="external">KBP(Knowledge Base Population)</a></strong> 对 ACE 定义的任务进一步修订，分了四个独立任务和一个整合任务，包括</p>
<ul>
<li><strong>Cold Start KB (CSKB)</strong><br>端到端的冷启动知识构建</li>
<li><strong>Entity Discovery and Linking (EDL)</strong><br>实体发现与链接</li>
<li><strong>Slot Filling (SF)</strong><br>槽填充</li>
<li><strong>Event</strong><br>事件抽取</li>
<li><strong>Belief/Sentiment (BeSt)</strong><br>信念和情感</li>
</ul>
<p>至于 SemEval 主要是词义消歧评测，目的是增加人们对词义、多义现象的理解。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/SemEval.png"></p>
<p>ACE 的 17 类关系<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>具体的应用实例<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/3.jpg" class="ful-image" alt="3.jpg"></p>
<p>常用的 Freebase relations<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">people/person/nationality,</div><div class="line">people/person/profession,</div><div class="line">biology/organism_higher_classification,</div><div class="line">location/location/contains</div><div class="line">people/person/place-of-birth</div><div class="line">film/film/genre</div></pre></td></tr></table></figure></p>
<p>还有的一些世界范围内知名的高质量大规模开放知识图谱，如包括 DBpedia、Yago、Wikidata、BabelNet、ConceptNet 以及 Microsoft Concept Graph等，中文的有开放知识图谱平台 OpenKG……</p>
<h1 id="实体抽取"><a href="#实体抽取" class="headerlink" title="实体抽取"></a>实体抽取</h1><p>实体抽取或者说命名实体识别（NER）在信息抽取中扮演着重要角色，主要抽取的是文本中的原子信息元素，如<strong>人名、组织/机构名、地理位置、事件/日期、字符值、金额值</strong>等。实体抽取任务有两个关键词：<strong>find &amp; classify</strong>，找到命名实体，并进行分类。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/2.jpg" class="ful-image" alt="2.jpg"></p>
<p><strong>主要应用：</strong></p>
<ul>
<li>命名实体作为索引和超链接</li>
<li>情感分析的准备步骤，在情感分析的文本中需要识别公司和产品，才能进一步为情感词归类</li>
<li>关系抽取（Relation Extraction）的准备步骤</li>
<li>QA 系统，大多数答案都是命名实体</li>
</ul>
<h2 id="传统机器学习方法"><a href="#传统机器学习方法" class="headerlink" title="传统机器学习方法"></a>传统机器学习方法</h2><p>标准流程：<br><strong>Training:</strong></p>
<ol>
<li>收集代表性的训练文档</li>
<li>为每个 token 标记命名实体(不属于任何实体就标 Others O)</li>
<li>设计适合该文本和类别的特征提取方法</li>
<li>训练一个 sequence classifier 来预测数据的 label</li>
</ol>
<p><strong>Testing:</strong></p>
<ol>
<li>收集测试文档</li>
<li>运行 sequence classifier 给每个 token 做标记</li>
<li>输出命名实体</li>
</ol>
<h3 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h3><p>看一下最常用的两种 sequence labeling 的编码方式，<strong>IO encoding</strong> 简单的为每个 token 标注，如果不是 NE 就标为 O(other)，所以一共需要 C+1 个类别(label)。而 <strong>IOB encoding</strong> 需要 2C+1 个类别(label)，因为它标了 NE boundary，B 代表 begining，NE 开始的位置，I 代表 continue，承接上一个 NE，如果连续出现两个 B，自然就表示上一个 B 已经结束了。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/3.jpg" class="ful-image" alt="3.jpg"></p>
<p>在 Stanford NER 里，用的其实是 IO encoding，有两个原因，一是 IO encoding 运行速度更快，二是在实践中，两种编码方式的效果差不多。IO encoding 确定 boundary 的依据是，如果有连续的 token 类别不为 O，那么类别相同，同属一个 NE；类别不相同，就分割，相同的 sequence 属同一个 NE。而实际上，两个 NE 是相同类别这样的现象出现的很少，如上面的例子，Sue，Mengqiu Huang 两个同是 PER 类别，并不多见，更重要的是，在实践中，虽然 IOB encoding 能规定 boundary，而实际上它也很少能做对，它也会把 Sue Mengqiu Huang 分为同一个 PER，这主要是因为更多的类别会带来数据的稀疏。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>Features for sequence labeling:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">• Words</div><div class="line">    Current word (essentially like a learned dictionary)</div><div class="line">    Previous/next word (context)</div><div class="line">• Other kinds of inferred linguistic classification</div><div class="line">    Part of speech tags</div><div class="line">    Dependency relations</div><div class="line">• Label context</div><div class="line">    Previous (and perhaps next) label</div></pre></td></tr></table></figure></p>
<p>再来看两个比较重要的 feature</p>
<p><strong>Word substrings</strong><br>Word substrings (包括前后缀)的作用是很大的，以下面的例子为例，NE 中间有 ‘oxa’ 的十有八九是 drug，NE 中间有 ‘:’ 的则大多都是 movie，而以 field 结尾的 NE 往往是 place。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/4.jpg" class="ful-image" alt="4.jpg"></p>
<p><strong>Word shapes</strong><br>可以做一个 mapping，把 <strong>单词长度(length)、大写(capitalization)、数字(numerals)、希腊字母(Greek eltters)、单词内部标点(internal punctuation)</strong> 这些字本身的特征都考虑进去。<br>如下表，把所有大写字母映射为 X，小写字母映射为 x，数字映射为 d…<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/5.jpg" class="ful-image" alt="5.jpg"></p>
<h3 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h3><p>NLP 的很多数据都是序列类型，像 sequence of characters, words, phrases, lines, sentences，我们可以把这些任务当做是给每一个 item 打标签，如下图：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>常见的序列模型有 <strong>有向图模型</strong> 如 HMM，假设特征之间相互独立，找到使得 P(X,Y) 最大的参数，生成式模型；<strong>无向图模型</strong> 如 CRF，没有特征独立的假设，找到使得 P(Y|X) 最大的参数，判别式模型。相对而言，CRF 优化的是联合概率（整个序列，实际就是最终目标），而不是每个时刻最优点的拼接，一般而言性能比 CRF 要好，在小数据上拟合也会更好。</p>
<p>整个流程如图所示：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/7.jpg" class="ful-image" alt="7.jpg"></p>
<p>讨论下最后的 inference<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/8.jpg" class="ful-image" alt="8.jpg"></p>
<p>最基础的是 “decide one sequence at a time and move on”，也就是一个 <strong>greedy inference</strong>，比如在词性标注中，可能模型在位置 2 的时候挑了当前最好的 PoS tag，但是到了位置 4 的时候，其实发现位置 2 应该有更好的选择，然而，greedy inference 并不会 care 这些。因为它是贪婪的，只要当前最好就行了。除了 greedy inference，比较常见的还有 beam inference 和 viterbi inference。</p>
<h4 id="Greedy-Inference"><a href="#Greedy-Inference" class="headerlink" title="Greedy Inference"></a>Greedy Inference</h4><p><strong>优点:</strong></p>
<ol>
<li>速度快，没有额外的内存要求</li>
<li>非常易于实现</li>
<li>有很丰富的特征，表现不错</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>贪婪</li>
</ol>
<h4 id="Beam-Inference"><a href="#Beam-Inference" class="headerlink" title="Beam Inference"></a>Beam Inference</h4><ul>
<li>在每一个位置，都保留 top k 种可能(当前的完整序列)</li>
<li>在每个状态下，考虑上一步保存的序列来进行推进</li>
</ul>
<p><strong>优点:</strong></p>
<ol>
<li>速度快，没有额外的内存要求</li>
<li>易于实现(不用动态规划)</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>不精确，不能保证找到全局最优</li>
</ol>
<h4 id="Viterbi-Inference"><a href="#Viterbi-Inference" class="headerlink" title="Viterbi Inference"></a>Viterbi Inference</h4><ul>
<li>动态规划</li>
<li>需要维护一个 fix small window</li>
</ul>
<p><strong>优点:</strong></p>
<ol>
<li>非常精确，能保证找到全局最优序列</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>难以实现远距离的 state-state interaction</li>
</ol>
<h2 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h2><p>最经典的 <a href="hiheng Huang, Wei Xu, Kai Yu. Bidirectional LSTM-CRF Models for Sequence Tagging. CoRR. 2015">LSTM+CRF</a>，端到端的判别式模型，LSTM 利用过去的输入特征，CRF 利用句子级的标注信息，可以有效地使用过去和未来的标注来预测当前的标注。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/LSTM%2BCRF.png">
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>评估 IR 系统或者文本分类的任务，我们通常会用到 precision，recall，F1 这种 set-based metrics，见<a href="http://www.shuang0420.com/2016/09/20/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/">信息检索评价的 Unranked Boolean Retrieval Model 部分</a>，但是在这里对 NER 这种 sequence 类型任务的评估，如果用这些 metrics，可能出现 boundary error 之类的问题。因为 NER 的评估是按每个 entity 而不是每个 token 来计算的，我们需要看 entity 的 boundary。</p>
<p>以下面一句话为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">First Bank of Chicago announced earnings...</div></pre></td></tr></table></figure></p>
<p>正确的 NE 应该是 First Bank of Chicago，类别是 ORG，然而系统识别了 Bank of Chicago，类别 ORG，也就是说，右边界(right boundary)是对的，但是左边界(left boundary)是错误的，这其实是一个常见的错误。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">正确的标注：</div><div class="line">ORG - (1,4)</div><div class="line"></div><div class="line">系统：</div><div class="line">ORG - (2,4)</div></pre></td></tr></table></figure>
<p>而计算 precision，recall 的时候，我们会发现，对 ORG - (1,4) 而言，系统产生了一个 false negative，对 ORG - (2,4) 而言，系统产生了一个 false positive！所以系统有了 2 个错误。<strong>F1 measure</strong> 对 precision，recall 进行加权平均，结果会更好一些，所以经常用来作为 NER 任务的评估手段。另外，专家提出了别的建议，比如说给出 partial credit，如 MUC scorer metric，然而，对哪种 case 给多少的 credit，也需要精心设计。</p>
<h2 id="其他-实体链接"><a href="#其他-实体链接" class="headerlink" title="其他-实体链接"></a>其他-实体链接</h2><p>实体识别完成之后还需要进行归一化，比如万达集团、大连万达集团、万达集团有限公司这些实体其实是可以融合的。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5.png" class="ful-image" alt="%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5.png"></p>
<p>主要步骤如下：</p>
<ol>
<li><strong>实体识别</strong><br>命名实体识别，词典匹配</li>
<li><strong>候选实体生成</strong><br>表层名字扩展，搜索引擎，查询实体引用表</li>
<li><strong>候选实体消歧</strong><br>图方法，概率生成模型，主题模型，深度学习</li>
</ol>
<p>补充一些开源系统：</p>
<ul>
<li><a href="http://acube.di.unipi.it/tagme" target="_blank" rel="external">http://acube.di.unipi.it/tagme</a></li>
<li><a href="https://github.com/parthatalukdar/junto" target="_blank" rel="external">https://github.com/parthatalukdar/junto</a></li>
<li><a href="http://orion.tw.rpi.edu/~zhengj3/wod/wikify.php" target="_blank" rel="external">http://orion.tw.rpi.edu/~zhengj3/wod/wikify.php</a></li>
<li><a href="https://github.com/yahoo/FEL" target="_blank" rel="external">https://github.com/yahoo/FEL</a></li>
<li><a href="https://github.com/yago-naga/aida" target="_blank" rel="external">https://github.com/yago-naga/aida</a></li>
<li><a href="http://www.nzdl.org/wikification/about.html" target="_blank" rel="external">http://www.nzdl.org/wikification/about.html</a></li>
<li><a href="http://aksw.org/Projects/AGDISTIS.html" target="_blank" rel="external">http://aksw.org/Projects/AGDISTIS.html</a></li>
<li><a href="https://github.com/dalab/pboh-entity-linking" target="_blank" rel="external">https://github.com/dalab/pboh-entity-linking</a></li>
</ul>
<h1 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h1><p><strong>关系抽取</strong> 需要从文本中抽取两个或多个实体之间的语义关系，主要方法有下面几类：</p>
<ul>
<li><strong>基于模板的方法(hand-written patterns)</strong><ul>
<li>基于触发词/字符串</li>
<li>基于依存句法</li>
</ul>
</li>
<li><strong>监督学习(supervised machine learning)</strong><ul>
<li>机器学习</li>
<li>深度学习（Pipeline vs Joint Model）</li>
</ul>
</li>
<li><strong>半监督/无监督学习(semi-supervised and unsupervised)</strong><ul>
<li>Bootstrapping</li>
<li>Distant supervision</li>
<li>Unsupervised learning from the web</li>
</ul>
</li>
</ul>
<h2 id="基于模板的方法"><a href="#基于模板的方法" class="headerlink" title="基于模板的方法"></a>基于模板的方法</h2><h3 id="基于触发词-字符串"><a href="#基于触发词-字符串" class="headerlink" title="基于触发词/字符串"></a>基于触发词/字符串</h3><p>首先是基于字符串的 pattern，举一个 IS-A 的关系</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Agar is a substance prepared from a mixture of red algae, **such as** Gelidium, for laboratory or industrial use</div></pre></td></tr></table></figure>
<p>通过 such as 可以判断这是一种 IS-A 的关系，由此可以写的规则是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">“Y such as X ((, X)* (, and|or) X)”</div><div class="line">“such Y as X”</div><div class="line">“X or other Y”</div><div class="line">“X and other Y”</div><div class="line">“Y including X”</div><div class="line">“Y, especially X”</div></pre></td></tr></table></figure></p>
<p>另一个直觉是，更多的关系是在特定实体之间的，所以可以用 NER 标签来帮助关系抽取，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">•  located-in (ORGANIZATION, LOCATION)</div><div class="line">•  founded (PERSON, ORGANIZATION)</div><div class="line">•  cures (DRUG, DISEASE)</div></pre></td></tr></table></figure></p>
<p>也就是说我们可以把基于字符串的 pattern 和基于 NER 的 pattern 结合起来，就有了下面的例子。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/5.jpg" class="ful-image" alt="5.jpg"></p>
<p>对应的工具有 Stanford CoreNLP 的 tokensRegex。</p>
<h3 id="基于依存句法"><a href="#基于依存句法" class="headerlink" title="基于依存句法"></a>基于依存句法</h3><p>通常可以以动词为起点构建规则，对节点上的词性和边上的依存关系进行限定。流程为:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/DT.png" class="ful-image" alt="DT.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/DT2.png" class="ful-image" alt="DT2.png"></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>手写规则的 <strong>优点</strong> 是：</p>
<ul>
<li>人工规则有高准确率(high-precision)</li>
<li>可以为特定领域定制(tailor)</li>
<li>在小规模数据集上容易实现，构建简单</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>低召回率(low-recall)</li>
<li>特定领域的模板需要专家构建，要考虑周全所有可能的 pattern 很难，也很费时间精力</li>
<li>需要为每条关系来定义 pattern</li>
<li>难以维护</li>
<li>可移植性差</li>
</ul>
<h2 id="监督学习-机器学习"><a href="#监督学习-机器学习" class="headerlink" title="监督学习-机器学习"></a>监督学习-机器学习</h2><h3 id="研究综述"><a href="#研究综述" class="headerlink" title="研究综述"></a>研究综述</h3><p>漆桂林,高桓,吴天星.知识图谱研究进展[J].情报工程,2017,3(1):004-025</p>
<blockquote>
<p>Zhou[13] 在 Kambhatla 的基础上加入了基本词组块信息和 WordNet，使用 SVM 作为分类器，在实体关系识别的准确率达到了 55.5%，实验表明实体类别信息的特征有助于提高关系抽取性能； Zelenko[14] 等人使用浅层句法分析树上最小公共子树来表达关系实例，计算两颗子树之间的核函数，通过训练例如 SVM 模型的分类器来对实例进行分。但基于核函数的方法的问题是召回率普遍较低，这是由于相似度计算过程匹配约束比较严格，因此在后续研究对基于核函数改进中，大部分是围绕改进召回率。但随着时间的推移，语料的增多、深度学习在图像和语音领域获得成功，信息抽取逐渐转向了基于神经模型的研究，相关的语料被提出作为测试标准，如 SemEval-2010 task 8[15]。基于神经网络方法的研究有，Hashimoto[16] 等人利用 Word Embedding 方法从标注语料中学习特定的名词对的上下文特征，然后将该特征加入到神经网络分类器中，在 SemEval-2010 task 8 上取得了 F1 值 82.8% 的效果。基于神经网络模型显著的特点是不需要加入太多的特征，一般可用的特征有词向量、位置等，因此有人提出利用基于联合抽取模型，这种模型可以同时抽取实体和其之间的关系。联合抽取模型的优点是可以避免流水线模型存在的错误累积[17-22]。其中比较有代表性的工作是[20]，该方法通过提出全新的全局特征作为算法的软约束，进而同时提高关系抽取和实体抽取的准确率，该方法在 ACE 语料上比传统的流水线方法 F1 提高了 1.5%，；另一项工作是 [22]，利用双层的 LSTM-RNN 模型训练分类模型，第一层 LSTM 输入的是词向量、位置特征和词性来识别实体的类型。训练得到的 LSTM 中隐藏层的分布式表达和实体的分类标签信息作为第二层 RNN 模型的输入，第二层的输入实体之间的依存路径，第二层训练对关系的分类，通过神经网络同时优化 LSTM 和 RNN 的模型参数，实验与另一个采用神经网络的联合抽取模型[21]相比在关系分类上有一定的提升。但无论是流水线方法还是联合抽取方法，都属于有监督学习，因此需要大量的训练语料，尤其是对基于神经网络的方法，需要大量的语料进行模型训练，因此这些方法都不适用于构建大规模的 Knowledge Base。</p>
</blockquote>
<p>[13] Guodong Z, Jian S, Jie Z, et al. ExploringVarious Knowledge in relation Extraction.[c]// acl2005, Meeting of the Association for ComputationalLinguistics, Proceedings of the Conference, 25-30 June, 2005, University of Michigan, USA. DBLP.2005:419-444.<br>[14] Zelenko D, Aone C, Richardella A. KernelMethods for relation Extraction[J]. the Journal ofMachine Learning Research, 2003, 1083-1106.<br>[15] Hendrickx I, Kim S N, Kozareva Z, et al.semEval-2010 task 8: Multi-way classification ofsemantic relations between Pairs of nominals[c]//the Workshop on semantic Evaluations: recentachievements and Future Directions. association forComputational Linguistics, 2009:94-99.<br>[16] Hashimoto K, Stenetorp P, Miwa M, et al. Task-oriented learning of Word Embeddings for semanticRelation Classification[J], Computer Science,2015:268-278.<br>[17] Singh S, Riedel S, Martin B, et al. JointInference of Entities, Relations, and Coreference[C]//the Workshop on automated Knowledge baseConstruction ,San Francisco, CA, USA, October27-november 1. 2013:1-6.<br>[18] Miwa M, Sasaki Y. Modeling Joint Entity andrelation Extraction with table representation[c]//conference on Empirical Methods in naturalLanguage Processing. 2014:944-948.<br>[19] Lu W, Dan R. Joint Mention Extraction andclassification with Mention Hypergraphs[c]//conference on Empirical Methods in naturallanguage Processing. 2015:857-867.<br>[20] Li Q, Ji H. Incremental Joint Extraction of EntityMentions and relations[c]// annual Meeting of theAssociation for Computational Linguistics. 2014:402-412.<br>[21] Kate R J, Mooney R J. Joint Entity andrelation Extraction using card-pyramid Parsing[c]//conference on computational natural languagelearning. 2010:203-212.<br>[22] Miwa M, Bansal M. End-to-End Relation Extraction using lstMs on sequences and tree structures[c]// annual Meeting of the association for computational linguistics. 2016:1105-1116.</p>
<h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3><p>标准流程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">- 预先定义好想提取的关系集合</div><div class="line">- 选择相关的命名实体集合</div><div class="line">- 寻找并标注数据</div><div class="line"> 选择有代表性的语料库</div><div class="line"> 标记命名实体</div><div class="line"> 人工标注实体间的关系</div><div class="line"> 分成训练、开发、测试集</div><div class="line">- 设计特征</div><div class="line">- 选择并训练分类器</div><div class="line">- 评估结果</div></pre></td></tr></table></figure></p>
<p>为了提高 efficiency，通常我们会训练两个分类器，第一个分类器是 yes/no 的二分类，判断命名实体间是否有关系，如果有关系，再送到第二个分类器，给实体分配关系类别。这样做的好处是通过排除大多数的实体对来加快分类器的训练过程，另一方面，对每个任务可以使用 task-specific feature-set。</p>
<p>可以采用的分类器可以是 <strong>MaxEnt、Naive Bayes、SVM</strong> 等。</p>
<h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><p>直接上例子：<br>E.g., <strong>American Airlines</strong>, a unit of AMR, immediately matched the move, spokesman <strong>Tim Wagner</strong> said</p>
<p><strong>Mention 1:</strong> American Airlines<br><strong>Mention 2:</strong> Tim Wagner</p>
<p>用到的特征可以有：<br><strong>Word features</strong></p>
<ul>
<li>Headwords of M1 and M2, and combination<ul>
<li>M1: Airlines,  M2: Wagner, Combination: Airlines-Wagner</li>
</ul>
</li>
<li>Bag of words and bigrams in M1 and M2<ul>
<li>{American, Airlines, Tim, Wagner, American Airlines, Tim Wagner}</li>
</ul>
</li>
<li>Words or bigrams in particular positions left and right of M1/M2<ul>
<li>M2: -1 spokesman</li>
<li>M2: +1 said</li>
</ul>
</li>
<li>Bag of words or bigrams between the two entities<ul>
<li>{a, AMR, of, immediately, matched, move, spokesman, the, unit}</li>
</ul>
</li>
</ul>
<p><strong>Named Entities Type and Mention Level Features</strong></p>
<ul>
<li>Named-entities types<br>M1: ORG<br>M2: PERSON</li>
<li>Concatenation of the two named-entities types<br>ORG-PERSON</li>
<li>Entity Level of M1 and M2 (NAME, NOMINAL, PRONOUN)<br>M1: NAME     [it or he would be PRONOUN]<br>M2: NAME     [the company would be NOMINAL]</li>
</ul>
<p><strong>Parse Features</strong></p>
<ul>
<li>Base syntactic chunk sequence from one to the other<br>NP NP PP VP NP NP</li>
<li>Constituent path through the tree from one to the other<br>NP ↑ NP ↑ S ↑ S ↓ NP</li>
<li>Dependency path<br>Airlines matched Wagner said</li>
</ul>
<p><strong>Gazetteer and trigger word features</strong></p>
<ul>
<li>Trigger list for family: kinship terms<br>parent, wife, husband, grandparent, etc. [from WordNet]</li>
<li>Gazetteer:<br>List of useful geo or geopolitical words<br>  Country name list<br>  Other sub-entities</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/7.jpg" class="ful-image" alt="7.jpg">
<p>或者从另一个角度考虑，可以分为</p>
<ul>
<li><strong>轻量级</strong><br>实体的特征，包括实体前后的词，实体类型，实体之间的距离等</li>
<li><strong>中等量级</strong><br>考虑 chunk，如 NP，VP，PP 这类短语</li>
<li><strong>重量级</strong><br>考虑实体间的依存关系，实体间树结构的距离，及其他特定的结构信息</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/features2.png" class="ful-image" alt="features2.png">
<h2 id="监督学习-深度学习"><a href="#监督学习-深度学习" class="headerlink" title="监督学习-深度学习"></a>监督学习-深度学习</h2><p>深度学习方法又分为两大类，pipeline 和 joint model</p>
<ul>
<li><strong>Pipeline</strong><br>把实体识别和关系分类作为两个完全独立的过程，不会相互影响，关系的识别依赖于实体识别的效果</li>
<li><strong>Joint Model</strong><br>实体识别和关系分类的过程共同优化</li>
</ul>
<p>深度学习用到的特征通常有：</p>
<ul>
<li>Position embeddings</li>
<li>Word embeddings</li>
<li>Knowledge embeddings</li>
</ul>
<p>模型通常有 CNN/RNN + attention，损失函数 ranking loss 要优于 softmax。</p>
<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><h4 id="CR-CNN"><a href="#CR-CNN" class="headerlink" title="CR-CNN"></a>CR-CNN</h4><p><a href="https://arxiv.org/pdf/1504.06580.pdf" target="_blank" rel="external">Santos et. al Computer Science 2015</a><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/CR-CNN.png" class="ful-image" alt="CR-CNN.png"></p>
<p>输入层 word embedding + position embedding，用 6 个卷积核 + max pooling 生成句子向量表示，与关系（类别）向量做点积求相似度，作为关系分类的结果。<br>损失函数用的是  <strong>pairwise ranking loss function</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/CR-CNN_loss.png" class="ful-image" alt="CR-CNN_loss.png">
<p>训练时每个样本有两个标签，正确标签 y+ 和错误标签 c-，m+ 和 m- 对应了两个 margin，$\gamma$ 用来缩放，希望 $s(x)_{y+}$ 越大越好，$s(x)_{c-}$ 越小越好。</p>
<p>另外还有一些 tips：</p>
<ul>
<li>负样本选择 $s(x)_c$ 最大的标签，便于更好地将比较类似的两种 label 分开</li>
<li>加了一个 Artifical Class，表示两个实体没有任何关系，可以理解为 Other/拒识，训练时不考虑这一类，损失函数的第一项直接置 0，预测时如果其他 actual classes 的分数都为负，那么就分为 Other，对于整体的 performance 有提升</li>
<li>position feature 是每个 word 与两个 entity 的相对距离，强调了两个实体的作用，认为距离实体近的单词更重要，PE 对效果的提升明显，但实际上只用两个实体间的 word embedding 作为输入代替整个句子的 word embedding+position embedding，也有相近效果，且输入更少实现更简单。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/PE.png" class="ful-image" alt="PE.png">
<h4 id="Att-CNN"><a href="#Att-CNN" class="headerlink" title="Att-CNN"></a>Att-CNN</h4><p><a href="http://iiis.tsinghua.edu.cn/~weblt/papers/relation-classification.pdf" target="_blank" rel="external">Relation Classification via Multi-Level Attention CNNs</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/AttCNN.png" class="ful-image" alt="AttCNN.png">
<p>用了两个层面的 Attention，一个是输入层对两个 entity 的注意力，另一个是在卷积后的 pooling 阶段，用 attention pooling 代替 max pooling 来加强相关性强的词的权重。</p>
<p>输入特征还是 word embedding 和 position embedding，另外做了 n-gram 的操作，取每个词前后 k/2 个词作为上下文信息，每个词的 embedding size 就是 $(d_w + 2d_p)*k$。这个滑动窗口的效果其实和卷积一样，但因为输入层后直接接了 attention，所以这里先做了 n-gram。</p>
<p>第一层 input attention 用两个对角矩阵分别对应两个 entity，对角线各元素是输入位置对应词与实体间的相关性分数 $A^j_{i,i}=f(e_j, w_i)$，通过词向量內积衡量相关性，然后 softmax 归一化，每个词对两个实体各有一个权重 $\alpha_1, \alpha_2$，然后进行加权把权重与输入 $z_i$ 融合，有三种融合方法， <strong>求平均、拼接、相减</strong>（类似 transE 操作，把 relation 看做两个权重的差）。这一层的 attention 捕捉的是句中单词与实体的词向量距离，但其实有些线索词如 caused 与实体的相似度不高但很重要。</p>
<p>接着做正常卷积，然后第二层用 attention pooling 代替 max-pooling，bilinear 方法计算相关度，然后归一化，再做 max pooling 得到模型最后的输出 $w^O$。</p>
<p>另外，这篇 paper 还改进了 Santos 提出的 Ranking loss，Ranking loss 里的 distance function 直接用了网络的输出，而这里定义了新的 distance function 来衡量模型输出 $w^O$ 和正确标签对应的向量 relation embedding $W^L_y$ 的距离：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/AttCnn_loss.png" class="ful-image" alt="AttCnn_loss.png"></p>
<p>用了 L2 正则，然后基于这一距离定义了目标函数：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/AttCNN_loss2.png" class="ful-image" alt="AttCNN_loss2.png"></p>
<p>两个距离分别为网络输出与正例和与负例的距离，负例照例用了所有错误类别中与输出最接近的，margin 设置的 1。</p>
<p><strong>这应该是目前最好的方法，SemEval-2010 Task 8 上的 F1 值到了 88。</strong></p>
<h4 id="Att-BLSTM-模型"><a href="#Att-BLSTM-模型" class="headerlink" title="Att-BLSTM 模型"></a>Att-BLSTM 模型</h4><p><a href="http://www.aclweb.org/anthology/P16-2034" target="_blank" rel="external">Peng Zhou et. al ACL 2016</a></p>
<p>CNN 可以处理文本较短的输入，但是长距离的依赖还是需要 LSTM，这一篇就是中规中矩的 BiLSTM+Attn 来做关系分类任务。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/AttBLSM.png" class="ful-image" alt="AttBLSM.png"></p>
<h4 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h4><p>各方法在 SemEval-2010 Task 8 上的评测：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/pipeline_perf.png" class="ful-image" alt="pipeline_perf.png"></p>
<h3 id="Joint-Model"><a href="#Joint-Model" class="headerlink" title="Joint Model"></a>Joint Model</h3><p>Pipeline 的方法会导致误差的传递，端到端的方法。</p>
<h4 id="LSTM-RNNs"><a href="#LSTM-RNNs" class="headerlink" title="LSTM-RNNs"></a>LSTM-RNNs</h4><p><a href="https://arxiv.org/pdf/1601.00770.pdf" target="_blank" rel="external">Miwa et. al ACL 2016</a></p>
<p>用端到端的方式进行抽取，实体识别和关系分类的参数共享，不过判断过程并没有进行交互。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/joint_model.png" class="ful-image" alt="joint_model.png"></p>
<p>三个表示层</p>
<ul>
<li><strong>Embedding layer (word embeddings layer)</strong><br>用到了词向量 $v_w$、词性 POS tags $v_p$、依存句法标签 Dependency types $v_d$、实体标签 entity labels $v_e$</li>
<li><strong>Sequence layer (word sequence based LSTM-RNN layer)</strong><br>负责实体识别<br>BiLSTM 对句子进行编码，输入是 word embedding 和 POS embedding 的拼接，输出是两个方向的隐层单元输出的拼接 $s_t$<br>然后进行实体识别，还是序列标注任务，两层 NN 加一个 softmax 输出标签。打标签的方法用 BILOU(Begin, Inside, Last, Outside, Unit)，解码时考虑到当前标签依赖于上一个标签的问题，输入在 sequence layer 层的输出上还加了上一时刻的 label embedding，用 schedule sampling 的方式来决定用 gold label 还是 predict label<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/entity_detection.png" class="ful-image" alt="entity_detection.png"></li>
<li><p><strong>Dependency layer (dependency subtree based LSTM-RNN layer )</strong><br>负责关系分类<br>用 tree-structured BiLSTM-RNNs 来表示 relation candidate，捕捉了 top-down 和 bottom-up 双向的关系，输入是 sequence layer 的输出 $s_t$，dependency type embedding $v_d$，以及 label embedding $v_e$，输出是 $d_p$<br>关系分类主要还是利用了依存树中两个实体之间的最短路径（shortest path）。主要过程是找到 sequence layer 识别出的所有实体，对每个实体的最后一个单词进行排列组合，再经过 dependency layer 得到每个组合的 $d_p$，然后同样用两层 NN + softmax 对该组合进行分类，输出这对实体的关系类别。<br>$d_p$ 第一项是 bottom-up LSTM-RNN 的 top LSTM unit，代表实体对的最低公共父节点（the lowest common ancestor of the target word pair p），第二、三项分别是两个实体对应的 top-down LSTM-RNN 的 hidden state。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/dp.png" class="ful-image" alt="dp.png">
<p>​</p>
</li>
</ul>
<p>不同模型在 SemEval-2010 Task 8 数据集上的效果比较：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/joint_perf.png" class="ful-image" alt="joint_perf.png"></p>
<p>与我们的直觉相反，joint model 不一定能起正作用。不过上面的比较能得到的另一个结论是：<strong>外部资源可以来优化模型</strong>。</p>
<h2 id="监督学习-评价指标"><a href="#监督学习-评价指标" class="headerlink" title="监督学习-评价指标"></a>监督学习-评价指标</h2><p>最常用的 Precision, Recall, F1</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/8.jpg" class="ful-image" alt="8.jpg">
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>如果测试集和训练集很相似，那么监督学习的准确率会很高，然而，它对不同 genre 的泛化能力有限，模型比较脆弱，也很难扩展新的关系；另一方面，获取这么大的训练集代价也是昂贵的。</p>
<h2 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h2><h3 id="Seed-based-or-bootstrapping-approaches"><a href="#Seed-based-or-bootstrapping-approaches" class="headerlink" title="Seed-based or bootstrapping approaches"></a>Seed-based or bootstrapping approaches</h3><p>半监督学习主要是利用少量的标注信息进行学习，这方面的工作主要是<strong>基于 Bootstrap 的方法</strong>以及<strong>远程监督方法（distance supervision）</strong>。<strong>基于 Bootstrap 的方法</strong> 主要是利用少量实例作为初始种子(seed tuples)的集合，然后利用 pattern 学习方法进行学习，通过不断迭代从非结构化数据中抽取实例，然后从新学到的实例中学习新的 pattern 并扩充 pattern 集合，寻找和发现新的潜在关系三元组。<strong>远程监督</strong> 方法主要是对知识库与非结构化文本对齐来自动构建大量训练数据，减少模型对人工标注数据的依赖，增强模型跨领域适应能力。</p>
<p>漆桂林,高桓,吴天星.知识图谱研究进展[J].情报工程,2017,3(1):004-025</p>
<blockquote>
<p>Brin[23]等人通过少量的实例学习种子模板，从网络上大量非结构化文本中抽取新的实例，同时学习新的抽取模板，其主要贡献是构建了 DIPRE 系统；Agichtein[24]在 Brin 的基础上对新抽取的实例进行可信度的评分和完善关系描述的模式，设计实现了 Snowball 抽取系统；此后的一些系统都沿着 Bootstrap 的方法，但会加入更合理的对 pattern 描述、更加合理的限制条件和评分策略，或者基于先前系统抽取结果上构建大规模 pattern；如 NELL（Never-EndingLanguage Learner）系统[25-26]，NELL 初始化一个本体和种子 pattern，从大规模的 Web 文本中学习，通过对学习到的内容进行打分来提高准确率，目前已经获得了 280 万个事实。</p>
</blockquote>
<p>[23] brin s. Extracting Patterns and relations fromthe World Wide Web[J]. lecture notes in computerScience, 1998, 1590:172-183.<br>[24] Agichtein E, Gravano L. Snowball : Extractingrelations from large Plain-text collections[c]// acMConference on Digital Libraries. ACM, 2000:85-94.<br>[25] Carlson A, Betteridge J, Kisiel B, et al. Toward anarchitecture for never-Ending language learning.[c]// twenty-Fourth aaai conference on artificialIntelligence, AAAI 2010, Atlanta, Georgia, Usa, July.DBLP, 2010:529-573.<br>[26] Mitchell T, Fredkin E. Never-ending Languagelearning[M]// never-Ending language learning.Alphascript Publishing, 2014.</p>
<h4 id="Relation-Bootstrapping"><a href="#Relation-Bootstrapping" class="headerlink" title="Relation Bootstrapping"></a>Relation Bootstrapping</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">•  Gather a set of seed pairs that have relation R</div><div class="line">•  Iterate:</div><div class="line">1.  Find sentences with these pairs</div><div class="line">2.  Look at the context between or around the pair and generalize the context to create patterns</div><div class="line">3.  Use the patterns for grep for more pairs</div></pre></td></tr></table></figure>
<p>看一个完整的例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>从 5 对种子开始，找到包含种子的实例，替换关键词，形成 pattern，迭代匹配，就为 $(authoer, book)$ 抽取到了 relation pattern，<strong><em>x, by y</em></strong>, 和 <strong><em>x, one of y’s</em></strong></p>
<p><strong>优点：</strong></p>
<ul>
<li>构建成本低，适合大规模构建</li>
<li>可以发现新的关系（隐含的）</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对初始给定的种子集敏感</li>
<li>存在语义漂移问题</li>
<li>结果准确率较低</li>
<li>缺乏对每一个结果的置信度的计算</li>
</ul>
<h4 id="Snowball"><a href="#Snowball" class="headerlink" title="Snowball"></a>Snowball</h4><p>对 Dipre 算法的改进。Snowball 也是一种相似的迭代算法，Dipre 的 X,Y 可以是任何字符串，而 Snowball 要求 X,Y 必须是命名实体，并且 Snowball 对每个 pattern 计算了 confidence value<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Group instances w/similar prefix, middle, suffix, extract patterns</div><div class="line"> •  But require that X and Y be named entites</div><div class="line"> •  And compute a confidence for each pattern</div><div class="line"></div><div class="line">ORGANIZATION &#123;&apos;s, in, headquaters&#125; LOCATION</div><div class="line">LOCATION &#123;in, based&#125; ORGANIZATION</div></pre></td></tr></table></figure></p>
<h3 id="Distant-Supervision"><a href="#Distant-Supervision" class="headerlink" title="Distant Supervision"></a>Distant Supervision</h3><p><strong>基本假设：</strong>两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系。</p>
<p><strong>具体步骤：</strong></p>
<ol>
<li>从知识库中抽取存在关系的实体对</li>
<li>从非结构化文本中抽取含有实体对的句子作为训练样例，然后提取特征训练分类器。</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/9.jpg" class="ful-image" alt="9.jpg">
<p>Distant Supervision 结合了 bootstrapping 和监督学习的长处，使用一个大的 corpus 来得到海量的 seed example，然后从这些 example 中创建特征，最后与有监督的分类器相结合。</p>
<p>与监督学习相似的是这种方法用大量特征训练了分类器，通过已有的知识进行监督，不需要用迭代的方法来扩充 pattern。<br>与无监督学习相似的是这种方法采用了大量没有标注的数据，对训练语料库中的 genre 并不敏感，适合泛化。</p>
<h4 id="PCNN-Attention"><a href="#PCNN-Attention" class="headerlink" title="PCNN + Attention"></a>PCNN + Attention</h4><p><a href="https://www.semanticscholar.org/paper/Distant-Supervision-for-Relation-Extraction-with-Ji-Liu/b8da823ad81e3b8e5b80d82f86129fdb1d9132e7" target="_blank" rel="external">Kang Liu et.al AI 2017</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/DS.png" class="ful-image" alt="DS.png">
<ol>
<li><strong>PCNN</strong><br>单一池化难以刻画不同上下文对句向量的贡献，而进行分段池化，根据两个实体把句子分成三段然后对不同部分分别进行池化，刻画更为精准。<br>另见 <a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP203.pdf" target="_blank" rel="external">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</a></li>
<li><strong>Sentence-level attention</strong><br>远程监督常用的 multi-instance learning，只选取最有可能的一个句子进行训练预测，丢失了大部分信息，句子层面的 attention 对 bag 里所有句子进行加权作为 bag 的特征向量，保留尽可能多的信息，能动态减少噪声句的权重，有利于解决错误标记的问题。<br>另见 <a href="http://www.aclweb.org/anthology/P16-1200" target="_blank" rel="external">Neural Relation Extraction with Selective Attention over Instances</a><br>这里对两个实体向量作差来表示 relation 向量 $v_{relation}$，如果一个实例能表达这种关系，那么这个实例的向量表达应该和 $v_{relation}$ 高度相似，根据这个假设来计算句向量和关系向量的相关性，其中 $[b_i; v_{relation}]$ 表示垂直级联，$b_i$ 是 PCNN 得到的特征输出，softmax 归一化再进行加权，最后再过softmax 进行分类。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/DS_sum.png" class="ful-image" alt="DS_sum.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/DS_att.png" class="ful-image" alt="DS_att.png"></li>
<li><strong>Entity representation</strong><br>引入了实体的背景知识（Freebase 和 Wikipedia 提供的实体描述信息），增强了实体表达（entity representation），D 是 (entity, description) 的集合表示，$e_i$ 是实体表示，$d_i$ 通过另一个传统 CNN 对收集到的实体的描述句抽特征得到<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/ed.png" class="ful-image" alt="ed.png">
希望 $e_i$ 和 $d_i$ 尽可能相似，定义两者间的误差：<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/errors.png" class="ful-image" alt="errors.png">
</li>
</ol>
<p>最后的损失函数是交叉熵和实体描述误差的加权和：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%9F%A5%E8%AF%86%E6%8A%BD%E5%8F%96-%E5%AE%9E%E4%BD%93%E5%8F%8A%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/final_loss.png" class="ful-image" alt="final_loss.png"></p>
<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p><strong>优点：</strong></p>
<ul>
<li>可以利用丰富的知识库信息，减少一定的人工标注</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>假设过于肯定，引入大量噪声，存在语义漂移现象</li>
<li>很难发现新的关系</li>
</ul>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><blockquote>
<p>Bollegala[27]从搜索引擎摘要中获取和聚合抽取模板，将模板聚类后发现由实体对代表的隐含语义关系; Bollegala[28]使用联合聚类(Co-clustering)算法，利用关系实例和关系模板的对偶性，提高了关系模板聚类效果，同时使用 L1 正则化 Logistics 回归模型，在关系模板聚类结果中筛选出代表性的抽取模板，使得关系抽取在准确率和召回率上都有所提高。</p>
<p>无监督学习一般利用语料中存在的大量冗余信息做聚类，在聚类结果的基础上给定关系，但由于聚类方法本身就存在难以描述关系和低频实例召回率低的问题，因此无监督学习一般难以得很好的抽取效果。</p>
</blockquote>
<p>[27] Bollegala D T, Matsuo Y, Ishizuka M. Measuringthe similarity between implicit semantic relationsfrom the Web[J]. Www Madrid! track semantic/dataWeb, 2009:651-660.<br>[28] Bollegala D T, Matsuo Y, Ishizuka M. RelationalDuality: Unsupervised Extraction of semantic relations between Entities on the Web[c]//International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, Usa, April. DBLP, 2010:151-160.</p>
<p><strong>Open Information Extraction</strong> 从网络中抽取关系，没有训练数据，没有关系列表。过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">1. Use parsed data to train a “trustworthy tuple” classifier</div><div class="line">2. Single-pass extract all relations between NPs, keep if trustworthy</div><div class="line">3. Assessor ranks relations based on text redundancy</div><div class="line"></div><div class="line">E.g.,</div><div class="line">(FCI, specializes in, sobware development)</div><div class="line">(Tesla, invented, coil transformer)</div></pre></td></tr></table></figure></p>
<h2 id="半监督-无监督学习的评价指标"><a href="#半监督-无监督学习的评价指标" class="headerlink" title="半监督/无监督学习的评价指标"></a>半监督/无监督学习的评价指标</h2><p>因为抽取的是新的关系，并不能准确的计算 precision 和 recall，所以我们只能估计，从结果集中随机抽取一个关系的 sample，然后人工来检验准确率</p>
<p>$$\hat P = {\text {Number of correctly extracted relations in the sample} \over \text {Total number of extracted relations in the sample}}$$</p>
<p>也可以计算不同 recall level 上的 precision，比如说分别计算在前 1000，10,000，100,000 个新的关系中的 precision，在各个情况下随机取样。</p>
<p>然而，并没有方法来计算 recall。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Knowledge Graph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Information Extraction </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[99/100 天纪念日 - 第 1 期]]></title>
      <url>http://www.shuang0420.com/2018/08/06/love99/</url>
      <content type="html"><![CDATA[<script src="/crypto-js.js"></script><script src="/mcommon.js"></script><h3 id="encrypt-message">Please enter the password to read the blog.</h3><link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css"> <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap-theme.min.css"> <script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script> <script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script> <div id="security"> <div> <div class="input-group"> <input type="text" class="form-control" aria-label="Enter the password." id="pass"/> <div class="input-group-btn"> <button type="button" class="btn btn-default" onclick="decryptAES()">Decrypt</button> </div> </div> </div> </div> <div id="encrypt-blog" style="display:none"> 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 </div>]]></content>
      
        
        <tags>
            
            <tag> 小心情 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[2018 CCF-GAIR 参会笔记 - NLP 专场]]></title>
      <url>http://www.shuang0420.com/2018/08/06/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/</url>
      <content type="html"><![CDATA[<p>这篇居然忘了发了……<br><a id="more"></a></p>
<h1 id="孙茂松-漫谈基于深度学习的中文计算"><a href="#孙茂松-漫谈基于深度学习的中文计算" class="headerlink" title="孙茂松 - 漫谈基于深度学习的中文计算"></a>孙茂松 - 漫谈基于深度学习的中文计算</h1><p>强调的一个概念是 <strong>中文自然语言处理需要加入专家知识</strong>。主要介绍的工作是 <strong>词表学习</strong>：</p>
<ul>
<li><strong>嵌入字信息的词表学习</strong><br>实践中还是经常会用到的技巧；<br>词向量本身对高频词是没问题的，比如说取相近词 K 近邻，猪肉/鸡肉语义是相似的，但对低频词/新词像马肉/龙肉，语义相关性就不高了；<br>因此在词向量出现歧义时可以加入字向量，相当于平滑作用，在这个刻度上没有这种信息，就进行回退；<br>提到了一些其他 trick<ul>
<li><strong>Position-based character embeddings</strong> 区分字在词中出现的位置，也就是用 char+pos 来表示字，idea 是通常一个字可能出现在词的开始、中间、尾部（用 $c^B$, $c^M$, $c^E$ 表示），却分别代表不同的含义，如车道、人行道和道法、道经中的道就不是一个含义；</li>
<li><strong>Cluster-based character embeddings</strong>，对每个字的所有 occurrence 进行聚类，然后对每个类建一个 embedding<br>参考论文：<a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/ijcai2015_character.pdf" target="_blank" rel="external">Joint Learning of Character and Word Embeddings</a></li>
</ul>
</li>
<li><strong>嵌入中文资源，基于知网的词表学习</strong><br>作用大概是消歧，利用 hownet 解决中文词语的多义性，类比 wordnet 用来加强英语的多义性学习一样<br>当然 HowNet 和 WordNet 的构造还是有很大不同的。HowNet 对十几万汉语常用词进行了描述，描述用的三要素分别是 sememe，sense 和 word，比如 apple 包含了两个 sense，sense1 是水果，sense2 是电脑，对每个 sense，sememe 可以描述其对应的属性，这些属性会通过相对复杂的层级结构来对目标 sense 进行说明。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/hownet.png" class="ful-image" alt="hownet.png">
在基于 hownet 的词表学习里，<strong>sememe 是最小的语义单元</strong>，数量有限，大概两千个。每个单词可能对应多个 sense，将每个 sense 对应的 sememe 看成是一个集合，相似的 sense 会包含相同的 sememe。训练模型基于经典的 skip-gram，考虑上下文的同时，也考虑了词的 sememe 信息以及 sememe 与 sense 之间的关系。提供了三种融合方法，SSA/SAC/SAT，<strong>SSA</strong> 对每个 target word 取它对应所有 sememe embeddings 的平均值，<strong>SAC</strong> 对 context words 进行消歧来更好的学习目标单词，也就是 context 用 sememe embedding 来表示 ，target word embedding 可以看做是为 context word embedding 选择最合适的 sense 和 sememe 的一个 attention 机制，而 <strong>SAT</strong> 中 context 用原来的 embedding 表示，但 target word 用 sememe embedding 表示，把 context words 看做是 target word 不同 sense 上的 attention。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/SAC.png" class="ful-image" alt="SAC.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/SAT.png" class="ful-image" alt="SAT.png">
基于 HowNet 的词表学习里，sememe, sense, word 之间能够互相打通。在对低频词和新词问题上，由于多了词与词之间共享的 sememe embeddings，低频词能够被解码成 sememe 并通过其他词得到良好的训练，相比于传统 WRL 模型能有更好的表现。<br>参考论文：<a href="http://nlp.csai.tsinghua.edu.cn/~xrb/publications/ACL-17_sememe.pdf" target="_blank" rel="external">Improved Word Representation Learning with Sememes</a></li>
<li>最后还介绍了清华出品的古诗系统，提到了要通过与情感结合、与知识图谱结合等方法来增强作诗系统。</li>
</ul>
<h1 id="赵军-开放域事件抽取"><a href="#赵军-开放域事件抽取" class="headerlink" title="赵军 - 开放域事件抽取"></a>赵军 - 开放域事件抽取</h1><p>主要讲的还是关系抽取中的远程监督（Distant Supervision）问题。远程监督基本假设是“<strong>两个实体如果在知识库中存在某种关系，则包含该两个实体的非结构化句子均能表示出这种关系</strong>”，这样的假设太强，带来的问题是<strong>噪声很多</strong>，一个解决方案是引入<strong>多示例学习</strong>，假定<strong>至少有一个句子表示了这种关系</strong>而不是每个句子都表示这种关系，把最有可能的句子标注出来，以提高性能。介绍的 paper 是 <a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP203.pdf" target="_blank" rel="external">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</a>，EMNLP 2015 挺有名的一篇文章，用分段卷积神经网络 PCNN 来自动学习特征，以及加入 multi-instance learning 来解决远程监督引起的噪声问题。主要 idea 是在池化层通过两个实体的位置把句子分成三个部分，分别池化，再把三个部分的向量结合起来，做整个句子的向量化表示。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/PCNN.png" class="ful-image" alt="PCNN.png"><br>后面还讲了开放域更复杂的事件抽取，像是缺少触发词，可以通过从一堆要素中定位核心要素，用核心要素到句子当中找到触发词，将触发词和前面的要素关联到一起，再回标，然后在文本当中找到更多数据。</p>
<h1 id="秦兵-机器智能中的情感计算"><a href="#秦兵-机器智能中的情感计算" class="headerlink" title="秦兵 - 机器智能中的情感计算"></a>秦兵 - 机器智能中的情感计算</h1><p>分享了文本情感计算的六个维度：</p>
<ol>
<li><strong>情感分类</strong><br>面向评价对象的情感分类（aspect-based sentiment analysis）比较典型的还是利用上下文信息，采用注意力机制，使某个评价对象和词语进行更好的搭配，然后分类</li>
<li><strong>隐式情感</strong><br>不含情感词的情感表达（即隐式情感）在情感表达中约占 20%-30%，类型有事实型、比喻型、反问型等，事实型情感占多数，比如住酒店时说“桌子上有一层灰”，实际表达的就是不满。要判断这种情感需要依赖上下文，如 “桌子上有一层灰“ 后面一句是 ”我很不高兴”，就可以把 “桌子上有一层灰” 定义为贬义。找不到上下文可以考虑跨文档，在其他文档中找与之类似的句子再判定情感<br>同时，这类情感的计算通常也需要借助外部知识如隐式情感语料库等，尤其是修辞型的隐式情感，比如隐喻，可以借助隐喻语料库<img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/%E4%BA%8B%E5%AE%9E%E5%9E%8B%E9%9A%90%E5%BC%8F.png" class="ful-image" alt="%E4%BA%8B%E5%AE%9E%E5%9E%8B%E9%9A%90%E5%BC%8F.png"></li>
<li><strong>情感溯因</strong><br>类似问答系统，有情感词、有原文，可以通过记忆网络判别哪句话是原因</li>
<li><strong>个性化</strong><br>在情感计算中加入用户特征/用户画像信息，包括自然属性、社会属性、兴趣属性、心理属性等，融入到已有的神经网络模型，来做情感分类</li>
<li><strong>跨领域</strong><br>利用领域无关词和领域相关词的链接关系，分别进行聚类；<br>通过神经网络的隐层参数提取与情感相关、但与领域无关的词的特征来分类</li>
<li><strong>情感生成</strong><br>根据指定的情感类别生成情感表达，应用如产品评论生成、聊天系统、对情感表达进行情感极性变换、润色等</li>
</ol>
<p>还有一个有意思的应用是中考、高考时经常看到的诗词鉴赏。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/2018%20CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0%20-%20NLP%20%E4%B8%93%E5%9C%BA/%E8%AF%97%E8%AF%8D%E9%89%B4%E8%B5%8F.png" class="ful-image" alt="%E8%AF%97%E8%AF%8D%E9%89%B4%E8%B5%8F.png"></p>
<h1 id="钟黎-从-0-到1-打造下一代智能对话引擎"><a href="#钟黎-从-0-到1-打造下一代智能对话引擎" class="headerlink" title="钟黎 - 从 0 到1 打造下一代智能对话引擎"></a>钟黎 - 从 0 到1 打造下一代智能对话引擎</h1><p>这个和之前的项目/工作经验高度相关，感觉更像是梳理了一遍之前的工作~~<br>业界通用智能问答平台要解决的问答类型：</p>
<ul>
<li><strong>任务驱动型（Task Oriented Dialogue）</strong><br>用户希望去完成一些任务，比如查天气、查汇率等，包括词槽填充、多轮会话、对话管理等</li>
<li><strong>信息获取型（Information &amp; Answers）</strong><br>目前业界落地最多的一种问答系统类型，包括搜索、单轮对话，根据数据类型划分有下面几类<ol>
<li>结构化知识，比如 CommunityQA（eg., FAQ）和 KBQA</li>
<li>半结构化/非结构化知识，比如说 TableQA（表格），PassageQA（文档）</li>
<li>多模态、跨媒体问答，比如说 VQA，存在视频、音频问答的语料库</li>
</ol>
</li>
<li><strong>通用闲聊型（General Conversation）</strong><br>基础会话，包括闲聊、情感联系、用户信息等，使对话系统更富于人性化</li>
</ul>
<p>重点讲的是第二类，具体讲了两个部分，一是<strong>快速召回</strong>，二是<strong>深度匹配</strong>。</p>
<h2 id="无监督-快速检索"><a href="#无监督-快速检索" class="headerlink" title="无监督-快速检索"></a>无监督-快速检索</h2><p>提高快速召回（无监督的快速检索）的三种方案，<strong>基于词汇计数（Lexical term counting）</strong>、<strong>基于语言模型</strong>、<strong>基于向量化</strong>。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201807/5b433cd974e8a.png?imageMogr2/format/jpg/quality/90" alt="腾讯知文团队负责人钟黎：从 0 到1  打造下一代智能对话引擎 | CCF-GAIR 2018"></p>
<p>很多是信息检索的思路，在 <strong><a href="http://www.shuang0420.com/categories/NLP/Search-Engines/">信息检索专题类</a></strong> 的博客都有探讨过。</p>
<h2 id="有监督-深度匹配"><a href="#有监督-深度匹配" class="headerlink" title="有监督-深度匹配"></a>有监督-深度匹配</h2><p>深度匹配的两类常用方法，<strong>Siamese 网络</strong> 和 <strong>基于交互矩阵的网络</strong>。</p>
<p><strong>Siamese 网络</strong>的基本思路：两个输入用同一个编码器进行编码，然后做相似度的计算，特点是共享网络结构和参数；</p>
<p><strong>基于交互矩阵的网络</strong>：除了最后的相关性度量，中间过程里两个输入的某些词也会有交互。</p>
<p>问句较短时/短文档时两类网络一般能打成平手，但对长文档而言，基于交互矩阵的网络就会有更好的表现。</p>
<p>再后面还讲了如何在非结构化文档里寻找信息和答案，具体应用是机器阅读理解（MRC），<strong><a href="http://www.shuang0420.com/tags/阅读理解/">系列博客</a></strong>也有提到。</p>
<p>最后总结了下业界问答系统建设的一些心得：</p>
<ol>
<li>要重视 Baseline。</li>
<li>尽早建立起整个流程的 pipeline。</li>
<li>没有免费午餐定理，不存在万能算法。</li>
<li>领域相关的数据准备、数据清洗非常重要。</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> NLP </tag>
            
            <tag> chatbot </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文梳理：问题生成(QG)与答案生成(QA)的结合]]></title>
      <url>http://www.shuang0420.com/2018/07/08/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90(QG)%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90(QA)%E7%9A%84%E7%BB%93%E5%90%88/</url>
      <content type="html"><![CDATA[<p>继续 QG，梳理一下 MSRA 其他 3 篇关于 QG 的 paper：<br><a id="more"></a></p>
<ul>
<li>Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</li>
<li>Question Answering and Question Generation as Dual Tasks</li>
<li>A Joint Model for Question Answering and Question Generation</li>
</ul>
<p>QG 系列其他的笔记：</p>
<ul>
<li><a href="http://www.shuang0420.com/2018/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/">论文笔记 - Machine Comprehension by Text-to-Text Neural Question Generation</a></li>
<li><a href="http://www.shuang0420.com/2018/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/">论文笔记 - Semi-Supervised QA with Generative Domain-Adaptive Nets</a></li>
</ul>
<p>目前的 QA 大多是<strong>抽取式（extractive）</strong>的任务，答案是文本中的一个连续片段，通常是命名实体这类语义概念，而 QG 是<strong>生成式的（abstractive）</strong>，问句是完整句子，部分单词可能是文档中没出现过的，很多情况下，问句和答案的语言结构不同，因此甚至可以看做两种不同类型的数据。所以第 1 篇 <strong>SynNet 就把答案生成当作序列标注任务，把 QG 当作生成任务</strong>；第 3 篇 <strong>Joint Model 从另一个角度出发，把 QA 和 QG 都当作生成任务，放到同一个 encoder-decoder 框架下，用转变输入数据来实现联合训练</strong>，用 pointer-softmax 来处理抽取/生成问题。</p>
<p>另外，QA 和 QG 任务在概率上是有联系的，可以通过 q、a 的联合概率分布联系起来，P(q|a) 就是 QG 模型，P(a|q) 类似 QA 模型，于是第 2 篇 <strong>dual tasks 就把这两个任务当做对偶任务，用一个正则项把两个任务联合在一起</strong>。<br>$$P(q,a)=P(a)P(q|a)=P(q)P(a|q)$$</p>
<h1 id="Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension"><a href="#Two-Stage-Synthesis-Networks-for-Transfer-Learning-in-Machine-Comprehension" class="headerlink" title="Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension"></a>Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</h1><p>我们知道 MRC 系统的输入是 (passage, question, answer) 三元组，q 和 a 依赖人工标注，这是制约 MRC 落地应用的最大问题之一，这篇 paper 提出了 SynNet，利用已有领域中可用的监督数据为基础进行训练，训练完成后迁移到新的领域中，根据新领域的文档模型能自动合成与文档 p 相关的 (q, a) 对，替代昂贵的人工标注，为 MRC 的迁移落地提供了便利。</p>
<p>SynNet 把 QA 对（question-answer pair）的生成过程 P(q,a|p) 分解为条件概率 P(a|p) P(q|p,a) ，也就是下面两个步骤：</p>
<ol>
<li><strong>基于文档生成答案 P(a|p)</strong><br>学习文档中的 potential “interestingness” pattern，包括文章中可作为常见问题答案的关键知识点、命名实体或语义概念<br>由于答案是文档的片段，所以看做序列标注任务</li>
<li><strong>基于文档和答案生成问题 P(q|p,a)</strong><br>学习生成自然语言的完整问句<br>作为生成任务</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/SynNet1.png" class="ful-image" alt="SynNet1.png">
<p><strong>答案合成模块（Answer Synthesis Module）</strong>，序列标注问题，训练了一个 IOB tagger （4 种标记，start, mid, end, none）来预测段落里的每个单词是不是答案。结构很简单，BiLSTM 对 p 的词向量进行编码，然后加两个 FC 层和一个 Softmax 产生每个单词的 tag likelihoods，选择连续的 span 作为 candidate answer chunks，喂给问题生成模块。</p>
<p><strong>问题合成模块（Question Synthesis Module）</strong>学习的是 $P(q_1,…q_n|p_1…p_n,a_{start},a_{end})$。模型结构是 encoder-decoder + attention + copy mechanism。通过在段落词向量中加入一个 0/1 特征来表示单词是不是出现在答案中。</p>
<p><strong>训练算法：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/SynNet_transfer.png" class="ful-image" alt="SynNet_transfer.png"></p>
<p>在源领域上训练 SynNet，产生新领域的 QA 对，然后和源领域的数据一起来 finetune 源领域的 MC 模型（用 SGD），源领域和新领域的数据采样比是 k:1（paper 里设的 k=4），这主要是为了处理合成数据的噪音问题而进行的正则化操作。</p>
<p>测试阶段也就是用 finetune 完成的 MC 模型回答新领域的问题时，可以对不同时刻的 checkpoints 的 answer likelihoods 做平均，然后用 DP 找到最佳的 answer span ($p_s, p_e$)，最大化 $p_sp_e$，复杂度是 linear，和 BiDAF 的逻辑相同。</p>
<p>难得的是这篇 paper 还提供了实现细节，其中一个 trick 是，在训练问题合成模块时，他们只用了 SQuAD 的训练集，但是在答案合成模块，还引入了 NER Tagger 来增强答案数据，基本假设任何命名实体都可以被当做某个问题的潜在答案。</p>
<p>在 Ablation Studies 和 Error Analysis 中还提到了一些有趣的发现，具体可以看论文。待解决的问题一个是 copy 机制导致的产生的问句和 paragraph 高度相似的问题，可以通过改进 cost function 在促进解码过程的多样化，另一篇 paper 有提到。还有一个问题是 SynNet 在解决比如数字、人名这种问题的效果很好，但是在需要一些推理的问题，像是 what was / what did 这些问题就很弱了，这也是后续的研究方向。</p>
<p>这篇 paper 个人非常喜欢，实现细节和一些结果的分析都很赞。</p>
<h1 id="Question-Answering-and-Question-Generation-as-Dual-Tasks"><a href="#Question-Answering-and-Question-Generation-as-Dual-Tasks" class="headerlink" title="Question Answering and Question Generation as Dual Tasks"></a>Question Answering and Question Generation as Dual Tasks</h1><p>把 QA 和 QG 当作对偶任务。关键还是下面这个式子：<br>$$P(q,a)=P(a)P(q|a)=P(q)P(a|q)$$</p>
<p>P(q|a) 即 QG 模型，和 P(a|q) 即 QA 模型可以通过联合概率联系起来，于是这里把 QA 和 QG 当作对偶任务，Seq2Seq 实现 QG，RNN 实现 QA，通过一个正则项把两个任务联系起来，联合训练一起学习 QA 和 QG 的参数，损失函数服从下面的条件：</p>
<p>$$P_a(a)P(q|a;\theta_{qg})=P_q(q)P(a|q;\theta_{qa})$$<br>其中 $P_a(a)$ 和 $P_q(q)$ 分别对应答案句和问句的语言模型。</p>
<p>这里 QA 任务不再是在 context 里选 answer span 的任务，而被看作是在一组候选答案句集合中选择可能性最高的 answer sentence 的一个<strong>排序任务</strong>。也就是说，这里的 a 是答案所在的句子，而不是前一篇 paper 提到的简单的语义概念/实体。</p>
<p>QG 任务还是一个生成任务，输入是答案句 a。要注意的是这里 QA 和 QG 的输入都没有 p，都只考虑了句子层面的信息。</p>
<p>和之前介绍的 <a href="(http://www.shuang0420.com/2018/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/">GDAN</a>) 不同的是，这里 <strong>QA 和 QG 的地位是相同的</strong>，也并不需要预训练 QA。</p>
<p>下面看一下模型细节：<br><strong>QA 模型</strong> 分别用 BiGRU 对 q 和 a 进行编码，拼接 last hidden state 作为向量得到 $v_q$ 和 $v_a$，question-answer pair 的表达由四个向量拼接构成 $v(q,a)=[v_q;v_a;v_q⊙v_a;e_{c(q,a)}]$，c(q,a) 表示 q,a 的共现词，对应的词向量表达通过引入额外的 embedding 矩阵 $L_c \in R^{d_c * |V_c|}$ 实现，$d_c$ 表示词共现向量的维度，$|V_c|$ 则是词汇表大小。$f_{qa}(a,q)$ 也就是 qa 相关性函数通过对 $v(q,a)$ 进行线性变换加 tanh 激活得到，最后 softmax 得到概率，损失函数还是 negative log-likelihood。</p>
<p><strong>QG 模型</strong> 还是经典的 encoder-decoder + attention 模型，输入是 answer sentence，还是用 BiGRU 进行编码，连接两个方向的 last hidden state 作为 encoder 的输出以及 deocder 的初始状态。对 attention 做了改进，希望模型能记住 answer sentence 中哪些 context 被使用过了，在产生 question words 的时候就不再重复使用。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/dual_task_attn.png" class="ful-image" alt="dual_task_attn.png">
<p>拼接 $s_t$ 和 $c_t$，接一个 linear layer 和 softmax 得到输出单词在词汇表上的概率分布，一个 trick 是softmax 输出维度取 top frequent question words，OOV 用 attention probability 最高的词替换，相当于对文档单词的一个 copy 机制，当然也可以用 pointer network 来做。</p>
<p>模型每次输入 m 个 QA 对正例和 m 个负例，通过 QG 和 QA 各自模型计算各自 loss，再加上一个正则化项一起计算参数梯度并更新参数。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/dual_task_algo.png" class="ful-image" alt="dual_task_algo.png"></p>
<p>正则化 dual 项利用了 QA 和 QG 的对偶关系：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/dual_task_term.png" class="ful-image" alt="dual_task_term.png"></p>
<p>考虑到 $P(a|q;\theta_{qa})$ 和 QA 模型的输出有差异，因此给定 q，sample 一系列 answer sentences A’，从中得到 $P(a|q;\theta_{qa})$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/dual_task_pa.png" class="ful-image" alt="dual_task_pa.png"></p>
<p>在实现细节里提到模型对 question words 和 answer words 用了不同的 emebdding 矩阵来学习特定的语义。另外 sampled answer sentence 来自其他的 passage，这降低了 QA 的难度。</p>
<p>结果分析再次证明了 word co-occurrence 是一个简单但非常有效的特征。</p>
<p>实验设计部分不大能看出模型的实际效果，不明白为什么不直接刷榜看一下结果。另外 QG 部分的评价指标也只用了 BLEU-4 分数，对 fluency 没有进行说明。</p>
<blockquote>
<p>We ﬁrst report results on the MARCO and SQUAD datasets. As the dataset is splitted by ourselves, we do not have pre- viously reported results for comparison. </p>
</blockquote>
<h1 id="A-Joint-Model-for-Question-Answering-and-Question-Generation"><a href="#A-Joint-Model-for-Question-Answering-and-Question-Generation" class="headerlink" title="A Joint Model for Question Answering and Question Generation"></a>A Joint Model for Question Answering and Question Generation</h1><p>这篇和上篇都是讲怎么同时生成答案和问题，不同的是上篇通过一个 dual regularization term 将两者联合起来训练，这里把 QA 和 QG 任务都作为生成任务，模型基本结构还是 <strong>Seq2Seq + Attention + Pointer Softmax</strong>，和之前提到的一篇  <a href="http://www.shuang0420.com/2018/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/">论文笔记 - Machine Comprehension by Text-to-Text Neural Question Generation</a> 差不多。输入是文档，以及一个 condition sequence，在 QA 任务里表现为 question word sequence，给定 question 生成 answer；QG 任务里表现为 answer word sequence，给定 answer 生成 qestion，condition 由一个0/1 变量来控制，表示收到的数据是给 a-gen 还是给 q-gen。Joint training 通过对输入数据的转换实现。</p>
<p>Pointer-softmax 一定程度上能解决 extractive/abstractive 的混合问题，通过选择 copy 文档的单词还是生成词汇表的单词来产生下一个单词，表达了 extractive/abstractive 的切换。这带来的一个额外好处是<strong>可以产生 abstractive answer</strong>。</p>
<p>具体来讲，Encoder 里，词向量和字向量拼接得到 word embedding，其中字向量用 BiLSTM 产生，word embedding 经过另一个 BiLSTM 编码得到文档编码 $h^d_i$ 和条件序列的编码 $h^c_j$。</p>
<p>条件序列的另一种编码是“抽取式的”，也就是从 document encoding 中直接抽取出出现在 condition sequence 中的单词的对应部分，这和 <a href="http://www.shuang0420.com/2018/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/">论文笔记 - Machine Comprehension by Text-to-Text Neural Question Generation</a> 原理相同。然后抽取的向量经过 BiLSTM 产生对应编码 $h^e_k$。两种条件序列的编码 $h^c_j$ 和 $h^e_k$ 的 final state 分别为 $h^c_J$ 和 $h^e_K$。在 a-gen mode 也就是对问句进行编码是采用 $h^c_J$，在 q-gen mode 也就是对答案进行编码时采用 $h^e_K$，相当于模拟了 extractive 和 abstractive 的特性。</p>
<p>Decoder 用了 pointer-softmax mechanism，比之前的工作复杂一些。用了两个 LSTM cell $c_1$、$c_2$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model1.png" class="ful-image" alt="joint_model1.png"></p>
<p>context vector:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model2.png" class="ful-image" alt="joint_model2.png"></p>
<p>distribution over the document word position:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model3.png" class="ful-image" alt="joint_model3.png"></p>
<p>Generative mode 由两层 MLP产生：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model4.png" class="ful-image" alt="joint_model4.png"></p>
<p>每个 step 的 switch scalar，由三层 MLP 得到，前两层用 tanh 激活，最后一层用 sigmoid，第一第二层之间是 highway 连接，在最后一层的输入加入 softmax distribution 的 entropy 来进一步提高 performance，相当于给了 point or generate 的更多信息。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model5.png" class="ful-image" alt="joint_model5.png"></p>
<p>最后结果：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model6.png" class="ful-image" alt="joint_model6.png"></p>
<p>损失函数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model7.png" class="ful-image" alt="joint_model7.png"></p>
<p>实现细节里，encoder 用了整个词表，decoder 用了训练数据里 gold question 中的频率最高的 100 个单词的词表。另外一个 trick 是 decoder 保留了之前产生的单词的历史来防止输出的重复。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86%EF%BC%9A%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%28QG%29%E4%B8%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%28QA%29%E7%9A%84%E7%BB%93%E5%90%88/joint_model_res.png" class="ful-image" alt="joint_model_res.png">
<p>联合训练下，a-gen 的表现有显著提升，q-gen 略有下降。一个直观结论是，模型并没有提高 QA 任务的效果，但是增加了 QG 的能力。</p>
<p>过去大多模型都把 QA 当做 point to answer span within a document 而不是 NLG 任务，这一篇的创新之处就在于把 QA 也当作了生成问题，与 QG 放到同一个框架下，用 pointer-softmax 来调节生成/抽取的比率，给 QA 也增加了“生成”的能力。</p>
<p>一个显著优势是，和上一篇 paper 相同，这里不需要预训练 QA，可以直接用 QG 辅助 QA 的实现同时给模型提供QG 的能力。</p>
<blockquote>
<p>a key distinction of our model is that we harness the process of asking questions to benefit question answering, without training the model to answer the generated questions.</p>
</blockquote>
<p>MSRA 出品的 QG 系列的 paper 在各自模型及实现上有共性也有个性，一些 trick 基本是通用的，具体的实用性能还待具体领域的实践检验。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> Question Generation </tag>
            
            <tag> 问题生成 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Machine Comprehension by Text-to-Text Neural Question Generation]]></title>
      <url>http://www.shuang0420.com/2018/06/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/</url>
      <content type="html"><![CDATA[<p>继续来瞅瞅问题生成~<br><a id="more"></a></p>
<p>QG 的应用还是挺广泛的，像是<strong>为 QA 任务产生训练数据、自动合成 FAQ 文档、自动辅导系统（automatic tutoring systems）</strong>等。</p>
<p>传统工作主要是利用<strong>句法树</strong>或者<strong>知识库</strong>，基于规则来产生问题。如<strong>基于语法</strong>（Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015），<strong>基于语义</strong>（Mannem et al., 2010; Lindberg et al., 2013），大多是利用规则操作句法树来形成问句。还有是<strong>基于模板</strong>（templates），定好 slot，然后从文档中找到实体来填充模板（Lindberg et al., 2013; Chali and Golestanirad, 2016）。</p>
<p>深度学习方面的工作不多，有意思的有下面几篇：</p>
<ul>
<li><a href="http://www.aclweb.org/anthology/P16-1056" target="_blank" rel="external">Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus</a><br>将 KB 三元组转化为问句</li>
<li><a href="https://www.aclweb.org/anthology/P16-1170" target="_blank" rel="external">Generating natural questions about an image</a><br>从图片生成问题</li>
<li><a href="https://arxiv.org/abs/1702.02206" target="_blank" rel="external">Semi-supervised QA with generative domain-adaptive nets</a><br>用 domain-adaptive networks 的方法做 QA 的数据增强<br><a href="http://www.shuang0420.com/2018/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/">论文笔记</a></li>
</ul>
<p>神经网络做 QG 基本套路还是 encoder-decoder 模型，对 P(q|d) 或者 P(q|d, a) 进行建模。像是 17年 ACL 的 paper  <a href="https://arxiv.org/abs/1705.00106" target="_blank" rel="external">Learning to Ask: Neural Question Generation for Reading Comprehension</a>，就是用一个基本的 attention-based seq2seq 模型对 P(q|d) 进行建模，并在 encoder 引入了句子和段落级的编码。</p>
<p>这一篇 Microsoft Maluuba 出的 paper 把 answer 作为先验知识，对 P(q|d, a) 进行建模。同时用监督学习和强化学习结合的方法来训练 QG，先用最大似然预训练一波，然后用 policy gradient 方法进行 fine-tune ，最大化能反映问题质量的一些 rewards。</p>
<h1 id="Encoder-Decoder-Model"><a href="#Encoder-Decoder-Model" class="headerlink" title="Encoder-Decoder Model"></a>Encoder-Decoder Model</h1><p>基础架构是 encoder-decoder，加了 attention mechanism (Bahdanau et al. 2015)和 pointer-softmax coping mechanism (Gulcehre et al. 2016)。</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>输入：</p>
<ul>
<li>document $D=(d_1, …, d_n)$</li>
<li>answer $A = (a_1, …, a_m)$</li>
</ul>
<p>$d_i, a_j \in R^{D_e}$ 是词向量。</p>
<p>在文档词向量后面拼了个二维特征表示文档单词是否在答案中出现。然后过 Bi-LSTM 对文档表示进行编码得到 annotation vectors $h_d=(h^d_1,…h^d_n)$，$h^d_i \in R^D_h$, $h^d_i$ 是每一时刻前向和后向 hidden state 的拼接。</p>
<p>接着对 answer 编码。主要根据 answer 在 document 的位置找到对应的 annotation vector，然后把它和 answer 的词向量拼接起来也就是 $[h^d_j;a_j], s&lt;=j &lt;=e$，s,e 表示 answer 在 document 的起始结束位置，经过第二个 biLSTM 得到 $h^a \in R^{D_h}$，$h_a$ 是两个方向 final hidden state 的拼接。</p>
<p>计算 decoder 的初始状态 $s_0 \in R^D_s$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/decoder_s0.png" class="ful-image" alt="decoder_s0.png">
<p>$L \in R^{D_h * D_h}, W_0 \in R^{D_s * D_h}, b_0 \in R^{D_s}$</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>解码器产生输出，输出单词从 $p_\theta(y_t|y_{&lt;t}, D, A)$ 分布中得到。</p>
<p>为了在问句中直接产生文档中的一些短语和实体，在 decoder 的时候采用了 pointer-softmax，也就是两个输出层，shortlist softmax 和 location softmax，shortlist softmax 就是传统的 softmax，产生 predefined output vocabulary，对应 copynet 中的 generate-mode，location softmax 则表示某个词在输入端的位置，对应 copynet 中的 copy-mode。</p>
<p>Decoder：<br>$$s_t=LSTM(s_{t-1}, y_{t-1}, v_t)$$<br>$v_t$ 是从 document 和 answer encoding 计算得到的 context vector，用了 attention 机制，$a_{tj}$ 同时可以用作location softmax。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/decoder_attn.png" class="ful-image" alt="decoder_attn.png">
<p>context vector:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/decoder_context_vec.png" class="ful-image" alt="decoder_context_vec.png"></p>
<p>shortlist softmax vector $o_t$ 用了 deep output layer (Pascanu et al., 2013)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/decoder_deep_output.png" class="ful-image" alt="decoder_deep_output.png"></p>
<p>最后的 $p_t \in R^{|V|+|D|}$ 由 $z_t$ 对两个 softmax 输出进行加权和拼接得到。$z_t$ 由 MLP 产生，输入也是 $s_t, v_t, y_{t-1}$，两个隐层然后输出层 sigmoid 激活得到 $z_t$。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/decoder_output.png" class="ful-image" alt="decoder_output.png"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>三个 loss:</p>
<ol>
<li>negative log-likelihood<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/loss1.png" class="ful-image" alt="loss1.png">
用了 teacher forcing，也就是 $y_{t-1}$ 不是从模型输出得到的，而是来自 source sequence</li>
<li>not to generate answer words in question<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/loss2.png" class="ful-image" alt="loss2.png">
$\hat a$ 表示在 answer 中出现但没有在 groud-truth question 中出现的单词</li>
<li>Variety<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/loss3.png" class="ful-image" alt="loss3.png">
最大化信息熵来鼓励输出多样性</li>
</ol>
<h1 id="Policy-Gradient-Optimization"><a href="#Policy-Gradient-Optimization" class="headerlink" title="Policy Gradient Optimization"></a>Policy Gradient Optimization</h1><p>Teacher forcing 会带来一个问题，训练阶段和测试阶段的结果会存在很大差异。在训练阶段，tearcher force 使得模型不能从错误中学习，因为最大化 groud-truth likelihood 并不能教模型给没有 groud-truth 的 example 分配概率。于是就有了 RL 方法。在预训练一波 maximum likelihood 之后，使用一些和问题质量相关的 rewards，来进行 policy gradient optimzation。</p>
<h2 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h2><ol>
<li><strong>Question answering</strong><br>好的问题能被回复<br>把 model-generated question 喂给预训练好的 QA 系统（论文用的 MPCM 模型），然后用 QA 系统的 accuracy（比如 F1） 作为 reward<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/reward_qa.png" class="ful-image" alt="reward_qa.png"></li>
<li><strong>Fluency (PPL)</strong><br>是否符合语法，过一个语言模型计算 perplexity<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/reward_ppl.png" class="ful-image" alt="reward_ppl.png"></li>
<li><strong>Combination</strong><br>两者加权<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/reward_comb.png" class="ful-image" alt="reward_comb.png">
</li>
</ol>
<h2 id="Reinforce"><a href="#Reinforce" class="headerlink" title="Reinforce"></a>Reinforce</h2><p>“loss”:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/reinforce_loss.png" class="ful-image" alt="reinforce_loss.png"></p>
<p>$\pi$ 是要训练的 policy，是action 的概率分布，action space 就是 decoder output layer 的词汇表，可以通过 beam-search 采样选择 action，采样结果通过 decoder teacher-force 还原得到 state，计算 reward 进行梯度更新。</p>
<p>Policy gradient:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/reinforce_gradient.png" class="ful-image" alt="reinforce_gradient.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Baseline Seq2Seq 可以产生更符合语法更流畅的英文问题，但是语义可能更加模糊，这篇 paper 提出的系统可以产生更具体的问题，虽然没那么流畅。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/evaluation.png" class="ful-image" alt="evaluation.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Machine%20Comprehension%20by%20Text-to-Text%20Neural%20Question%20Generation/examples.png" class="ful-image" alt="examples.png">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> Question Generation </tag>
            
            <tag> 问题生成 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Making Neural QA as Simple as Possible but not Simpler（FastQA）]]></title>
      <url>http://www.shuang0420.com/2018/05/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/</url>
      <content type="html"><![CDATA[<p>阅读理解系列的框架很多大同小异，但这篇 paper 真心觉得精彩，虽然并不是最新最 state-of-art~<br><a id="more"></a></p>
<p>现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 <strong>emedding-, encoding-, interaction-, answer-layer</strong>），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层。而这篇 paper 提供了抽取式 QA 基于神经网络的两个 baseline，BoW- 和 RNN-based nerual QA (FastQA) ，创新的以 bottom-up 的方式分析了框架复杂性以及主流 interaction layer 的作用。</p>
<p>一个基本认识，构建好的 QA 系统必不可少的两个要素是：</p>
<ol>
<li>在处理 context 时对 question words 的意识</li>
<li>有一个超越简单的 bag-of-words modeling 的函数，像是 RNN</li>
</ol>
<p>另外，作者还发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans：</p>
<ol>
<li><strong>与 question 对应的 answer type 匹配</strong><br>比如说问 when 就回答 time</li>
<li><strong>与重要的 question words 位置上临近</strong><br>如下图的 St. Kazimierz Church</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/heuristic.png" class="ful-image" alt="heuristic.png">
<p>FastQA 的表现对额外的复杂度，尤其是 interaction 的复杂交互，提出了质疑。</p>
<h1 id="A-BoW-Neural-QA-System"><a href="#A-BoW-Neural-QA-System" class="headerlink" title="A BoW Neural QA System"></a>A BoW Neural QA System</h1><p>比照传统思路来构建。</p>
<ol>
<li><strong>Embedding</strong><br>词向量和字向量的拼接，字向量用 CNN 进行训练，$x=[x^w; x^c] \in R^d$</li>
<li><strong>Type matching</strong><br>抽取 question words 得到 lexical answer type(LAT)。抽哪些？<ul>
<li>who, when, why, how, how many, etc.</li>
<li>what, which 后面的第一个名词短语，如 what year did…<br>将 LAT 的第一个和最后一个单词的 embedding，以及 LAT 所有单词的平均的 embedding 拼接起来，再通过全连接层和 tanh 做一个非线性变换得到 $\hat z$。<br>用同样方法对每个 potential answer span(s, e) 做编码。所有 span，最长为 10 个单词，同样把 span 里第一个和最后一个单词的 embedding 和所有单词的 embedding 进行拼接，又因为 potential answer span 周围的单词会对 answer span type 提供线索（比如上文提到的 St. Kazimierz Church），所以额外的拼接了 span 往左、往右 5 个单词的平均 embedding，这样一共就是 5 个 embedding，接 FC 层和 tanh 非线性变换，得到 $\hat x_{s,e}$<br>最后，拼接 LAT 和 span 的表示，$[\hat z; \hat x_{s, e}; \hat z \ ☉ \ \hat x_{s,e}]$，用一个前馈网络计算每个 span(s,e) 和 LAT 的分数 $g_{type}(s,e)$</li>
</ul>
</li>
<li><strong>Context Matching</strong><br>引入两个 word-in-question 特征，对 context 中的每个单词 $x_j$<ul>
<li><strong>binary</strong><br>$wiq^b$ ，如果 $x_j$ 出现在了 question 中，就为 1，否则为 0</li>
<li><strong>weighted</strong><br>计算 $q_i$ 和 $x_j$ 的词向量相似性<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/weighted_wiq.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/weighted_wiq.png">
Softmax 保证了 infrequent occurrences of words are weighted more heavily.<br>对每个 answer span(s,e)，计算往左、往右 5/10/20 token-windows 内 $wiq^b$ 和 $wiq^w$ 的平均分数，也就是计算 2(kind of features) <em> 3(windows) </em> 2(left/right)=12个分数的加权和得到 context-matching score $g_{ctxt}(s,e)$，各分数的权重由训练得到</li>
</ul>
</li>
<li><strong>Answer span scoring</strong><br>最后每个 span(s,e) 的分数就是 type matching score 和 context matching score 的和<br>$$g(s,e)=g_{type}(s,e)+g_{ctxt}(s,e)$$</li>
</ol>
<p>最小化 softmax-cross-entropy loss 进行训练。</p>
<h1 id="FastQA"><a href="#FastQA" class="headerlink" title="FastQA"></a>FastQA</h1><p>上面的方法中语义特征完全被缩减成了 answer-type 和 word-in-question features，另外 answer span 也受到了长度限制，对语义的捕捉很弱。</p>
<p>BiRNN 在识别 NER 上面非常有优势，context matching 也可以通过给 BiRNN 喂 wiq-features 得到，answer-type 会间接由网络学习得到。</p>
<p>模型相对简单，就三层 <strong>embedding-, encoding-, answer layer</strong>。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/fastQA.png" class="ful-image" alt="fastQA.png">
<ol>
<li><strong>Embedding</strong><br>和 BoW baseline 相同。</li>
<li><strong>Encoding</strong><br>为了让 question 和 context embedding 可以交互，先映射到 n 维向量，再过一个 highway layer。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding1.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding1.png">
然后加上 wiq features<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding2.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding2.png">
再一起过一个 BiRNN，输出再做个 projection<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding3.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/encoding3.png">
初始化 project matrix B 为 $[I_n; I_n]$，$I_n$ 是 n 维的 identity matrix，H 是 forawrd 和 backward LSTM 的输出的加和。<br>question 和 context 的参数共享，question 对应的两个 wiq 特征设为 1。projection matrix B 不共享。</li>
<li><strong>Answer layer</strong><br>context x $H=[h_1,…,h_{L_X}]$<br>question Q $Z=[Z_1,…Z_{L_Q}]$<br>对 Z 做一个变换，同样是 context-independent<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer1.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer1.png">
answer 的开始位置的概率 $p_s$ 由 2 个前馈网络加一个 ReLU 激活得到。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer2.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer2.png">
结束位置：<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer3.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/answer3.png">
$$p(s,e)=p_s(s)•p_e(e|s)$$</li>
</ol>
<p>最小化 p(s,e) 的交叉熵来训练。</p>
<p>在预测的时候，可以用 beam-search。</p>
<h1 id="FastQA-Extended"><a href="#FastQA-Extended" class="headerlink" title="FastQA Extended"></a>FastQA Extended</h1><p>相当于主流模型的 <strong>interaction layer</strong>。对当前的 context state，考虑和剩下的 context（intra）或者和 question（inter）做注意力计算，将其余 context/question 的信息融入当前 context。</p>
<ul>
<li><strong>Intra-fustion</strong><br>between passages of the context</li>
<li><strong>Inter-fusion</strong><br>between question and context</li>
</ul>
<p>实验结果：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/res1.png" class="ful-image" alt="res1.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/res2.png" class="ful-image" alt="res2.png">
<p>一些小结论：</p>
<ol>
<li>简单的 $wiq^b$ 特征能大幅度提升 performance，原因是让 encoder 有了真实 question 的部分知识后，encoder 就可以有选择性的追踪问题相关的信息并进一步将具体的实体抽象为对应的类型，如果在问题中提到了人名，那么 context encoder 就会记住 “question-person” 而不是具体名字。</li>
<li>Beam-search 可以微弱提升结果，因为最可能的开始位置不一定是最好的 answer span</li>
<li>额外的 character embedding 对结果有显著提升</li>
<li>进一步的 fusion 对结果也有帮助，但并没有那么显著</li>
</ol>
<p>讨论 <strong>Do we need additional interaction?</strong><br>对比试验，FastQA 与 FastQAExt 和 DCN 相比，快两倍，而且少 2-4 倍的显存。分析了结果发现 FastQAExt 泛化能力更强些，但并没有 systematic advantage，并不会对某类问题（主要分析了推理）有一致性的提升。</p>
<h1 id="Qualitative-Analysis"><a href="#Qualitative-Analysis" class="headerlink" title="Qualitative Analysis"></a>Qualitative Analysis</h1><p>对 FastQA 的错误结果进行了一些分析，大部分的错误来自：</p>
<ol>
<li>缺乏对句法结构的理解</li>
<li>不同词位相似语义的词的细粒度语义之间的区分</li>
</ol>
<p>其他很多的错误也是来自人工标注偏好。</p>
<p>举了一些典型的错误例子，像 例1 是缺乏对某些答案类型的细化理解。例2 缺乏指代消解和上下文缩略语的认识，例3 模型有时难以捕捉基本的句法结构，尤其是对于重要的分隔符如标点符号和连词被忽略的嵌套句子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Making%20Neural%20QA%20as%20Simple%20as%20Possible%20but%20not%20Simpler/error.png" class="ful-image" alt="error.png">
<p>现有 top-down 模型用到实际业务当中通常需要为了 fit 进显存或者是满足一定的响应时间而进行模型的各种简化，FastQA 在显存占用和响应速度上有着绝对优势，感觉还是非常有意义的~</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> FastQA </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Semi-Supervised QA with Generative Domain-Adaptive Nets]]></title>
      <url>http://www.shuang0420.com/2018/04/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/</url>
      <content type="html"><![CDATA[<p>GDAN，Question Generation 和 Question Answering 相结合，利用少量的有标注的 QA 对 + 大量的无标注的 QA 对来训练 QA 模型。<br><a id="more"></a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>看到这篇论文，看到来自 CMU，就忍不住推测作者估计是 LTI 的，估计还上过 411/611/711，毕竟 idea 和 final project 太像了。。</p>
<p>回顾下 CMU 11411/611/711 的 final project，项目是阅读理解，分为 Asking System 和 Answering System 两个子系统。17年初的时候，Alan 鼓励用课上学到的东西 &amp; 隐晦的不鼓励用 DL，anyway 那时候也并没有看到用 DL 做 QG 的 paper，网上唯几和 QG 相关的 paper 都是 CMU 的，估计和这门课相辅相成。</p>
<p>611 的 asking system 和 answering system 都没有标注，只是纯粹的 wiki 文本，asking system 基于 document 产生 question 以及 answer，answering system 根据 question 和 document 产生 answer。具体见之前的两篇博文：<br><a href="http://www.shuang0420.com/2017/03/02/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/">NLP 笔记 - Question Answering System</a><br><a href="http://www.shuang0420.com/2017/04/06/QA%20system%20-%20Question%20Generation/">QA system - Question Generation</a></p>
<p>因为没有标注，所以两个系统其实是相互补充相互促进的。如果产生的 question 太简单，和原文太过相近，那么 answering system 的泛化能力有可能就很差，而如果 question 太难，answering system 也就学很难学习很难训练。</p>
<p>评价产生的 question 的好坏的标准除了流畅、符合语法等基于 question 本身的特点外，我们还希望好的问题能找到答案，这些逻辑在这篇论文中都有所体现。</p>
<p>回到 paper，主要思想其实就是用 <strong>少量的有标注的 QA 对 + 大量的无标注的 QA 对</strong> 来训练 QA 模型。主要做法是，给部分 unlabelled text，用 tagger 抽一些答案，训练 generative model 来生成对应的问题，然后补充训练集，再训练 QA model。实际是用改进的 GAN 方法来构建一个半监督问答模型。</p>
<h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><h2 id="Generative-Model-seq2seq-with-attention-and-copy"><a href="#Generative-Model-seq2seq-with-attention-and-copy" class="headerlink" title="Generative Model - seq2seq with attention and copy"></a>Generative Model - seq2seq with attention and copy</h2><p>对 P(q|p,a) 进行建模。输入是 unlabelled text p 和从中抽取的答案 a，输出是 q，或者说 (q, p, a)。答案 a 的抽取依赖 POS tagger + constituency parser + NER tagger。生成模型这里用的是 <strong>seq2seq model</strong>(Sutskever et al., 2014) + <strong>copy mechanism</strong>(Gu et al., 2016; Gulcehre et al., 2016)。</p>
<p>Encoder 用一个 GRU 把 paragraph 编码成 sequence of hidden states H。注意论文在 paragraph token 的词向量上加了额外的一维特征来表示这个词是否在答案中出现，如果出现就为 1，否则为 0。<br>Decoder 用另一个 GRU + Attention 对 H 进行解码，在每一个时刻，生成/复制单词的概率是：</p>
<p>$$P_{overall} = g_tp_{vocab}+(1-g_t)p_{copy}$$<br>$$g_t=\sigma(w^T_gh_t)$$</p>
<p>具体细节不多说了，相关可以看 Copy or Generate。</p>
<p>生成模型 G 产生的 (q, p, a) 作为判别模型的输入。</p>
<h2 id="Discriminative-Model-gated-attention-reader"><a href="#Discriminative-Model-gated-attention-reader" class="headerlink" title="Discriminative Model - gated-attention reader"></a>Discriminative Model - gated-attention reader</h2><p>对 P(a|p,q)进行建模。输入是人为标注数据 L 以及模型产生的数据 U，由于 L 和 U 来自不同分布，所以引入了 domain tag 来区分两类数据，“true”来表示人为标记数据 L，“gen”标签来表示模型生成数据 U（Johnson et al., 2016; Chu et al., 2017）。在测试时，只加入 d_true。</p>
<p>论文这里用了 GA (gated-attention) Reader 作为基本结构，也是 CMU 出的模型，当然事实上别的模型也可以。模型很简单，embedding 层用词向量，encoder 层用双向 GRU 分别得到 $H_q$ 和 $H^k_p$，context-query attention 层用 gated attention($H^k_p$, $H_q$ 做 element-wise 乘法)做下一层网络的输入，重复进入 encoder 和 attention 层进行编码和乘法（共 k 层），最后将 p, q 做內积（inner product）得到一个最终向量输入 output 层，output 层用两个 softmax 分别预测答案在段落中的起始和结束位置。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/GA_reader.png" class="ful-image" alt="GA_reader.png">
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>整体的目标函数：</p>
<p>$$max_D \ J(L, d_{true, D})+ J(U_G, d_{gen}, D)$$<br>$$max_G \ J(U_G, d_{true}, D)$$</p>
<h2 id="Training-Algorithm"><a href="#Training-Algorithm" class="headerlink" title="Training Algorithm"></a>Training Algorithm</h2><p>主要要解决下面两个问题。</p>
<h3 id="Issue-1-discrepancy-between-datasets"><a href="#Issue-1-discrepancy-between-datasets" class="headerlink" title="Issue 1: discrepancy between datasets"></a>Issue 1: discrepancy between datasets</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/issue1.png" class="ful-image" alt="issue1.png">
<p>如上，判别模型很容易在 U 上 overfit，所以才用了 domain tag 做区分。</p>
<h3 id="Issue-2-jointly-train-G-and-D"><a href="#Issue-2-jointly-train-G-and-D" class="headerlink" title="Issue 2: jointly train G and D"></a>Issue 2: jointly train G and D</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/issue2.png" class="ful-image" alt="issue2.png">
<p>如上，如果用 auto-encoder，容易让 question 和 answer 的表达非常接近，question 甚至可能完全 copy answer，所以这里用了判别模型。</p>
<blockquote>
<p>Intuitively, the goal of G is to generate “useful” questions where the usefulness is measured by the probability that the generated questions can be answered correctly by D</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/issue22.png" class="ful-image" alt="issue22.png">
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/model.png" class="ful-image" alt="model.png">
<p>分两个阶段：<br><strong>第一阶段:</strong> 固定 G，利用 d_true 和 d_gen，用 SGD 来更新 D。在 L 上计算 MLE 来完成 G 的初始化，对 D 进行随机初始化。<br><strong>第二阶段:</strong> 固定 D，利用 d_true，用 RL 和 SGD 更新 G。由于 G 的输出是不可导的，所以用到了 reinforce algorithm。action space 是长度为 T’ 的所有可能的 questions，reward 是 $J(U_G,d_{true}, D)$。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Semi-Supervised%20QA%20with%20Generative%20Domain-Adaptive%20Nets/algo.png" class="ful-image" alt="algo.png">
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>QANet 那篇论文中提到了另一篇 Question Generation 的论文：</p>
<blockquote>
<p>Zhou et al. (2017) improved the diversity of the SQuAD data by generating more questions. However, as reported by Wang et al. (2017), their method did not help improve the performance.</p>
</blockquote>
<p>相信 GDAN 在一定程度上一定能缓解 QA 中标注数据稀少的问题，但是能否在数据较为充足，模型较为优势的情况下提升 performance，估计难说，下次尝试后再来填这个坑了。Anyway，看到了曾经思考过的问题有人做出了实践还是万分开心的~</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> Question Generation </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Bi-Directional Attention Flow for Machine Comprehension]]></title>
      <url>http://www.shuang0420.com/2018/04/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Bi-Directional%20Attention%20Flow%20for%20Machine%20Comprehension/</url>
      <content type="html"><![CDATA[<p>BiDAF，相对复杂 attention 机制。<br><a id="more"></a></p>
<p><strong>论文：</strong> <a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="external">Bidirectional Attention Flow for Machine Comprehension</a><br><strong>代码：</strong> <a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="external">allenai/bi-att-flow</a></p>
<h1 id="Attention-Summary"><a href="#Attention-Summary" class="headerlink" title="Attention Summary"></a>Attention Summary</h1><p>这篇论文主要对 attention 机制做了改进，为此作者总结了 MC 任务上过去常用的三类 attention：</p>
<ol>
<li><strong>Attention Reader。</strong>通过动态 attention 机制从文本中<strong>提取相关信息（context vector）</strong>，再依据该信息给出预测结果。<br>代表论文：Bahdanau et al. 2015, Hermann et al. 2015, Chen et al. 2016, Wang &amp; Jiang 2016</li>
<li><strong>Attention-Sum Reader。</strong>只计算一次 attention weights，然后直接喂给输出层做最后的预测，也就是利用 attention 机制直接获取文本中各位置作为答案的概率，和 pointer network 类似的思想，效果很依赖对 query 的表示<br>代表论文：Kadlec et al. 2016, Cui et al. 2016</li>
<li><strong>Multi-hop Attention</strong>。计算多次 attention<br>代表论文：<a href="http://www.shuang0420.com/2017/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/">Memory Network(Weston et al., 2015)</a>，Sordoni et al., 2016; Dhingra et al., 2016., Shen et al. 2016.</li>
</ol>
<p>在此基础上，作者对注意力机制做出了改进，具体 BiDAF attention 的特点如下：</p>
<ol>
<li>并没有把 context 编码进固定大小的 vector，而是让 vector 可以流动，减少早期加权和的信息损失</li>
<li>Memory-less，在每一个时刻，仅仅对 query 和当前时刻的 context paragraph 进行计算，并不直接依赖上一时刻的 attention，这使得后面的 attention 计算不会受到之前错误的 attention 信息的影响</li>
<li>计算了 query-to-context（Q2C） 和 context-to-query（C2Q）两个方向的 attention 信息，认为  C2Q 和 Q2C 实际上能够相互补充。实验发现模型在开发集上去掉 C2Q 与 去掉 Q2C 相比，分别下降了 12 和 10 个百分点，显然 C2Q 这个方向上的 attention 更为重要</li>
</ol>
<h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Bi-Directional%20Attention%20Flow%20for%20Machine%20Comprehension/bidaf.png" class="ful-image" alt="bidaf.png">
<p><strong>论文提出的是六层结构：</strong><br><strong>Character Embedding Layer -&gt; Word Embedding Layer -&gt; Contextual Embedding Layer -&gt; Attention Flow Layer -&gt; Modeling Layer -&gt; Output Layer</strong></p>
<p>然而我还是压缩成五层结构来讲吧：</p>
<ol>
<li><strong>Input embedding layer</strong> = Character Embedding Layer + Word Embedding Layer<br>和其他模型差不多，word embedding + character embedding，预训练词向量，OOV 和字向量可训练，字向量用 CNN 训练<br>单词 w 的表示由词向量和字向量的拼接然后经过两层 highway network 得到，得到 context vector $X \in R^{d*T}$ 和 query vector $Q \in R^{d*J}$</li>
<li><strong>Embedding encoder layer</strong> = Contextual Embedding Layer<br>对上一步的结果 X 和 Q 分别使用 Bi-LSTM 编码，捕捉 X 和 Q 各自单词间的局部关系，拼接双向 LSTM 的输出，得到 $H \in R^{2d*T}$ 和 $U \in R^{2d*J}$<br>这前面的两层（or 原文三层）用来捕捉 query 和 context 各自不同粒度（character, word, phrase）上的特征</li>
<li><p><strong>Context-query attention layer</strong> = Attention Flow Layer</p>
<blockquote>
<p>The attention flow layer is not used to summarize the query and context into single feature vectors. instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to <strong>flow through to the subsequent modeling layer</strong>. This reduces the information loss caused by early summarization.</p>
</blockquote>
<p>输入是 H 和 U，输出是 context words 的 query-aware vector G，以及上一层传下来的 contextual embeddings。做 context-to-query 以及 query-to-context 两个方向的 attention。做法还是一样，先计算相关性矩阵，再归一化计算 attention 分数，最后与原始矩阵相乘得到修正的向量矩阵。<br>c2q 和 q2c 共享相似度矩阵，$S \in R^{T*J}$，相似度计算方式是：<br>$$S_{tj}=\alpha(H_{:t}, U_{:j}) \in R$$<br>$$\alpha(h,u)=w^T_{(S)}[h;u;h⊙u]$$<br>$S_{tj}$ : 第 t 个 context word 和第 j 个 query word 之间的相似度<br>$\alpha$: scalar function<br>$H_{:t}$: H 的第 t 个列向量<br>$U_{:j}$：U 的第 j 个列向量<br>⊙：element-wise multiplication<br>[;]：向量在行上的拼接</p>
<ul>
<li><strong>context-to-query attention(C2Q):</strong> 计算对每一个 context word 而言哪些 query words 和它最相关。前面得到了相关性矩阵，现在 softmax 对列归一化然后计算 query 向量加权和得到 $\hat U$<br>$$a_t=softmax(S_{t:}) \in R^J$$<br>$$\hat U_{:t}=\sum_ja_{tj}U_{:j}$$</li>
<li><strong>query-to-context attention(Q2C):</strong> 计算对每一个 query word 而言哪些 context words 和它最相关，这些 context words 对回答问题很重要。取相关性矩阵每列最大值，对其进行 softmax 归一化计算 context 向量加权和，然后 tile T 次得到 $\hat H \in R^{2d*T}$。<br>$$b=softmax(max_{col}(S)) \in R^T$$<br>$$\hat h=\sum_tb_tH_{:t} \in R^{2d}$$<br>$\hat U$ 和 $\hat H$ 都是 2dxT 的矩阵<br>将三个矩阵拼接起来得到 G<br>$$G_{:t}=\beta (H_{:t}, \hat U_{:t}, \hat H_{:t}) \in R^{d_G}$$<br>$\beta$ 可以是多层 perceptron，不过如上简单的拼接效果也不错。<br>$$\beta(h, \hat u, \hat h)=[h;\hat u; h⊙\hat u; h⊙\hat h] \in R^{8d*T}$$<br>于是就得到了 context 中单词的 query-aware representation。</li>
</ul>
</li>
<li><strong>Model encoder layer</strong> = Modeling Layer<br>输入是 G，再经过一次 Bi-LSTM 得到 $M \in r^{2D * T}$，捕捉的是 interaction among the context words conditioned on the query<br>M 的每一个列向量都包含了对应单词关于整个 context 和 query 的上下文信息</li>
<li><p><strong>Output layer</strong><br>预测开始位置 p1 和结束位置 p2<br>$$p_1=softmax(W^T_{(p^1)}[G; M]), \ \ \ p_2=softmax(W^T_{(p^2)}[G; M^2])$$<br>M 再经过一个 Bi-LSTM 得到 $M^2 \in R^{2d * T}$，用来得到结束位置的概率分布</p>
<p>​<br>最后的目标函数：<br>$$L(\theta)=-{1 \over N} \sum^N_i[log(p^1_{y_i^1})+log(p^2_{y_i^2})]$$<br>$y^1_i$ 和 $y^2_i$ 分别是第 i 个样本的 groundtruth 的开始和结束位置</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> BiDAF </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution]]></title>
      <url>http://www.shuang0420.com/2018/03/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Fast%20and%20Accurate%20Reading%20Comprehension%20by%20Combining%20Self-Attention%20and%20Convolution/</url>
      <content type="html"><![CDATA[<p>CMU &amp; Google 出品的 Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution，SQuAD 榜单上对应模型 QANet，这名字是不是太随意了TAT…<br><a id="more"></a></p>
<h1 id="Fast-and-Accurate-Reading-Comprehension-by-Combining-Self-Attention-and-Convolution"><a href="#Fast-and-Accurate-Reading-Comprehension-by-Combining-Self-Attention-and-Convolution" class="headerlink" title="Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution"></a>Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution</h1><p>CMU 和 Google Brain 新出的文章，SQuAD 目前的并列第一，两大<strong>特点：</strong></p>
<ol>
<li><strong>模型方面创新的用 CNN+attention 来完成阅读理解任务</strong><br> 在编码层放弃了 RNN，只采用 CNN 和 self-attention。CNN 捕捉文本的局部结构信息（ local interactions），self-attention 捕捉全局关系（ global interactions），在没有牺牲准确率的情况下，加速了训练（训练速度提升了 3x-13x，预测速度提升 4x-9x）</li>
<li><strong>数据增强方面通过神经翻译模型（把英语翻译成外语（德语/法语）再翻译回英语）的方式来扩充训练语料，增加文本多样性</strong></li>
</ol>
<p>其实目前多数 NLP 的任务都可以用 word vector + RNN + attention 的结构来取得不错的效果，虽然我挺偏好 CNN 并坚定相信 CNN 在 NLP 中的作用（捕捉局部相关性&amp;方便并行），但多数情况下也是跟着主流走并没有完全舍弃过 RNN，这篇论文还是给了我们很多想象空间的。</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Fast%20and%20Accurate%20Reading%20Comprehension%20by%20Combining%20Self-Attention%20and%20Convolution/cnn_self_attention.png" class="ful-image" alt="cnn_self_attention.png">
<p>先看模型，在 BiDAF 基础上的一些改进，主要在 embedding encoder 层。还是阅读理解经典五层结构：</p>
<ol>
<li><strong>Input embedding layer</strong><br>和其他模型差不多，word embedding + character embedding，预训练词向量，OOV 和字向量可训练，字向量用 CNN 训练<br>单词 w 的表示由词向量和字向量的拼接 $[x_w; x_c] \in R^{p_1+p_2}$然后经过两层 highway network 得到，这个和 BiDAF 相同</li>
<li><strong>Embedding encoder layer</strong><br>重点是这一层上的改变，由几个基本 block 堆叠而成，每个 block 的结构是：<br><strong>[convolution-layer x # + self-attention-layer + feed-forward-layer]</strong><br>卷积用的 <strong>separable convolutions</strong> 而不是传统的 convolution，因为更加 memory efficient，泛化能力也更强。核心思想是将一个完整的卷积运算分解为 <strong>Depthwise Convolution</strong> 和 <strong>Pointwise Convolution</strong> 两步进行，两幅图简单过一下概念<br>先做 depthwise conv， 卷积在二维平面进行，filter 数量等于上一次的 depth/channel，相当于对输入的每个 channel 独立进行卷积运算，然后就结束了，这里没有 ReLU<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Fast%20and%20Accurate%20Reading%20Comprehension%20by%20Combining%20Self-Attention%20and%20Convolution/depth_conv.png" class="ful-image" alt="depth_conv.png">
然后做 pointwsie conv，和常规卷积相似，卷积核尺寸是 1x1xM，M 为上一层的 depth，相当于将上一步depthwise conv 得到的 map 在深度上进行加权组合，生成新的 feature map<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Fast%20and%20Accurate%20Reading%20Comprehension%20by%20Combining%20Self-Attention%20and%20Convolution/pointwise_conv.png" class="ful-image" alt="pointwise_conv.png">
<strong>Self-attention-layer</strong> 用的是多头注意力机制（head=8），常用的也不多说了。<br>注意的是这里每个基本运算（conv/self-attention/ffn）之间是 <strong>残差连接</strong>，对输入 x 和操作 f，输出是 f (layernorm(x))+x，也就是说某一层的输出能够直接跨越几层作为后面某一层的输入，有效避免了信息损失<br>4 个卷积层，1 个 encoding block</li>
<li><strong>Context-query attention layer</strong><br>几乎所有 machine reading comprehension 模型都会有，而这里依旧用了 context-to-query 以及 query-to-context 两个方向的 attention，先计算相关性矩阵，再归一化计算 attention 分数，最后与原始矩阵相乘得到修正的向量矩阵。相似度函数这里用的<br>$$f(q,c)=W_0[q,c,q⊙c]$$<br>对行、列分别做归一化得到 S’ 和 S’’，最后 context-to-query attention 就是 $A=S’Q^T$，query-to-context attention 就是 $B=S’S’’^TC^T$，用了 DCN attention 的策略</li>
<li><strong>Model encoder layer</strong><br>和 BiDAF 差不多，不过这里依旧用 CNN 而不是 RNN。这一层的每个位置的输入是 [c, a, c⊙a, c⊙b]，a, b 是 attention 矩阵 A,B 的行，参数和 embedding encoder layer 相同，除了 cnn 层数不一样，这里是每个 block 2 层卷积，一共 7 个 block</li>
<li><strong>Output layer</strong><br>再次和 BiDAF 相同<br>$p1=softmax(W_1[M_0; M_1]), p2=softmax(W_2[M_0; M_2])$<br>目标函数：<br>$$L(\theta)=-{1 \over N} \sum^N_i[log(p^1_{y_i^1})+log(p^2_{y_i^2})]$$<br>其中 $y^1_i$ 和 $y^2_i$ 分别是第 i 个样本的 groundtruth 的开始和结束位置</li>
</ol>
<h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>CNN 速度快所以有条件用更多的数据来训练啦，然后进一步增强模型的泛化能力啦。这里数据增强的基本 idea 就是通过 NMT 把数据从英文翻译成法文（English-to-French），另一个翻译模型再把法文翻回英文（French-to-English）<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Fast%20and%20Accurate%20Reading%20Comprehension%20by%20Combining%20Self-Attention%20and%20Convolution/data_augmentation.png" class="ful-image" alt="data_augmentation.png"><br>看图说话，对段落中每个句子先用 English-to-French 模型的 beam decoder 得到 k 个法语翻译，然后对每一条翻译，都再经过一个 reversed translation model 的 beam decoder，这最后就得到了 k^2 个改写的句子（paraphrases），然后从这 k^2 个句子中随机选一个</p>
<p>具体到 SQuAD 任务就是 (d,q,a) -&gt; (d’, q, a’)，问题不变，对文档 d 翻译改写，由于改写后原始答案 a 现在可能已经不在改写后的段落 d’ 里了，所以需要从改写后的段落 d’ 里抽取新的答案 a’，采用的方法是计算 s’ 里每个单词和原始答案里 start/end words 之间的 character-level 2-gram score，分数最高的单词就被选择为新答案 a’ 的 start/end word<br>这个方法还可以从 quality 和 diversity 改进，quality 方面用更好的翻译模型，diversity 方面可以考虑引入问题的改写，也可以使用其他的数据增广的方法（Raiman&amp;Miller, 2017）</p>
<p>实验结论是英文语料：法语语料：德语语料是 3:1:1 的比例时效果最好，EM 提升了 1.5，F1 提升了 1.1</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Comprehension </tag>
            
            <tag> 阅读理解 </tag>
            
            <tag> BiDAF </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[扯扯 Semi-hard Negative Samples]]></title>
      <url>http://www.shuang0420.com/2018/03/17/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/</url>
      <content type="html"><![CDATA[<p>谈谈排序模型的 negative sampling 问题。取不好题目TAT…<br><a id="more"></a></p>
<p>之前在用 Dual Encoder 这类 ranking model 时，发现在一些问题上尽管大方向上匹配的很好，比如人名问题答的都是人名，歌曲问题回答的都是歌曲名，但是具体到再细的粒度就完全匹配不上了。明明知道这显然是 negative sampling 的问题，却一直没找到好的解决方案，毕竟 google 一下 negative sampling 发现前 5 页都是在讲 word2vec……</p>
<p>感觉这应该算是 ranking 模型中一个非常实际也非常常见的问题。在训练的时候，我们一般不会在整个模板库上进行排序，这样效率太低也容易错，而是会做随机负采样，比如随机选 19 个负样本，和正确答案一起作为候选答案（ranking size=20）。但在实际应用中，又往往需要对整个模板库排序，或者至少要用别的方法召回一批样本（包含正确答案）再进行预测。这样一来，如果训练时随机的负样本太弱太简单（和正确答案差异性很大），而在预测时候选答案又太难太挑战（和正确答案很相似），效果当然就不好了。</p>
<p>当时自己折腾的时候尝试过很多方法，比如扩大 ranking size，多采样负样本，发现收益并不大，也试过先召回一部分和答案相似的样本，再做 reranking，或者在 DE 前或者后加别的 ranking 来进一步缩小范围，也是收效甚微，另外资源限制很多实验都没跑的很完美就暂时把这个问题放下了。直到最近看到了 <a href="https://github.com/tambetm/allenAI" target="_blank" rel="external">tambetm/allenAI</a> 这个，发现这个小哥哥用了一种不错的 negative sampling 的方法，看上去很有道理的样子，借鉴的还是 15 年的工作，哎都怪我读书少之前木有接触过现在才发现(╥﹏╥)</p>
<p>简单来说就是选择难度适当的错误回复（<strong>semi-hard negative samples</strong>）作为负样本，这个 idea 来自 15年 FaceNet 那篇文章，过去瞅了瞅，就从 FaceNet 讲起吧。FaceNet 最大特点应该就是提出了 Triplet Loss 的概念，也就是在向量空间内，希望保证单个个体的图像 $x^a_i (anchor)$和该个体的所有其它图像 $x^p_i(positive)$ 之间的特征距离尽可能的小，而与其它个体的图像 $x^n_i(negitive)$ 之间的特征距离要尽可能的大（差不多就是 LDA 的思路嘛，最大化类间距离最小化类内距离）。</p>
<blockquote>
<p>Here we want to ensure that an image $x^a_i (anchor)$ of a specific person is closer to all other images $x^p_i(positive)$ of the same person than it is to any image  $x^n_i(negitive)$  of any other person.</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/triple_loss.png" class="ful-image" alt="triple_loss.png">
<p>然后 loss 的设定就是说通过学习，使得类间距离大于类内距离，$\alpha$ 作为 positive/negtive 边界，是一个常量。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/triple_loss_for.png" class="ful-image" alt="triple_loss_for.png"></p>
<p>Triplets 显然不能穷举，这样一来筛选 triplets 就很重要，为了最快收敛考虑当然是首选最难区分的图像对了，也就是 hard positive ($argmax_{x^p_i}||f(x^a_i)-f(x^p_i)||^2_2$) 和 hard negative ($argmin_{x^n_i}||f(x^a_i)-f(x^n_i)||^2_2$)。举个例子，如果整体样本集是 1000 个人每个人 40 张图片，给定某个人的图片来选 triple，那么自然选这个人另外 39 张图片中和它最不相似的图片作为 hard positive，以及剩下 40*999 张图片中和它最相似的图片作为 hard negative。</p>
<p>挑选 hard positive 和 hard negative 有 offline (generate triplets every n steps) 和 online (select triplets within a mini-batch) 方法。论文重点聚焦在了 online 方法，在一个大的 mini-batch （1800 个样本）中选择所有的 anchor-positive pairs，同时，来选择一定的 hard anchor-negative pairs，但选择 hardest negatives 在实际当中容易导致在训练最开始的时候就陷入局部最优，所以实际会选择 semi-hard negatives，使挑选样本满足下面的式子：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/form.png" class="ful-image" alt="form.png"></p>
<p>这个约束就是 semi-hard。再回过头来看开始的 repo，模型用 RNN 对 question 和 answer 进行编码，然后用 cosine ranking loss，也就是让 question 和 right answer 的 cosine 距离小于 question 和 wrong answer 的 cosine 距离。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/cosine_loss.png" class="ful-image" alt="cosine_loss.png"></p>
<p>wrong answer 采用 neagtive sampling，选择难度适当的 wrong answer，或者说选择当前模型能正确分类但是没那么 confident 的 wrong answer。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%89%AF%E6%89%AF%20Semi-hard%20Negative%20Samples/semi_hard.png" class="ful-image" alt="semi_hard.png"></p>
<p>他们是发现 too hard answer 会使模型难以收敛，而实验表明：</p>
<blockquote>
<p>Semi-hard negative samples, which are further than the right answer, but still within the margin - works best</p>
</blockquote>
<p>参考连接：</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/24837264" target="_blank" rel="external">谷歌人脸识别系统FaceNet解析</a><br><a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="external">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> negative sampling </tag>
            
            <tag> 负采样 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - 基于神经网络的推理]]></title>
      <url>http://www.shuang0420.com/2018/03/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%8E%A8%E7%90%86/</url>
      <content type="html"><![CDATA[<p>DeepMind Relational Reasoning(RNs)<br><a id="more"></a></p>
<h1 id="Relational-reasoning-RNs"><a href="#Relational-reasoning-RNs" class="headerlink" title="Relational reasoning(RNs)"></a>Relational reasoning(RNs)</h1><ul>
<li>论文：<a href="https://arxiv.org/abs/1706.01427" target="_blank" rel="external">A simple neural network module for relational reasoning(2017)</a></li>
<li>github代码: <a href="https://github.com/siddk/relation-network" target="_blank" rel="external">https://github.com/siddk/relation-network</a></li>
</ul>
<p>关系推理的传统方法有<strong>基于符号的方法（symbolic approaches）</strong>和<strong>基于统计的方法（statistical learning）</strong>。基于符号的方法存在着 symbol grounding 的问题，在小任务（small task）和输入变化（input variations）的问题上也不够鲁棒，学习能力不强；而基于统计的方法像深度学习，虽然泛化能力强，但是对数据稀疏但关系复杂的问题也是束手无策。DeepMind 2017年出的这篇论文提出的<strong>Relation network(RN)</strong>，是用于关系推理（relational reasoning）的一个神经网络模块（NN module），能直接加载到已有的神经网络架构中。与 GNN 等网络结构相比，更为简单和灵活，即插可用（plug-and-play），在一些关系推理的测试上的准确率已经超过了人类。</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>RN 的网络结构是真的很简单（不然也不会说是”simple neural network”），以至于通篇下面一个公式就可以概括，核心就是利用神经网络来找出任意 pairwise 对象之间的潜在关系。</p>
<p>$$RN(O) = f_\phi (\sum_{i,j}g_\theta(o_i, o_j))$$</p>
<ul>
<li>Inputs: O={$o_1, …, o_n$}</li>
<li>MLPs: $f_\phi$, $g_\theta$<ul>
<li>$g_\theta$: 使用一个全连接的神经网络来量化 $o_i$ 和 $o_j$ 的关系，任意两个对象之间的关系使用同一套参数 $g_\theta(•,•)$</li>
<li>$f_\phi (\sum_{i,j}g_\theta(o_i, o_j))$: 考虑所有组合的关系，相当于考虑一个完全连接图，在这个图上计算各个边的权重，把重要的关系凸显出来，f 函数就计算了这个重要关系的集合</li>
</ul>
</li>
</ul>
<p>用在自然语言处理里，就是把每个句子当做一个对象，每个句子与句子的 pair 用 g 计算关系，再把所有关系加权和放到最终的预测网络里。</p>
<p>小结一下，RNs有以下三个特点：</p>
<ol>
<li><strong>可以学习推理</strong>。这里 RNs 计算了所有的两个对象之间的关系，当然也可以只计算部分两个对象之间的关系，这里的“部分”需要预定义</li>
<li><strong>RNs的数据效率更高(data efficient)</strong>。RNs 使用一个 gθ 函数来计算所有的关系，任意两个对象之间的关系使用同一套参数，泛化能力更强</li>
<li><strong>RNs作用在一个集合上</strong>，对输入和输出都是与顺序无关的（input/output invariation）</li>
</ol>
<h2 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h2><p>简单提一下和 NLP 有关的任务。</p>
<h3 id="VQA"><a href="#VQA" class="headerlink" title="VQA"></a>VQA</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%8E%A8%E7%90%86%20/RN_vqa.png" class="ful-image" alt="RN_vqa.png">
<p>RN 在 VQA 任务上的结构也很简单，CNN 处理图像，LSTM 编码 question，然后两两配对的 spatial cell（红蓝；黄红；蓝黄…）和 question embedding 拼接，后面接几个 FC 层，最后 softmax 得到某个 answer word。</p>
<p>Word-embedding: dim32; LSTM: dim128<br>$g_\theta$: 4-layer MLP, dim256-256-256, RELU<br>$f_\phi$: 3-layer MLP, dim256-256-29, RELU<br>$f_\phi (\sum_{i,j}g_\theta(o_i, o_j, q)$: 综合所有组合 $g_\theta(o_i, o_j;q)$，implicitly 提取有用的组合预测最终答案</p>
<h3 id="bAbI"><a href="#bAbI" class="headerlink" title="bAbI"></a>bAbI</h3><p>RN 在 bAbI 测试集上的结构，每个问题之前的最多 20个句子作为 support set，使用 LSTM-dim32 把 support set 连同每个句子在 set 里的相对位置编码转化为 RN 的 object set，同时使用另一个 LSTM-dim32 的 encoding state 表示问题。</p>
<p>$g_\theta$: 4-layer MLP, dim256-256-256-256<br>$f_\phi$: 3-layer MLP, dim 256-512-159</p>
<p>在 joint training 也就是 20 个任务一起训练一个 QA 模型的情况下，通过了 18/20 bAbI test。DNC 在 path finding 任务上表现不错，但在 basic induction 上误差达到 55.1%，而 RN 达到了 2.1% 的误差水平。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Relational reasoning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - 从神经图灵机 NTM 到可微分神经计算机 DNC]]></title>
      <url>http://www.shuang0420.com/2018/01/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/</url>
      <content type="html"><![CDATA[<p>Neural Turing Machine(NTC) 和 Differentiable Neural Machine(DNC) 的相关笔记。<br><a id="more"></a></p>
<p>涉及论文：</p>
<ul>
<li><strong>Neural Turing Machine</strong><br>Neural Turing Machine(2014)</li>
<li><strong>Differentiable Neural Machine</strong><br>Hybrid computing using a neural network with dynamic external memory (2016)</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>当今所有的计算机体系都源自冯诺依曼体系(Von Neumann, 1945)，三大要素：</p>
<ol>
<li><strong>elementary operations</strong><br>基本操作，如加减乘除</li>
<li><strong>logical flow control (branching)</strong><br>逻辑流程控制，如 if-else-then, for, while</li>
<li><strong>external memory</strong><br>外部存储器，内存和硬盘</li>
</ol>
<p>RNN 被认为是 Turing-complete 的，理论上可以拟合任何函数，有模拟流程的能力，然而理论上可以，实际实现并没有那么简单。NTM/DNC 的重点是 <strong>存储管理</strong>，它能够通过一个大的、可寻址的外部存储器来扩展标准 RNN 能力，大大简化算数等任务。</p>
<h2 id="Turing-Machine"><a href="#Turing-Machine" class="headerlink" title="Turing Machine"></a>Turing Machine</h2><p>NTM/DNC 的灵感来自于图灵机。图灵机就是一种简单的计算机模型，摘一段概念：</p>
<blockquote>
<p>A <strong>Turing machine</strong> is a simple model of the computer. Like modern computers, it encapsulates the idea of having an external memory as well as some sort of processor. Essentially, a Turing machine consists of a tape with instructions written on it and the device that can read up and down the tape. Based on what it reads on the tape, it can decide to move in a different direction to write a new symbol or erase a symbol, and so on.</p>
</blockquote>
<p>很简单，由 <strong>外部存储器（写有指令的磁带）和存储器（能够沿着磁带读取的设备）</strong> 组成，根据磁带上读取到的指令，计算机能够决定在磁带上不同的方向上移动来进行写入或者擦除新符号等操作。</p>
<p>神经图灵机的灵感就来自图灵机的架构，也有 <strong>控制器（神经网络）和外部存储器（memory）</strong> 构成，试图去解决一些计算机能够解决的很好但机器学习模型很难解决的问题，比如说算法，或者说上面提到的冯诺依曼体系的一些要素。</p>
<h2 id="NTM-DNC-vs-TM"><a href="#NTM-DNC-vs-TM" class="headerlink" title="NTM/DNC vs. TM"></a>NTM/DNC vs. TM</h2><p>NTM/DNC 灵感来自于 TM，最关键的区别是神经图灵机是 <strong>可微分的（differentiable）</strong> 图灵机。计算机/图灵机的计算是绝对的，要么是 0 要么是 1，计算机在非此即彼的逻辑或者整数中运作。然而大多数的神经网络和机器学习更多会使用实数，使用更平滑的曲线，这样会更容易训练（如 BP，可以通过输出追踪回去调整参数以得到希望的输出）。神经图灵机采用基本的图灵机思想，但同时找到了平滑的模拟函数，也就是说，在图灵机磁带上，神经图灵机 NTM 可以决定“稍微”向左或者向右移动，而不是单纯的向左跳一个或者向右跳一个。</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><ul>
<li><strong>Learn simple algorithms(Copy, repeat, recognize simple formal languages)</strong><br>能够学习简单的算法（夸张一点，本质上就是尝试着取代程序员）<br>NTM/DNC 能够接受输入和输出，并且学习得到能够从输入映射到输出的算法<br>如说复制任务，它能够学会接受相对短的序列，并重复几次。这让 LSTM 来做它就会崩溃，因为 LSTM 并不是在学习算法，而是试图一次性解决一个整体问题，它意识不到前两次所做的事情就是它们之后应该做的<br>再比如说识别平衡的括号，这涉及到了栈（stack）的算法，NTM/DNC 可以像程序员一样完成这个任务</li>
<li><strong>Generalize</strong><br>NTM/DNC 的计算图是对所有任务通用的</li>
<li><strong>Do well at language modeling</strong><br>擅长语言建模<br>比如做完型填空，能够猜测一个单词在句子或者文档语境中的意思</li>
<li><strong>Do well at bAbI</strong><br>擅长推理<br>在 bAbI 数据集上效果很好</li>
</ul>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><ol>
<li><strong>Architecture dependent</strong><br>实现的时候需要谨慎做出如对每一时刻的输入能读取/写入多少向量这类的决策，否则很有可能永远都得不到一个合理的结果</li>
<li><strong>Large number of parameters</strong><br>参数量非常大 =&gt; RAM 压力很大</li>
<li><strong>Dosen’t benefit much from GPU acceleration</strong><br>序列输入，每一步输入都依赖之前的输入，也就很难并行化，不能受益于 GPU 加速 =&gt; 很难训练</li>
</ol>
<p>很难训练还表现在：</p>
<ol>
<li><strong>Numerical Instability</strong><br>数值不稳定性。在试图学习算法时会更倾向于犯大错误，而如果在算法中犯了一个错误，所有的输出结果都会是不正确的。换言之，训练时神经图灵机总是很难找到需要的算法</li>
<li><strong>Using memory is hard</strong><br>大量数据+足够时间，大多数神经网络都会得到一些结果，而神经图灵机经常会卡住，它们经常一遍又一遍地一味地产生那些经常重复的值<br>因为使用记忆是很困难的，不仅要学会记住之后解决问题需要的东西，还不能意外的忘记它</li>
<li><strong>Need smart optimization</strong><br>1+2 =&gt; 需要很好的优化<br>如 <strong>gradient clipping, loss clipping, RMSprop, Adam, try different initialization, curriculum learning…</strong></li>
</ol>
<p>上面这些问题会让神经图灵机很难在实际中应用。</p>
<h2 id="NTM-DNC-vs-MemNN"><a href="#NTM-DNC-vs-MemNN" class="headerlink" title="NTM/DNC vs. MemNN"></a>NTM/DNC vs. MemNN</h2><p><strong>异：</strong><br><strong>DNC 侧重记忆管理</strong>。MemNN 侧重 memory 查询，在记忆管理上非常简单，一般以 QA 为例，就是每句话 encode 成 vector，然后保存下来，所谓的更新大多是把不重要的 memory 清出去空个位出来给新的 memory。而 DNC 花更多努力在记忆管理上，注重更新 memory 和 memory 的时间关系，包含更多的操作，更新、删除、添加等。<br>另外，<strong>NTM/DNC 侧重算法任务</strong>，能够自动从数据中学习算法，而一般而言 MemNN 侧重的是 QA 任务。</p>
<p><strong>同：</strong><br>都是从 model architecture 层面，将多个 machine learning model 联合起来处理复杂的任务，比如 LSTM 通常是来处理线性数据，MemNN/NTM 可能包含多个 LSTM，能够处理多个线性结构（类似图结构）</p>
<h1 id="Neural-Turing-Machines-NTM"><a href="#Neural-Turing-Machines-NTM" class="headerlink" title="Neural Turing Machines(NTM)"></a>Neural Turing Machines(NTM)</h1><h2 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h2><p>主要创新是将神经网络与外部存储器（external memory）结合来扩展神经网络的能力（通过注意力机制进行交互），可以类比图灵机，不过 NTM 是端到端可微的，所以可以使用梯度下降进行高效训练。</p>
<p>两个 <strong>主要元件</strong> 是 <strong>controller</strong> 和 <strong>memory bank</strong>。类比计算机来看 <strong>基本思路</strong>，实际是把神经网络看成是 CPU，把 memory 看做是计算机内存。CPU 根据任务来确定到内存的哪个位置读写信息，不过计算机的内存位置是离散数据，而 NTM 里是连续可导的。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/basic_idea.png" class="ful-image" alt="basic_idea.png"></p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>一张图读懂架构，取 <a href="http://people.idsia.ch/~rupesh/rnnsymposium2016/slides/graves.pdf" target="_blank" rel="external">DeepMind DNC Slides</a> 做了部分修改。最重要的概念还是那句话，每个组件都是可微分的，<strong>所有操作皆可导</strong>，这就可以直接用梯度下降训练来训练。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/ntm_arch.png" class="ful-image" alt="ntm_arch.png"></p>
<p>主要来研究读写操作。</p>
<h2 id="Read-Heads"><a href="#Read-Heads" class="headerlink" title="Read Heads"></a>Read Heads</h2><p>读操作和普通的 MemNN 相似，使用 attention 原理计算每个 memory vector 的权重向量，然后对 memory vector 进行加权生成读操作的结果。<br>$$r_t \leftarrow \sum^N_{i=1} w_t(i)M_t(i)$$</p>
<p>$M_t$ 是一个 NxM 的矩阵，表示 t 时刻的 memory，N 是 memory 的数量，M 是 memory vector 的维度。$w_t$是 t 时刻产生的权重向量，和 memory 数量相同，进行了归一化，$\sum_iw_t(i)=1, \ \ 0 \le w_t(i) \le 1, \forall i$</p>
<h2 id="Write-Heads"><a href="#Write-Heads" class="headerlink" title="Write Heads"></a>Write Heads</h2><h3 id="Erase-and-Add"><a href="#Erase-and-Add" class="headerlink" title="Erase and Add"></a>Erase and Add</h3><p>写操作包含两个步骤：先<strong>擦除（erase）</strong> 后<strong>添加（add）</strong></p>
<ul>
<li><strong>Erase</strong><br>input + controller 产生 erase vector $e_t \in R^M$, $e_t(d) \in (0,1)$<br>input + memory 产生 $M_t^{erased}(i) \leftarrow M_{t-1}(i)[1-w_t(i)e_t]$<br>和读操作一样，需要由 attention 机制得到 weight vector，表示每个 memory vector 被改动的幅度的大小，有多少个 memory 就有多少个 $w_t$<br>将上一时刻每个 memory vector 都乘上一个 0-1 之间的 vector，就是擦除操作，相当于 forget gate<br>如果 $w_t$ 和 $e_t$ 都为 1，memory 就会被重置为 0，如果两者有一个为 0，那么 memory 保持不变。多个 erase 操作可以以任意顺序叠加</li>
<li><strong>Add</strong><br>input + controller 产生 add vector $a_t \in R^M$ ，$a_t(d) \in (0,1)$<br>input + memory 产生 $M_t(i) \leftarrow M_t^{erased}(i) + w_t(i)a_t$<br>相当于 update gate<br>同样的，多个 add 操作的顺序并没有关系</li>
</ul>
<p>所有的 memory vector 共享 $e_t, \ a_t$，erase 和 add 操作中的 $w_t$ 也是共享的。再次注意 erase vector 和 add vector 都是由 controller 对 input 做编码得到的。<br>擦除和添加动作都有 M 个独立的 component，使得对每个 memory location 的修改可以在更细的粒度上进行。</p>
<h3 id="Addressing"><a href="#Addressing" class="headerlink" title="Addressing"></a>Addressing</h3><p>知道了怎么读写，现在来看看权重是如何产生的。主要通过两种方式来进行寻址，一是 <strong>content-based addressing</strong>，基于 controller 提供的状态向量 key vector 和当前 memory vector 的相似度来决定对内存地址的聚焦程度，另一个是 <strong>location-based addressing</strong>，通过地址来寻址，可能还会伴随权重的位移（rotational shift）。</p>
<p>整个 Addressing 过程的流程图：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/ntm_write.png" class="ful-image" alt="ntm_write.png"></p>
<h4 id="Content-based-addressing"><a href="#Content-based-addressing" class="headerlink" title="Content-based addressing"></a>Content-based addressing</h4><p>Controller 会产生一个长度为 M 的状态向量 key vector $k_t$，基于每一个 memory vector $M_t(i)$ 和 controller 状态向量  $k_t$ 的相似程度 K[·,·]，计算每个 memory vector 的 attention 权重。其中相似度可以用 cosine similarity 计算，$K(u,v)={u v \over ||u|| ||v||}$</p>
<p>使用 softmax 将相似度转化为概率分布：<br>$$w^c_t(i) \leftarrow {exp(\beta_tK[k_t, M_t(i)]) \over \sum_j exp(\beta_tK[k_t, M_t(j)])}$$</p>
<p>其中，$\beta_t$ 可以放大或减弱聚焦的程度。$\beta_t=1$ 时就是标准的 softmax，而 $\beta$ 的值越大，越会强化最大相似度分数的 memory vector 的优势，可以看做赢者通吃。要注意的是 $\beta$ 不是超参数，而是 controller 预测得到的。举个具体的例子，在对话领域，如果输入时“呵呵”这类没有太多信息量的句子，那么 controller 就会产生一个非常接近 0 的 $\beta$，表示没有明确倾向去访问某个特定的信息；反之，如果是包含很多信息的输入，产生的 $\beta$ 值会很大。</p>
<p>content addressing 完成的下面一个流程，$k_t$ 可以看做是 controller 对输入进行编码产生的状态向量，$\beta_t$ 是标量，一个 concentration 参数。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/ntm_content.png" class="ful-image" alt="ntm_content.png">
<h4 id="Location-based-addressing"><a href="#Location-based-addressing" class="headerlink" title="Location-based addressing"></a>Location-based addressing</h4><p>不是所有的问题都可以通过 content-based addressing 来解决的。在一些特定任务尤其是有 variable-binding 的任务中，变量的内容是任意的，但变量还是需要一个可识别的名字/地址来 refer。比如说算数任务，x, y 代表任意值，要计算 $f(x,y)=x * y$，这时候 controller 就会把 x,y 存到对应的地址上，然后通过地址而不是数值内容来获取它们并进行乘法操作。</p>
<p>Content-based addressing 比 location-based addressing 更为通用，因为 Content-based addressing 本身可能包含地址信息。然而 location-based addressing 对某些形式的通用化很有必要，所以论文同时引入了两种寻址机制。</p>
<h5 id="1-Interpolation-Gate"><a href="#1-Interpolation-Gate" class="headerlink" title="1. Interpolation Gate"></a>1. Interpolation Gate</h5><p>第一步要进行插值计算。<br>$$w_t^g \leftarrow g_tw^c_t + (1-g_t)w_{t-1}$$</p>
<p>基于内容的 weight vector $w^c_t$ 和上一个时间的 weight vector $w_{t-1}$ 的线性组合，线性组合的参数 $g_t$ 是一个 （0, 1）之间的标量，由 controller 产生，表示多大程度上使用当前时刻基于内容的寻址，多大程度上使用上一时刻产生的 $w_{t-1}$</p>
<p>如果 g=0，那么 content-weighting 整个就被忽略了，只用上一时刻的权重；如果 g=1，那么上一时刻的权重就被忽略了，只使用 content-based addressing。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/ntm_inter.png" class="ful-image" alt="ntm_inter.png"></p>
<h5 id="2-Shifting-and-Sharpening"><a href="#2-Shifting-and-Sharpening" class="headerlink" title="2. Shifting and Sharpening"></a>2. Shifting and Sharpening</h5><p>基于地址的寻址机制既可以用做简单的 memory bank 遍历，也可以用于随机访问，通过对 weighting 的旋转位移操作来实现。如果当前 weight 全力聚焦在一个单一地址上，那么一个为 1 的旋转可以把部分焦点位移到下一个地址，一个负的位移则相反。</p>
<p>具体是在 Interpolation 之后进行。controller 产生的 shift weighting $s_t$ 定义了所有允许的整数位移值上的归一化分布。如果 -1 到 1 间的位移是被允许的，那么 $s_t$ 就有三个对应位移值 -1，0，1 上的概率分布。最简单是用 softmax 来预测 shift weighting，不过这里用了另一种方法，controller 产生一个 single scalar 表示均匀分布的下界（the lower bound of a width one uniform distribution over shifts），也就是如果 shift scalar=6.7，那么 $s_t(6)=0.3$，$s_t(7)=0.7$，剩下的 $s_t(i)$ 都是 0。</p>
<p><strong>Shift attention:</strong><br>$$\hat w_t \leftarrow \sum^{R-1}_{j=0}w^g_t(j)s_t(i-j)$$<br>$s_t$ 其实相当于一个 convolution filter，每一个元素表示当前位置和对应位置的相关程度，像是定义了一个滑动窗里的权重，shift attention 将滑动窗里的 vector 做加权平均。<br>如果位移权重不是 sharp 的，也就是说权重分布相对均匀，那么这个卷积操作会使权重随时间变化更加发散。例如，如果给 -1，0，1 的对应的权重 0.1，0.8 和 0.1，旋转位移就会将聚焦在一个点上的权重轻微分散到三个点上。</p>
<p>controller 还会给出一个标量 $\gamma_t$ 用来 sharpen 最终的权重，作用和之前讲过的 $\beta_t$ 差不多，值越大权重大的越突出。<br><strong>Sharpening:</strong><br>$$w_t(i) \leftarrow {\hat w_t(i)\gamma_t \over \sum_j \hat w_t(i)\gamma_t}$$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/ntm_post.png" class="ful-image" alt="ntm_post.png"></p>
<p>结合<strong>权重插值（weighting interpolation）</strong>、<strong>内容寻址（content-based addressing）</strong>和<strong>地址寻址（location-based addressing）</strong>的寻址系统可以在三种补充模式下工作：</p>
<ol>
<li>权重可以完全由 content-based addressing 来自主选择而不受 location system 的影响</li>
<li>由 content-based addressing 产生的权重可以被选择然后进行位移。这使得 focus 能够跳跃到通过内容寻址产生的地址附近而不是具体一个点。在计算方面，这使得 head 可以访问一个连续的数据块，然后访问这个块中的特定数据</li>
<li>来自上一个时刻的权重可以在没有任何 content-based addressing 的输入的情况下被旋转，以便权重可以以相同的时间间隔连续地访问一个地址序列（allows the weighting to iterate through a sequence of addresses by advancing the same distance at each time-step）。</li>
</ol>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>input 被 controller 加工产生 key vector 和一些中间变量，基于 key vector 和 memory bank 里的记忆向量的相似程度，用 attention 机制将 memory 检索结果转化成 vector 返回给读操作；基于 controller 加工后的外界输入，把一些信息写到 memory 里面，实现更新 memory 的效果（擦除+更新，都需要 weight vector 来确定 memory vector 的权重，权重由 content-base+location-base 产生，表示输入与记忆的相似程度，记忆与记忆的相似程度）。</p>
<p>NTM 架构有三个 free parameters：</p>
<ul>
<li>size of memory</li>
<li>number of read and write heads</li>
<li>range of allowed location shifts</li>
</ul>
<p>但最重要的选择还是用作 controller 的网络模型。来探讨下 recurrent 和 feedforward network 的选择</p>
<ul>
<li>递归网络像 LSTM 拥有自己的 internal memory，可以对矩阵中更大的存储器起到补充作用。想象 controller 是 CPU，memory 是 RAM，那么 RN 里的 hidden activations 相当于处理器的寄存器（rigisters），允许 controller 跨时间操作时可以共享信息</li>
<li>前馈网络 FN 可以通过每一时刻读写同一个记忆地址来模拟 RN，在网络操作上有更大透明度，读写 memory matrix 的模式比 RNN 的中间状态更容易解释。但局限是并行 read/write heads 的数量有限，在 NTM 计算时会成为瓶颈。单个 read head 在每个时刻只能对单个 memory vector 进行一元变换，而两个 read heads 可以进行二元向量变换，以此类推。递归控制器能够存储上一时刻的读出的向量，不会受到这个限制。</li>
</ul>
<h1 id="Differentiable-Neural-Computer-DNC"><a href="#Differentiable-Neural-Computer-DNC" class="headerlink" title="Differentiable Neural Computer(DNC)"></a>Differentiable Neural Computer(DNC)</h1><p>NTM 的第二个版本。更加复杂一些吧。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch.png">
<p>和 NTM 一样，controller 由若干个神经网络组成，负责和输入、输出交互，产生一些中间变量（又叫 interface parameter）。根据这些中间变量，可以进行 memory 的读写操作，注意这里是 <strong>先写再读！</strong> 绿色的方块表示写操作，可以看到先是会释放某些区域的记忆，然后分配记忆，写入东西，完成后输出，表示 done，可以重新分配记忆了，然后交替，整个是动态分配的过程。</p>
<p>紫色的方块表示读操作，更新完 memory，会从更新过的 memory 里定位和读取信息。有多个互相独立的 read model，互相独立的来从 memory 里读取信息并拼到一起，可以从各个角度来衡量信息的有用程度。</p>
<p>记忆区的右边（d Memory usage and temporal links）有一个附加的链表，追踪上面的箭头能够“回想”最近输入和输出的过程。</p>
<p>和 Seq2seq 类比一下，图中 controller 指向自己的箭头其实相当于 RNN decoder里的 RNN state vector，黄线从 memory 到 controller 的路径其实相当于 attention 提取的 context vector。</p>
<h2 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a>Architecture</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch2.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch2.png">
<p>梳理一下，有三大部分：</p>
<ol>
<li><strong>Controller 产生 interface parameters 和 output parameters</strong><br>$x_t=[x_t;r^1_{t-1};…;r^R_{t-1}]$<br>$(\xi t, v_t)=NNC([x_1;…;x_t];\theta)$</li>
<li><strong>对 memory 的操作，先写再读</strong><br>Content-based writing and reading weight<br>History-based writing weight =&gt; final writing weights<br>History-based reading weight =&gt; final reading weights</li>
<li><strong>Memory 结果（类比对话系统里的上下文 context vector）和 controller 产生的 $v_t$（类比RNN decoder 的 state vector）拼到一起产生最后的输出</strong><br>$y_t=W_r[r^1_t,…r^R_t]+v_t$</li>
</ol>
<p>完整的连续输入的结构图如下，上一时刻读的信息也会作为输入放到 controller 里做预测，也就是说除了 input，controller 还会用到历史信息。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch3.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_arch3.png"></p>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p>下面说一下 memory 读写操作的细节。沿着下图走一遍，图中六角形部分都是控制器的输出也就是 interface parameter<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_details.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%9B%BE%E7%81%B5%E6%9C%BA%20NTM%20%E5%88%B0%E5%8F%AF%E5%BE%AE%E5%88%86%E7%A5%9E%E7%BB%8F%E8%AE%A1%E7%AE%97%E6%9C%BA%20DNC/DNC_details.png"></p>
<h3 id="1-Content-based-weighting-for-memory-write"><a href="#1-Content-based-weighting-for-memory-write" class="headerlink" title="1. Content-based weighting for memory write"></a>1. Content-based weighting for memory write</h3><p>这一部分 $c^w_t$ 和 NMT.v1 相同。D 是相似度函数，可以取 cosine similarity; $\beta$ 是 concentration parameter，表示 key strength，是大于 1 的 scalar</p>
<p>$$C(M_{t-1}, k^w_t, \beta^w_t)[i]={exp(D(k^w_t, M_{t-1}[i, ·])\beta_t^w \over \sum_j exp(D(k^w_t, M_{t-1}[j,·])\beta^w_t))}$$</p>
<h3 id="2-History-based-write-weighting-Dynamic-memory-allocation"><a href="#2-History-based-write-weighting-Dynamic-memory-allocation" class="headerlink" title="2. History-based write weighting(Dynamic memory allocation)"></a>2. History-based write weighting(Dynamic memory allocation)</h3><p>对应 NTM 的 remove 和 add 操作。每一个存储单元都是一个等长 vector，里面有若干个 element，只要 element 没有全部被占用，就可以写入新数据。如果被完全占用了，也可以通过一定方法将存储单元释放。</p>
<p>Controller 有三种选择：</p>
<ol>
<li>不写</li>
<li>写 &amp; 写到新分配的位置（未使用过的/最新释放的存储单元）</li>
<li>写 &amp; 写到内容相似且还没有被完全占用的存储单元<br>也就是更新这个存储单元的信息</li>
</ol>
<p>如果所有存储空间都用完了，那么控制器必须释放空间才可以进行写操作。每一个时刻的写操作完成后，位置信息会和 L 矩阵也就是 links of association 连接起来，记录信息存储的顺序。</p>
<ul>
<li><strong>memory retention vector:</strong> $\psi_t = \prod^R_{i=1}(1-f^i_t \omega^{r,i}_{t-1})$<br>$\omega^{w,i}_{t-1}$ 是上一时刻的 write weights，$f^i_t$ 是 controller 产生的 free gates，决定最近读取的位置要不要被释放。i 是 read head 的 index。$\psi_t \in [0,1]$ 也就是 memory retention vector，表示每个位置上有多少信息被保留下来</li>
<li><strong>usage vector:</strong> $u_t=(u_{t-1}+\omega^w_{t-1}-(u_{t-1}⊙\omega_{t-1}^w))⊙ \psi_t$<br>$u_t$ 是 t 时刻的 usage vector，衡量每一个 memory vector 最近被用到的程度，如果很久没用&amp;需要腾地方，就会优先把它给踢掉<br>$u_t$ 被 $\psi_t$ 限制，在 0-1 之间，⊙ 表示 element-wise 乘法</li>
<li><strong>least used location:</strong> $\phi_t=SortIndicesAscending(u_t)$<br>升序排序，于是 $\phi_t[1]$ 表示最少使用到的位置</li>
<li><strong>allocation weighting:</strong> $a_t[\phi_t[j]]=(1-u_t[\phi_t[j]])\prod^{j-1}_{i=1}u_t[\phi_t[j]]$<br>最近读取的 memory 在更新 memory 的时候有更大的权重<br>$a_t$ 是 allocation weighting，提供写操作的新的位置，如果所有的 u 都是 1，那么 $a_t=0$，必须先释放存储空间才能够分配空间</li>
</ul>
<h3 id="3-Final-write-weight"><a href="#3-Final-write-weight" class="headerlink" title="3. Final write weight"></a>3. Final write weight</h3><p>Usage attention 和 content attention 通过 gate 线性组合, gate 也是 interface parameter, 再做后续 filter。</p>
<ul>
<li>$\omega^w_t=g^w_t[g^a_ta_t+(1-g^a_t)c^w_t]$<br> $g^a_t$ 是线性组合的 allocation gate，$g^w_t$是 write gate，如果 write gate 是 0，那么啥都不写</li>
<li>$M_t=M_{t-1}⊙(E-\omega^w_te^T_t)+\omega^w_tv^T_t = M_{t-1}-M_{t-1}⊙\omega^w_te^T_t+\omega^w_tv^T_t$<br> Memory writes，改变 t-1 时刻的 memory bank</li>
</ul>
<h3 id="4-Content-based-weighting-for-memory-read"><a href="#4-Content-based-weighting-for-memory-read" class="headerlink" title="4. Content-based weighting for memory read"></a>4. Content-based weighting for memory read</h3><p>$$C(M_t, k^{r,i}_t,\beta^{r,i}_t)[k]={exp(D(k^{r,i}_t, M_t[k, ·])\beta^{r,i}_t) \over \sum_j exp(D(k^{r,i}_t,M_t[j, ·])\beta^{r,i}_t)}$$<br>总共读 R 次（$R\ge 1,$超参数），从更新过以后的 memory $M_t$ 里读信息</p>
<h3 id="5-History-based-reading-weights"><a href="#5-History-based-reading-weights" class="headerlink" title="5. History-based reading weights"></a>5. History-based reading weights</h3><p>通过有时间信息的历史记录（temporal links），和 content-based read 挑出来的 C，进一步挑选出不直接相关但历史上有间接联系的 memory vector。举个例子理解一下，t 时刻的 input 通过内容的相似程度定位到了第 5 个 memory vector，然后 L 矩阵通过写操作的历史记录，发现第 5 个 vector 和第 2，4，6 个 vector 非常相关，所以尽管 t 时刻的 input 和 vector 2，4，6 在这一时刻表面上不相关，但有了第 5 个 vector 作为中介找到了 2，4，6，就有了间接关系，这可以通过 f, b 挑选出来。</p>
<p>$p_0=0$<br>$p_t=(1-\sum^N_{i=1}\omega^w_t[i])p_{t-1}+\omega^w_t$<br>$L_0[i,j]=0 \ \ \forall i,j $<br>$L_t[i,i]=0 \ \ \forall i $<br>$L_t[i,j]=(1-\omega^w_t[i]-\omega^w_t[j])L_{t-1}[i,j]+\omega^w_t[i]p^w_{t-1}[j]$</p>
<p>$L_t[i,j]$: <em>the degree to which location i was the location written to after location j,the rows and columns of Lt represent the weights of the temporal links going into and out from particular memory slots, respectively</em></p>
<p>每次修改存储空间时，链接矩阵都会更新，删除旧位置的链接，添加最后写入的位置的新链接。 </p>
<p>从 L 中挑选：</p>
<ul>
<li>$b^i_t$ backward weighting<br>$b^i_t=L^T_t\omega^{r,i}_{t-1}$</li>
<li>$f^i_t$ forward weighting<br>$f^i_t=L_t\omega^{r,i}_{t-1}$</li>
</ul>
<p>这种 temporal links 特别适用于NLP的任务。比如一段文本以单词为单位输入喂给 DNC，每个单词输入下 memory 都会被更新，但根据 L 可以知道上一个单词都影响了哪些 memory，也就是能够去关注单词的时序关系，这对捕捉文本意义有重要作用。</p>
<h3 id="6-Final-read-weighting"><a href="#6-Final-read-weighting" class="headerlink" title="6. Final read weighting"></a>6. Final read weighting</h3><ul>
<li>$\omega^{r,i}_t=\pi^i_t[1]b^i_t+\pi^i_t[2]c^{r,i}_t+\pi^i_t[3]f^i_t$<br>$\pi^i_t$ 表示读的模式，如果 $\pi^i_t[2]$ 占主导，那么只用 content lookup，如果  $\pi^i_t[3]$ 占主导，那么就按 memory 写入的顺序来遍历，如果  $\pi^i_t[1]$ 占主导，按 memory 写入顺序反向遍历</li>
<li>$r^i_t=M^T_t\omega^{r,i}_t$<br>Read from memory，三个 weight vector 加权相加得到最后的 weight vector，然后从更新过的 memory vector 把 memory 读出来加权平均返回给控制器</li>
</ul>
<p>小结一下，对于读操作，控制器可以从多个位置读取记忆，可以基于内容读取，也可以基于 associative temporal links 读取（向前/向后以顺序或反序的方式回调依次读取写入的信息）。读取的信息可以用来生成问题答案或者在环境中采取的行为。</p>
<h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><p>controller 可以用简单的 RNN，复杂些的 LSTM，也可以用 RL。论文还举了一些 DNC 的应用例子，比如学会找最短路径、伦敦地铁的路径规划、家族树（回答 who is Freya’s maternal great uncle 这类问题）、移动块问题等。</p>
<h1 id="DNC-vs-NTM"><a href="#DNC-vs-NTM" class="headerlink" title="DNC vs. NTM"></a>DNC vs. NTM</h1><p>DNC 是 NTM 的第二版，它改进了 NTM 的寻址机制，去掉了 index shift，更好支持了对记忆的 allocate 和 de-allocate 的功能。具体来说表现在下面几个方面：</p>
<ol>
<li><strong>No index shift based addressing</strong><br>NTM 是沿着磁带（或者说记忆）左右移动，而 DNC 则尝试基于输入直接在记忆中搜索给定的 vector</li>
<li><strong>Can ‘allocate’ and ‘deallocate’ memory</strong><br>DNC 可以分配和释放记忆。NTM 不能保证多个存储单元之间互不重叠、互不干扰（=&gt; allocate a free space），也不能释放存储单元，这意味着不能重复使用存储单元，也就很难处理很长的序列（=&gt; free gates used for deallocation）<br>另外，这种机制也很容易能将记忆中的某个区域标记为禁止访问，避免在以后意外地删除它们，这有助于优化</li>
<li><strong>Remembers recent memory use</strong><br>NTM 中，序列信息只能通过连续位置的写操作来顺序保存，并回到 memory 起点将它们读出来。一旦写操作跳跃到一个很远的位置，那么跳跃前和跳跃后的存储顺序就丢失了，读操作是没办法获取的<br>而 DNC 有一个 temporal link matrix 记录了写操作的顺序，也就是说 DNC 有某种形式的 temporal memory，在某个瞬时记忆下 DNC 可以回想起上一步做的事，以及上一步的上一步，以此类推，也就是可以遍历由它们需要做的事组成地一个链表</li>
</ol>
<blockquote>
<p>参考链接<br><a href="http://people.idsia.ch/~rupesh/rnnsymposium2016/slides/graves.pdf" target="_blank" rel="external">DeepMind DNC Slides</a><br><a href="http://blog.talla.com/neural-turing-machines-perils-and-promise" target="_blank" rel="external">Neural Turing Machines: Perils and Promise</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Memory Networks </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP笔记 - 多轮对话之对话管理(Dialog Management)]]></title>
      <url>http://www.shuang0420.com/2018/01/03/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86(Dialog%20Management)/</url>
      <content type="html"><![CDATA[<p>开始涉猎多轮对话，这一篇想写一写对话管理（Dialog Management），感觉是个很庞大的工程，涉及的知识又多又杂，在这里只好挑重点做一个引导性的介绍，后续会逐个以单篇形式展开。–持续更新中–<br><a id="more"></a></p>
<p>放一张多轮语音对话流程图，理解下 DM 在整个对话流程中处于什么地位。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92.png" class="ful-image" alt="%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92.png"></p>
<p>简单描述一下这个流程图常见的一种信息流动方式，首先是语音识别 ASR，产生语音识别结果也就是用户话语 $u_u$ ；语义解析模块 NLU 将 $u_u$ 映射成用户对话行为 $a_u$；对话管理模块 DM 选择需要执行的系统行为$a_m$；如果这个系统行为需要和用户交互，那么语言生成模块 NLG 会被触发，生成自然语言或者说是系统话语 $u_m$；最后，生成的语言由语音合成模块 TTS 朗读给用户听。 </p>
<p>这一篇第一部分介绍下对话管理及重要的几个小知识点，第二部分介绍对话管理的一些方法，主要有三大类：</p>
<ul>
<li><strong>Structure-based Approaches</strong><ul>
<li>Key phrase reactive</li>
<li>Tree and FSM</li>
<li>…</li>
</ul>
</li>
<li><strong>Principle-based Approaches</strong><ul>
<li>Frame</li>
<li>Information-State</li>
<li>Plan</li>
<li>…</li>
</ul>
</li>
<li><strong>Statistical Approaches</strong><ul>
<li>这一类其实和上面两类有交叉…不过重点想提的是：</li>
<li><strong>Reinforcement Learning</strong></li>
</ul>
</li>
</ul>
<p>方法不等于模型，这里只介绍一些重要概念，不会涉及模型细节。最后一部分会介绍一下 DM 设计工程上的一些 tips。</p>
<h1 id="Dialog-Management"><a href="#Dialog-Management" class="headerlink" title="Dialog Management"></a>Dialog Management</h1><p><strong>对话管理（Dialog Management, DM）</strong>控制着人机对话的过程，DM 根据对话历史信息，决定此刻对用户的反应。最常见的应用还是任务驱动的多轮对话，用户带着明确的目的如订餐、订票等，用户需求比较复杂，有很多限制条件，可能需要分多轮进行陈述，一方面，用户在对话过程中可以不断修改或完善自己的需求，另一方面，当用户的陈述的需求不够具体或明确的时候，机器也可以通过询问、澄清或确认来帮助用户找到满意的结果。</p>
<p>总的来说，对话管理的任务大致有下面一些：</p>
<ul>
<li><strong>对话状态维护（dialog state tracking, DST）</strong><br>维护 &amp; 更新对话状态<br>t+1 时刻的对话状态 $s_{t+1}$，依赖于之前时刻 t 的状态 $s_t$，和之前时刻 t 的系统行为 $a_t$，以及当前时刻 t+1 对应的用户行为 $o_{t+1}$。可以写成 $s_{t+1} \leftarrow s_t+a_t+o_{t+1}$</li>
<li><strong>生成系统决策（dialog policy）</strong><br>根据 DST 中的对话状态（DS），产生系统行为（dialog act），决定下一步做什么<br>dialog act 可以表示观测到的用户输入（用户输入 -&gt; DA，就是 NLU 的过程），以及系统的反馈行为（DA -&gt; 系统反馈，就是 NLG 的过程）<br>DA 的具体介绍将在 NLU 系列中展开</li>
<li><strong>作为接口与后端/任务模型进行交互</strong></li>
<li><strong>提供语义表达的期望值（expectations for interpretation）</strong><br>interpretation: 用户输入的 internal representation，包括 speech recognition 和 parsing/semantic representation 的结果</li>
</ul>
<p>本质上，任务驱动的对话管理实际就是一个决策过程，系统在对话过程中不断根据当前状态决定下一步应该采取的最优动作（如：提供结果，询问特定限制条件，澄清或确认需求…）从而最有效的辅助用户完成信息或服务获取的任务。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/dm1.png" class="ful-image" alt="dm1.png">
<p>如图，DM 的<strong>输入</strong>就是用户输入的语义表达（或者说是用户行为，是 NLU 的输出）和当前对话状态，<strong>输出</strong>就是下一步的系统行为和更新的对话状态。这是一个循环往复不断流转直至完成任务的过程，其中，<strong>语义输入就是流转的动力，DM 的限制条件（即通过每个节点需要补充的信息/付出的代价）就是阻力</strong>，输入携带的语义信息越多，动力就越强；完成任务需要的信息越多，阻力就越强。</p>
<p>一个例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/DM_eg.png" class="ful-image" alt="DM_eg.png"></p>
<p>实际上，DM 可能有更广泛的职责，比如融合更多的信息（业务+上下文），进行第三方服务的请求和结果处理等等。</p>
<h2 id="Initiative"><a href="#Initiative" class="headerlink" title="Initiative"></a>Initiative</h2><p>对话引擎根据对话按<strong>对话由谁主导</strong>可以分为三种类型：</p>
<ul>
<li><strong>系统主导</strong><br>系统询问用户信息，用户回答，最终达到目标</li>
<li><strong>用户主导</strong><br>用户主动提出问题或者诉求，系统回答问题或者满足用户的诉求</li>
<li><strong>混合</strong><br>用户和系统在不同时刻交替主导对话过程，最终达到目标<br>有两种类型，一是用户/系统转移任何时候都可以主导权，这种比较困难，二是根据 prompt type 来实现主导权的移交<br>Prompts 又分为 open prompt（如 ‘How may I help you‘ 这种，用户可以回复任何内容 ）和 directive prompt（如 ‘Say yes to accept call, or no’ 这种，系统限制了用户的回复选择）。</li>
</ul>
<h2 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h2><p><strong>Ground and Repair</strong><br>对话是对话双方共同的行为，双方必须不断地建立<strong>共同基础（common ground, Stalnaker, 1978）</strong>，也就是双方都认可的事物的集合。共同基础可以通过听话人<strong>依靠（ground）</strong>或者<strong>确认（acknowledge）</strong>说话人的话段来实现。<strong>确认行为（acknowledgement）</strong>由弱到强的 5 种方法（Clark and Schaefer 1989）有：<strong>持续关注（continued attention），相关邻接贡献（relevant next contribution），确认（acknowledgement），表明（demonstration），展示（display）</strong>。</p>
<p>听话人可能会提供<strong>正向反馈（如确认等行为）</strong>，也可能提供<strong>负向反馈（如拒绝理解/要求重复/要求 rephrase等）</strong>，甚至是<strong>要求反馈（request feedback）</strong>。如果听话人也可以对说话人的语段存在疑惑，会发出一个<strong>修复请求（request for repair）</strong>，如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">A: Why is that?</div><div class="line">B: Huh?</div><div class="line">A: Why is that?</div></pre></td></tr></table></figure>
<p>还有的概念如 <strong>speech acts，discourse</strong> 这类，之前陆陆续续都介绍过一些了。</p>
<h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>人的复杂性（complex）、随机性（random）和非理性化（illogical）的特点导致对话管理在应用场景下面临着各种各样的<strong>问题</strong>，包括但不仅限于：</p>
<ul>
<li><strong>模型描述能力与模型复杂度的权衡</strong></li>
<li><strong>用户对话偏离业务设计的路径</strong><br>如系统问用户导航目的地的时候，用户反问了一句某地天气情况</li>
<li><strong>多轮对话的容错性</strong><br>如 3 轮对话的场景，用户已经完成 2 轮，第 3 轮由于ASR或者NLU错误，导致前功尽弃，这样用户体验就非常差</li>
<li><strong>多场景的切换和恢复</strong><br>绝大多数业务并不是单一场景，场景的切换与恢复即能作为亮点，也能作为容错手段之一</li>
<li><strong>降低交互变更难度，适应业务迅速变化</strong></li>
<li><strong>跨场景信息继承</strong></li>
</ul>
<h1 id="Structure-based-Approaches"><a href="#Structure-based-Approaches" class="headerlink" title="Structure-based Approaches"></a>Structure-based Approaches</h1><h2 id="Key-Pharse-Reactive-Approaches"><a href="#Key-Pharse-Reactive-Approaches" class="headerlink" title="Key Pharse Reactive Approaches"></a>Key Pharse Reactive Approaches</h2><p>本质上就是关键词匹配，通常是通过捕捉用户最后一句话的<strong>关键词/关键短语</strong>来进行回应，比较知名的两个应用是 <strong>ELIZA</strong> 和 <strong>AIML</strong>。AIML （人工智能标记语言），XML 格式，支持 ELIZA 的规则，并且更加灵活，能支持一定的上下文实现简单的多轮对话（利用 that），支持变量，支持按 topic 组织规则等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;category&gt;</div><div class="line">&lt;pattern&gt;DO YOU KNOW WHO * IS&lt;/pattern&gt; </div><div class="line">&lt;template&gt;&lt;srai&gt;WHO IS &lt;star/&gt;&lt;/srai&gt;&lt;/template&gt; </div><div class="line">&lt;/category&gt;</div><div class="line"></div><div class="line">&lt;category&gt;</div><div class="line">&lt;pattern&gt;MOTHER&lt;/pattern&gt;</div><div class="line">&lt;template&gt; Tell me more about your family. &lt;/template&gt; </div><div class="line">&lt;/category&gt;</div><div class="line"></div><div class="line">&lt;category&gt;</div><div class="line">&lt;pattern&gt;YES&lt;/pattern&gt;</div><div class="line">&lt;that&gt;DO YOU LIKE MOVIES&lt;/that&gt; </div><div class="line">&lt;template&gt;What is your favorite movie?&lt;/template&gt; </div><div class="line">&lt;/category&gt;</div></pre></td></tr></table></figure>
<p>附上自己改写的 <a href="https://github.com/Shuang0420/aiml" target="_blank" rel="external">aiml</a> 地址，在原有基础上增添了一些功能：</p>
<ul>
<li>支持 python3</li>
<li>支持中文</li>
<li>支持 * 扩展</li>
</ul>
<h2 id="Trees-and-FSM-based-Approaches"><a href="#Trees-and-FSM-based-Approaches" class="headerlink" title="Trees and FSM-based Approaches"></a>Trees and FSM-based Approaches</h2><p>Trees and FSM-based approach 通常把对话建模为通过树或者有限状态机（图结构）的路径。 相比于 simple reactive approach，这种方法融合了更多的上下文，能用一组有限的信息交换模板来完成对话的建模。这种方法<strong>适用于</strong>：</p>
<ul>
<li>系统主导</li>
<li>需要从用户收集特定信息</li>
<li>用户对每个问题的回答在有限集合中</li>
</ul>
<p>这里主要讲 FSM，把对话看做是在有限状态内跳转的过程，每个状态都有对应的动作和回复，如果能从开始节点顺利的流转到终止节点，任务就完成了。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/fsa.png" class="ful-image" alt="fsa.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/credit_card_eg.png" class="ful-image" alt="credit_card_eg.png">
<p>FSM 的<strong>状态</strong>对应系统问用户的问题，<strong>弧线</strong>对应将采取的行为，依赖于用户回答。</p>
<p>FSM-based DM 的<strong>特点</strong>是：</p>
<ul>
<li>人为定义对话流程</li>
<li>完全由系统主导，系统问，用户答</li>
<li>答非所问的情况直接忽略</li>
<li>建模简单，能清晰明了的把交互匹配到模型</li>
<li>难以扩展，很容易变得复杂</li>
<li>适用于简单任务，对简单信息获取很友好，难以处理复杂的问题</li>
<li>缺少灵活性，表达能力有限，输入受限，对话结构/流转路径受限</li>
</ul>
<p>对特定领域要设计 task-specific FSM，简单的任务 FSM 可以比较轻松的搞定，但稍复杂的问题就困难了，毕竟要考虑对话中的各种可能组合，编写和维护都要细节导向，非常耗时。一旦要扩展 FSM，哪怕只是去 handle 一个新的 observation，都要考虑很多问题。实际中，通常会加入其它机制（如变量等）来扩展 FSM 的表达能力。</p>
<h1 id="Principle-based-Approaches"><a href="#Principle-based-Approaches" class="headerlink" title="Principle-based Approaches"></a>Principle-based Approaches</h1><h2 id="Frame-based-Approaches"><a href="#Frame-based-Approaches" class="headerlink" title="Frame-based Approaches"></a>Frame-based Approaches</h2><p>Frame-based approach 通过允许多条路径更灵活的获得信息的方法扩展了基于 FSM 的方法，它将对话建模成一个填槽的过程，<strong>槽</strong>就是多轮对话过程中将初步用户意图转化为明确用户指令所需要补全的信息。一个槽与任务处理中所需要获取的一种信息相对应。槽直接没有顺序，缺什么槽就向用户询问对应的信息。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/slot_filling.png" class="ful-image" alt="slot_filling.png">
<p>Frame-based DM 包含下面一些<strong>要素：</strong></p>
<ul>
<li><strong>Frame：</strong> 是槽位的集合，定义了需要由用户提供什么信息</li>
<li><strong>对话状态：</strong>记录了哪些槽位已经被填充</li>
<li><strong>行为选择：</strong>下一步该做什么，填充什么槽位，还是进行何种操作<br>行为选择可以按槽位填充/槽位加权填充，或者是利用本体选择</li>
</ul>
<p>基于框架/模板的系统本质上是一个生成系统，不同类型的输入激发不同的生成规则，每个生成能够灵活的填入相应的模板。常常用于用户可能采取的行为相对有限、只希望用户在这些行为中进行少许转换的场合。</p>
<p><strong>Frame-based DM 特点：</strong></p>
<ul>
<li>用户回答可以包含任何一个片段/全部的槽信息</li>
<li>系统来决定下一个行为</li>
<li>支持混合主导型系统</li>
<li>相对灵活的输入，支持多种输入/多种顺序</li>
<li>适用于相对复杂的信息获取</li>
<li>难以应对更复杂的情境</li>
<li>缺少层次</li>
</ul>
<p>槽的更多信息可以参考<a href="http://www.pmcaff.com/article/index/971158746030208?from=related&amp;pmc_param%5Bentry_id%5D=950709304427648" target="_blank" rel="external">填槽与多轮对话 | AI产品经理需要了解的AI技术概念</a></p>
<h2 id="Agenda-Frame-CMU-Communicator"><a href="#Agenda-Frame-CMU-Communicator" class="headerlink" title="Agenda + Frame(CMU Communicator)"></a>Agenda + Frame(CMU Communicator)</h2><p><strong>Agenda + Frame(CMU Communicator)</strong> 对 frame model 进行了改进，有了层次结构，能应对更复杂的信息获取，支持话题切换、回退、退出。主要<strong>要素</strong>如下：</p>
<ul>
<li><strong>product</strong><br>树的结构，能够反映为完成这个任务需要的所有信息的顺序<br>相比于普通的 Tree and FSM approach，这里产品树（product tree）的创新在于它是动态的，可以在 session 中对树进行一系列操作比如加一个子树或者挪动子树</li>
<li><strong>process</strong><ul>
<li><strong>agenda</strong><br>相当于任务的计划（plan）<br>类似栈的结构（generalization of stack）<br>是话题的有序列表（ordered list of topics）<br>是 handler 的有序列表（list of handlers），handler 有优先级</li>
<li><strong>handler</strong><br>产品树上的每个节点对应一个 handler，一个 handler 封装了一个 information item</li>
</ul>
</li>
</ul>
<p>从 product tree 从左到右、深度优先遍历生成 agenda 的顺序。当用户输入时，系统按照 agenda 中的顺序调用每个 handler，每个 handler 尝试解释并回应用户输入。handler 捕获到信息就把信息标记为 consumed，这保证了一个 information item 只能被一个 handler 消费。</p>
<p>input pass 完成后，如果用户输入不会直接导致特定的 handler 生成问题，那么系统将会进入 output pass，每个 handler 都有机会产生自己的 prompt（例如，departure date handler 可以要求用户出发日期）。</p>
<p>可以从 handler 返回代码中确定下一步，选择继续 current pass，还是退出 input pass 切换到 output pass，还是退出 current pass 并等待来自用户输入等。handler 也可以通过返回码声明自己为当前焦点（focus），这样这个 handler 就被提升到 agenda 的顶端。为了保留特定主题的上下文，这里使用 sub-tree promotion 的方法，handler 首先被提升到兄弟节点中最左边的节点，父节点同样以此方式提升。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/rotate_subtree.png" class="ful-image" alt="rotate_subtree.png">
<p>系统还能处理产品树中节点之间的依赖关系。典型的依赖关系在父节点和子节点之间。通常父节点的值取决于其子节点。每个节点都维护一个依赖节点的列表，并且会通知依赖节点值的变化，然后依赖节点可以声明自己是无效的并成为当前对话的候选主题。</p>
<p>给一个例子，能够回应用户的显式/隐式话题转移（A1-A3, U11），也能够动态添加子树到现有的 agenda（A8-A10）。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/cmu_ex.png" class="ful-image" alt="cmu_ex.png"></p>
<p>具体还是看论文吧<br><a href="http://www.cs.cmu.edu/~xw/asru99-agenda.pdf" target="_blank" rel="external">AN AGENDA-BASED DIALOG MANAGEMENT ARCHITECTURE FOR SPOKEN LANGUAGE SYSTEMS</a></p>
<h2 id="Information-State-Approaches"><a href="#Information-State-Approaches" class="headerlink" title="Information-State Approaches"></a>Information-State Approaches</h2><p>Information State Theories 提出的背景是：</p>
<ul>
<li><strong>很难去评估各种 DM 系统</strong></li>
<li><strong>理论和实践模型存在很大的 gap</strong><br>理论型模型有：logic-based, BDI, plan-based, attention/intention<br>实践中模型大多数是 finite-state 或者 frame-based<br>即使从理论模型出发，也有很多种实现方法</li>
</ul>
<p>因此 Information State Models 作为对话建模的形式化理论，为工程化实现提供了理论指导，也为改进当前对话系统提供了大的方向。Information-state theory 的<strong>关键</strong>是识别对话中流转信息的 relevant aspects，以及这些成分是怎么被更新的，更新过程又是怎么被控制的。idea 其实比较简单，不过执行很复杂罢了。理论架构如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/IS_arch.png" class="ful-image" alt="IS_arch.png"></p>
<p>介绍下简单的一些<strong>要素</strong>：<br><strong>Statics</strong></p>
<ul>
<li><strong>Informational components</strong><br>包括上下文、内部驱动因子（internal motivating factors）<br>e.g., QUD, common ground, beliefs, intentions, dialogue history, user models, etc.</li>
<li><strong>Formal representations</strong><br>informational components 的表示<br>e.g., lists, records, DRSs,…</li>
</ul>
<p><strong>Dynamics</strong></p>
<ul>
<li><strong>dialog moves</strong><br>会触发更新 information state 的行为的集合<br>e.g., speech acts</li>
<li><strong>update rules</strong><br>更新 information state 的规则集合<br>e.g., selection rules</li>
<li><strong>update strategy</strong><br>更新规则的选择策略，选择在给定时刻选用哪一条 update rules</li>
</ul>
<p>意义在于可以遵循这一套理论体系来构建/分析/评价/改进对话系统。基于 information-state 的系统有：</p>
<ul>
<li><strong>TrindiKit Systems</strong><br>–  GoDiS (Larsson et al) – information state: Questions Under Discussion<br>–  MIDAS – DRS information state, first-order reasoning (Bos &amp;Gabsdil, 2000)<br>–  EDIS – PTT Information State, (Matheson et al 2000)<br>–  SRI Autoroute –Conversational Game Theory (Lewin 2000)</li>
<li><strong>Successor Toolkits</strong><br>–  Dipper (Edinburgh)<br>–  Midiki (MITRE)</li>
<li><strong>Other IS approaches</strong><br>–  Soar (USC virtual humans)<br>–  AT&amp;T MATCH system</li>
</ul>
<h2 id="Plan-based-Approaches"><a href="#Plan-based-Approaches" class="headerlink" title="Plan-based Approaches"></a>Plan-based Approaches</h2><p>一般指大名鼎鼎的 <strong>BDI (Belief, Desire, Intention)</strong> 模型。起源于三篇经典论文：</p>
<ul>
<li>Cohen and Perrault 1979</li>
<li>Perrault and Allen 1980</li>
<li>Allen and Perrault 1980</li>
</ul>
<p>基本假设是，一个试图发现信息的行为人，能够利用标准的 plan 找到让听话人告诉说话人该信息的 plan。这就是 Cohen and Perrault 1979 提到的 <strong>AI Plan model</strong>，Perrault and Allen 1980 和 Allen and Perrault 1980 将 BDI 应用于理解，特别是间接言语语效的理解，本质上是对 Searle 1975 的 speech acts 给出了可计算的形式体系。</p>
<p>官方描述（Allen and Perrault 1980）：</p>
<blockquote>
<p>A has a goal to acquire certain information. This causes him to create a plan that involves asking B a question. B will hopefully possess the sought information. A then executes the plan, and thereby asks B the question. B will now receive the question and attempt to infer A’s plan. In the plan there might be goals that A cannot achieve without assistance. B can accept some of these obstacles as his own goals and create a plan to achieve them. B will then execute his plan and thereby respond to A’s question.</p>
</blockquote>
<p>重要的概念都提到了，<strong>goals, actions, plan construction, plan inference</strong>。理解上有点绕，简单来说就是 agent 会捕捉对 internal state (beliefs) 有益的信息，然后这个 state 与 agent 当前目标（goals/desires）相结合，再然后计划（plan/intention）就会被选择并执行。对于 communicative agents 而言，plan 的行为就是单个的 speech acts。speech acts 可以是复合（composite）或原子（atomic）的，从而允许 agent 按照计划步骤传达复杂或简单的 conceptual utterance。</p>
<p>这里简单提一下重要的概念。</p>
<ul>
<li><strong>信念（Belief）</strong><br>基于谓词 KNOW，如果 A 相信 P 为真，那么用 B(A, P) 来表示</li>
<li><strong>期望（Desire）</strong><br>基于谓词 WANT，如果 S 希望 P 为真（S 想要实现 P），那么用 WANT(S, P) 来表示，P 可以是一些行为的状态或者实现，W(S, ACT(H)) 表示 S 想让 H 来做 ACT</li>
</ul>
<p>Belief 和 WANT 的逻辑都是基于公理。最简单的是基于 action schema。每个 action 都有下面的参数集：</p>
<ul>
<li><strong>前提（precondition）</strong><br>为成功实施该行为必须为真的条件</li>
<li><strong>效果（effect）</strong><br>成功实施该行为后变为真的条件</li>
<li><strong>体（body）</strong><br>为实施该行为必须达到的部分有序的目标集（partially ordered goal states）</li>
</ul>
<p><strong>计划推理（Plan Recognition/Inference, PI）：</strong><br>根据 B 实施的行为，A 试图去推理 B 的计划的过程。</p>
<ul>
<li>PI.AE Action-Effect Rule（行为-效果规则）</li>
<li>PI.PA Precondition-Action Rule（前提-行为规则）</li>
<li>PI.BA Body-Action Rule（体-行为规则）</li>
<li>PI.KB Know-Desire Rule（知道-期望规则）</li>
<li>E1.1 Extended Inference Rule（扩展推理规则）</li>
</ul>
<p><strong>计划构建（Plan construction）：</strong></p>
<ul>
<li>找到从当前状态（current state）达到目标状态（goal state）需要的行为序列（sequence of actions）</li>
<li>Backward chaining，大抵是说，试图找到一个行为，如果这个行为实施了能够实现这个目标，且它的前提在初始状态已经得到满足，那么计划就完成了，但如果未得到满足，那么会把前提当做新的目标，试图满足前提，直到所有前提都得到满足。（find action with goal as effect then use preconditions of action as new goal, until no unsatisfied preconditions）<br>backward chaining 在 <a href="http://www.shuang0420.com/2017/04/07/NLP%20笔记%20-%20Meaning%20Representation%20Languages/">NLP 笔记 - Meaning Representation Languages</a> 中提到过。</li>
</ul>
<p>还有个重要的概念是 speech acts，在 <a href="http://www.shuang0420.com/2017/09/20/NLP%20笔记%20-%20Discourse%20Analysis/">NLP 笔记 - Discourse Analysis</a> 中提到过，之后会细讲。</p>
<p>更多见 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.8451&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Plan-based models of dialogue</a></p>
<p>值得一提的是，<strong>基于 logic 和基于 plan</strong> 的方法虽然有更强大更完备的功能，但实际场景中并不常用，大概是因为大部分的系统都是相对简单的单个领域，任务小且具体，并不需要复杂的推理。</p>
<h1 id="Statistical-Approaches"><a href="#Statistical-Approaches" class="headerlink" title="Statistical Approaches"></a>Statistical Approaches</h1><h2 id="RL-Based-Approaches"><a href="#RL-Based-Approaches" class="headerlink" title="RL-Based Approaches"></a>RL-Based Approaches</h2><p>前面提到的很多方法还是需要人工来定规则的（hand-crafted approaches），然而人很难预测所有可能的场景，这种方法也并不能重用，换个任务就需要从头再来。而一般的基于统计的方法又需要大量的数据。再者，对话系统的评估也需要花费很大的代价。这种情况下，强化学习的优势就凸显出来了。RL-Based DM 能够对系统理解用户输入的不确定性进行建模，让算法来自己学习最好的行为序列。首先利用 simulated user 模拟真实用户产生各种各样的行为（捕捉了真实用户行为的丰富性），然后由系统和 simulated user 进行交互，根据 reward function 奖励好的行为，惩罚坏的行为，优化行为序列。由于 simulated user 只用在少量的人机互动语料中训练，并没有大量数据的需求，不过 user simulation 也是个很难的任务就是了。</p>
<p>对话仿真的整体框架如下图：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E4%B9%8B%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86%28Dialog%20Management%29/rl_arch.png" class="ful-image" alt="rl_arch.png"></p>
<h1 id="DM-设计"><a href="#DM-设计" class="headerlink" title="DM 设计"></a>DM 设计</h1><p>一般在冷启动阶段，会先用规则方法打造一个 DM，快速上线并满足业务需求，收集数据之后再转换成模型。</p>
<p>DM 的<strong>设计理念：</strong></p>
<ul>
<li><strong>完整性</strong><br>具备建模各种类型对话的能力（不仅仅是slot filling）</li>
<li><strong>独立性</strong><br>当设计（变更）一个场景时，不需要考虑当前场景跳转到其他场景的情况</li>
<li><strong>模块化</strong><br>一些常用的业务组件(如：确认，slot filling，翻页等)，能呈模块化复用(同时允许业务自定义内部的多种属性)</li>
</ul>
<p>DM 里可以有很多小的 dialogs，这些 dialogs 的特点是：</p>
<ul>
<li>可以重用（reusable modules）</li>
<li>完成一个简单操作（Perform a single operation）</li>
<li>可以被其他 dialog 调用（callable from other dialogs）</li>
<li>可以是全局的（”global”）</li>
</ul>
<p>Global dialog 的特点是：</p>
<ul>
<li>在 recognizer 能够匹配时 trigger </li>
<li>可以提供一些 conversation support，像是 help/cancel 这些全局功能</li>
<li>应对各种话题切换（Tangents）</li>
</ul>
<p>通常只有一个 root dialog，在满足下面两个条件的情况下被唤醒</p>
<ul>
<li>dialog stack 里没有其他的 dialog 剩余了</li>
<li>当前时刻 recognizer 并不能 trigger 其他的 dialog </li>
</ul>
<p>dialog stack 会存放目前已经被激活但还没完成的 dialog。dialog 一旦完成就会被 pop 出去。</p>
<blockquote>
<p><strong>参考链接：</strong><br><a href="https://www.slideshare.net/YunChaoHe1/ss-73195982" target="_blank" rel="external">多轮对话 multi-turn dialog for task-oriented system</a><br><a href="https://www.youtube.com/watch?v=FiytlAalO_g" target="_blank" rel="external">Dialog Management in Bot Framework</a><br><a href="http://www.cs.cmu.edu/~xw/asru99-agenda.pdf" target="_blank" rel="external">AN AGENDA-BASED DIALOG MANAGEMENT ARCHITECTURE FOR SPOKEN LANGUAGE SYSTEMS</a><br><a href="http://people.ict.usc.edu/~traum/Papers/traumlarsson.pdf" target="_blank" rel="external">The Information State Approach to Dialogue Management</a><br><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.8451&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Plan-based models of dialogue</a><br><a href="https://yq.aliyun.com/articles/276269" target="_blank" rel="external">对话管理的一些思考</a><br><a href="http://www.pmcaff.com/article/index/971158746030208?from=related&amp;pmc_param%5Bentry_id%5D=950709304427648" target="_blank" rel="external">填槽与多轮对话 | AI产品经理需要了解的AI技术概念</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> NLU </tag>
            
            <tag> Dialog Management </tag>
            
            <tag> DM </tag>
            
            <tag> slot-filling </tag>
            
            <tag> FSM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Learning to Remember Translation History with a Continuous Cache]]></title>
      <url>http://www.shuang0420.com/2017/12/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/</url>
      <content type="html"><![CDATA[<p>介绍一种 cache-like memory network。<br><a id="more"></a></p>
<p><strong>涉及论文：</strong></p>
<ul>
<li>Learning to Remember Translation History with a Continuous Cache</li>
</ul>
<p><strong>相关博客：</strong></p>
<ul>
<li><a href="http://www.shuang0420.com/2017/07/10/-NLP%20笔记%20-%20Machine%20Translation-Neuron%20models/">NLP 笔记 - Neural Machine Translation</a></li>
<li><a href="http://www.shuang0420.com/2017/12/04/论文笔记%20-%20Memory%20Networks/">论文笔记 - Memory Networks</a></li>
</ul>
<p>比较传统的 NMT 把文档当做一系列独立的句子来进行翻译，忽略了句子之间的关系，或者说是忽略了篇章信息，这样会带来两个问题：</p>
<ol>
<li><strong>一致性问题（inconsistency）</strong><br>如时态一致性问题，以及术语选择的一致性问题，这些通常都需要联系上下文/篇章信息</li>
<li><strong>歧义问题（ambiguity）</strong><br>NMT 基本的翻译单位是词向量，也是通过 word-by-word 的方式产生翻译的。向量表示的泛化问题会导致歧义进一步放大，如机遇/挑战，教师/培训这些词在空间里比较靠近，很容易导致翻译错误</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/nmt_drawback.png" class="ful-image" alt="nmt_drawback.png">
<p>在 SMT 中，引入 cross-sentence 的作用对解决上面两个问题非常有效，因此 NMT 方面也有人做了一些尝试，比如说用分层 RNN 来总结前 K 个 source sentences 的语境信息（Wang et al. 2017a），或者用额外的 encoder 和 attention model 来动态选择聚焦前一个 source sentence 的某个部分 （Jean et al. 2017），这些方法有一定效果，但是都只考虑了单语的信息，没有用到目标端的信息，并且仍然是从 discrete lexicon 中产生 context，词级别的错误会继续传递下去，这在口语字幕的语料上表现的更为明显（不能得到多大改善）。另外，这两个模型计算量也很大，不利于 scale。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/performance.png" class="ful-image" alt="performance.png">
<p>这篇论文用到了 cache-like memory networks 的思想，用一个额外的 cache model  把源端表示作为 KEY，目标端表示作为 VALUE，从 memory 里定位相关的信息，然后把相关信息也作为输入，翻译时能得到更多方面比如说时态的信息。这样的好处 <strong>一是可以规模化，通过 cache 获得更长的历史信息，二是因为用的是 internal representation （并且是 attention 后的片段信息）而不是单词，能缓解错误传播的问题，也考虑进了目标端的信息。</strong></p>
<p>与 Standard NMT 的对比如下：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/architecture.png" class="ful-image" alt="architecture.png">
<p>主要还是读取/写入 cache 的过程。</p>
<h1 id="Reading-from-Cache"><a href="#Reading-from-Cache" class="headerlink" title="Reading from Cache"></a>Reading from Cache</h1><h2 id="Key-Matching"><a href="#Key-Matching" class="headerlink" title="Key Matching"></a>Key Matching</h2><p>cache lookup 最简单用点积来做，也可以加中间转换矩阵或者用 attention 方法，不过点积最简单高效，不用学新的参数也能学到相似度</p>
<p>$$P_m(c_i|c_t)={exp(c^T_tc_i)\over \sum^I_{i’=1}exp(c^T_tc_i’)}$$</p>
<p>$c_t$ 是 t step 的 attention context，$c_i$ 是 cache 里第 i 个位置的 representation，I 是 cache slots 的总量</p>
<h2 id="Value-Reading"><a href="#Value-Reading" class="headerlink" title="Value Reading"></a>Value Reading</h2><p>得到概率分布后对每个 value 进行加权</p>
<p>$$m_t=\sum_{(c_i, s_i) \in cache} P_m(c_i|c_t)s_i$$</p>
<p>$P_m(c_i|c_t)$ 可以解释为给定 source side context $c_t$，从 cache 里检索得到相似的 target-side info $m_t$，答案是和过去产生的相似的 target words 相关的语境</p>
<h2 id="Representation-Combining"><a href="#Representation-Combining" class="headerlink" title="Representation Combining"></a>Representation Combining</h2><p>用原始的 decoder state $s_t$ 和当前的 output vector $m_t$ 进行线性组合，相当于 GRU 里的 update gate</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/repres_combing.png" class="ful-image" alt="repres_combing.png">
<p>其中 lambda 是一个动态调节的 weight vector，在每个 decoding step 都要重新计算</p>
<p>$$\lambda_t = \sigma (Us_t + Vc_t+Wm_t)$$</p>
<p>U(dxd), V(dxl), W(dxd) 是参数矩阵</p>
<h1 id="Writing-to-Cache"><a href="#Writing-to-Cache" class="headerlink" title="Writing to Cache"></a>Writing to Cache</h1><p>在整个句子翻译完后，再写入 cache，写入 cache 的内容包括</p>
<ul>
<li><strong>generated translation sentence</strong><br>  {$y_1,…,y_t…,y_T$}</li>
<li><strong>attention vector sequence</strong><br>  {$c_1,…,c_t,…c_T$}</li>
<li><strong>decoder state sequence</strong><br>  {$s_1,…,s_t,…,s_T$}</li>
</ul>
<p>如果 $y_t$ 在 cache 里不存在，那么会选择一个空的 slot 或者覆盖一个 LRU(least recently used) slot，key 是 $c_t$，value 是 $s_t$，indicator 是 $y_t$</p>
<p>如果 $y_t$ 已经存在，那么更新 key, value，$k_i=(k_i+c_t)/2, \ v_i=(v_i+s_t)/2$，像是一个 exponential decay，每一次更新之前的 key, value 都会减半，基本逻辑是最近的历史会有更高的重要性。通过可视化图可以看一下效果：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Remember%20Translation%20History%20with%20a%20Continuous%20Cache/vis.png" class="ful-image" alt="vis.png">
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>理解下来还是非常简单的。亮点还是 cache 的设计，一方面用到了历史信息，另一方面用到了源端和目标端的信息，并且是单词粒度之上的信息（attention context vector）。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Memory Networks </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Memory Networks]]></title>
      <url>http://www.shuang0420.com/2017/12/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/</url>
      <content type="html"><![CDATA[<p>Memory Networks 相关笔记。<br><a id="more"></a></p>
<p>这一篇会覆盖下面三个版本的 Memory Networks</p>
<ul>
<li>Memory Network with strong supervision</li>
<li>End-to-End Memory Network</li>
<li>Dynamic Memory Network</li>
</ul>
<p>涉及下面一些论文：</p>
<ul>
<li>Memory Networks (2015)</li>
<li>End-To-End Memory Networks (2015)   </li>
<li>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing (2016)</li>
<li>Dynamic Memory Networks for Visual and Textual Question Answering (2016)</li>
</ul>
<p>首先要明确的是，Memory Networks 只是一种思想或者说一个框架，像一个 base class，里面的各个 module 都可以自己定制。其中基本的一些思路：</p>
<ul>
<li><strong>分层 RNN 的 context RNN</strong><br>与 context RNN 类似，Memory Network 同样以句子为单位来提取、保存语境信息</li>
<li><strong>Attention 原理</strong><br>使用多个 state vector 来保存信息，并从中寻找有用的记忆，而不是寄希望于 final state 存储的信息</li>
<li><strong>Incorporate reasoning with attention over memory(RAM): Memory Network</strong><br>使用记忆以及记忆上的 attention 来做推理</li>
</ul>
<h1 id="Memory-Networks-2015"><a href="#Memory-Networks-2015" class="headerlink" title="Memory Networks (2015)"></a>Memory Networks (2015)</h1><p>对应论文：Memory Networks (2015)</p>
<p>Memory Networks 提出的基本动机是我们需要 <strong>长期记忆（long-term memory）</strong>来保存问答的知识或者聊天的语境信息，而现有的 RNN 在长期记忆中表现并没有那么好。</p>
<h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><p>4 个重要组件：</p>
<ul>
<li><strong>I: input feature map</strong><br>把输入映射为特征向量（input -&gt; feature representation）<br>通常以句子为单位，一个句子对应一个向量</li>
<li><strong>G: generalization</strong><br>使用新的输入数据更新 memories</li>
<li><strong>O: output</strong><br>给定新的输入和现有的 memory state，在特征空间里产生输出<br>类比 attention RNN decoder 产生 outputs/logits</li>
<li><strong>R: response</strong><br>将输出转化为自然语言</li>
</ul>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks1.png" class="ful-image" alt="Memory%20Networks/memory_networks1.png">
<p>上面的 4 个 component 就对应了整个工作流程：</p>
<ol>
<li><strong>把输入 x 映射为特征向量 I(x)</strong><br>可以选择多种特征，如 bag of words, RNN encoder states, etc.<br>如果用 embedding model 来表达文本的话，一个郁闷的地方是 embdding 的参数在不断变化，所以训练时保存的 vector 也要变化……当然这在测试时就成了优势，因为测试时 embedding 参数就固定啦</li>
<li><strong>更新 memory mi</strong>，$m_i = G(m_i, I(x), m), \forall i.$<br>将输入句子的特征 x 保存到下一个合适的地址 $m_n$，可以简单的寻找下一个空闲地址，也可以使用新的信息更新之前的记忆<br>简单的函数如 $m_{H(x)}=I(x)$，H(x) 是一个寻址函数（slot choosing function），G 更新的是 m 的 index，可以直接把新的输入 I(x) 保存到下一个空闲的地址 $m_n$，并不更新原有的 memory，当然更复杂的 G 函数可以去更新更早的 memory 甚至是所有的 memory</li>
<li><strong>给定新的输入和 memory，计算 output feature o: o=O(I(x),m)</strong><br>Addressing，寻址，给定 query Q，在 memory 里寻找相关的包含答案的记忆<br>$qUU^Tm$： 问题 q 和事实 m 的相关程度，当然这里的 q，m 都是特征向量，可以用同一套参数也可以用不同的参数<br>U：bilinear regression 参数，相关事实的 $qUU^Tm_{true}$ 分数高于不相关事实的分数 $qUU^Tm_{random}$<br>n 条记忆就有 n 条 bilinear regression score<br>回答一个问题可能需要寻找多个相关事实，先根据 query 定位到第一条最相关的记忆，再用第一条 fact 和 query 通过加总或拼接等方式得到 u1 然后一起定位第二条<br>$o_1 = O_1(q,m) = argmax_{i=1,…N} \ s_o(q, m_i)$<br>$o_2 = O_2(q,m) = argmax_{i=1,…N} \ s_o([q, o_1], m_i)$</li>
<li><strong>对 output feature o 进行解码，得到最后的 response: r=R(o)</strong><br>将 output 转化为自然语言的 response<br>$r = argmax_{w \in W} \ s_R([q, o_1, o_2], w)$<br>$s_R(x,y)=xUU^Ty$<br>可以挑选并返回一个单词比如说 playground<br>在词汇表上做一个 softmax 然后选最有可能出现的单词做 response，也可以使用 RNNLM 产生一个包含回复信息的句子，不过要求训练数据的答案就是完整的句子，比如说 football is on the playground</li>
</ol>
<h2 id="Huge-Memory-问题"><a href="#Huge-Memory-问题" class="headerlink" title="Huge Memory 问题"></a>Huge Memory 问题</h2><p><strong>如果 memory 太大怎么办？</strong></p>
<ol>
<li>可以按 entity 或者 topic 来存储 memory，这样 G 就不用在整个 memories 上操作了</li>
<li>如果 memory 满了，可以引入 forgetting 机制，替换掉没那么有用的 memory，H 函数可以计算每个 memory 的分数，然后重写</li>
<li>还可以对单词进行 hashing，或者对 word embedding 进行聚类，总之是把输入 I(x) 放到一个或多个 bucket 里面，然后只对相同 bucket 里的 memory 计算分数</li>
</ol>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数如下，选定 2 条 supporting fact (k=2)，response 是单词的情况：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks_loss.png" class="ful-image" alt="Memory%20Networks/memory_networks_loss.png"></p>
<p>(6) 有没有挑选出正确的第一句话<br>(7) 正确挑选出了第一句话后能不能正确挑出第二句话<br>(6)+(7) 合起来就是能不能挑选出正确的语境，用来训练 attention 参数<br>(8) 把正确的 supporting fact 作为输入，能不能挑选出正确的答案，来训练 response 参数</p>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>在 bAbI QA 部分数据集上的结果<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks_performance.png" class="ful-image" alt="memory_networks_performance.png"></p>
<p>部分 QA 实例：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks_performance1.png" class="ful-image" alt="memory_networks_performance1.png"></p>
<h2 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h2><ol>
<li><strong>需要很强的标记信息</strong><br>bAbI 提供了 supporting fact 的 ID，但对大多数 QA 数据而言，并不存在明确的 supporting fact 标记</li>
<li><strong>Loss2 无法 backpropagate 到模型的左边部分，BP 过程到 m 就停了，并不能 end-to-end 进行训练</strong><br>这相当于一个链状的贝叶斯网络，考虑 A-&gt;B-&gt;C 的有向图，B 对应 m，B 不确定的时候，C 依赖于 A，但是当 B 确定的时候，C 就不依赖于 A 了。 也就是说，在给定 m 的情况下，loss2 和前面的参数是独立的，所以优化 loss2 并不能优化左边的参数</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/memory_networks_drawback.png" class="ful-image" alt="Memory%20Networks/memory_networks_drawback.png">
<h1 id="End-to-End-learning"><a href="#End-to-End-learning" class="headerlink" title="End-to-End learning"></a>End-to-End learning</h1><p>对应论文：End-To-End Memory Networks (2015)   </p>
<p>End-to-End learning 用了 soft attention 来估计每一个 supporting fact 的相关程度，实现了端到端的 BP 过程。</p>
<h2 id="Single-Layer"><a href="#Single-Layer" class="headerlink" title="Single Layer"></a>Single Layer</h2><p>一张图就解决了：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc.png"></p>
<p>模型输入:</p>
<ul>
<li><strong>Input:</strong> $x_1, …, x_n$，会被存储到 memory 中</li>
<li><strong>Query:</strong> q</li>
<li><strong>Answer:</strong> a</li>
</ul>
<p>具体过程（以单层为例）：</p>
<ol>
<li><strong>映射到特征空间</strong><br>{$x_i$} $\xrightarrow{A}$ {$m_i$}<br>$q \xrightarrow{B} u$</li>
<li><strong>计算 attention，得到 query 和 memory 的匹配度，有多少个 memory 就有多少个 $p_i$</strong><br>$p_i = Softmax(u^Tm_i)$</li>
<li><strong>得到 context vector</strong><br>$o = \sum_ip_ic_i$<br>和 Memory Networks with Strong Supervision 版本不同，这里的 output 是加权平均而不是一个 argmax</li>
<li><strong>context vector 和 query 一起，预测最后答案，通常是一个单词</strong><br>$\hat a=Softmax(W(o+u))$</li>
<li><strong>对 $\hat a$ 进行解码，得到自然语言的 response</strong><br>$\hat a \xrightarrow{C} a$</li>
</ol>
<p>其中，<br>A: intput embedding matrix<br>C: output embedding matrix<br>W: answer prediction matrix<br>B: question embedding matrix</p>
<p>损失函数是交叉熵，用 SGD 训练。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc2.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_struc2.png"></p>
<h2 id="Multi-hop-Architecture"><a href="#Multi-hop-Architecture" class="headerlink" title="Multi-hop Architecture"></a>Multi-hop Architecture</h2><p>多层结构（K hops）也很简单，相当于做多次 addressing/多次 attention，每次 focus 在不同的 memory 上，不过在第 k+1 次 attention 时 query 的表示需要把之前的 context vector 和 query 拼起来，其他过程几乎不变。<br>$u^{k+1}=u^k + o^k$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_multi_hop.png" class="ful-image" alt="%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/end_to_end_multi_hop.png">
<h3 id="一些技术细节"><a href="#一些技术细节" class="headerlink" title="一些技术细节"></a>一些技术细节</h3><p>通常来说 encoding 和 decoding 的词向量参数是不一样的，因为一个是单词-词向量，一个是 隐状态-词向量。</p>
<ul>
<li><strong>Adjacent</strong><br>前一层的输出是这一层的输入<br>$A^{k+1}=C^k$<br>$W^T=C^L$<br>$B=A^1$</li>
<li><strong>Layer-wise(RNN-like)</strong><br>不同层之间用同样的 embedding<br>$A^1=A^2=…=A^K$<br>$C^1=C^2=…=C^K$<br>可以在 hop 之间加一层线性变换 H 来更新 $\mu$<br>$u^{k+1}=Hu^k+o^k$</li>
</ul>
<h1 id="Dynamic-Memory-Networks"><a href="#Dynamic-Memory-Networks" class="headerlink" title="Dynamic Memory Networks"></a>Dynamic Memory Networks</h1><p>对应论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing (2016)</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN_struc1.png" class="ful-image" alt="DMN_struc1.png">
<h2 id="Input-Module"><a href="#Input-Module" class="headerlink" title="Input Module"></a>Input Module</h2><p>这一部分像是一个 semantic memory。<strong>输入</strong>可以是一个/多个句子，一篇/几篇文章，包含语境信息和知识库等，使用 RNN 进行 encoding，每一个句子编码成固定维度的 state vector。<br>具体做法是把句子拼到一起（每个句子结尾加标记符 EOS），用 GRU-RNN 进行编码，如果是单个句子，就<strong>输出</strong>每个词的 hidden state；如果是多个句子，就<strong>输出</strong>每个句子 EOS 标记对应的 hidden state，其实相当于分层 RNN 的下面一层。</p>
<p>$$h_t=GRU(L[w_t], h_{t-1})$$</p>
<p>$L[w_t] $ 是 word embedding。</p>
<h2 id="Question-Module"><a href="#Question-Module" class="headerlink" title="Question Module"></a>Question Module</h2><p><strong>输入</strong>是 question 对应的词序列，同样用 GRU-RNN 进行编码。</p>
<p>$$q_t=GRU([L[w_t^Q], q_{t-1})$$</p>
<p>同样的，L 是词向量参数，和 input module 的 L 相同。<strong>输出</strong>是 final hidden state</p>
<h2 id="Episodic-Memory-Module"><a href="#Episodic-Memory-Module" class="headerlink" title="Episodic Memory Module"></a>Episodic Memory Module</h2><p>由 internal memory, attention mechansim, memory update mechanism 组成。 <strong>输入</strong>是 input module 和 question module 的输出。</p>
<p>把 input module 中每个句子的表达（fact representation c）放到 episodic memory module 里做推理，使用 attention 原理从 input module 中提取相关信息，同样有 multi-hop architecture。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN_process.png" class="ful-image" alt="DMN_process.png">
<h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>觉得论文里的 attention mechanism 和 memory update mechanism 描述的有点问题，个人以为 attention mechanism 的目的还是生成 context vector，memory update mechanism 的目的是更新 memory，所以把部分公式按自己的理解移动了下，便于理解。</p>
<p>计算 query 和 fact 的分数，query 和上一个 memory $m^{i-1}$ 作为<strong>输入</strong>产生<strong>输出</strong> episode $e^i$。要注意的是，End-to-End MemNN 的 attention 用的是 linear regression，DMN 用的是  gating function，一个两层的前向神经网络。</p>
<p>在每一轮迭代中，都有<strong>输入</strong>：</p>
<ol>
<li>$c_t$（input module 中第 t 个位置的 fact vector)</li>
<li>上一轮迭代得到的 memory $m_{i-1}$</li>
<li>question q  </li>
</ol>
<p>利用 gating function 计算第 t 个位置的得分  $g^i_t=G(c_t, m_{i-1}, q)$。G 是一个两层的前向神经网络的 score function，不过描述 input, memory, question 相似度的 feature vector z(c,m,p) 是人工定义的，这也成为了之后 DMN+ 的一个优化点。</p>
<p>计算完每一次迭代每一个位置的分数后，来更新 episode $e^i$，或者说产生 context vector。<strong>输入</strong>是 $c_1, …, c_{T_C}$，和它们的 gate score $g^i_t$。</p>
<p>$$ h^i_t=g^i_tGRU(c_t, h^i_{t-1})+(1-g^i_t)h^i_{t-1}$$</p>
<p>$$e^i=h^i_{T_C}$$</p>
<p>总结一下，这部分 attention mechanism 的目的是生成 episode $e^i$，$e^i$ 是第 t 轮迭代的所有相关 input 信息的 summary。与 End-to-End MemNN 不同的是，End-to-End 用了 soft attention，也就是加权和来计算 context，而这里用了 GRU。</p>
<h3 id="Memory-Update-Mechanism"><a href="#Memory-Update-Mechanism" class="headerlink" title="Memory Update Mechanism"></a>Memory Update Mechanism</h3><p>上一步算的 episode $e^i$ 以及上一轮迭代的 memory $m^{i-1}$ 作为<strong>输入</strong> 来更新 episodic memory $m^i$。</p>
<p>$$m^i=GRU(e^i, m^{i-1})$$</p>
<p><strong>输出</strong>是最后一次迭代的 $m=m^{T_M}$</p>
<p>End-to-End MemNN 的 memory update 过程里，第 k+1 次 query vector 直接是上一个 context vector 和 query 的拼接，$u^{k+1}=u^k + o^k$。而 DMN 里，采用了 RNN 做非线性映射，用 episode $e^i$ 和上一个 memory $m^{i-1}$ 来更新 episodic memory，其 GRU 的初始状态包含了 question 信息，$m^0=q$。</p>
<p>Episodic Memory Module 需要一个停止迭代的信号。一般可以在输入中加入一个特殊的 end-of-passes 的信号，如果 gate 选中了该特殊信号，就停止迭代。对于没有显性监督的数据集，可以设一个迭代的最大值。</p>
<p>总结一下，这部分 memory update mechanism 的目的是生成 t 时刻的 episode memory $m^t$，最后一个 pass 的$m^T$ 将包含用于回答问题 q 的所有信息。</p>
<h3 id="Example-Understanding"><a href="#Example-Understanding" class="headerlink" title="Example Understanding"></a>Example Understanding</h3><p>来用例子解释下 Episodic Memory Module 上面那张图，question 是 where is the football? 第一次迭代找到的是第 7 个句子，John put down the football，第二次找到第 6 个句子 John went to the hallway，第三次找到第 2 个句子 John moved to the bedroom。</p>
<p>多次迭代第一次找到的是字面上最相关的 context，然后一步步迭代会逐渐定位到真正语义相关的 context，这感觉上就是一个推理的过程。</p>
<h3 id="Answer-Module"><a href="#Answer-Module" class="headerlink" title="Answer Module"></a>Answer Module</h3><p>使用了 GRU-RNN 作为 decoder。<strong>输入</strong>是 question module 的输出 q 和上一时刻的 hidden state $a_{t-1}$，初始状态是episode memory module 的输出 $a_0=m^{T_M}$</p>
<p>$$<br>  \begin{aligned}<br>  y_t=Softmax(W^{(a)}a_t) \\<br>  a_t=GRU([y_{t-1}, q], a_{t-1}) \\<br>  \end{aligned}<br>$$</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>使用 cross-entroy 作为目标函数。如果数据集有 gate 的监督数据，还可以将 gate 的 cross-entroy 加到总的 cost上去，一起训练。训练直接使用 backpropagation 和 gradient descent 就可以。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总的来说，DMN 在 addressing，memory 提取，query/memory update 部分都用了 DL 手段，相比于 End-to-End MemNN 更为复杂。</p>
<h1 id="DMN"><a href="#DMN" class="headerlink" title="DMN+"></a>DMN+</h1><p>对应论文：Dynamic Memory Networks for Visual and Textual Question Answering (2016)</p>
<p>DMN 存在的两个问题：</p>
<ol>
<li>输入模块只考虑了过去信息，没考虑到将来信息</li>
<li>只用 word level 的 GRU，很难记忆远距离 supporting sentences 之间的信息</li>
</ol>
<p>这一部分重点讲与 DMN 不同的地方。</p>
<h2 id="Input-Module-1"><a href="#Input-Module-1" class="headerlink" title="Input Module"></a>Input Module</h2><p>DMN+ 把 single GRU 替换成了类似 hierarchical RNN 结构，一个 sentence reader 得到每个句子的 embedding，一个 input infusion layer 把每个句子的 embedding 放入另一个 GRU 中，得到 context 信息，来解决句子远距离依赖的问题。HRED 相关思路见 <a href="http://www.shuang0420.com/2017/11/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20HRED%20%E4%B8%8E%20VHRED/">论文笔记 - HRED 与 VHRED</a>。这里还做了一些微调，sentence reader 用的是 positional encoding，input fusion layer 用了双向 GRU，兼顾了过去和未来的信息。</p>
<p>用 positional encoding 的原因是在这里用 GRU/LSTM 编码句子计算量大而且容易过拟合（毕竟 bAbI 的单词量很小就几十个单词。。），这种方法反而更好。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_textual_input.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_textual_input.png"></p>
<p>除了处理文本信息，作者也提出了图像信息的方案，CNN+RNN，把局部位置的图像信息当做 sentence 处理，在这不多介绍。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_image_input.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_image_input.png"></p>
<h2 id="Episodic-Memory-Module-1"><a href="#Episodic-Memory-Module-1" class="headerlink" title="Episodic Memory Module"></a>Episodic Memory Module</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2BMemoryUpdate.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2BMemoryUpdate.png">
<h3 id="Attention-Mechanism-1"><a href="#Attention-Mechanism-1" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>仍然是人工构造特征向量来计算 attention，但与之前版本的 DMN 相比更为简化，省去了两项含有参数的部分，减少了计算量。另外与 DMN 不同的是，gate 值不是简单的二层前馈网络的结果，而是接着计算了一个 sofmax 评分向量。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_gate.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/DMN%2B_gate.png">
<p>也就是说，从公式上看，相对于 DMN，式 8 更为简洁，式 9 不变（结果就是 DMN 的 gate 值），增加了式 10。 </p>
<p>进行到下一步关于 context vector 的计算，两种方案，一种是 soft attention，简单的加权求和，另一种是 attention based GRU。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/AttnGRU.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/AttnGRU.png">
<p>AttnGRU 考虑了输入 facts 的位置和顺序信息（position and ordering），或者说是时序信息。在得到 attenion 后，把 attention 作为 gate，如上图，把传统 GRU 中的 update gate $u_i$ 替换成了 attention 的输出 $g^t_i$，这样 gate 就包含了 question 和前一个 episode memory 的知识，更好的决定了把多少 state 信息传递给下一个 RNN cell。同时这也大大简化了 DMN 版本的 context 计算。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/AttnGRU2.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/AttnGRU2.png">
<p>$\hat h$ 是更新的 state，$h_{i-1}$ 是传入的上一时刻的 state，$g^t_i$ 是 attention value，是一个由 softmax 产生的标量（scalar）而不是 sigmoid 激活产生的 vector $u_i \in R^{n_H}$，context vector 是 GRU 的 final hidden state。</p>
<p>AttnGRU 要优于 weighted sum，因为使用了一些时间上的关系，比如小明在操场，小明回了家，小明进了卧室，这些事实实际上有先后关系，而 weighted sum 不一定能反映这种时序关系。</p>
<h3 id="Memory-Update-Mechanism-1"><a href="#Memory-Update-Mechanism-1" class="headerlink" title="Memory Update Mechanism"></a>Memory Update Mechanism</h3><p>DMN 中 memory 更新采用以 q 向量为初始隐层状态的 GRU 进行更新，用同一套权重，这里替换成了一层 ReLU 层，实际上简化了模型。</p>
<p>$$m^t = ReLU(W^t[m^{t-1};c^t;q]+b)$$</p>
<p>其中 ; 表示拼接，这能进一步提高近 0.5% 的准确率。</p>
<h2 id="Performance-1"><a href="#Performance-1" class="headerlink" title="Performance"></a>Performance</h2><p>最后上一个不同模型的 performance 比较图。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/final_performance.png" class="ful-image" alt="../../static/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Memory%20Networks/final_performance.png"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Memory Networks </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习策略(Andrew Ng. DL 笔记)]]></title>
      <url>http://www.shuang0420.com/2017/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5(Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0)/</url>
      <content type="html"><![CDATA[<p>Andrew Ng. Deep Learning Course 3 Structuring Machine Learning Projects 的重点笔记。<br><a id="more"></a></p>
<h1 id="常用策略及考虑问题"><a href="#常用策略及考虑问题" class="headerlink" title="常用策略及考虑问题"></a>常用策略及考虑问题</h1><p><strong>常用的 ML 策略：</strong></p>
<ul>
<li>收集更多数据（collect more data）</li>
<li>收集更多样化的训练集（collect more diverse training set）</li>
<li>梯度下降训练更长时间（train algorithm longer with gradient descent）</li>
<li>尝试 Adam 算法（try Adam instead of gradient descent）</li>
<li>尝试更大的网路（try bigger network）</li>
<li>尝试小一点的网络（try smaller network）</li>
<li>尝试 dropout（try dropout）</li>
<li>加 L2 正则（add L2 regularization）</li>
<li>改善网络结构（network architecture）<ul>
<li>激活函数（activation functions）</li>
<li>隐藏单元数量（number of hidden units）</li>
<li>…</li>
</ul>
</li>
</ul>
<p><strong>要考虑的几个问题：</strong></p>
<ul>
<li>训练集上拟合良好 fit training set well on cost function</li>
<li>训练开发集上拟合良好 fit training-dev set well on cost function<br>在训练集和开发/测试集来自不同分布时考虑</li>
<li>开发集上拟合良好 fit dev set well on cost function</li>
<li>测试集上拟合良好 fit test set well on cost function</li>
<li>现实世界中表现良好 performs well in real world</li>
</ul>
<p>另外，调优要注意的是，尽量用 <strong>正交化（Orthogonalization）</strong> 的手段，比如说 <strong>early stopping</strong> 其实不是一个优先的选择，因为它不那么正交化，会同时影响模型对训练集的拟合以及开发集的表现，同时影响了两件事情，对误差分析造成干扰。</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p><strong>单实数评价指标</strong>（Using a single number evaluation metric）<br>用单实数评价指标，能够提高比较各种模型的效率，便于优化模型。 一个简单的例子是用 precision 和 recall，我们知道二者不可兼得，也就是比较哪个模型好的时候我们会发现 A 的 precision 高，B 的 recall 高，选哪个呢？这时候不妨引入 F1 来综合 precision 和 recall。</p>
<p><strong>满足和优化指标</strong><br>如果有多个评价指标，可以选择线性组合，如 cost = accuracy - β*time，也可以选其中一个为 optimizing metric，其他为 satisfying metrics。</p>
<p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">分类器	| 准确率 | 耗时</div><div class="line">  A     |  90%  | 80ms</div><div class="line">  B	|  92% |	95ms</div><div class="line">  C	|  95% |	1500ms</div></pre></td></tr></table></figure></p>
<p>我们更关心准确率，所以准确率是优化目标，同时希望耗时不要太长，所以运行时间是满足指标，最后整体指标就是在 100ms 运行时间的条件下准确率的大小。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">maximize accuracy</div><div class="line">subject to running time &lt;= 100ms</div></pre></td></tr></table></figure></p>
<p><strong>什么时候改变评价指标？</strong><br>模型在 metric + dev/test 上表现很好，但是在实际应用中表现不好的时候，就应该考虑改变 metric 了。</p>
<p>假设现在有两个算法:</p>
<ul>
<li>算法 A:  喵咪图片识别误差是 3%，但是会把少儿不宜的图片分类为猫</li>
<li>算法 B：误差是 5%，但是不会给用户推送不健康的图片</li>
</ul>
<p>我们的 dev + metric 偏好 A，但我们的用户偏好 B，两者存在分歧，这时我们就需要改变评价指标了。根据上面的例子，假设原来的评价指标是：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/error_metric0.png" class="ful-image" alt="error_metric0.png"></p>
<p>那么现在可以给“把少儿不宜的图片分类为猫”这个错误一个大的惩罚权重，当然前提是先把这些少儿不宜的图片标注好。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/error_metric1.png" class="ful-image" alt="error_metric1.png"></p>
<h1 id="训练-开发-测试集"><a href="#训练-开发-测试集" class="headerlink" title="训练/开发/测试集"></a>训练/开发/测试集</h1><p>机器学习通常会把数据集分为<strong>训练/开发/测试集</strong>，这一篇讲一讲这三个子集扮演的角色。</p>
<p>最重要的一点是：<br><strong>dev set + single metric =&gt; target（瞄准的目标）</strong></p>
<p>开发集和测试集的选择通常是现实中希望去处理的数据，很重要的一点是 <strong>开发集和测试集必须服从同一分布</strong>。举个例子，复习考试，训练集是复习资料，开发集像是考试前的模拟试卷，测试集则是真实考卷，我们最终目标是在真实考卷（测试集）中取得好成绩，准备过程中优化的是模拟试题（开发集）上的成绩，而复习资料（训练集）的选择会影响我们逼近这个目标有多快。而如果开发集和测试集来自不同分布，比如说在英语模拟试卷上考了 99 分，但最后你去参加了语文考试，这不是白复习白训练了么~</p>
<p><strong>训练/开发/测试集的大小</strong><br>之前传统 ML 时代数据集比较小， 一般 &lt; 10,000，所以 train/dev/test 的划分通常是 60%/20%/20%，而现在的数据量很大，动辄百万级，划分 40% 的数据处理做开发/测试集显然是浪费，所以比例可以是 98%/1%/1%。</p>
<p>可避免偏差 <strong>available bias</strong> 和<strong>Human-level performance</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/bayes_bias.png" class="ful-image" alt="bayes_bias.png"></p>
<p>Humans error 与 Training Error之间的差距是 <strong>Avoidable bias</strong>，Training Error 与 Dev Error之间的差距是  <strong>Variance</strong>，提到过好多次啦，具体见 <a href="http://www.shuang0420.com/2017/03/17/会议笔记%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/">会议笔记 - Nuts and Bolts of Applying Deep Learning</a>，这里简单附个图吧。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/improve_model.png" class="ful-image" alt="improve_model.png"></p>
<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p><strong>误差分析</strong> 的作用在于弄清楚误差最主要来自哪个部分，然后给未来的工作指明优化方向，便于解决主要矛盾，节省时间。</p>
<p>还是猫分类器的例子，假设我们分析模型的预测结果后发现，预测错误的数据中有一部分狗的图片被错误标成了猫。这时可能会想着设计一些处理狗的特征/分类的算法功能来提高猫分类器。然而，假如 100 个错误标记的开发集样本中，只有 5 个是狗，那么这意味着你针对狗的算法提高最终也只能优化误差的 5%，比如原来误差是 10%，最好的情况也只是优化到 9.5%，这是优化上限。</p>
<p>把误差的来源以及在总误差的占比列个表统计，就能清楚的发现解决哪类误差对模型优化帮助最大。这个过程中可能会发现新的错误类型，比如滤镜导致的误差。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/error_analysis.png" class="ful-image" alt="error_analysis.png"></p>
<p>统计完成后根据误差占比、问题难度、团队的时间精力，来选择其中优先级高的几个进行优化。</p>
<p><strong>如果训练数据中有一些标记错误的例子怎么办？</strong></p>
<p>如果这种误差是 <strong>随机误差</strong>，人没有注意而随机产生的，那么不用花太多时间修复它们，只要数据集足够大，对最后的结果不会有太大影响，因为<strong>深度学习算法对随机误差有一定健壮性（robustness）。</strong></p>
<p>但如果这种误差是 <strong>系统性误差</strong>，比如把把所有白色的狗都标注成了猫，那么问题就大了，学习之后所有白色的狗都会被分类成猫，这就需要重新标记了。</p>
<p><strong>如果开发/测试集中有一些标记错误的例子怎么办？</strong><br>在误差分析中加一列 incorrectly labeled，然后看是否值得修正这些标记错误的例子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/error_analysis2.png" class="ful-image" alt="error_analysis2.png">
<p>如果人工错误标记引起的错误样本比例是 0.6%，而总体开发集误差是 10%，那么应该集中精力解决剩下的 9.4%，而如果总体开发集误差是 2%，那么就应该考虑去纠正这些人为错误了。</p>
<p>但要注意的是，不管要不要纠正人为误差，都要同时作用在开发集和测试集上，确保两者服从同一分布。同时，也可以考虑再次检查那些分类正确的例子，因为本来 no 的例子可能被标记成了 yes，不过正常情况下判断错的次数比判断对的次数要少的多，所以检查这部分数据花的时间也长的多。</p>
<h1 id="训练-测试集来自不同分布"><a href="#训练-测试集来自不同分布" class="headerlink" title="训练/测试集来自不同分布"></a>训练/测试集来自不同分布</h1><p><strong>如果训练集的分布和开发/测试集不一样怎么办？</strong> 这种情况并不少见。比如说，我们有网上爬取的猫的图片 20w，但只有用户在手机 APP 上传的图片 1w。</p>
<p>这时候，可能会想到把这 21w 条数据 shuffle 然后来划分，这样的好处是训练/开发/测试集来自同一分布，但是！你会发现开发/测试集上的很多图片都来自网页下载，这并不是我们真心关心的数据分布，也就是说，只有一小部分的数据是我们的模型真心要瞄准、要优化的目标，而实际上我们大部分精力都在优化网页下载的图片！</p>
<p>再回顾一下核心思想，<strong>开发集/测试集是真正关心的要优化的目标数据</strong>。所以更恰当的做法是，<strong><em>把手机上传的一半图片 5k 条划给训练集，剩下的 5k 条全部划分为开发/测试集</em></strong>。</p>
<p>要注意的是，训练/测试集来自不同分布会对 <strong>方差/偏差分析</strong> 造成影响，因为这不再是正交化的分析了。比如说</p>
<ul>
<li>training error 1%</li>
<li>dev error 10%</li>
</ul>
<p>如果训练集和开发集来自同一分布，很简单，这是出现了高方差的问题。但如果训练集和开发集来自不同的分布，那么可能这里就不是高方差的问题了，造成这种情况的原因有两种：</p>
<ol>
<li>算法看不到开发集数据，难以泛化</li>
<li>训练集和开发集来自不同的分布</li>
</ol>
<p>我们需要知道哪个因素影响更大，才能判断是不是高方差的原因。这时需要定义一个新的数据集 <strong>training-dev set</strong>，通过随机打散训练集，分出一部分训练集和开发集一起作为训练-开发集，这样的话，training-dev set 和 training set 来自同一分布，同时 trainging-dev set 和 dev/test set 也来自同一分布。</p>
<p>现在只在训练集上训练模型</p>
<ul>
<li>training error    1%    </li>
<li>training-dev error  9%    </li>
<li>dev error                10%</li>
</ul>
<p>这就是方差问题，模型泛化能力差。</p>
<ul>
<li>training error    1%    </li>
<li>training-dev error1.5%</li>
<li>dev error10%</li>
</ul>
<p>这时候就是 <strong>数据不匹配问题（data mismatch problem ）</strong>了。</p>
<p>也就是说，当训练集和开发/测试集来自不同分布的时候，我们需要考虑下面 5 种 error：</p>
<ul>
<li>human error: 0%</li>
<li>training error: 10%</li>
<li>training-dev error: 11%</li>
<li>dev error: 20%</li>
<li>test error: 20%</li>
</ul>
<p>这个例子就是 <strong>高方差+数据不匹配</strong> 问题。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/mismatch.png" class="ful-image" alt="mismatch.png">
<p><strong>如果误差分析显示有数据不匹配的问题该怎么办</strong></p>
<ul>
<li>可以人工做误差分析，了解训练集和开发测试集的差异</li>
<li>收集更多与开发集、测试集相似的训练数据，人工数据合成<br>比如 clear audio + car noise =&gt; synthesized in-car audio</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> 过拟合 </tag>
            
            <tag> overfitting </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - HRED 与 VHRED]]></title>
      <url>http://www.shuang0420.com/2017/11/17/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20HRED%20%E4%B8%8E%20VHRED/</url>
      <content type="html"><![CDATA[<p>介绍一下经典的 HRED 和 VHRED。<br><a id="more"></a></p>
<p>主要涉及到下面几篇论文</p>
<ul>
<li><a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="external">Building end-to-end dialogue systems using generative hierarchical neural network models</a></li>
<li><a href="https://arxiv.org/abs/1605.06069" target="_blank" rel="external">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a></li>
<li><a href="https://arxiv.org/abs/1511.06349" target="_blank" rel="external">Generating Sentences From a Continuous Spaces motivation</a></li>
</ul>
<h1 id="HRED"><a href="#HRED" class="headerlink" title="HRED"></a>HRED</h1><p>HRED 在之前<a href="http://www.shuang0420.com/2017/10/05/经典的端到端聊天模型/">经典的端到端聊天模型</a> 提到过，这里拎出来再具体分析一下。</p>
<p>传统 Seq2Seq 在对话任务上对上下文依赖考虑有限，<a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="external">Building end-to-end dialogue systems using generative hierarchical neural network models</a> 论文提出了一种 <strong>分层 RNN 结构 -  HRED（Hierarchical Recurrent Encoder-Decoder）</strong>，能同时对句子和对话语境（上下文）进行建模，来实现多轮对话。</p>
<p>先来看一下如果不使用分层 RNN，在传统 Seq2Seq 模型基础上，如果我们想得到 context 信息应该怎么做。 </p>
<p>第一个想法是将上一个句子的 final state 作为下一个句子的 initial state，然后将句子信息不断传递下去，这样的话 context vector 里的信息会在 propagation 的过程中会被新句子的词语逐步稀释，对信息/梯度的传播极不友好。</p>
<p>因此为了让信息更好的传递，我们可能会考虑把 final state 传递到下一个句子的 last state，而不是 initial state，然后用拼接或者非线性的方法来表达之前的和当前的句子信息。</p>
<p>更干脆的，直接将语境中的多个 utterance vector 提取出来再用一个 RNN 来处理，捕捉 context 信息，这就有了分层 RNN。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/HRED.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/HRED.png">
<p>简单来说，HRED 在传统 encoder-decoder 模型上，额外增加了一个 encoder，相比于普通的 RNN-LM 来说，考虑了 turn-taking nature，能够对上下文进行建模，减少了相邻句子间的计算步骤，有助于信息/梯度的传播，从而实现多轮对话。整个过程有下面三个阶段：</p>
<ol>
<li><strong>encoder RNN</strong><br>第一个 encoder 和标准的 seq2seq 相同，将一句话编码到固定长度的 utterance vector，也就是 RNN 的 last hidden state<br>encoder RNN 或者说 utterance RNN 记忆的是对话的细节</li>
<li><strong>context RNN</strong><br>n 个句子的 utterance vector 作为第二个 encoder 也就是 <strong>context-level encoder</strong> 各个时间上的的输入，对应长度为 n 的 sequence，产生一个 context vector 实现对语境的编码，也就是 RNN 的 output (注意这里不是 last hidden state)<br>context RNN 记忆的是更为全局的语义信息</li>
<li><strong>decoder RNN</strong><br>上一个句子的 utterance vector 作为 response 的初始状态，目前为止产生的 context vector 和上一个单词的 word embedding 拼接作为 decoder 的输入</li>
</ol>
<p>然而 HRED 相对于传统的 Seq2Seq 模型的提高并不明显，bootstrapping 的作用更加明显。一方面可以用 pre-trained word embedding，另一方面可以使用其他 NLP 任务的数据预训练我们的模型，使得模型的参数预先学到一些对自然语言的理解，然后再来学习聊天任务。</p>
<h1 id="VHRED"><a href="#VHRED" class="headerlink" title="VHRED"></a>VHRED</h1><p>先简单看一下 auto-encoder 的两个变体 dAE 和 VAE。</p>
<p><strong>Denoising auto-encoder(dAE)</strong> 在输入数据引入一些随机噪声，要求 auto-encoder 去重构加噪声之前的原始观测值，来增加模型鲁棒性，避免过拟合。</p>
<p>而 <strong>Variational Autoencoder(VAE)</strong> 则是在中间层（hidden layer）引入噪音，来重构输入数据，因此 auto-encoder 出来的样本具有更高的全局性。</p>
<p><strong>VHRED(Latent Variable Hierarchical Recurrent Encoder-Decoder Model)</strong>，就是在 HRED 基础上引入了 VAE 的思想，不同的是在 reconstruction 时生成的是下一个 utterance 而不是原来的 input。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20HRED%20%E4%B8%8E%20VHRED/VHRED.png" class="ful-image" alt="VHRED.png">
<p>VHRED 是为了解决 RNNLM 和 HRED 很难产生有意义的、高质量的回复而提出的。传统 Seq2Seq 倾向于产生短的安全回答（safe response），因为它有确定性的编码和解码过程，着重拟合具体、有限的回复样本，缺少对 response 语义信息的理解。另外 decoder 时两个目标，一是生成下一个 token，二是占据控制真实输出路径的 embedding space 的一个位置，来影响之后 token 的生成，而由于梯度衰减的影响，模型会更聚焦第一个目标，response 的产生更容易限于 token level，尤其对于 high-entropy 的句子，模型更偏好短期预测而不是长期预测，所以模型很难产生长的、高质量的回复。</p>
<p>VHRED 针对这个问题引入了全局（语义层面）的随机因素，一是能增强模型的 robustness，二是能捕捉 high level concepts。Latent variable 使得 response 不再和一个/几个固定的句子绑定，鼓励了回复的多样性。</p>
<p>和 HRED 不同的是，VHRED 在第二个 context RNN 产生 context vector c 后，由 c sample 了一些高斯随机变量 z（latent variable），期望值和标准差由 c 决定（前向网络+矩阵乘法得到 $\mu$，+矩阵乘法和 softplus 得到 $\Sigma$），高斯变量和 context vector 拼接就得到了包含全局噪声的 vector，作为每个时间的观测值，和 query vector 一起放到 decoder 里产生 response。</p>
<p>训练时，z 从后验采样，测试时由于 KL 已经把分布拉近，z 可以从先验采样。模型的训练技巧如 KL annealing 等借鉴了 <a href="https://arxiv.org/abs/1511.06349" target="_blank" rel="external">Generating Sentences From a Continuous Spaces motivation</a> 这篇论文的思想。</p>
<p>因为 z 是从 context state 计算出来的，latent variable 的期望值和标准差包含一些 high-level 的语义信息，更鼓励模型抽取抽象的语义概念。实验表明，HRED 能产生更长的回复，更好的 diversity。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Chatbot </tag>
            
            <tag> 多轮对话 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[关于微信公众号和知乎专栏的开通]]></title>
      <url>http://www.shuang0420.com/2017/11/11/%E5%85%B3%E4%BA%8E%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%92%8C%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F%E7%9A%84%E5%BC%80%E9%80%9A/</url>
      <content type="html"><![CDATA[<p>终于等到你~<br><a id="more"></a></p>
<blockquote>
<p><em>贵有恒何必五更起三更眠</em>，<em>最无益只怕一日曝十日寒</em></p>
</blockquote>
<p>写博客也有一年多了，回国后忧伤的遇到了 GitPage 加载速度慢、CodingNet 容量限制、百度收录速度慢（收录条目不及谷歌的一半）等诸多问题。入乡随俗，大刀阔斧租了服务器，上了 CDN，注册了域名，折腾了好一番。想要喘口气的时候想到之前有小伙伴嫌弃我博客不能主动推送（嗯，她不用 RSS），于是就干脆趁热打铁，开个 <strong>公众号/专栏</strong> 吧。</p>
<p>欢迎关注。名字是不变的，谷歌搜索<strong>徐阿衡</strong>，不出意外第二条应该是知乎专栏地址，公众号的话二维码在文末。<strong>主题</strong> 也是不变的，依然是 <strong>自然语言处理</strong> 与 <strong>深度学习</strong> 的相关内容，只是 <strong>定位</strong> 可能会有所不同。</p>
<p><strong>公众号</strong> 的定位是 <strong>短文</strong>，可能是零散的知识点，新的 idea，也可能是论文的分享，总之，是便于碎片时间阅读的内容，许多个短文串联才可能覆盖一篇完整的博文。<strong>更新频率</strong>大概会是 2-3 天。除分享外，也在计划上线部分自己正在做的项目，比如说带个人风格的机器人等，在这立个 FLAG 先~</p>
<p><strong>知乎和博客</strong> 则是 <strong>长文</strong>，是相关知识的一个框架性整理。<strong>更新频率</strong> 大概是 1-2 周。</p>
<p>大胆假设，小心求证。不能保证我对知识的理解、我的 idea、我的代码是完全准确无 bug 的，欢迎讨论，欢迎批评指正，但希望各位客观评论，以礼相待。</p>
<p>学习之路，道阻且长。道阻且长，行则将至。希望和小伙伴们一起，每天进步一点点，嗯，话不多说，就这样。</p>
<p><strong>公众号：</strong> xu_a_heng<br><strong>知乎专栏：</strong> <a href="https://zhuanlan.zhihu.com/c_136690664" target="_blank" rel="external">徐阿衡-自然语言处理</a></p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 公众号 </tag>
            
            <tag> 知乎 </tag>
            
            <tag> 专栏 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Copy or Generate]]></title>
      <url>http://www.shuang0420.com/2017/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20CopyNet%20or%20Generate/</url>
      <content type="html"><![CDATA[<p>关于 Point Network &amp; CopyNet 的几篇论文笔记。<br><a id="more"></a></p>
<p>普通的 Seq2Seq 的 output dictionary 大小是固定的，对输出中包含有输入单词(尤其是 OOV 和 rare word) 的情况很不友好。一方面，训练中不常见的单词的 word embedding 质量也不高，很难在 decoder 时预测出来，另一方面，即使 word embedding 很好，对一些命名实体，像人名等，word embedding 都很相似，也很难准确的 reproduce 出输入提到的单词。Point Network 及 CopyNet 中的 copy mechanism 就可以很好的处理这种问题，decoder 在各 time step 下，会学习怎样直接 copy 出现在输入中的关键字。</p>
<p><strong>涉及到的论文：</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="external">Pointer Networks. NIPS 2015</a></li>
<li><a href="https://arxiv.org/abs/1603.06393" target="_blank" rel="external">Incorporating Copying Mechanism in Sequence-to-Sequence Learning. ACL 2016.</a></li>
<li><a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="external">Get To The Point: Summarization with Pointer-Generator Networks. ACL 2017</a></li>
<li><a href="http://link.zhihu.com/?target=http%3A//www.nlpr.ia.ac.cn/cip/shizhuhe/articles/acl2017-coreqa.pdf" target="_blank" rel="external">Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning. ACL 2017</a></li>
</ul>
<h1 id="Pointer-Network-Ptr-Net"><a href="#Pointer-Network-Ptr-Net" class="headerlink" title="Pointer Network(Ptr-Net)"></a>Pointer Network(Ptr-Net)</h1><p>主要来解决传统 Seq2Seq 中 output dictionary 大小固定(fixed prior)的问题。思路非常简单，实际上就是 attention Seq2Seq 的一个简化版本，不过产生的不是输出序列，而是指向输入序列的一堆指针(或者从输入序列中挑选一个元素进行输出)。论文解释了这种结构可以用来解决 <strong>旅行商(Travelling Salesman Problem)、凸包(convex hulls)</strong> 等问题。</p>
<p><strong>结构对比:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/PtrNet.png" class="ful-image" alt="PtrNet.png"></p>
<p>还是先来看一下经典的 attention-based seq2seq， 在每一个 decoder step，先计算 $e_{ij}$ 得到对齐概率(或者说 how well input position j matches output position i)，然后做一个 softmax 得到 $a_{ij}$，再对 $a_{ij}$ 做一个加权和作为 context vector $c_i$，得到这个 context vector 之后在固定大小的 output dictionary 上做 softmax 预测输出的下一个单词。</p>
<p><strong>经典 Attention:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/birnnattention.png" class="ful-image" alt="birnnattention.png"></p>
<p>而 Ptr-Net 不过是简化了后面的步骤，有了 $e_{ij}$ 后直接做 sofmax，可以得到一系列指向输入元素的指针 $a_{ij}$，最直观的用法就是对输入元素进行排序输出了。</p>
<h1 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h1><p><strong>CopyNet</strong> 实现的是能够把输入中的部分信息原封不动的保留到输出中，相当于一个 refer back。一个简单的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Q: 你好呀，我叫毛毛</div><div class="line">A: 毛毛，你好，很高兴认识你</div></pre></td></tr></table></figure>
<p>这个”毛毛”，可能是 OOV，也可能是其他实体或者是日期等很难被 decoder “还原” 出来的信息，CopyNet 可以更好的处理这类的信息。</p>
<p>那么问题主要有两个：</p>
<ul>
<li><strong>What to copy: 输入中的哪些部分应该被 copy?</strong> </li>
<li><strong>Where to paste: 应该把这部分信息 paste 到输出的哪个位置？</strong></li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/CopyNet1.png" class="ful-image" alt="CopyNet1.png">
<p>框架还是基于 attention-based encoder-decoder，不过在 decoder 的时候，做了部分改进，总的来说，下一个单词的预测是由一个 generate-mode g 和 copy-mode c 的混合概率模型决定的。</p>
<p>$$p(y_t|s_t, y_{t-1}, c_t, M) = p(y_t, g|s_t, y_{t-1}, c_t, M) + p(y_t, c|s_t, y_{t-1}, c_t, M) $$</p>
<p>要实现 CopyNet 需要有两个词汇表，一个是通常意义的高频词词汇表 V={$v_1,…,v_N$} 加上 OOV，另一个是在输入中出现的所有 unique words X={$x_1, …, x_{T_S}$}，这部分可能会包含没有在 V 里出现的单词，也就是 OOV 单词，第二个词汇表用来支持 CopyNet，最终输入的词汇表是三者的并集 $V  ∪ UNK ∪ X$。</p>
<p>对 encoder 部分(双向 RNN) 的输出 $h_1, …, h_{T_S}$，记做 M，M 其实同时包含了 <strong>语义</strong> 和 <strong>位置</strong> 信息。decoder 部分对 M 的读取有两种形式：</p>
<ul>
<li><strong>Content-base</strong><br>Attentive read from word-embedding</li>
<li><strong>location-base</strong><br>Selective read from location-specific hidden units</li>
</ul>
<p><strong>两种模式对应的 score function</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/CopyNet_mode.png" class="ful-image" alt="CopyNet_mode.png"></p>
<p>$$<br>  \begin{aligned}<br>  φ(y_t=v_i) &amp;= v_i^TW_os_t, \ \ \ \ v_i ∈ V ∪ UNK \\<br>  φ(y_t=x_j) &amp;= σ(h_j^TW_c)s_t, \ \ \ \ vi ∈ V ∪ UNK \\<br>  \end{aligned}<br>$$</p>
<p>$φ(y_t=v_i)$ 和普通的 RNN encoder-decoder 计算相同，$φ(y_t=x_j)$ 将 $h_t$ 和 $s_t$ 映射到了同一个语义空间，$\sigma$ 发现用 tanh 比较好，注意 $p(y_t, c|・)$ 的计算加总了所有 $x_j=y_t$ 的情况。</p>
<p><strong>$s_t$ 的更新：</strong><br>在用 $y_{t-1}$ 更新 t 时刻的状态 $s_t$ 时，CopyNet 不仅仅考虑了词向量，还使用了 M 矩阵中特定位置的 hidden state，或者说，$y_{t-1}$ 的表示中就包含了这两个部分的信息 $[e(y_{t-1}); ζ(y_{t-1})]$，$e(y_{t-1})$ 是词向量，$ζ(y_{t-1})$ 和 attention 的形式差不多，是 M 矩阵中 hidden state 的加权和</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/similar_attention.png" class="ful-image" alt="similar_attention.png">
<p>K 是 normalization term，直观上，是去找输入中也出现 $y_{t-1}$ 的单词对应的 hidden state，考虑到 $y_{t-1}$ 可能在输入中出现多次，$ρ_{tT}$ 更关注这些多次出现的词。</p>
<p><strong>整条路径：</strong></p>
<p>$$ζ(y_{t-1}) \longrightarrow{update} \ s_t \longrightarrow predict \ y_t \longrightarrow sel. read \  ζ(y_t)$$</p>
<p>一些结果：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/res.png" class="ful-image" alt="res.png"></p>
<h1 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><p>Copy 机制在文本摘要中的应用。传统 attention-based seq2seq 存在下面两个问题</p>
<ol>
<li><strong>无法正确产生文中的事实细节</strong><br>e.g. <em>Germany beat Argentina 3-2</em><br>尤其是对 OOV 或者 rare word。在训练中不常见的单词的 word embedding 质量也不高，很难 reproduce，而即使 word embedding 很好，对一些命名实体，像是人名之类的，含义都很相似，也很难准确的 reproduce 出来<br>所以作者引入了一个 <strong>pointer-generator network</strong>，在 generation 的基础上，加入了 copy 输入中的一些词的能力来提高摘要的准确性，相当于引入了部分 extractive summary 的能力</li>
<li><strong>倾向重复一些词组</strong><br>e.g. <em>Germany beat Germany beat Germany beat…</em><br>主要是因为 decoder 过程太过依赖于上一个单词，而不是 longer-term 的信息，所以一个重复的单词会 trigger 出死循环，比如上面这个例子就陷入了 Germany beat Germany beat Germany beat… 的死循环，产生不出 Germany beat Germany 2-0 这样的句子<br>所以有了 <strong>coverage</strong>，来追踪哪些部分已经被 summarize 过了，之后的 attention 就不会注意这些部分</li>
</ol>
<h2 id="Pointer-generation-network"><a href="#Pointer-generation-network" class="headerlink" title="Pointer-generation network"></a>Pointer-generation network</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/pointer_generator.png" class="ful-image" alt="pointer_generator.png">
<p>上图展示了 decoder 的第三个 step，预测 Germany beat 后面一个单词，像之前一样，我们会计算一个 attention distribution 和 vocabulary distribution，不过同时，也会计算一个产生概率 $p_{gen}$，是 0-1 间的一个值，表示从 vocabulary 中产生一个单词的概率，相应的 $1-p_{gen}$ 就是从输入中 copy 一个单词的概率</p>
<p>$$<br>  \begin{aligned}<br>  P(w) &amp;= p_{gen}P_{vocab}(w) + (1-p_{gen})\sum_{i:w_i=w}a_i^t \\<br>  p_{gen} &amp;= \sigma(w^T_{h^*}h^*_t + w^T_ss_t+w^T_xx_t+b_{ptr}) \\<br>  \end{aligned}<br>$$</p>
<p>其中 $h^*_t$ 是 context vector，在前面我们用的是 $c_i$ 来表示。如果 w 是 OOV， 那么 $P_{vocab}=0$，如果 w 没有在输入中出现，那么 $\sum_{i:w_i=w}a_i^t=0$。</p>
<h2 id="Coverage"><a href="#Coverage" class="headerlink" title="Coverage"></a>Coverage</h2><p>用 attention distribution 来记录哪些部分已经总结过了，来惩罚再次加入计算的部分。decoder 的每个时刻，有一个 coverage vector $c_t$ 来将记录之前所有时刻 attention 的总和。</p>
<p>$$c^t=\sum^{t-1}_{t’=0}a^{t’}$$</p>
<p>这个 coverage 会作为 attention 计算的一个输入<br>$$e^t_i=v^Ttanh(W_hh_i+W_ss_t+w_cc^t_i+b_{attn})$$</p>
<p>对应的，有一个 coverage loss<br>$$covloss_t=\sum_imin(a^t_i,c^t_i)$$</p>
<p>整体的 loss 就是<br>$$loss_t=-logP(w^*_t)+\lambda \sum_i min(a^t_i, c^t_i)$$</p>
<h1 id="Generating-Natural-Answers-by-Incorporating-Copying-and-Retrieving-Mechanisms-in-Sequence-to-Sequence-Learning"><a href="#Generating-Natural-Answers-by-Incorporating-Copying-and-Retrieving-Mechanisms-in-Sequence-to-Sequence-Learning" class="headerlink" title="Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning"></a>Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</h1><p>Copy 机制在问答系统中的应用。论文的模型用三种不同模式获取词汇并进行选取：<strong>用拷贝方式取得问句中的实体、用预测方式产生让答案更自然的连接词、用检索方式获取相关事实</strong> 并结合多个相关事实产生复杂问句的自然形式的答案。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/COREQA1.png" class="ful-image" alt="COREQA1.png">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Q: Do you know where was Jet Li from ?</div><div class="line">A1: Beijing.</div><div class="line">A2: Jet Li was born in Beijing. He is now a Singaporean citizen.</div></pre></td></tr></table></figure>
<p>传统的 QA 系统只会返回 A1，一个孤零零的实体，这对用户并不友好，A2 才是自然语言形式的答案。COREQA 利用 <strong>拷贝(copy)、检索(retrieval)和预测(prediction)</strong> 从不同来源获取不同类型的词汇，产生复杂问句的自然答案。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/COREQA2.png" class="ful-image" alt="COREQA2.png">
<p>具体过程，以 <strong>Do you know where was Jet Li from</strong> 这个问题为例来说明：</p>
<ol>
<li><strong>知识检索:</strong> 首先识别问题中的包含的 topic entities。这里我们识别出的 topic entity 是  Jet Li 。然后根据 entity 从知识库中检索出相关的三元组(subject, property, object)。针对李连杰这个实体，我们可以检索出(Jet Li, gender, Male)，(Jet Li, birthplace, Beijing)，(Jet, nationality, Singapore) 等三元组。</li>
<li><strong>编码(Encoder):</strong> 将问题和检索到的知识编码成向量<br><strong>问题编码:</strong> 双向 RNN(Bi-RNN)，把前向和后向对应的 hidden state 拼接起来形成每一时刻的 short-term memory $M_Q={h_t}$，两个方向 RNN 的最后一个向量拿出来拼在一起就得到向量 q 来表示整个问题<br><strong>知识编码:</strong> 使用了记忆网络(Memory Network)，对知识检索阶段得到的知识三元组 spo 分别进行编码得到 $e_s, e_p, e_o$，拼接成一个 $f_i$ 来表示这个三元组，所有这些三元组向量形成一个 list，用 $M_{KB}$ 表示<br><strong>分数计算: </strong> $S(q, s_t, f_j) = DNN_1(q, s_t, f_j)$</li>
<li><strong>解码(Decoder):</strong> 根据 $M_Q$ 和 $M_{KB}$ 来生成自然答案。单词预测有三种模式，predict-mode, copy-mode 和 retrieve-mode，predict-mode 和普通 seq2seq 原理相同，生成词汇表中的单词，copy-mode 从问句中复制单词，retrieve-mode 从知识库中选取单词。过程和 CopyNet 差不多，也有两种读取方式，一种是读取语义，一种是读取位置。</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/COREQA_formula1.png" class="ful-image" alt="COREQA_formula1.png">
<p>$p_{pr}, p_{co}, p_{re}$ 以及对应的 score function 和前面的 CopyNet 非常相似，包括之后的 state update 部分也和 CopyNet 差不多，不过是多了 $r_{kb}$ 而已。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Copy%20or%20Generate/COREQA_formula2.png" class="ful-image" alt="COREQA_formula2.png">
<blockquote>
<p><strong>参考链接：</strong><br><a href="https://zhuanlan.zhihu.com/p/26826765" target="_blank" rel="external">让问答更自然 - 基于拷贝和检索机制的自然答案生成系统研究 | 论文访谈间 #02</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> text summarization </tag>
            
            <tag> CopyNet </tag>
            
            <tag> Pre-Net </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[经典的端到端聊天模型]]></title>
      <url>http://www.shuang0420.com/2017/10/05/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p>介绍一下经典的 End-to-end 聊天模型及应用，包括检索式模型、生成式模型，以及 Google 邮件自动回复的应用。<br><a id="more"></a></p>
<p>主要涉及到下面几篇论文</p>
<ul>
<li><a href="https://arxiv.org/pdf/1506.08909.pdf" target="_blank" rel="external">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</a></li>
<li><a href="https://arxiv.org/abs/1506.05869" target="_blank" rel="external">A Neural Conversational Model</a></li>
<li><a href="http://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" target="_blank" rel="external">Smart reply, automated response suggestion in email</a></li>
<li><a href="https://arxiv.org/abs/1603.08023" target="_blank" rel="external">How NOT To Evaluate Your Dialogue System</a></li>
<li><a href="https://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts" target="_blank" rel="external">Zhengdong Lu &amp; Hang Li, 2013, A Deep Architecture for Matching Short Texts</a></li>
<li><a href="https://arxiv.org/abs/1408.6988" target="_blank" rel="external">Zongcheng Ji, et al., 2014, An Information Retrieval Approach to Short Text Conversation</a></li>
<li><a href="http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf" target="_blank" rel="external">Baotian Hu, et al., 2015, Convolutional Neural Network Architectures for Matching Natural Language Sentences</a></li>
<li><a href="https://pdfs.semanticscholar.org/73d8/26d4c2363701b88e3e234fe3b8756c0f9671.pdf" target="_blank" rel="external">Aliaksei Severyn, et al., 2015, Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="external">Building end-to-end dialogue systems using generative hierarchical neural network models</a></li>
</ul>
<h1 id="Modular-system-vs-end-to-end-system"><a href="#Modular-system-vs-end-to-end-system" class="headerlink" title="Modular system vs end-to-end system"></a>Modular system vs end-to-end system</h1><p>第一部分先简单比较一下对话系统中 modular system 和 end-to-end 的不同。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/end-to-end.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/end-to-end.png">
<p>如上图，传统的一个对话系统由 <strong>Speech Recognizer, Language Interpreter, State Tracker, Response Generator, Natural Language Generator, Speech Synthesizer</strong> 这么多个子模块拼接而成，这种系统称为 Modular system，在系统中每个组件单独训练，来优化一个单独的中间目标(如 slot-filling)。而 end-to-end system 相当于用一个系统替代了上图中框起来的四个组件，来比较一下</p>
<p><strong>Modular system vs end-to-end system</strong></p>
<ul>
<li><strong>目标函数</strong><br>modular system 有两个及以上的目标函数<br>end-to-end 通常只有一个目标函数</li>
<li><strong>所需数据</strong><br>modular system 更容易训练，需要的数据少<br>end-to-end 需要大量数据</li>
<li><strong>人工标注</strong><br>modular system 需要大量的人工的特征工程，需要预先定义 state, action spaces 等等<br>end-to-end 不需要预先定义的 state/action spaces</li>
<li><strong>效果</strong><br>modular system 在 highly structured tasks/narrow domain 上的效果更出色，但泛化能力有限<br>end-to-end 在 general purpose 的效果上比较好</li>
</ul>
<h1 id="Retrieval-based-models-vs-Generative-models"><a href="#Retrieval-based-models-vs-Generative-models" class="headerlink" title="Retrieval-based models vs Generative models"></a>Retrieval-based models vs Generative models</h1><p>对话模型分 <strong>检索式(Retrieval-based models )</strong> 和 <strong>生成式(Generative models)</strong> 两种，检索式的聊天模型有一个预先定义好的模板库，给定一个 query，来从模板库里选择最好的 response。回复的产生依赖于模板库，不可能产生模板库没有的句子。而生成式模型不依赖于模板库，而是直接产生的，生成式模型的方法大多依赖于机器翻译的技术，但不是从一个语言翻译到另一个语言，而是从一个输入映射到回复。</p>
<p>两种方法都有利有弊，检索式模型得到的回复不会产生语法错误，但没法处理在模板库里不存在回复的用户输入，另外，检索式模型也很难结合上下文信息。而生成式模型更加的“聪明”，可以结合语境，然而更难训练，也更容易犯语法错误(尤其是长句)，需要的训练数据也很大。</p>
<h1 id="Retrieval-Based-Models"><a href="#Retrieval-Based-Models" class="headerlink" title="Retrieval-Based Models"></a>Retrieval-Based Models</h1><p>可以看做是一个 <strong>检索/排序/匹配</strong> 问题，有一个预先定义好的模板库(看做是检索系统的文档集)，给定一个 query，来从模板库里选择最好的 response，这里需要计算一个 score(query, response) 来衡量 query 和 response 的匹配程度，score 越高，response 越可能是一个合适的回复。response 也可以替换成标准 query，这就把问题转换为用户 query 和标准 query 的一个相似度计算问题，这里的 score 就是相似度分数。</p>
<p>需要学习的一个是语义表达，一个是 score 的计算。score 可以单独用传统方法做，也可以在神经网络的 MLP 层做，还可以在语义表达产生的过程中做。</p>
<p>有下面一些经典的论文，(Q, Q’)，(Q, A) 或 (Q, D) 的匹配在这里统一表示为 (Q, D)，D 可以是标准 query，可以是 answer，也可以是标准 query + answer</p>
<ol>
<li><a href="https://papers.nips.cc/paper/5019-a-deep-architecture-for-matching-short-texts" target="_blank" rel="external">Zhengdong Lu &amp; Hang Li, 2013, A Deep Architecture for Matching Short Texts</a><br>DeepMatch，先用 (Q, D) 语料训练 LDA 主题模型，得到其 topic words，这些主题词被用来检测两个文本是否存在语义相关性(Localness)；每次指定不同的 topic 个数分别训练 LDA 模型，得到几个不同分辨率的主题模型(Hierarchy)，高分辨率的 topic words 更具体，低分辨率的更抽象，这可以避免短文本词稀疏带来的问题，并得到不同的抽象层级</li>
<li><a href="https://arxiv.org/abs/1408.6988" target="_blank" rel="external">Zongcheng Ji, et al., 2014, An Information Retrieval Approach to Short Text Conversation</a><br>从不同角度构造匹配特征，作为 ranking 模型的特征输入，构造的特征包括：1）Query-ResponseSimilarity；2）Query-Post Similarity；3）Query-Response Matching in Latent Space；4）Translation-based Language Model；5）Deep MatchingModel；6）Topic-Word Model；7）其它匹配特征</li>
<li><a href="http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf" target="_blank" rel="external">Baotian Hu, et al., 2015, Convolutional Neural Network Architectures for Matching Natural Language Sentences</a><br>基于 CNN，Q 和 D 分别经过多次一维卷积和池化，得到的固定维度的两个 sentence embedding，然后输入到 Siamese 结构的 MLP 层，得到文本的相似度分数。这种方法的监督信号在最后的输出层才出现，在这之前，Q 和 D 的 embedding 相互独立生成，可能会丢失语义相关信息，所以有第二种结构，在第 1 层卷积后就把 Q 和 D 做融合，融合方式是分别对 Q 和 D 做 1D 卷积，然后针对两者卷积得到的 feature map，构造其所有可能的组合(在两个方向上拼接对应的 feature map)，这样就构造出一个 2D 的 feature map，然后对其做 2D MAX POOLING，多次 2D 卷积和池化操作后，输出固定维度的向量，接着输入 MLP 层，最终得到文本相似度分数。实验表明优于 DeepMatch</li>
<li><a href="https://pdfs.semanticscholar.org/73d8/26d4c2363701b88e3e234fe3b8756c0f9671.pdf" target="_blank" rel="external">Aliaksei Severyn, et al., 2015, Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a><br>分别对 Q 和 D 做 wide 1D 卷积和 MAX 池化，得到文本的语义向量，接着通过 M 矩阵变换得到语义向量的相似度，然后把 Q 语义向量、Q&amp;D 的语义相似度、D 语义向量、外部特征拼接成 n 维向量，输入一个非线性变换隐层，最终用 softmax 做概率归一化。用 softmax 的输出作为监督信号，采用 cross-entropy 作为损失函数进行模型训练</li>
</ol>
<p>这里介绍的是 <strong>Dual-Encoder</strong> 模型(Ryan Lowe, et al., 2016, The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems)，通过对偶的 RNN 模型分别把 context 和 response 编码成语义向量，然后通过 M 矩阵变换计算语义相似度，相似度得分作为监督信号在标注数据集上训练模型。</p>
<h2 id="句子表达"><a href="#句子表达" class="headerlink" title="句子表达"></a>句子表达</h2><p>先来看一下怎么表达 query 和 response。语义特征方面很容易想到 TFIDF，然而它忽略了词序，表达效果没那么强，所以考虑用 sentence embedding。sentence embedding 可以用 RNN/LSTM 来获取。还是 Encoder-decoder 的思想，不过这里把 decoder 给替换了成了另一个 encoder，也就成了 Dual-RNN 的结构。两个独立的 RNN 分别对 context/ query和 response 进行编码，每个 RNN 最后一个 hidden state 相当于是对整个 input(context/response) 的一个总结。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/dual_lstm.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/dual_lstm.png">
<h2 id="分数计算"><a href="#分数计算" class="headerlink" title="分数计算"></a>分数计算</h2><p>模型的学习目标其实是一个 binary 分类器<br>$\sigma (score(Query, Response_true)) -&gt; 1$<br>$\sigma (score(Query, Response_false)) -&gt; 0$</p>
<p>先来看下<strong>如何计算分数</strong>。<br>$$p(flag=1|c,r,M)=\sigma(c^TMr+b)$$</p>
<p>这个过程可以看做是一个产生模型，给定 input response，用 $c’=Mr$ 产生一个 context (M 是 dxd 的参数矩阵)，然后利用点乘来及计算这个产生的 context 和真实 context 的相似度分数，再用 sigmoid 将这个分数转化为概率，最小化交叉熵损失函数来将进行训练。</p>
<p>简单一个例子来理解这个过程，假设下图 5x5 的表格，i 行 j 列代表 $(query_i, response_j)$，我们希望对角线的 probability 最大，因为对角线对应着正确的 (query, response)，真实的 label 是一个 identity matrix，我们的 prediction 是 5x5 的 score，现在对每一行进行  softmax cross-entropy 损失函数的计算，其实就相当于直接优化 retrieval metrics (Recall@k)，即 (query, response) 在所有 pair 里的排名。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/matrix.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/matrix.png">
<p>从代码角度理解一下，下面是训练阶段的一个 minibatch 过程，假设 minibatch 大小为 5</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">if self.is_training:</div><div class="line">    # 训练阶段, 使用 minibatch 内其他样本的 response 作为 negative response</div><div class="line">    # 用 LSTM 做 encoding，query/response 都是 5x128 的表达</div><div class="line">    # W: 128x128</div><div class="line">    response_final_state = tf.matmul(response_final_state[-1].h, W)</div><div class="line">    # 得到 5x5 的 score matrix</div><div class="line">    logits = tf.matmul(</div><div class="line">        a = query_final_state[-1].h, b = response_final_state,</div><div class="line">        transpose_b = True)</div><div class="line">    self.losses = tf.losses.softmax_cross_entropy(</div><div class="line">        onehot_labels = self.labels,</div><div class="line">        logits = logits)</div><div class="line">    self.mean_loss = tf.reduce_mean(self.losses, name=&quot;mean_loss&quot;)</div><div class="line">    train_loss_summary = tf.summary.scalar(&apos;loss&apos;, self.mean_loss)</div><div class="line">    self.training_summaries = tf.summary.merge(</div><div class="line">                             inputs = [train_loss_summary], name=&apos;train_monitor&apos;)</div><div class="line"></div><div class="line">    opt = tf.train.AdamOptimizer(</div><div class="line">        learning_rate=self.args.learningRate,</div><div class="line">        beta1=0.9,</div><div class="line">        beta2=0.999,</div><div class="line">        epsilon=1e-08</div><div class="line">    )</div><div class="line">    self.optOp = opt.minimize(self.mean_loss)</div></pre></td></tr></table></figure>
<p><strong>测试阶段</strong>，计算的是 Recall@k (给定一个 query，选择 k 个最有可能的 response，看正确的 response 在不在这 k 个里)。举个例子，从整个 test data/validation data 随机抽取 19 个错误答案，对每一个样本，计算 20 个 response 的 score，看真实回复的 score 是否排名前 k。response 矩阵连续的 20 行对应一个 query，第 1 行、21行、41行…对应真实的 response，其他是错误 response。将 query 复制 20 次，就得到 100x128 的 query 和 100x128 的 response (假设 rnn_dim=128)，点乘得到每个 (query, response) 的 score，也就是 100x100 的矩阵，然后再 reshape 成 5x20 的矩阵，再计算 Recall@k</p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">else:</div><div class="line">   # 测试阶段，每一个样本的response是固定的</div><div class="line">   # query: [batch_size, rnn_dim]</div><div class="line"># respones: [batch_size x 20, rnn_dim]</div><div class="line">   # [batch_size x 20, rnn_dim]</div><div class="line">   response_final_state = tf.matmul(response_final_state[-1].h, W)</div><div class="line">   query_final_state = tf.reshape(</div><div class="line">           tf.tile(query_final_state[-1].h, [1, 20]),</div><div class="line">           [-1, self.args.hiddenSize])</div><div class="line">   # [batch_size, batch_size x 20]</div><div class="line">   logits = tf.reduce_sum(</div><div class="line">           tf.multiply(</div><div class="line">               x = query_final_state,</div><div class="line">               y = response_final_state),</div><div class="line">           axis = 1,</div><div class="line">           keep_dims = True)</div><div class="line">   logits = tf.reshape(logits, [-1, 20])</div><div class="line">   # top_k percentage</div><div class="line">   self.response_top_1 = tf.reduce_mean(</div><div class="line">           tf.cast(tf.nn.in_top_k(</div><div class="line">               predictions = logits,</div><div class="line">               targets = self.targets,</div><div class="line">               k = 1,</div><div class="line">               name = &apos;prediction_in_top_1&apos;),</div><div class="line">           dtype = tf.float32))</div><div class="line">   self.response_top_3 = tf.reduce_mean(</div><div class="line">           tf.cast(tf.nn.in_top_k(</div><div class="line">               predictions = logits,</div><div class="line">               targets = self.targets,</div><div class="line">               k = 3,</div><div class="line">               name = &apos;prediction_in_top_3&apos;),</div><div class="line">           dtype = tf.float32))</div><div class="line">   self.response_top_5 = tf.reduce_mean(</div><div class="line">           tf.cast(tf.nn.in_top_k(</div><div class="line">               predictions = logits,</div><div class="line">               targets = self.targets,</div><div class="line">               k = 5,</div><div class="line">               name = &apos;prediction_in_top_5&apos;),</div><div class="line">           dtype = tf.float32))</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/recall_k.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/recall_k.png">
<h1 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h1><p>产生模型并不是从模板库里选一个分数最高的 response 出来，而是去自动生成这样一个 response。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/seq2seq.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/seq2seq.png"></p>
<p>将 MT 问题中引入的 seq2seq 模型应用到对话任务上。给出 (query, response) 以后，seq2seq 对 query 进行编码，最后一个 hidden state 包含 query 的所有信息，结合开始标记 EOS 进行解码，得到 response。不过这种方法对上下文依赖考虑有限，<strong>HRED(Hierarchical Recurrent Encoder-Decoder)</strong> 为解决这个问题做了改进。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/HRED.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/HRED.png">
<p>HRED 在传统 encoder-decoder 模型上，额外增加了一个 encoder，相比于普通的 RNN-LM 来说，考虑了 turn-taking nature，能够对上下文进行建模，有助于信息/梯度的传播，从而实现多轮对话。有下面三个阶段：</p>
<ol>
<li><strong>encoder RNN</strong><br>第一个 encoder 和标准的 seq2seq 相同，将一句话编码到固定长度的 utterance vector，也就是 RNN 的 last hidden state</li>
<li><strong>context RNN</strong><br>n 个句子的 utterance vector 作为第二个 encoder 也就是 <strong>context-level encoder</strong> 各个时间上的的输入，对应长度为 n 的 sequence，产生一个 context vector 实现对语境的编码，也就是 RNN 的 output (注意这里不是 last hidden state)</li>
<li><strong>decoder RNN</strong><br>上一个句子的 utterance vector 作为 response 的初始状态，目前为止产生的 context vector 和上一个单词的 word embedding 拼接作为 decoder 的输入</li>
</ol>
<p>然而 HRED 相对于传统的 Seq2Seq 模型的提高并不明显，bootstrapping 的作用更加明显。一方面可以用 pre-trained word embedding，另一方面可以使用其他 NLP 任务的数据预训练我们的模型，使得模型的参数预先学到一些对自然语言的理解，再来学习聊天任务。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/seq2seq_eval.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/seq2seq_eval.png">
<p>看上面的例子，可以发现 seq2seq 模型可以在一定程度上记住知识，理解语境，进行简单的推理(图左)，然而并不能保留记忆和性格，对相同语义的不同表达会返回不同的答复(图右)。另外要注意的是，这个场景下 seq2seq 的训练目标和真实目标实际是不一样的，尤其在闲聊场景中。训练阶段关注的是真实 response 出现的概率和怎么最大化这个概率，而测试阶段或者说真实场景下，对话侧重于交流信息，以及长时间的连贯性，考虑到回复的灵活性(如一个 query 可以有多种合适的回复)，以及经产生模型的自由度(并不需要在模板库里面选择回复，可以是全新的句子)，因此使用合适的 Metric 来衡量产生的句子实际是非常困难的问题。</p>
<blockquote>
<p><strong>objective function</strong> being optimized does not capture the <strong>actual objective achieved through human communication</strong>, which is typically <strong>longer term</strong> and based on <strong>exchange of information</strong> rather than <strong>next step prediction</strong></p>
</blockquote>
<h1 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h1><p>Metrics 的设计目标是<strong>使得 metric 的判断和人为判断尽量相似</strong>。</p>
<h2 id="Retrieval-Metrics-Recall-k"><a href="#Retrieval-Metrics-Recall-k" class="headerlink" title="Retrieval Metrics: Recall@k"></a>Retrieval Metrics: Recall@k</h2><p>Recall@k 是信息检索里的评估方法，给定一个 query，选择 k 个最有可能的 response，看正确的 response 在不在这 k 个里。</p>
<h2 id="Generative-Metrics"><a href="#Generative-Metrics" class="headerlink" title="Generative Metrics"></a>Generative Metrics</h2><ol>
<li>相对于机器翻译，对话中回复的选择空间大很多； 看起来完全无关的两句话都可以是合适的回复</li>
<li>而这两个正确的回复如果不看context的话，无论是从词频 还是语义来看都是不相关不想似的句子</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/eva_eg1.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/eva_eg1.png">
<h3 id="Word-Overlap-based-Metrics"><a href="#Word-Overlap-based-Metrics" class="headerlink" title="Word Overlap-based Metrics"></a>Word Overlap-based Metrics</h3><p>主要有 <strong>BLEU</strong>，<strong>ROUGE</strong> 和 <strong>METEOR</strong>，最初用于衡量机器翻译的效果。<strong>BLEU</strong> 主要看人为/测试的句子里的单词的 overlap (机器产生的待评测句子中的 ngram 正确匹配人工产生的参考句子中 ngram 与机器产生的句子中所有 ngram 出现次数的比值)，加入 BP(Brevity Penalty) 惩罚因子可以评价句子的完整性。然而 BLEU 不关心语法，只关心内容分布，适用于衡量数据集量级的表现，在句子级别的表现不佳。</p>
<blockquote>
<p>“BLEU is designed to approximate human judgement at a corpus level, and performs badly if used to evaluate the quality of individual sentences.”——wikipedia</p>
</blockquote>
<p><strong>ROUGE</strong> 是一种基于召回率的相似性度量方法，与 BLEU 类似，但计算的是 ngram 在参考句子和待评测句子的共现概率，包含 ROUGE-N, ROUGE-L(最长公共子句, Fmeasure), ROUGE-W(带权重的最长公共子句, Fmeasure), ROUGE-S(不连续二元组, Fmeasure) 四种，具体不多说。</p>
<p><strong>METEOR</strong> 改进了 BLEU，考虑了参考句子和待评测句子的对齐关系，和人工判断的结果有更高的相关性。</p>
<h3 id="Embedding-based-Metrics"><a href="#Embedding-based-Metrics" class="headerlink" title="Embedding-based Metrics"></a>Embedding-based Metrics</h3><p>侧重比较生成的句子和真实样本的语义相似度。</p>
<ul>
<li><strong>Embedding average score</strong><br>将句中每个单词的词向量作平均来作为句子的特征，计算生成的句子和真实句子的特征的 cosine similarity</li>
<li><strong>Greedy matching score</strong><br>寻找生成的句子和真实句子中最相似的一对单词，把这对单词的相似度近似为句子的距离</li>
<li><strong>Vector extrema score</strong><br>对句中单词词向量的每一个维度提取最大(小)值作为句子向量对应维度的数值，然后计算cosine similarity</li>
</ul>
<h2 id="Human-judgement"><a href="#Human-judgement" class="headerlink" title="Human judgement"></a>Human judgement</h2><blockquote>
<p> “We ﬁnd that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models”</p>
</blockquote>
<p>在 <a href="https://arxiv.org/abs/1603.08023" target="_blank" rel="external">How NOT To Evaluate Your Dialogue System</a> 这篇论文中，宣称和人工判断相比，上述的所有 metric 都是垃圾</p>
<ul>
<li>在闲聊性质的数据集上，上述 metric 和人工判断有一定微弱的关联 (only a small positive correlation on chitchat oriented Twitter dataset)</li>
<li>在技术类的数据集上，上述 metric 和人工判断完全没有关联(no correlation at all on the technical UDC)</li>
<li>当局限于一个特别具体的领域时，BLEU会有不错的表现</li>
</ul>
<h2 id="Learning-to-Evaluate-Dialogue-Responses"><a href="#Learning-to-Evaluate-Dialogue-Responses" class="headerlink" title="Learning to Evaluate Dialogue Responses"></a>Learning to Evaluate Dialogue Responses</h2><p>可以尝试使用机器学习的方法来学习一个好的 metric，用语境 c，真实回复 r，机器回复 $\hat r$，训练一个 regression 模型，使得 score 和人工打分的 score 接近。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/eva_formula.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/eva_formula.png">
<h1 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h1><p>讲一个生成式模型的具体应用，然而用预先定义好的模板库对生成的 response 做了一个限制。具体任务是如何对邮件进行自动回复。来自 <a href="http://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" target="_blank" rel="external">Smart reply, automated response suggestion in email</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/email.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/email.png">
<p>达到的目标是收到一封邮件，系统发现这个邮件适合 Smart Reply，就会自动推荐 3 个回复语句给用户选择。</p>
<p>框架面临的几个挑战是</p>
<ul>
<li><strong>Response quality</strong><br>怎么保证生成的回复的质量，如果质量不高，根本没有推荐的必要</li>
<li><strong>Utility</strong><br>怎样选择推荐的回复，能最大化用户选中的概率</li>
<li><strong>Scalability</strong><br>怎样提高效率，大规模处理</li>
<li><strong>Privacy</strong><br>在开发系统的过程中怎么保护隐私，加密</li>
</ul>
<p>如何应对上面的挑战，也是文章的亮点</p>
<ul>
<li><strong>Response selection</strong><br>对应 Scalability 问题<br>将模板库里的句子组织成一个 trie，从左到右用 beam search 的方法进行每次遍历，只保留在 trie 中出现的 hypothese，这样对每个 response candidate 评分的复杂度就由 O(Rl) 降到了 O(bl)，R 是模板库的大小，l 是最长回复的长度，b 是 beam size</li>
<li><strong>Response set generation</strong><br>对应 Response quality, Scalability, 以及 Utility 问题<br>生成一个带 intent 标记的模板库，回复只从这个模板库里产生</li>
<li><strong>Diversity</strong><br>对应 Utility 问题<br>去掉 generic 的回复，兼顾正面、负面回复，在得分最高的回复中，每个 intent 只选择一个回复</li>
<li><strong>Triggering model</strong><br>对应 Utility 问题<br>Binary 分类器判断是否要 trigger 自动回复，不需要回复的，不适合短回复的</li>
</ul>
<p>具体过程是，来一封邮件，首先看是否 trigger Smart Reply(采用一个 feedforward neural network)，如果是，就跑一遍 LSTM，生成候选的 n 个 response。</p>
<p>特征方面，预处理后的邮件采取的特征有 unigram, bigram，发件方是否在收件方的地址簿里，是否在收件方的社交网络里，收件方是否在过去回复过发件方等等。稀疏特征类型(如 unigram, bigram)的 embedding 是单独训练的，然后每个稀疏类型特征下的 embedding 进行加总，再和 dense feature(如数值、布尔类型的特征)拼接作为输入。</p>
<p>重点看一下模板库的生成。</p>
<h2 id="Response-set-generation"><a href="#Response-set-generation" class="headerlink" title="Response set generation"></a>Response set generation</h2><p>模板库的存在可以限制产生的 response 的范围，提高 response 的质量以及选择回复的速度。另外，这里模板库里的句子都有一个 intent 标记，而标记了 intent 类别的回复模板库可以增加回复的 diversity。</p>
<p>产生模板库用了 <strong>Expander graph learning approach</strong>，是一种半监督的方法。<br>搜集邮件数据后用传统 nlp 方法进行预处理，包括</p>
<ul>
<li>去掉非英语的样本</li>
<li>Tokenization</li>
<li>将内容分割成句子为单位</li>
<li>使用特殊符号替换不常用的单词(e.g. 人名，url，邮件地址）</li>
<li>去掉引用和转发的邮件部分</li>
<li>去掉问候和致敬部分</li>
</ul>
<p>处理好的数据中只选择<strong>短的、最常出现的、匿名的</strong>回复。</p>
<p>首先利用 dependency parser 将类似的句子如 “Thanks for your kind update”,“Thank you for updating!”, , “Thanks for the status update” 等转换为 canonical 形式，即 “Thanks for the update.”<br>然后进行做语义聚类，每个 cluster 对应一个意图（intent）.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/graph.png" class="ful-image" alt="%E7%BB%8F%E5%85%B8%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/graph.png"></p>
<p><strong>过程：</strong></p>
<ol>
<li><strong>初始化:</strong> 标记～100个类别(cluster)，每个类别～3个人工选择的样本</li>
<li>使用 (original, response), (response1, response2, feature) 对模板库里的样本建立关系</li>
<li>使用 Expander 算法给未标记的句子打标记</li>
<li>对于新的类别的发现，<strong>Iteration：</strong><br><strong>Inference:</strong> 用 label propagation 算法迭代 5 次推测未标记样本的 cluster 类别<br><strong>Update:</strong> 从图中剩下的未标记的样本中随机 sample 100 个作为潜在的新类别，用 canonicalized representation 来标记<br>​ 重新运行 label propagation 直到收敛(不再发现新的类别，或者每个类的成员不再变化)</li>
<li>最后进行 <strong>Validation:</strong> 提取每个 cluster 的 top-k 个回复样本，人工验证</li>
</ol>
<blockquote>
<p>参考链接<br><a href="https://zhuanlan.zhihu.com/p/26879507" target="_blank" rel="external">PaperWeekly 第37期 | 论文盘点：检索式问答系统的语义匹配模型（神经网络篇）</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Chatbot </tag>
            
            <tag> 多轮对话 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Discourse Analysis]]></title>
      <url>http://www.shuang0420.com/2017/09/20/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/</url>
      <content type="html"><![CDATA[<p>CMU 11611 笔记。讲语篇分析的一些概念，包括 coreference, cohesion, speech acts 等。<br><a id="more"></a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><blockquote>
<p><strong>Discourse</strong> is the coherent structure of language above the level of sentences or clauses. A discourse is a coherent structured group of sentences.</p>
</blockquote>
<p><strong>Discourse Analysis</strong> 中文对应过来通常是<strong>语篇/篇章分析</strong>。前面讲到了语素级别的(morphemes)、词汇级别的(lexical)、短语级别的(segmentation)、句子级别的(syntactic parsing)种种概念及分析，现在到了 <strong>beyond sentences</strong> 这一级别，只有两句或两个句子/从句以上的句子，才被称为 discourse。广义来讲，discourse 其实有很多含义，大多都能归到下面三类。</p>
<ul>
<li><strong>Language beyond sentences</strong><br>语言学家(Linguistists)关注的概念，分析句子之间是怎么<strong>联系/衔接</strong>的，也是本篇的重点</li>
<li><strong>Language in use</strong><br>可以理解为实际发生的对话(conversation)，这是应用语言学家(Applied Linguistists)会关注的，比如说他们会来研究医患人员的对话来看医生是怎么在对话过程中建立权威的(结合 speech acts)</li>
<li><strong>A broader range of social practice that includes nonliguistic and nonspecific instances of language</strong><br>不止是语言学的内容，而是 linguistic + social practice + ideological assumption 结合的产物，社会学家对这个更感兴趣，会关注特定时间特定场景下的行为，比如来研究种族歧视等社会现象(结合 contexts)</li>
</ul>
<p>具体见<a href="A broader range of social practice that includes nonliguistic and nonspecific instances of langue">The Handbook of Discourse Analysis</a></p>
<p>语篇分析的应用很广泛，比如说<strong>自动文摘、自动作文评分、会议理解、对话系统</strong>等。</p>
<h1 id="Coreference"><a href="#Coreference" class="headerlink" title="Coreference"></a>Coreference</h1><p>一个重要的概念是 <strong>coreference</strong>，表示共指关系。自然语言的所指现象非常丰富，有<strong>不定名词短语(indefinite noun phrase)、有定名词短语(definite noun phrase)、代词(pronoun)、指示词(demonstrative)、单个复指(one-anaphora)</strong>等，所指对象类型有<strong>推理对象(inferrable)、不连续集(discontinuous set)和类属(generic)</strong>。下面主要以<strong>代词指代</strong>来讲共指的概念。</p>
<p>先来看下两个概念，<strong>anaphora</strong> 和 <strong>cataphora</strong>，两者都是指代，不同的是 referent 和 referring expression 出现的先后顺序</p>
<ul>
<li><strong>anaphora:</strong> the use of a word referring to or replacing a word used earlier in a sentence</li>
<li><strong>cataphora:</strong> the use of a word or phrase that refers to or stands for a later word or phrase</li>
</ul>
<p><strong>E.g., Anphora </strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">I went to see my grandfather at the hopital.</div><div class="line">The old man has been there for weeks.</div><div class="line">He had surgery a few days ago.</div></pre></td></tr></table></figure></p>
<p>the old man 和 he 是 referring expression，在这个场景下也是 anaphora，都指代前面的 my grandfather，my grandfather 又称为先行词(antecedents)。</p>
<p><strong>Referring expressions:</strong> the old man, he<br><strong>Antecedents:</strong> my grandfather</p>
<p><strong>E.g., Cataphora </strong><br>先出现 She，再出现 She 指代的 Mary。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">– R: She didn’t like it!</div><div class="line">– D: What do you mean?</div><div class="line">– R: She didn’t like it!</div><div class="line">– D: Who didn’t like what?</div><div class="line">– R: Mary.</div><div class="line">– D: What didn’t Mary like?</div><div class="line">– R: She didn’t like the article I read in the *New Yorker*.</div></pre></td></tr></table></figure></p>
<p>指代问题在单个句子或者多个句子中都会出现，这一章讲 discourse，只讨论多个句子中的指代消解问题(<strong>Reference Resolution</strong>)。</p>
<h2 id="Pronoun-reference-resolution"><a href="#Pronoun-reference-resolution" class="headerlink" title="Pronoun reference resolution"></a>Pronoun reference resolution</h2><p>看一个简单的代词指代消解的例子。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">John saw Mary in the park. As every morning, she was walking her dog.</div></pre></td></tr></table></figure></p>
<p>我们需要来判断 she 指代谁。<br>第一步，找到所有的 candidate referents，也就是名词短语。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">- John</div><div class="line">- Mary</div><div class="line">- The park</div><div class="line">- Every morning</div><div class="line">- Her dog</div></pre></td></tr></table></figure></p>
<p>直觉上看，She 必须是 Person，所以 morning, park 删除，John 是男性，gender 不符，删除，her dog 难以确定 gender，最有可能的是 Mary。这个例子非常简单，仅仅通过一致性约束就判断出了正确的 referent，很多场景比这复杂的多。下面先来看一下代词指代消解相关的一些规则/模型。</p>
<h3 id="Filters-句法和语义约束"><a href="#Filters-句法和语义约束" class="headerlink" title="Filters: 句法和语义约束"></a>Filters: 句法和语义约束</h3><p>根据下面的一些规则约束可以 <strong>排除</strong> 一些 candidate referents。</p>
<ul>
<li><strong>Agreement constraints (一致性)</strong><br>gender, number, person, animacy</li>
<li><strong>Binding theory:</strong> <strong>reflexive</strong> required/prohibited (反身代词)<br>反身代词可以用于同指包含它的最近邻从句的主语，而非反身代词不能同指该主语<br>– John bought himself a new Ford. [himself=John]<br>– John bought him a new Ford. [him≠John]<br>– John said that Bill bought him a new Ford. [him≠Bill]<br>– J said that B bought himself a new F. [himself=Bill]<br>– He said that he bought J a new Ford. [both he≠J]</li>
<li><strong>Selectional restrictions</strong><br>选择限制，动词对论元(argument)施加的选择限制<br>John parked his car in the garage after driving it around for hours.<br>动词 drive 要求直接宾语是能够驾驶的事物，比如说 car</li>
</ul>
<h3 id="Preferences-优先级"><a href="#Preferences-优先级" class="headerlink" title="Preferences: 优先级"></a>Preferences: 优先级</h3><p>下面的一些偏好规则说明哪些 referents 有更大的可能性是正确的。</p>
<ul>
<li><strong>Parallelism</strong></li>
<li><strong>Sentence ordering: Recency </strong><br>邻近话段引入的实体比较远话段引入的显著性更高</li>
<li><strong>Grammatical Role</strong>: subj&gt;obj&gt;others</li>
<li><strong>Repeated mention</strong><br>Billy had been drinking for days.<br>He went to the bar again today. Jim went with him. He ordered rum.</li>
<li><strong>Verb semantics</strong><br>有些动词的出现会对其中一个论元的位置产生语义上的强调，这会造成对其后代词的理解偏差<br>John <strong>phoned/criticized</strong> Bill. He lost the laptop.<br>如果是 phone，明显应该是 John 丢了电脑，如果是 criticize，应该是 Bill 丢了。这被认为是动词的“隐含因果关系”，criticize 事件的隐含因果被认为是动词宾语，而 phone 被认为是动词主语</li>
</ul>
<h3 id="Discourse-model"><a href="#Discourse-model" class="headerlink" title="Discourse model"></a>Discourse model</h3><p>很多指代消解算法的第一步是建立 discourse model，discourse model 包含了 discourse 所指实体的表示以及它们所承担的关系，模型有两个基本操作，如下图所示，当第一次提到所指对象时，我们称它的表示为被唤起(evoke)而进入模型，之后当再次提及时，我们称从模型中访问(access)它的表示。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/discourse%20model.png" class="ful-image" alt="discourse%20model.png"></p>
<h2 id="Computational-approaches-to-pronouns-reference-resolution"><a href="#Computational-approaches-to-pronouns-reference-resolution" class="headerlink" title="Computational approaches to pronouns reference resolution"></a>Computational approaches to pronouns reference resolution</h2><p>有了前面的知识储备，现在可以来看一下代词指代消解的传统算法。</p>
<h3 id="Hobbs-Algorithm"><a href="#Hobbs-Algorithm" class="headerlink" title="Hobbs Algorithm"></a>Hobbs Algorithm</h3><p>非常早期的，1978 年 Hobbs 提出一种不依赖任何语义知识或语篇信息，只利用语法规则和完全解析树信息的指代消解算法，又叫<strong>树查询算法(Tree Search Algorithm)</strong>。算法会遍历当前句子和先行句(preceding sentences)的解析树，根据 binding theory, recency, 和 grammatical role preferences 选择合适的 NP 作为 referent，这种方法需要 parser，需要 gender 和 number 信息，也需要用于确定 NP gender 的 head rule 和 wordnet。现在实际系统中很少直接使用，一般只会拿来做 baseline。</p>
<p><strong>算法过程:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/HOBBS.png" class="ful-image" alt="HOBBS.png"></p>
<h3 id="Resolution-of-Anaphora-Procedure-RAP"><a href="#Resolution-of-Anaphora-Procedure-RAP" class="headerlink" title="Resolution of Anaphora Procedure(RAP)"></a>Resolution of Anaphora Procedure(RAP)</h3><p>1994 年 Lappin 和 Lease 提出的，综合考虑了 recency 和基于句法的优先关系的影响。使用 McCord 提出的 Slot Grammar 获得文档的句法结构，根据过滤规则过滤掉不合适的 referent，然后通过手工加权的各种语言特征计算剩下的 referent 重要性，确定 referent。1996 年 Kennedy 等人对 RAP 做了修改和扩展，避免了构建完整的解析树，只用 NLP 工具预处理得到词性标注和句法功能标注等浅层信息，2005 年 Luo 等人继续做了改进，尝试用最大熵模型来自动确定各种语言特征的权值。下面来看一下基础版本的 RAP 算法。</p>
<p>两个步骤，<strong>discourse model 的更新和代词的判定</strong>。遇到一个唤起的新的实体的名词短语时，必须为它添加一个表示以及用于计算的<strong>显著度(salience)</strong>，显著度由一组<strong>显著因子(salience factor)</strong>所指派的权值综合来计算。每处理一个新句子，discourse model 中的每个因子为实体所指派的权重就减一半。</p>
<p><strong>显著因子及其权重：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/salience%20weights.png" class="ful-image" alt="salience%20weights.png"></p>
<p><strong>RAP 过程:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/RAP.png" class="ful-image" alt="RAP.png"></p>
<p><strong>Example:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs1.png" class="ful-image" alt="egs1.png"></p>
<p>=&gt; he 指代 John</p>
<p><strong>Halve</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs2halv.png" class="ful-image" alt="egs2halv.png"></p>
<p>加入 he 的分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs3he.png" class="ful-image" alt="egs3he.png"></p>
<p>根据规则 =&gt; it 指代 Integra，加入 it 的分数</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs4.png" class="ful-image" alt="egs4.png">
<p>加入 Bill<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs5.png" class="ful-image" alt="egs5.png"></p>
<p><strong>Halve</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/egs6.png" class="ful-image" alt="egs6.png"></p>
<p>……</p>
<h3 id="Centering-theory"><a href="#Centering-theory" class="headerlink" title="Centering theory"></a>Centering theory</h3><p><strong>Centering algorithm</strong> 和 Lappin &amp; Leass 算法一样都采用了 discourse model 的表示，但同时引入了 center 的概念，center 表示语段中心成分，在语篇中联系不同语段的实体(entity)，在话语中的任何定点都有一个单独的实体被作为 center。center 细分为<strong>语段潜在中心</strong> (forward-looking center $C_f$) 和<strong>语段现实中心</strong>(backward-looking center $C_b$)。</p>
<p>对于由 $U_1, …, U_t$ 构成的语篇，语段 $U_n$ 的 $C_f$ 和 $C_b$ 由下面的制约条件：</p>
<ul>
<li>$C_f(U_n)$ 是当前语段中的所有实体组成的集合，在下一语段中至少会部分实现(realize)</li>
<li>$C_b(U_n)$ 是 $C_f(U_{n-1})$ 中的一个，而且是 rank 最高的那个 $C_b = \ most \ highly \ ranked \ C_f \ used \ from  \ prev. \ S$<br><strong>Rank:</strong> Subj &gt; ExistPredNom &gt; Obj &gt; IndObj-Obl &gt; DemAdvPP<br>语法角色层级和 Lappin &amp; Leass 算法相似，但并没有给实体附加权重值，只是简单的相互排序</li>
<li>每一个 $U_n$ 都可以有一组 $C_f$，但最多只能有一个 $C_b$，一般来说，篇章的第一个语段没有 $C_b$</li>
<li>在 $C_f$ 集合里，rank 最高的称为 Preferred Center $C_p$</li>
</ul>
<p><strong>转换规则：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/centering1.png" class="ful-image" alt="centering1.png"></p>
<p>有了上面的概念和规则，再来看一下基于 Center 的指代消解算法<br><strong>Step1：</strong> 为每个语段中的实体生成可能的 Cb-Cf组；<br><strong>Step2：</strong> 通过各种约束条件来过滤（比如：句法位置约束、语义选择限制，等等）；<br><strong>Step3：</strong> 通过转换顺序来给出排序：如果一个代词 R 的指代成分为 A 所得到的篇章连贯性高于指代成分为 B 时得到的篇章连贯性，则将 R 的指代成分确定为 A。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/centeringeg.png" class="ful-image" alt="centeringeg.png"></p>
<p>Centering 和 Hobbs 都假设输入时正确的句法结构。和 Lappin &amp; Leass 算法相似，中心算法的主要显著因子包括 Grammatical Role, Recency, Repeated Mention，但不同的是语法层级对显著性影响的方式是间接的，如果低级语法角色的所指对象导致的转换时较高级别的，它会比高级角色的所指对象优先，所以 centering algorithm 可能会将其他算法认为是相对较低显著性的所指对象判定一个代词的所指对象。比如说</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">– Bob opened a new dealership last week</div><div class="line">– John took a look at the Fords in his lot [Cb=Bob]</div><div class="line">– He ended up buying one</div><div class="line"></div><div class="line">Results: He=Bob =&gt; CONTINUE, He=John =&gt; SMOOTH</div></pre></td></tr></table></figure>
<h3 id="Log-linear-model"><a href="#Log-linear-model" class="headerlink" title="Log-linear model"></a>Log-linear model</h3><p>监督学习，需要手工标注同指关系，基于规则过滤，还是以上面的语段为例，特征如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/feature.png" class="ful-image" alt="feature.png"></p>
<h3 id="General-Coreference-Resolution"><a href="#General-Coreference-Resolution" class="headerlink" title="General Coreference Resolution"></a>General Coreference Resolution</h3><p>就是转化为一个分类问题，先识别文本中的 NP，然后对每一个 NP pair 进行一个二分类，看他们是否是共指关系，然后合并结果形成<strong>共指链(coreferential chain)</strong>，Coreference chains 其实是 cohesion 的一个部分，下面会具体讲到 cohesion。所以我们需要的是</p>
<ul>
<li>a choice of classifier</li>
<li>lots of labeled data</li>
<li>features</li>
</ul>
<p>可以选的特征有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">• Edit distance between the two NPs</div><div class="line">• Are the two NPs the same NER type?</div><div class="line">• Appositive syntax</div><div class="line">  – “Alan Shepherd, the first American astronaut…”</div><div class="line">• Proper/definite/indefinite/pronoun</div><div class="line">• Gender</div><div class="line">• Number</div><div class="line">• Distance in sentences</div><div class="line">• Number of NPs between</div><div class="line">• Grammatical role</div><div class="line">• etc.</div></pre></td></tr></table></figure></p>
<p>更多见<br>• Combine best: ENCORE (Bo Lin et al 2010)<br>• ML for Cross-Doc Coref (Rushin Shah et al 2011)</p>
<h1 id="Coherence-Cohesion"><a href="#Coherence-Cohesion" class="headerlink" title="Coherence, Cohesion"></a>Coherence, Cohesion</h1><h2 id="Coherence-Relations"><a href="#Coherence-Relations" class="headerlink" title="Coherence Relations"></a>Coherence Relations</h2><p>Cohesion 是衔接，强调句子构成成分之间的关联性，Coherence 是连贯，强调句子之间的语义连接关系。看下面三组句子，只有第一组是 make sense 的，而第二第三组的两个句子间没有任何关联。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">I saw Mary in the street. She was looking for a bookstore.</div><div class="line">? I saw Mary in the street. She has a cat.</div><div class="line">?? I saw Mary in the street. The pistons won.</div></pre></td></tr></table></figure></p>
<p>句子之间的衔接关系有很多种，比如说<strong>结果(result)、解释(explanation)、平行(parallel)、详述(elaboration)</strong>等。 <a href="https://en.wikipedia.org/wiki/William_C._Mann" target="_blank" rel="external">William Mann</a> 和 <a href="https://en.wikipedia.org/wiki/Sandra_Thompson_(linguist" target="_blank" rel="external">Sandra Thompson</a>) 提出了 <strong>RST (Rhetorical Structure Theory，修辞结构理论)</strong>，可以用来解释这种关系。RST 是基于局部文本之间的关系的文本组织理论，它认为语篇(discourse)的构成具有层次关系，通过修饰结构可以表示语篇结构。RST 在<strong>文本生成(text generation)</strong>、<strong>文本摘要(text summarization)</strong> 等场景下都有应用。</p>
<p>两个概念是 <strong>核心(nucleus)</strong> 和 <strong>外围(satellite)</strong>。RST 一般将文本的中心片段称为 <strong>核心(nucleus)</strong>，文本的周边片段称为 <strong>外围(satellite)</strong>，比如说下面的段落<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The carpenter was tired. He had been working all day.</div></pre></td></tr></table></figure></p>
<p>第二个句子详细描述了<strong>(elaborate)</strong>第一个句子，解释了 carpenter 为什么会 tired。更明显的是第二个句子以代词 He 开头，对第一个句子的依赖性很强，相对来说句子重要性更低，所以第一个句子是 Nucleus，第二个句子是 Satellite。</p>
<p><strong>Nucleus 和 Satellite 的关系</strong></p>
<ul>
<li>The satellite increases the belief in the relation described in the nucleus</li>
<li>Some relations have only a nucleus, others have two nuclei, yet others have on nucleus and one satellite</li>
</ul>
<p>再看一组 RST 关系的定义<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/RSTdef.png" class="ful-image" alt="RSTdef.png"></p>
<p>RST 是通过层级进行组合的，也就是说，我们可以采用一对相关文本作为其他高层关系的外围或者核心。和句法结构类似，这可以形成 discourse 的结构，可以看下几个例子，在下面的树里，代表一组局部连贯话段的节点被称为 discourse segment，相当于句法中的 consitute。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">S1: John went to the bank to deposit his paycheck</div><div class="line">S2: He then took a bus to Bill’s car dealership</div><div class="line">S3: He needed to buy a car</div><div class="line">S4: The company he works for now isn’t near a bus line</div><div class="line">S5: He also wanted to talk with Bill about their soccer league</div></pre></td></tr></table></figure>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/RSTeg1.png" class="ful-image" alt="RSTeg1.png">
<p>更多例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/RSTeg2.png" class="ful-image" alt="RSTeg2.png"></p>
<h2 id="Automatic-Coherence-Assignment"><a href="#Automatic-Coherence-Assignment" class="headerlink" title="Automatic Coherence Assignment"></a>Automatic Coherence Assignment</h2><p>给定句子/从句的序列，我们希望能自动的</p>
<ul>
<li>决定句子之间的衔接关系(<strong>coherence relation assignment</strong>)</li>
<li>抽取能够表示整个 discourse 的树/图结构(<strong>discourse parsing</strong>)</li>
</ul>
<h3 id="Use-cue-phrases-discourse-markers"><a href="#Use-cue-phrases-discourse-markers" class="headerlink" title="Use cue phrases/discourse markers"></a>Use cue phrases/discourse markers</h3><p>Automatic Coherence Assignment 是一个很难的任务，现有的一种方法是利用线索型的<strong>短语(cue phrases)</strong>，具体过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">John hid Bill’s car keys because he was drunk.</div><div class="line">The scarecrow came to ask for a brain. Similarly, the tin man wants a heart.</div></pre></td></tr></table></figure>
<ol>
<li><strong>Identify</strong><br>识别文本中的 cue phrases，如第一句中的 because，第二句中的 similarly</li>
<li><strong>Segment</strong><br>将文本分成 discourse segments</li>
<li><strong>Classify</strong><br>对每组相邻的 discourse segment 进行关系分类</li>
</ol>
<p><strong><a href="https://pdfs.semanticscholar.org/ba81/b31598b85259b20399e485a1ab156db5511f.pdf" target="_blank" rel="external">Marcu and Echihabi 2002</a></strong> 最早提出了关于文本结构的完全确定性的形式化模型，用无监督方法来自动识别 4 种 RST 关系(contrast, cause-explanation-evidence, condition, elaboration + non-relation)，利用 Word co-occurrence，训练了 Naive Bayes 关系分类器，取得了不错的结果，论文值得一看，是当时 discourse analysis 的一个重大突破，不过这种模型过度依赖 cue phrases，匹配模式也很简单，只能对文本进行颗粒度较粗的分析。</p>
<p>除了 <strong>cue phrases/discourse markers</strong>，还可以利用的是推理关系。</p>
<h3 id="Use-abduction-defeasible-inference"><a href="#Use-abduction-defeasible-inference" class="headerlink" title="Use abduction/defeasible inference"></a>Use abduction/defeasible inference</h3><p>基于推理的判定算法主要是通过推理来约束连贯关系。之前在<a href="http://www.shuang0420.com/2017/04/07/NLP%20笔记%20-%20Meaning%20Representation%20Languages/">NLP 笔记 - Meaning Representation Languages</a>中讲过<strong>取式推理(modus ponens)</strong>，是<strong>演绎(deduction)</strong>的规则，也是 sound inference 的一种形式，如果前提为真，结论必为真。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/modus.png" class="ful-image" alt="modus.png">
<p>然而很多 NLU 依赖的推理是不可靠的，比如说 <strong>abduction(溯因推理)</strong>，中心规则如下：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/abduction.png" class="ful-image" alt="abduction.png">
<p>这其实相当于 Peter Lipton 说的 <strong>Inference to the Best Explanation(IBE)</strong>，从结果中找最可能的原因。比如说我们知道 All Acuras are fast.，还知道 John’s car is fast，想来解释为什么 John’s car is fast，发现最合理的理由是  John’s car is an Acura。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">All Acuras are fast.</div><div class="line">John&apos;s car is fast.</div><div class="line">Maybe John&apos;s car is an Acura.</div></pre></td></tr></table></figure>
<p>然而这可能是一个不正确的推理，John 的汽车可能是由其他制造商生产同时速度很快。一个给定的结果 $\beta$ 可能会有很多潜在的原因 $\alpha$，要找到 BE，可以采用<strong>概率模型(Charniak and Goldman, 1988; Charniak and Shimony, 1990)</strong>，或者<strong>启发式方法(Charniak and McDermott)</strong>。</p>
<p>下面看一个具体的例子，下面两个句子应该是 <strong>Explanation</strong> 的关系。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">John hid Bill&apos;s car keys. He was drunk.</div></pre></td></tr></table></figure></p>
<p><strong>推理图:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/inference.png" class="ful-image" alt="inference.png"></p>
<p>下面看一下具体的过程。首先需要<strong>关于 cohesion 本身的公理</strong>，表示要确定两个事件有连贯关系，一种可能性是假定两者是解释(explanation)关系<br>$∀ e_i, e_j \ Explanation(e_i, e_j) =&gt; CoherenceRel(e_i, e_j)$</p>
<p>然后是需要<strong>关于解释(explanation)的公理</strong>，要求第二个话段是第一个话段的原因<br>$∀ e_i, e_j \ cause(e_j, e_i) =&gt; Explanation(e_i, e_j)$</p>
<p>再然后是<strong>代表世界常识的公理</strong>，第一条，如果某人喝醉了，我们就不让他开车<br>$∀ x, y, e_i \ drunk(e_i, x) =&gt; ∃ e_j, e_k diswant(e_j, y, e_k) ∧ drive(e_k, x) ∧  cause(e_i, e_j)$</p>
<p>第二条，如果某人不想让其他人开车，那么他们就不愿意让这个人拿到他的车钥匙<br>$∀ x, y, e_j, e_k diswant(e_j, y, e_k) ∧ drive(e_k, x) =&gt; ∃ z, e_l, e_m diswant(e_l, y, e_m) ∧ have(e_m, x, z)  ∧ carkeys(z, x)  ∧  cause(e_j, e_l)$</p>
<p>第三条，如果某人不想让其他人拥有某件东西，那么他可以将东西藏起来<br>$∀ x, y, z, e_i, e_j diswant(e_l, y, e_m) ∧ have(e_m, x, z)  =&gt; ∃e_n hide(e_n, y, x, z) ∧ cause(e_l, e_n) $</p>
<p>最后一个公理，原因是可传递的<br>$∀e_i, e_j e_k cause(e_i, e_j)  ∧ cause(e_j, e_k) =&gt; cause(e_i, e_k)$</p>
<p>开始假设连贯关系是 explanation，根据公理最后能得到<br>$diswant(e_3, John, e_5) ∧ have（e_5, Bill, carkey)$<br>$diswant(e_4, John, e_6) ∧ drive（e_6, Bi)$<br>=&gt; 推测出<br>$drunk(e_2, Bill)$</p>
<p>通过 coreference 可以把 he 和 Bill 绑定，这就确立了句子的连贯。然而要注意的是，Abduction 是<strong>非可靠</strong>的推理，是<strong>可废止的(defeasible)</strong>。比如说如果紧跟上面句子的下面的句子，那么我们不得不撤销连接之前两个句子的推理连，然后用事实(藏钥匙是恶作剧的一部分)来替代。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Bill&apos;s car isn&apos;t here anyway; John was just playing a practical joke on him.</div></pre></td></tr></table></figure>
<h2 id="Discourse-Segmentation"><a href="#Discourse-Segmentation" class="headerlink" title="Discourse Segmentation"></a>Discourse Segmentation</h2><p>还有一个任务是 <strong>discourse segmentation</strong>，目标是将文本切分为一个子话题(subtopics)的线性序列，比如说</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/segmentation.png" class="ful-image" alt="segmentation.png">
<p>常用的方法是 <strong>TextTiling</strong>，通过词汇层面的共现和分布模式(lexical co-occurrence and distribution)来寻找 subtopic 的边界。它的假设是认为描述 subtopic 的词会局部共现，从一个 subtopic 到另一个 subtopic 的 switch 以一个共现词集合的结束和另一个共现词集合的开始为标志。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/TextTiling.png" class="ful-image" alt="TextTiling.png"></p>
<p><strong>步骤:</strong></p>
<ol>
<li><strong>Tokenization</strong><br>进行 tokenization 得到 ABCDE 等 term 以及以句子为单位的词汇单元(sentence-sized units)，如上图 1-8，每一列都是一个 unit</li>
<li><strong>Lexical score determination</strong><br>方法有 blocks, vocabulary introductions 和 chains<br>上图表示 blocks 方法，把 k 个句子 group 成 block (一般 K=2)，计算 block lexical score，一般就是向量內积，比如第一个 block 的 score 就是 8=2x1(for A)+1x1(for B)+2x1(for C)+1x1(for D)+1x2(for E)<br>Block 其实就相当于一个 moving window</li>
<li><strong>Boundary identification</strong><br>如果一个较低的  lexical score 前面和后面都跟着一个高的 lexical score，那么较低的 lexical score 所在的位置可能就代表了一个 shift，或者说 subtopic change</li>
</ol>
<p>更多戳论文<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.5278&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">TextTiling: A Quantitative Approach to Discourse Segmentation</a></p>
<p>另外还有监督学习的方法，可以训练一个 binary classifier，在句子之间放个 marker，标注这是不是 discourse boundary，使用到的 feature 有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">• Discourse markers or cue words</div><div class="line">• Word overlap before/after boundary</div><div class="line">• Number of coreference chains that cross boundary</div><div class="line">...</div></pre></td></tr></table></figure></p>
<h1 id="Context-Speech-Acts"><a href="#Context-Speech-Acts" class="headerlink" title="Context, Speech Acts"></a>Context, Speech Acts</h1><p>最后来看一下 context 和 speech acts。</p>
<h2 id="Context"><a href="#Context" class="headerlink" title="Context"></a>Context</h2><p>同一个句子在不同的情境(context)下的含义可能是不同的。需要考虑的情境：</p>
<ul>
<li>Social context<br>Social identities, relationships, and setting</li>
<li>Physical context<br>Where? What objects are present? What actions?</li>
<li>Linguistic context<br>Conversation history</li>
<li>Other forms of context<br>Shared knowledge, etc.</li>
</ul>
<h2 id="Speech-Acts"><a href="#Speech-Acts" class="headerlink" title="Speech Acts"></a>Speech Acts</h2><blockquote>
<p>A <strong>speech act</strong> in <a href="https://en.wikipedia.org/wiki/Linguistics" target="_blank" rel="external">linguistics</a> and the <a href="https://en.wikipedia.org/wiki/Philosophy_of_language" target="_blank" rel="external">philosophy of language</a> is an utterance that has <a href="https://en.wikipedia.org/wiki/Performativity" target="_blank" rel="external">performative function</a> in language and communication.</p>
</blockquote>
<p><strong>语言行为理论(Speech Acts)</strong>最初由语言哲学家 Austin 提出，后来由其学生 Searle 进一步发展。核心理论是 <strong>Sentences perform actions</strong>，说话人只要说出了有意义的，可以被听众理解的话，就可以说他实施了某个行为，这个行为就是 Speech Act。</p>
<h3 id="Austin"><a href="#Austin" class="headerlink" title="Austin"></a>Austin</h3><p><a href="http://en.wikipedia.org/wiki/J._L._Austin" target="_blank" rel="external">Austin</a> 最初把言语分为两类，<strong>言有所述(constatives)</strong>和<strong>言有所为(performatives)</strong>。判断一个句子是不是 performative 的方法是加上 <strong>hereby</strong> 来验证，如果句子依然通顺，这个句子就含有 performative verb。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">– I hereby name this ship the Queen Elizabeth.</div><div class="line">– I hereby take this man to be my husband.</div><div class="line">– I hereby bequeath this watch to my brother.</div><div class="line">– I hereby declare war.</div></pre></td></tr></table></figure></p>
<p>加上 hereby 之后，上面的句子依旧 make sense，然而下面这两个句子就不能<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">– Birds hereby sing.</div><div class="line">– There is hereby fighting in Syria.</div></pre></td></tr></table></figure></p>
<p>然而后来 Austin 发现用 hereby 来判读一个句子是不是有 performative 有点牵强，于是提出了一个新的模式，一个人说话的时候，同时实施了三种行为，<strong>言内行为(locutionary act), 言外行为(illocutionary act), 和言后行为( perlocutionary act)</strong></p>
<ul>
<li><strong>Locuion</strong>: say some words<br>通过说话表达字面意义，包括说话时所用的发出的语音、音节、单词、短语、句子</li>
<li><strong>Illocution</strong>: an action performed <em>in</em> saying words<br>​     通过字面意义表达说话人的意图，比如说发出命令<br>​  Ask, promise, command</li>
<li><strong>Perlocution</strong>: an action performed <em>by</em> saying words, probably the effect that an illocution has on the listener.<br>说话人的话语作用在听众身上所带来的效果<br>Persuade, convince, scare, elicit an answer, etc.</li>
</ul>
<h3 id="Searle"><a href="#Searle" class="headerlink" title="Searle"></a>Searle</h3><p><a href="http://en.wikipedia.org/wiki/Speech_act" target="_blank" rel="external">Searle</a>进一步说明了<strong>人类交际的基本单位不是句子或其他任何表达手段，而是完成一定的行为</strong>，并提出了言外行为的几个分类</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/searle%27s%20speech%20acts.png" class="ful-image" alt="searle%27s%20speech%20acts.png">
<p>同时提出了<strong>间接言语行为理论(indirect speech acts)</strong>，通过某一个言语行为来做另一个言外行为，比如陈述句不是陈述，祈使句不是祈使，疑问句不是疑问的情况等。<br><strong>Indirect speech acts:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">– Can you pass the salt?</div><div class="line">​	• Has the form of a *question*, but the effect of a *directive*.</div></pre></td></tr></table></figure></p>
<p><a href="http://ccl.pku.edu.cn/doubtfire/NLP/Artificial_Intelligence/Searle/%D1%D4%D3%EF%D0%D0%CE%AA%C0%ED%C2%DB%C6%C0%CA%F6.htm" target="_blank" rel="external">言语行为理论评述</a></p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>Speech acts 的主要应用应该是对话系统，如下面是任务导向型对话系统的 speech acts 示例<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/speech_acts_eg1.png" class="ful-image" alt="speech_acts_eg1.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/speech_acts_eg2.png" class="ful-image" alt="speech_acts_eg2.png"></p>
<p>谈判的 speech acts<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/speech_acts_eg3.png" class="ful-image" alt="speech_acts_eg3.png"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> discourse analysis </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Neo4j Cypher Cheetsheet]]></title>
      <url>http://www.shuang0420.com/2017/09/11/Neo4j%20Cypher%20Cheetsheet/</url>
      <content type="html"><![CDATA[<p>内容主要来自 Coursera 课程 <strong>Big Data Graph Analytics</strong>，在这对 Cypher 语句做个整理，方便查阅。包括基本语句、以及路径分析、链接分析样例。<br><a id="more"></a></p>
<p>Neo4j 使用 Cypher 查询图形数据，Cypher 是描述性的图形查询语言，语法简单功能强大，由于 Neo4j 在图形数据库家族中处于绝对领先的地位，拥有众多的用户基数，Cypher 成为图形查询语言的事实上的标准。<br>和 SQL 很相似，Cypher 语言的关键字不区分大小写，但是属性值，标签，关系类型和变量是区分大小写的。</p>
<p>看一下基本的概念</p>
<ul>
<li><p><strong>变量(Variable)</strong><br>变量用于对搜索模式的部分进行命名，并在同一个查询中引用，在小括号()中命名变量，<strong>变量名是区分大小写的</strong>，示例代码创建了两个变量：n 和 b，通过 return 子句返回变量 b；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MATCH (n)--&gt;(b)</div><div class="line">RETURN b</div></pre></td></tr></table></figure>
</li>
<li><p><strong>访问属性(Property)</strong><br>在 Cypher 查询中，通过点来访问属性，格式是：Variable.PropertyKey，通过 id 函数来访问实体的 ID，格式是 id(Variable)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n)--&gt;(b)</div><div class="line">where id(n)=5 and b.age=18</div><div class="line">return b;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>节点(Node)</strong><br>节点模式的构成：<code>(Variable:Lable1 {Key1:Value1,Key2,Value2})</code><br>每个节点都有一个整数 ID，在创建新的节点时，Neo4j 自动为节点设置 ID 值，在整个数据库中，节点的 ID 值是递增和唯一的。<br>下面的 Cypher 查询创建一个节点，标签是 Person，具有两个属性 name 和 born，通过 RETURN 子句，返回新建的节点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create (n:Person &#123; name: &apos;Tom Hanks&apos;, born: 1956 &#125;) return n;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>匹配(Match)</strong><br>通过match子句查询数据库，match子句用于指定搜索的模式（Pattern），where子句为match模式增加谓词（Predicate），用于对Pattern进行约束；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match(n) return n;</div></pre></td></tr></table></figure>
</li>
<li><p><strong>关系(Relation)</strong><br>关系的构成：<code>StartNode - [Variable:RelationshipType {Key1:Value1, Key2:Value2}] -&gt; EndNode</code><br>创建关系时，必须指定关系类型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MATCH (a:Person),(b:Movie)</div><div class="line">WHERE a.name = &apos;Robert Zemeckis&apos; AND b.title = &apos;Forrest Gump&apos;</div><div class="line">CREATE (a)-[r:DIRECTED]-&gt;(b)</div><div class="line">RETURN r;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>本篇数据集：<br><strong>链接:</strong> <a href="http://pan.baidu.com/s/1dEHWQch" target="_blank" rel="external">http://pan.baidu.com/s/1dEHWQch</a><br><strong>密码:</strong> 00v4</p>
<h1 id="Create-and-Delete"><a href="#Create-and-Delete" class="headerlink" title="Create and Delete"></a>Create and Delete</h1><p>建图要求<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">==============</div><div class="line">Five Nodes</div><div class="line">N1 = Tom</div><div class="line">N2 = Harry</div><div class="line">N3 = Julian</div><div class="line">N4 = Michele</div><div class="line">N5 = Josephine</div><div class="line"></div><div class="line">Five Edges</div><div class="line">e1 = Harry ‘is known by’ Tom</div><div class="line">e2 = Julian ‘is co-worker of’ Harry</div><div class="line">e3 = Michele ‘is wife of’ Harry</div><div class="line">e4 = Josephine ‘is wife of’ Tom</div><div class="line">e5 = Josephine ‘is friend of’ Michele</div><div class="line"></div><div class="line">==============</div><div class="line">A simple text description of a graph</div><div class="line">N1 - e1 -&gt; N2</div><div class="line">N2 - e2 -&gt; N3</div><div class="line">N2 - e3 -&gt; N4</div><div class="line">N1 - e4 -&gt; N5</div><div class="line">N4 - e5 -&gt; N5</div><div class="line">==============</div></pre></td></tr></table></figure></p>
<p>创建完整的 graph<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">create (N1:ToyNode &#123;name: &apos;Tom&apos;&#125;) - [:ToyRelation &#123;relationship: &apos;knows&apos;&#125;] -&gt; (N2:ToyNode &#123;name: &apos;Harry&apos;&#125;),</div><div class="line">(N2) - [:ToyRelation &#123;relationship: &apos;co-worker&apos;&#125;] -&gt; (N3:ToyNode &#123;name: &apos;Julian&apos;, job: &apos;plumber&apos;&#125;),</div><div class="line">(N2) - [:ToyRelation &#123;relationship: &apos;wife&apos;&#125;] -&gt; (N4:ToyNode &#123;name: &apos;Michele&apos;, job: &apos;accountant&apos;&#125;),</div><div class="line">(N1) - [:ToyRelation &#123;relationship: &apos;wife&apos;&#125;] -&gt; (N5:ToyNode &#123;name: &apos;Josephine&apos;, job: &apos;manager&apos;&#125;),</div><div class="line">(N4) - [:ToyRelation &#123;relationship: &apos;friend&apos;&#125;] -&gt; (N5)</div><div class="line">;</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/create_1.png" class="ful-image" alt="create_1.png">
<p>ToyNode is a node type and ToyRelation is an edge type. ToyNode can have properties, so can ToyRelation.</p>
<p><strong>//View the resulting graph</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode)-[r]-(m) return n, r, m</div></pre></td></tr></table></figure></p>
<p><strong>//Delete all nodes and edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n)-[r]-() delete n, r</div></pre></td></tr></table></figure></p>
<p><strong>//Delete all nodes which have no edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n) delete n</div></pre></td></tr></table></figure></p>
<p><strong>//Delete only ToyNode nodes which have no edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode) delete n</div></pre></td></tr></table></figure></p>
<p><strong>//Delete all edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n)-[r]-() delete r</div></pre></td></tr></table></figure></p>
<p><strong>//Delete only ToyRelation edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n)-[r:ToyRelation]-() delete r</div></pre></td></tr></table></figure></p>
<p><strong>//Selecting an existing single ToyNode node</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode &#123;name:&apos;Julian&apos;&#125;) return n</div></pre></td></tr></table></figure></p>
<h1 id="Adding-or-Modify"><a href="#Adding-or-Modify" class="headerlink" title="Adding or Modify"></a>Adding or Modify</h1><p>Merge 子句的作用：当模式（Pattern）存在时，匹配该模式；当模式不存在时，创建新的模式，功能是 match 子句和 create 的组合。在 merge 子句之后，可以显式指定 on create 和 on match 子句，用于修改绑定的节点或关系的属性。</p>
<p>通过 merge 子句，可以指定图形中必须存在一个节点，该节点必须具有特定的标签，属性等，如果不存在，那么 merge 子句将创建相应的节点。</p>
<p><strong>//Adding a Node Correctly</strong><br>First find a node you wanna add to, then add the node.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode &#123;name:&apos;Julian&apos;&#125;)</div><div class="line">merge (n)-[:ToyRelation &#123;relationship: &apos;fiancee&apos;&#125;]-&gt;(m:ToyNode &#123;name:&apos;Joyce&apos;, job:&apos;store clerk&apos;&#125;)</div></pre></td></tr></table></figure></p>
<p><strong>//Adding a Node Incorrectly</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">create (n:ToyNode &#123;name:&apos;Julian&apos;&#125;)-[:ToyRelation &#123;relationship: &apos;fiancee&apos;&#125;]-&gt;(m:ToyNode &#123;name:&apos;Joyce&apos;, job:&apos;store clerk&apos;&#125;)</div></pre></td></tr></table></figure></p>
<p><strong>//Correct your mistake by deleting the bad nodes and edge</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode &#123;name:&apos;Joyce&apos;&#125;)-[r]-(m) delete n, r, m</div></pre></td></tr></table></figure></p>
<p><strong>//Modify a Node’s Information</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:ToyNode) where n.name = &apos;Harry&apos; set n.job = &apos;drummer&apos;</div><div class="line">match (n:ToyNode) where n.name = &apos;Harry&apos; set n.job = n.job + [&apos;lead guitarist&apos;]</div></pre></td></tr></table></figure></p>
<h1 id="Import"><a href="#Import" class="headerlink" title="Import"></a>Import</h1><p><strong>//One way to “clean the slate” in Neo4j before importing (run both lines):</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (a)-[r]-&gt;() delete a,r</div><div class="line">match (a) delete a</div></pre></td></tr></table></figure></p>
<p><strong>//Script to Import Data Set: test.csv (simple road network)</strong><br><strong>//[NOTE: replace any spaces in your path with %20, “percent twenty” ]</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">LOAD CSV WITH HEADERS FROM &quot;file:///test.csv&quot; AS line</div><div class="line">MERGE (n:MyNode &#123;Name:line.Source&#125;)</div><div class="line">MERGE (m:MyNode &#123;Name:line.Target&#125;)</div><div class="line">MERGE (n) -[:TO &#123;dist:line.distance&#125;]-&gt; (m)</div></pre></td></tr></table></figure></p>
<p><strong>//Script to import global terrorist data</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">LOAD CSV WITH HEADERS FROM &quot;file:///terrorist_data_subset.csv&quot; AS row</div><div class="line">MERGE (c:Country &#123;Name:row.Country&#125;)</div><div class="line">MERGE (a:Actor &#123;Name: row.ActorName, Aliases: row.Aliases, Type: row.ActorType&#125;)</div><div class="line">MERGE (o:Organization &#123;Name: row.AffiliationTo&#125;)</div><div class="line">MERGE (a)-[:AFFILIATED_TO &#123;Start: row.AffiliationStartDate, End: row.AffiliationEndDate&#125;]-&gt;(o)</div><div class="line">MERGE(c)&lt;-[:IS_FROM]-(a);</div></pre></td></tr></table></figure></p>
<p>When you are loading CSVs you get an error like “Couldn’t load the external resource at: file: […]”, put your csv file in the right path like <code>/Users/shuang/Documents/Neo4j/default.graphdb/import/</code>, the problem will be solved.</p>
<h1 id="Basic-Graph-Operations"><a href="#Basic-Graph-Operations" class="headerlink" title="Basic Graph Operations"></a>Basic Graph Operations</h1><p><strong>//Counting the number of nodes</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)</div><div class="line">return count(n)</div></pre></td></tr></table></figure></p>
<p><strong>//Counting the number of edges</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-&gt;()</div><div class="line">return count(r)</div></pre></td></tr></table></figure></p>
<p><strong>//Finding leaf nodes:</strong><br><strong>Leaf node: </strong> the node which have no outgoing edges<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r:TO]-&gt;(m)</div><div class="line">where not ((m)--&gt;())</div><div class="line">return m</div></pre></td></tr></table></figure></p>
<p><strong>//Finding root nodes:</strong><br><strong>Root node: </strong> the node which have no incoming edges<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (m)-[r:TO]-&gt;(n:MyNode)</div><div class="line">where not (()--&gt;(m))</div><div class="line">return m</div></pre></td></tr></table></figure></p>
<p><strong>//Finding triangles:</strong><br><strong>Triangle: </strong> a three cycle, consisting of three nodes and three edges where the beginning and end node are the same<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (a)-[:TO]-&gt;(b)-[:TO]-&gt;(c)-[:TO]-&gt;(a)</div><div class="line">return distinct a, b, c</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/triangle.png" class="ful-image" alt="triangle.png">
<p><strong>//Finding 2nd neighbors of D:</strong><br><strong>2nd neighbor: </strong> two nodes away from D<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (a)-[:TO*..2]-(b)</div><div class="line">where a.Name=&apos;D&apos;</div><div class="line">return distinct a, b</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/2nd_nei.png" class="ful-image" alt="2nd_nei.png">
<p>Some nodes appear to be only one node away from the node D but we can get to those nodes indirectly through another node, which means that they’re not only a first neighbor but they’re also a second neighbor.</p>
<p><strong>//Finding the types of a node:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n)</div><div class="line">where n.Name = &apos;Afghanistan&apos;</div><div class="line">return labels(n)</div></pre></td></tr></table></figure></p>
<p><strong>//Finding the label of an edge:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n &#123;Name: &apos;Afghanistan&apos;&#125;)&lt;-[r]-()</div><div class="line">return distinct type(r)</div></pre></td></tr></table></figure></p>
<p><strong>//Finding all properties of a node:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:Actor)</div><div class="line">return * limit 20</div></pre></td></tr></table></figure></p>
<p><strong>//Finding loops:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n)-[r]-&gt;(n)</div><div class="line">return n, r limit 10</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/loops.png" class="ful-image" alt="loops.png">
<p><strong>//Finding multigraphs:</strong><br><strong>Multigraph: </strong> any two nodes which have two or more edges between them<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n)-[r1]-&gt;(m), (n)-[r2]-(m)</div><div class="line">where r1 &lt;&gt; r2</div><div class="line">return n, r1, r2, m limit 10</div></pre></td></tr></table></figure></p>
<p>remember to apply a constraint in which the edges must be different for the same pairs of nodes<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/multi_graph.png" class="ful-image" alt="multi_graph.png"></p>
<p><strong>//Finding the induced subgraph given a set of nodes:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n)-[r:TO]-(m)</div><div class="line">where n.Name in [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;] and m.Name in [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;]</div><div class="line">return n, r, m</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/induced.png" class="ful-image" alt="induced.png">
<h1 id="Path-Analytics"><a href="#Path-Analytics" class="headerlink" title="Path Analytics"></a>Path Analytics</h1><p><strong>//Viewing the graph</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-&gt;(m)</div><div class="line">return n, r, m</div></pre></td></tr></table></figure></p>
<p><strong>//Finding paths between specific nodes:</strong><br>Use the match command to match p which is a variable we’re using to represent our path, = node a, going through an edge to node c. There’s something slightly different about this edge, and that is that we’re using a star to represent an arbitrary number of edges in sequence between a and c, and we’ll be returning all of those edges that are necessary to complete the path. And in this case we only want to return a single path.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match p=(a)-[:TO*]-(c)</div><div class="line">where a.Name=&apos;H&apos; and c.Name=&apos;P&apos;</div><div class="line">return p limit 1</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/HP.png" class="ful-image" alt="HP.png">
<p>*Your results might not be the same as the video hands-on demo. If not, try the following query and it should return the shortest path between nodes H and P:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">match p=(a)-[:TO*]-(c) where a.Name=&apos;H&apos; and c.Name=&apos;P&apos; return p order by length(p) asc limit 1</div></pre></td></tr></table></figure></p>
<p><strong>//Finding the length between specific nodes:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match p=(a)-[:TO*]-(c)</div><div class="line">where a.Name=&apos;H&apos; and c.Name=&apos;P&apos;</div><div class="line">return length(p) limit 1</div></pre></td></tr></table></figure></p>
<p><strong>//Finding a shortest path between specific nodes:</strong><br>Use a built-in command shortestPath<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match p=shortestPath((a)-[:TO*]-(c))</div><div class="line">where a.Name=&apos;A&apos; and c.Name=&apos;P&apos;</div><div class="line">return p, length(p) limit 1</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/shortest.png" class="ful-image" alt="shortest.png">
<p><strong>//All Shortest Paths:</strong><br>Use a built-in command allShortestPaths<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MATCH p = allShortestPaths((source)-[r:TO*]-(destination))</div><div class="line">WHERE source.Name=&apos;A&apos; AND destination.Name = &apos;P&apos;</div><div class="line">RETURN EXTRACT(n IN NODES(p)| n.Name) AS Paths</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/shortest_all.png" class="ful-image" alt="shortest_all.png">
<p><strong>//All Shortest Paths with Path Conditions:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MATCH p = allShortestPaths((source)-[r:TO*]-&gt;(destination))</div><div class="line">WHERE source.Name=&apos;A&apos; AND destination.Name = &apos;P&apos; AND LENGTH(NODES(p)) &gt; 5</div><div class="line">RETURN EXTRACT(n IN NODES(p)| n.Name) AS Paths,length(p)</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/shortest_cond.png" class="ful-image" alt="shortest_cond.png">
<p><strong>//Diameter of the graph:</strong><br><strong>Diameter: </strong> the longest shortest path between two nodes in the graph<br>Returned in the form of an array. We’re using a new term, extract, which is based on the following. Assuming we have matched our path p, we want to identify all of the nodes in p and extract their names. And we’ll return these names as a listing, which we’ll call the variable paths. If there’s more than one shortest path, we’ll get multiple listings of node names.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode), (m:MyNode)</div><div class="line">where n &lt;&gt; m</div><div class="line">with n, m</div><div class="line">match p=shortestPath((n)-[*]-&gt;(m))</div><div class="line">return n.Name, m.Name, length(p)</div><div class="line">order by length(p) desc limit 1</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/diameter.png" class="ful-image" alt="diameter.png">
<p><strong>//Extracting and computing with node and properties:</strong><br>Returned as the variable pathLength.<br><strong>Reduce line</strong> begins by setting a variable s equal to 0. And then define a variable e, which represents the set of relationships in a path that’s returned,<br>or in other words, the edges. And we pass that into this variable s, and add to it, the value of the distance that we’ve assigned to that edge.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">match p=(a)-[:TO*]-(c)</div><div class="line">where a.Name=&apos;H&apos; and c.Name=&apos;P&apos;</div><div class="line">return extract(n in nodes(p)|n.Name) as Nodes, length(p) as pathLength,</div><div class="line">reduce(s=0, e in relationships(p)| s + toInt(e.dist)) as pathDist limit 1</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/extract.png" class="ful-image" alt="extract.png">
<p>The path itself, as we know, begins in H and ends in P. And it has a pathLength of 8, but it has a pathDist of 40.<br>So we could interpret this to mean that even though there are 7 towns between the source town and the destination town, or a pathLength of 8,<br>the actual distance in miles would be a value of 40.</p>
<p><strong>//Dijkstra’s algorithm for a specific target node:</strong><br>This is not the path in our network with the least weights. It is the weight of the shortest path based on numbers of hops.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MATCH (from: MyNode &#123;Name:&apos;A&apos;&#125;), (to: MyNode &#123;Name:&apos;P&apos;&#125;),</div><div class="line">path = shortestPath((from)-[:TO*]-&gt;(to))</div><div class="line">WITH REDUCE(dist = 0, rel in rels(path) | dist + toInt(rel.dist)) AS distance, path</div><div class="line">RETURN path, distance</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/Dijkstra.png" class="ful-image" alt="Dijkstra.png">
<p><strong>//Dijkstra’s algorithm SSSP:</strong><br>What we’ve calculated is the shortest hop path with the weights added, the sum of the weights of the edges in that path. This is not the least weight path of the entire network.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MATCH (from: MyNode &#123;Name:&apos;A&apos;&#125;), (to: MyNode),</div><div class="line">path = shortestPath((from)-[:TO*]-&gt;(to))</div><div class="line">WITH REDUCE(dist = 0, rel in rels(path) | dist + toInt(rel.dist)) AS distance, path, from, to</div><div class="line">RETURN from, to, path, distance order by distance desc</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/error.png" class="ful-image" alt="error.png">
<p>Problem not solved. Refer to <a href="https://github.com/neo4j/neo4j/issues/9992" target="_blank" rel="external">allshortestPaths error start/end nodes the same with cypher.forbid_shortestpath_common_node=false</a></p>
<p><strong>//Graph not containing a selected node:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n)-[r:TO]-&gt;(m)</div><div class="line">where n.Name &lt;&gt; &apos;D&apos; and m.Name &lt;&gt; &apos;D&apos;</div><div class="line">return n, r, m</div></pre></td></tr></table></figure></p>
<p><strong>//Shortest path over a Graph not containing a selected node:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match p=shortestPath((a &#123;Name: &apos;A&apos;&#125;)-[:TO*]-(b &#123;Name: &apos;P&apos;&#125;))</div><div class="line">where not(&apos;D&apos; in (extract(n in nodes(p)|n.Name)))</div><div class="line">return p, length(p)</div></pre></td></tr></table></figure></p>
<p><strong>//Graph not containing the immediate neighborhood of a specified node:</strong><br>Remember to take leaf and root node into account.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">match (d &#123;Name:&apos;D&apos;&#125;)-[:TO]-(b)</div><div class="line">with collect(distinct b.Name) as neighbors</div><div class="line">match (n)-[r:TO]-&gt;(m)</div><div class="line">where</div><div class="line">not (n.Name in (neighbors+&apos;D&apos;))</div><div class="line">and</div><div class="line">not (m.Name in (neighbors+&apos;D&apos;))</div><div class="line">return n, r, m</div><div class="line">;</div><div class="line">match (d &#123;Name:&apos;D&apos;&#125;)-[:TO]-(b)-[:TO]-&gt;(leaf)</div><div class="line">where not((leaf)--&gt;())</div><div class="line">return (leaf)</div><div class="line">;</div><div class="line">match (d &#123;Name:&apos;D&apos;&#125;)-[:TO]-(b)&lt;-[:TO]-(root)</div><div class="line">where not((root)&lt;--())</div><div class="line">return (root)</div></pre></td></tr></table></figure></p>
<p>The result for first statement.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/no_immediate%20neighborhood.png" class="ful-image" alt="no_immediate%20neighborhood.png"></p>
<p><strong>//Graph not containing a selected neighborhood:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">match (a &#123;Name: &apos;F&apos;&#125;)-[:TO*..2]-(b)</div><div class="line">with collect(distinct b.Name) as MyList</div><div class="line">match (n)-[r:TO]-&gt;(m)</div><div class="line">where not(n.Name in MyList) and not (m.Name in MyList)</div><div class="line">return distinct n, r, m</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/no_contain.png" class="ful-image" alt="no_contain.png">
<h1 id="Connectivity-Analytics"><a href="#Connectivity-Analytics" class="headerlink" title="Connectivity Analytics"></a>Connectivity Analytics</h1><blockquote>
<p><strong>Connectivity analytics</strong> in terms of network robustness. In other words, a measure of how resistant a graph network is to being disconnected<br>Two ways of connectivity analytics: One computed the eigenvalues, and the second computed the degree distribution. For these examples, we’re going to use the second one, degree distributions.</p>
</blockquote>
<p><strong>//Viewing the graph</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-&gt;(m)</div><div class="line">return n, r, m</div></pre></td></tr></table></figure></p>
<p><strong>// Find the outdegree of all nodes</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-&gt;()</div><div class="line">return n.Name as Node, count(r) as Outdegree</div><div class="line">order by Outdegree</div><div class="line">union</div><div class="line">match (a:MyNode)-[r]-&gt;(leaf)</div><div class="line">where not((leaf)--&gt;())</div><div class="line">return leaf.Name as Node, 0 as Outdegree</div></pre></td></tr></table></figure></p>
<p><strong>// Find the indegree of all nodes</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)&lt;-[r]-()</div><div class="line">return n.Name as Node, count(r) as Indegree</div><div class="line">order by Indegree</div><div class="line">union</div><div class="line">match (a:MyNode)&lt;-[r]-(root)</div><div class="line">where not((root)&lt;--())</div><div class="line">return root.Name as Node, 0 as Indegree</div></pre></td></tr></table></figure></p>
<p><strong>// Find the degree of all nodes</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-()</div><div class="line">return n.Name, count(distinct r) as degree</div><div class="line">order by degree</div></pre></td></tr></table></figure></p>
<p><strong>// Find degree histogram of the graph</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-()</div><div class="line">with n as nodes, count(distinct r) as degree</div><div class="line">return degree, count(nodes) order by degree asc</div></pre></td></tr></table></figure></p>
<p><strong>//Save the degree of the node as a new node property</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode)-[r]-()</div><div class="line">with n, count(distinct r) as degree</div><div class="line">set n.deg = degree</div><div class="line">return n.Name, n.deg</div></pre></td></tr></table></figure></p>
<p><strong>// Construct the Adjacency Matrix of the graph</strong><br><strong>Philosophical issue：</strong><br>Every database will allow you some analytical computation and the remainder of the analytical computations must be done outside of the database. However, it is always a judicious idea to get the database to achieve an intermediate result formatted in a way that you would need for the next computation. And then, you use that intermediate result as the input to the next computation. We’ve seen that a number of computations in graph analytics start with the adjacency matrix. So we should be able to force Cypher to produce an adjacency matrix<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode), (m:MyNode)</div><div class="line">return n.Name, m.Name,</div><div class="line">case</div><div class="line">when (n)--&gt;(m) then 1</div><div class="line">else 0</div><div class="line">end as value</div></pre></td></tr></table></figure></p>
<p><strong>// Construct the Normalized Laplacian Matrix of the graph</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">match (n:MyNode), (m:MyNode)</div><div class="line">return n.Name, m.Name,</div><div class="line">case</div><div class="line">when n.Name = m.Name then 1</div><div class="line">when (n)--&gt;(m) then -1/(sqrt(toInt(n.deg))*sqrt(toInt(m.deg)))</div><div class="line">else 0</div><div class="line">end as value</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/matrix.png" class="ful-image" alt="matrix.png">
<h1 id="Scale-View"><a href="#Scale-View" class="headerlink" title="Scale View"></a>Scale View</h1><p>可以调整显示区的大小，浏览器调到 inspect 模式，在 d3 代码区域添加 scale 函数，如下。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/scale0.png" class="ful-image" alt="scale0.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Neo4j%20Cypher%20Cheetsheet/scale1.png" class="ful-image" alt="scale1.png">
<p><strong>References:</strong></p>
<blockquote>
<p><a href="https://neo4j.com/docs/cypher-refcard/current/" target="_blank" rel="external">Neo3j Cypher Refcard 3.2</a><br><a href="http://www.ttlsa.com/nosql/how-to-neo4j-cypher-query-language/" target="_blank" rel="external">Neo4j Cypher查询语言详解</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Knowledge Graph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Knowledge Graph </tag>
            
            <tag> 知识库 </tag>
            
            <tag> Neo4j </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[项目实战--知识图谱初探]]></title>
      <url>http://www.shuang0420.com/2017/09/05/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/</url>
      <content type="html"><![CDATA[<p>实践了下怎么建一个简单的知识图谱，两个版本，一个从 0 开始(start from scratch)，一个在 CN-DBpedia 基础上补充，把 MySQL，PostgreSQL，Neo4j 数据库都尝试了下。自己跌跌撞撞摸索可能踩坑了都不知道，欢迎讨论。<br><a id="more"></a></p>
<h1 id="CN-DBpedia-构建流程"><a href="#CN-DBpedia-构建流程" class="headerlink" title="CN-DBpedia 构建流程"></a>CN-DBpedia 构建流程</h1><p>知识库可以分为两种类型，一种是以 Freebase，Yago2 为代表的 Curated KBs，主要从维基百科和 WordNet 等知识库中抽取大量的实体及实体关系，像是一种结构化的维基百科。另一种是以 Stanford OpenIE，和我们学校 Never-Ending Language Learning (NELL) 为代表的 Extracted KBs，直接从上亿个非结构化网页中抽取实体关系三元组。与 Freebase 相比，这样得到的知识更加<strong>多样性</strong>，但同时精确度要低于 Curated KBs，因为实体关系和实体更多的是<strong>自然语言</strong>的形式，如“奥巴马出生在火奴鲁鲁。” 可以被表示为（“Obama”, “was also born in”, “ Honolulu”），</p>
<p>下面以 CN-DBpedia 为例看下知识图谱大致是怎么构建的。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/CN_DBpedia%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B.png" class="ful-image" alt="CN_DBpedia%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/CN_DBpedia.png" class="ful-image" alt="CN_DBpedia.png"></p>
<p>上图分别是 CN-DBpedia 的构建流程和系统架构。知识图谱的构建是一个浩大的工程，从大方面来讲，分为<strong>知识获取、知识融合、知识验证、知识计算和应用</strong>几个部分，也就是上面架构图从下往上走的一个流程，简单来走一下这个流程。</p>
<h2 id="数据支持层"><a href="#数据支持层" class="headerlink" title="数据支持层"></a>数据支持层</h2><p>最底下是知识获取及存储，或者说是<strong>数据支持层</strong>，首先从不同来源、不同结构的数据中<strong>获取</strong>知识，CN-DBpedia 的知识来源主要是通过爬取各种百科知识这类半结构化数据。</p>
<p>至于<strong>数据存储</strong>，要考虑的是选什么样的数据库以及怎么设计 schema。选<strong>关系数据库</strong>还是<strong>NoSQL 数据库</strong>？要不要用<strong>内存数据库</strong>？要不要用<strong>图数据库</strong>？这些都需要根据数据场景慎重选择。CN-DBpedia 实际上是基于 mongo 数据库，参与开发的谢晨昊提到，一般只有在基于特定领域才可能会用到图数据库，就知识图谱而言，基于 json(bson) 的 mongo 就足够了。用到图查询的领域如征信，一般是需要要找两个公司之间的关联交易，会用到最短路径/社区计算等。</p>
<p>schema 的重要性不用多说，高质量、标准化的 schema 能有效降低领域数据之间对接的成本。我们希望达到的效果是，对于任何数据，进入知识图谱后后续流程都是相同的。换言之，对于不同格式、不同来源、不同内容的数据，在接入知识图谱时都会按照预定义的 schema 对数据进行转换和清洗，无缝使用已有元数据和资源。</p>
<h2 id="知识融合层"><a href="#知识融合层" class="headerlink" title="知识融合层"></a>知识融合层</h2><p>我们知道，目前分布在互联网上的知识常常以<strong>分散、异构、自治</strong>的形式存在，另外还具有<strong>冗余、噪音、不确定、非完备</strong>的特点，清洗并不能解决这些问题，因此从这些知识出发，通常需要<strong>融合</strong>和<strong>验证</strong>的步骤，来将不同源不同结构的数据融合成统一的知识图谱，以保证知识的一致性。所以数据支持层往上一层实际上是融合层，主要工作是对获取的数据进行标注、抽取，得到大量的三元组，并对这些三元组进行融合，去冗余、去冲突、规范化，</p>
<p>第一部分 SPO <strong>三元组抽取</strong>，对不同种类的数据用不同的技术提取</p>
<ul>
<li>从结构化数据库中获取知识：D2R<br>难点：复杂表数据的处理</li>
<li>从链接数据中获取知识：图映射<br>难点：数据对齐</li>
<li>从半结构化（网站）数据中获取知识：使用包装器<br>难点：方便的包装器定义方法，包装器自动生成、更新与维护</li>
<li>从文本中获取知识：信息抽取<br>难点：结果的准确率与覆盖率</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96.png" class="ful-image" alt="%E7%9F%A5%E8%AF%86%E8%8E%B7%E5%8F%96.png">
<p>尤其是纯文本数据会涉及到的 <strong>实体识别、实体链接、实体关系识别、概念抽取</strong> 等，需要用到许多自然语言处理的技术，包括但不仅限于分词、词性标注、分布式语义表达、篇章潜在主题分析、同义词构建、语义解析、依存句法、语义角色标注、语义相似度计算等等。</p>
<p>第二部分才到融合，目的是将不同数据源获取的知识进行融合构建数据之间的关联。包括 <strong>实体对齐、属性对齐、冲突消解、规范化</strong> 等，这一部分很多都是 dirty work，更多的是做一个数据的映射、实体的匹配，可能还会涉及的是本体的构建和融合。最后融合而成的知识库存入上一部分提到的数据库中。如有必要，也需要如 Spark 等大数据平台提供高性能计算能力，支持快速运算。</p>
<p>知识融合的四个难点：</p>
<ul>
<li>实现不同来源、不同形态数据的融合</li>
<li>海量数据的高效融合</li>
<li>新增知识的实时融合</li>
<li>多语言的融合</li>
</ul>
<h2 id="知识验证"><a href="#知识验证" class="headerlink" title="知识验证"></a>知识验证</h2><p>再往上一层主要是<strong>验证</strong>，分为<strong>补全、纠错、外链、更新</strong>各部分，确保知识图谱的<strong>一致性和准确性</strong>。一个典型问题是，知识图谱的构建不是一个静态的过程，当引入新知识时，需要判断新知识是否正确，与已有知识是否一致，如果新知识与旧知识间有冲突，那么要判断是原有的知识错了，还是新的知识不靠谱？这里可以用到的证据可以是<strong>权威度、冗余度、多样性、一致性</strong>等。如果新知识是正确的，那么要进行相关实体和关系的更新。</p>
<h2 id="知识计算和应用"><a href="#知识计算和应用" class="headerlink" title="知识计算和应用"></a>知识计算和应用</h2><p>这一部分主要是基于知识图谱计算功能以及知识图谱的应用。<strong>知识计算</strong>主要是根据图谱提供的信息得到更多隐含的知识，像是通过<strong>本体或者规则推理</strong>技术可以获取数据中存在的隐含知识；通过<strong>链接预测</strong>预测实体间隐含的关系；通过<strong>社区计算</strong>在知识网络上计算获取知识图谱上存在的社区，提供知识间关联的路径……通过知识计算知识图谱可以产生大量的智能应用如专家系统、推荐系统、语义搜索、问答等。</p>
<p>知识图谱涉及到的技术非常多，每一项技术都需要专门去研究，而且已经有很多的研究成果。Anyway 这章不是来论述知识图谱的具体技术，而是讲怎么做一个 hello world 式的行业知识图谱。这里讲两个小 demo，一个是 <strong>爬虫+mysql+d3</strong> 的小型知识图谱，另一个是 <strong>基于 CN-DBpedia+爬虫+PostgreSQL+d3</strong> 的”增量型”知识图谱，要实现的是某行业上市公司与其高管之间的关系图谱。</p>
<h1 id="Start-from-scratch"><a href="#Start-from-scratch" class="headerlink" title="Start from scratch"></a>Start from scratch</h1><h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><p>第一个重要问题是，我们需要什么样的知识？需要爬什么样的数据？一般在数据获取之前会先做个<strong>知识建模</strong>，建立知识图谱的数据模式，可以采用两种方法：一种是<strong>自顶向下</strong>的方法，专家手工编辑形成数据模式；另一种是<strong>自底向上</strong>的方法，基于行业现有的标准进行转换或者从现有的高质量行业数据源中进行映射。数据建模都过程很重要，因为标准化的 schema 能有效降低领域数据之间对接的成本。</p>
<p>作为一个简单的 demo，我们只做上市公司和高管之间的关系图谱，企业信息就用公司注册的基本信息，高管信息就用基本的姓名、出生年、性别、学历这些。然后开始写爬虫，爬虫看着简单，实际有很多的技巧，怎么做优先级调度，怎么并行，怎么屏蔽规避，怎么在遵守互联网协议的基础上最大化爬取的效率，有很多小的 trick，之前博客里也说了很多，就不展开了，要注意的一点是，<strong>高质量的数据来源是成功的一半！</strong></p>
<p>来扯一扯爬取建议：</p>
<ul>
<li>从数据质量来看，优先考虑权威的、稳定的、数据格式规整且前后一致、数据完整的网页</li>
<li>从爬取成本来看，优先考虑免登录、免验证码、无访问限制的页面</li>
<li>爬下来的数据务必<strong>保存好爬取时间、爬取来源(source)或网页地址(url)</strong><br>source 可以是新浪财经这类的简单标识，url 则是网页地址，这些在后续数据清洗以及之后的纠错(权威度计算)、外链和更新中非常重要</li>
</ul>
<p>企业信息可以在天眼查、启信宝、企查查各种网站查到，信息还蛮全的，不过有访问限制，需要注册登录，还有验证码的环节，当然可以过五关斩六将爬到我们要的数据，然而没这个必要，换别个网站就好。</p>
<p>推荐两个数据来源：</p>
<ul>
<li><a href="http://data.cfi.cn/cfidata.aspx" target="_blank" rel="external">中财网数据引擎</a></li>
<li><a href="http://www.cninfo.com.cn/cninfo-new/index" target="_blank" rel="external">巨潮资讯</a></li>
</ul>
<p>其中巨潮资讯还可以同时爬取高管以及公告信息。看一下数据<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/cfi.png" class="ful-image" alt="cfi.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/cninfo.png" class="ful-image" alt="cninfo.png"></p>
<p>换句话说，我们直接能得到规范的<strong>实体(公司、人)</strong>，以及规范的<strong>关系(高管)</strong>，当然也可以把高管展开，用下一层关系，董事长、监事之类，这就需要做进一步的清洗，也可能需要做关系的对齐。</p>
<p>这里爬虫框架我用的是 scrapy+redis 分布式，每天可以定时爬取，爬下来的数据写好自动化清洗脚本，定时入库。</p>
<h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p>数据存储是非常重要的一环，第一个问题是选什么数据库，这里作为 starter，用的是关系型数据库 MySQL。设计了四张表，两张实体表分别存<strong>公司(company)</strong>和<strong>人物(person)</strong>的信息，一张关系表存公司和高管的<strong>对应关系(management)</strong>，最后一张 SPO 表存<strong>三元组</strong>。</p>
<p><strong>为什么爬下来两张表，存储却要用 4 张表？</strong><br>一个考虑是知识图谱里典型的<strong>一词多义</strong>问题，相同实体名但有可能指向不同的意义，比如说 Paris 既可以表示巴黎，也可以表示人名，怎么办？让作为地名的 “Paris” 和作为人的 “Paris” 有各自独一无二的ID。“Paris1”（巴黎）通过一种内在关系与埃菲尔铁塔相联，而 “Paris2”（人）通过取消关系与各种真人秀相联。这里也是一样的场景，同名同姓不同人，需要用 id 做唯一性标识，也就是说我们需要对原来的数据格式做一个转换，不同的张三要标识成张三1，张三2… 那么，用什么来区别人呢？拍脑袋想用姓名、生日、性别来定义一个人，也就是说我们需要一张人物表，需要 (name, birth, sex) 来作<strong>composite unique key</strong> 表示每个人。公司也是相同的道理，不过这里只有上市公司，股票代码就可以作为唯一性标识。</p>
<p>Person 表和 company 表是多对多的关系，这里需要做 normalization，用 management 这张表来把多对多转化为两个一对多的关系，(person_id, company_id) 就表示了这种映射。management 和 spo 表都表示了这种映射，为什么用两张表呢？是出于实体对齐的考虑。management 保存了原始的关系，”董事”、监事”等，而 spo 把这些关系都映射成”高管”，也就是说 management 可能需要通过映射才能得到 SPO 表，SPO 才是最终成型的表。</p>
<p>可能有更简单的方法来处理上述问题，思考中，待更新—-</p>
<p>我们知道知识库里的关系其实有两种，一种是<strong>属性(property)</strong>，一种是<strong>关系(relation)</strong>。那么还有一个问题是 <strong>SPO 需不需要存储属性？</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/spo.png" class="ful-image" alt="spo.png"></p>
<p>最后要注意的一点是，每条记录要保存创建时间以及最后更新时间，做一个简单的版本控制。</p>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>Flask 做 server，d3 做可视化，可以检索公司名/人名获取相应的图谱，如下图。之后会试着更新有向图版本。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/eg.png" class="ful-image" alt="eg.png"></p>
<h1 id="Start-from-CN-DBpedia"><a href="#Start-from-CN-DBpedia" class="headerlink" title="Start from CN-DBpedia"></a>Start from CN-DBpedia</h1><p>把 CN-DBpedia 的三元组数据，大概 6500 万条，导入数据库，这里尝试了 PostgreSQL。然后检索了 112 家上市公司的注册公司名称，只有 69 家公司返回了结果，属性、关系都不是很完善，说明了通用知识图谱有其不完整性(也有可能需要先做一次 mention2entity，可能它的标准实体并不是注册信息的公司名称，不过 API 小范围试了下很多是 Unknown Mention)。</p>
<p>做法也很简单，把前面 <strong>Start from scratch</strong> 中得到的 SPO 表插入到这里的 SPO 表就好了。这么简单？因为这个场景下不用做实体对齐和关系对齐。</p>
<h1 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h1><p>这只是个 hello world 项目，在此基础上可以进行很多有趣的拓展，最相近的比如说加入企业和股东的关系，可以进行<strong>企业最终控制人查询</strong>(e.g.,基于股权投资关系寻找持股比例最大的股东，最终追溯至自然人或国有资产管理部门)。再往后可以做企业社交图谱查询、企业与企业的路径发现、企业风险评估、反欺诈等等等等。具体来说：</p>
<ol>
<li>重新设计数据模型 引入”概念”，形成可动态变化的“概念—实体—属性—关系”数据模型，实现各类数据的统一建模</li>
<li>扩展多源、异构数据，结合实体抽取、关系抽取等技术，填充数据模型</li>
<li>展开知识融合(实体链接、关系链接、冲突消解等)、验证工作(纠错、更新等)</li>
</ol>
<p>最后补充一下用 Neo4j 方式产生的可视化图，有两种方法。一是把上面说到的 MySQL/PostgreSQL 里的 company 表和 person 表存成 node，node 之间的关系由 spo 表中 type == relation 的 record 中产生；二是更直接的，从 spo 表中，遇到 type == property 就给 node(subject) 增加属性({predicate:object})，遇到 type == relation 就给 node 增加关系((Nsubject) - [r:predicate]-&gt; node(Nobject))，得到下面的图，移动鼠标到相应位置就可以在下方查看到关系和节点的属性。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%88%9D%E6%8E%A2/show2.png" class="ful-image" alt="show2.png">
<p><a href="https://github.com/Shuang0420/knowledge_graph_demo" target="_blank" rel="external">项目地址</a></p>
]]></content>
      
        <categories>
            
            <category> Projects </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Knowledge Graph </tag>
            
            <tag> 知识库 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习-过拟合(Andrew Ng. DL 笔记)]]></title>
      <url>http://www.shuang0420.com/2017/08/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88(Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0)/</url>
      <content type="html"><![CDATA[<p>Andrew Ng. Deep Learning Course 2 Improving Deep Neural Networks 过拟合部分的笔记。<br><a id="more"></a></p>
<p><strong>高方差(high variance)</strong> 对应的问题就是 <strong>过拟合(overfitting)</strong>，模型在训练集上表现的非常完美，然而开发集和测试集却有很高的错误率。这时需要引入正则或者多加些数据来调优。这一篇就来讲过拟合的处理方法。方差/偏差的解释戳<a href="http://www.shuang0420.com/2017/03/17/会议笔记%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/">会议笔记 - Nuts and Bolts of Applying Deep Learning</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/overfit.jpg" class="ful-image" alt="overfit.jpg">
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><p><strong>正则化(Regularization)</strong> 是最常见的方法之一。在深度学习中的正则化中，我们保留所有的 unit，但是会压缩其权重。</p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>对损失函数加上一个正则化参数，一般形式<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/regu1.png" class="ful-image" alt="regu1.png"></p>
<p>其中 $\Omega(\theta)$ 是参数范数惩罚，$\alpha \in [0,+\infty)$ 是参数范数惩罚程度的超参数，$\alpha=0$ 代表没有正则化，$\alpha$ 越大正则化惩罚越大。</p>
<p> $\Omega(\theta)$ 有 L1 和 L2 范式，如果用 L1，W 最终会是稀疏的，也就是说 W 中会有很多 0；L2 参数正则化通常被称为<strong>权重衰减(weight decay)</strong>，实际过程中一般用的是 L2。</p>
<p>$L1: \ \ \ \ \Omega(\theta)=||w||_1$<br>$L2: \ \ \ \ \Omega(\theta)={1 \over 2}||w||^2_2$</p>
<p>在原来的损失函数基础上加上正则因子：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/l2_fw.jpg" class="ful-image" alt="l2_fw.jpg"></p>
<p>权重更新：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/l2_bp.jpg" class="ful-image" alt="l2_bp.jpg"></p>
<p>$min_{w^{[1]},b{[1]},…w^{[L]},b{[L]},  }J(w,b)={1 \over m}\sum^m_{i=1}L(\hat y_{(i)}, y^{(i)}) + {\lambda \over 2m}\sum^L_{l=1}||w||^2_2$</p>
<p>发现加入 L2 后，每次梯度更新前权重会先乘以 $1-\alpha {\lambda \over m} $，相当于收缩了权重，因此 L2 正则也叫权重衰减(weight decay)。</p>
<p>这里的正则化参数$\lambda$通常使用 dev_set 来配置。</p>
<h2 id="Why-regularization"><a href="#Why-regularization" class="headerlink" title="Why regularization"></a>Why regularization</h2><p>一个直观上的理解是如果 $\lambda$ 足够大，由上一部分的计算可以得出 W 接近于 0，也就是说很多 hidden units 的权重被降成了 0，这就消除了很多 hidden units 的影响，实际上就是从下面右图的结构转换到了左图。当然事实上并不能说消除了这些 hidden units 的影响，只能说是减少，网络变得更简单罢了。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/why%20reg1.png" class="ful-image" alt="why%20reg1.png">
<p>从另一个角度考虑，$\lambda$ 变大 =&gt; $W^{[l]}$ 变小 =&gt; z 变小，看一下激活函数，以 tanh 为例，在 z 小的部分，曲线趋于线性，计算接近线性函数的值。如果每一层都是线性的话，那么无论网络有多少层，输出都是输入的线性组合而已，当然就不会过拟合啦。所以说在需要做复杂的决策的时候，$\lambda$ 不能设太大。另外使用 L2 正则需要搜索合适的 $\lambda$，花费很大。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/why%20reg2.png" class="ful-image" alt="why%20reg2.png">
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>另一种方法是 Dropout，随机删除一些 unit。Dropout 会遍历网络每一层，设置消除网络中节点的概率，对待删除的节点，删除从该节点进出的连线，得到一个节点更少、规模更小的网络，然后用 BP 对这个新的小的网络训练，持续这个过程。因为丢弃的神经元在训练阶段对 BP 算法的前向和后向阶段都没有贡献，所以每一次训练都像是在训练一个新的网络。</p>
<p>这里要讲的是 <strong>Inverted dropout</strong>，看下代码，keep_prob 表示保留任意一个 hidden unit 的概率，清除任意一个 hidden unit 的概率是 1-keep_prob，在网络第三层，过程如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_prob</div><div class="line">a3 = np.multiply(a3, d3)</div><div class="line">a3 /= keep_prob</div></pre></td></tr></table></figure>
<p>没有第三行就只是普通的 <strong>dropout</strong>，在这种情况下，测试阶段我们必须关闭 dropout 模式，去模拟训练阶段的集成网络模型，因为我们不希望最后结果是随机的，不希望预测结果受到干扰。而加了第三行就变成了 <strong>inverted dropout</strong>，我们只要在训练阶段缩放激活函数的输出值，而不用在测试阶段改变什么(只用修改一个参数)。举个例子，第三层有 50 个 units，keep_prob=0.8，有 10% 的 units 被消除了，那么在下一层，$z^{[4]}=w^{[4]}*a^{[3]}+b^{[4]}$，$a^{[3]}$ 的大小减少了 20%，为了让 $z^[4]$ 的期望值不变，或者说 $a^{[3]}$ 的期望值不变，需要补上这 20%，所以 $w^{[4]}*a^{[3]}/0.8$。</p>
<h2 id="Why-dropout"><a href="#Why-dropout" class="headerlink" title="Why dropout"></a>Why dropout</h2><p>很直接的思路，每次迭代网络都变小了，自然就减少了过拟合的可能。另一种解释是，dropout 使得神经网络不能依靠任何一个特征，因为每个特征都有可能被随机清除，这样的将结果是网络不会给一个 unit 特别大的权重，而是会 spread out weight，给每个 unit 都增加一点权重。而分散所有权重其实就产生了和 L2 类似的压缩权重的效果。</p>
<p>相对于 L2 正则，dropout 可以处理多样化的输入，然而 dropout 方法不会阻止参数过大，参数之间也不会互相牵制，所以有时需要配合使用 L2 或者其他正则化来改变这个情况。</p>
<p>另外提到的一个技巧是，可以对不同层设置不同的 keep_prob。如下图，第一层 W 矩阵是 3x7，过拟合的可能性小一些，可以留下 70% 的 unit，第二层是 7x7，更可能过拟合，所以少保留一些，设 keep_prob=0.5，由此类推，给每一层设定不同的 keep_prob，对不需要担心过拟合的层，直接设为 1。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/why_dropout1.png" class="ful-image" alt="why_dropout1.png">
<p>还有要注意的是，dropout 开启的情况下损失函数不再是定义良好的，也就没法根据损失函数的效果图来 debug，所以一般在看代码有没有 bug 的时候先会关闭 dropout。</p>
<h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><ul>
<li><strong>Data Augmentation</strong><br>解决过拟合的另一个思路是使用更多的数据，所以当数据量不够的时候，会进行数据增强。<ul>
<li>数据集的各种变换，如对图像的平移、旋转、缩放、裁剪、扭曲变形。</li>
<li>在输入层注入噪声，如去噪自编码器，通过将随机噪声添加到输入再进行训练能够大大改善神经网络的健壮性。</li>
</ul>
</li>
<li><strong>Early Stopping</strong><br>同时画出 train error 和 dev error，会发现 dev error 先下降然后到某个点会上升，所以在迭代到某次觉得结果不错的时候，提前停止训练。<br>w 在最开始的时候值很小，到后面的时候会很大，在中间的时候可能得到一个中等大小的 Frobenius norm，和 L2 正则相似，我们要选择 W 范数较小的神经网络。找 small, large, middle 几个点，不用像 L2 正则那样尝试很多的 $\lambda$ 参数，这时 early stopping 的优势，然而过早的停止可能对损失函数 J 的优化不到位，loss 不够小。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%BF%87%E6%8B%9F%E5%90%88%28Andrew%20Ng.%20DL%20%E7%AC%94%E8%AE%B0%29/early_stop.jpg" class="ful-image" alt="early_stop.jpg">
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> 过拟合 </tag>
            
            <tag> overfitting </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[EMNLP 2017 北京论文报告会笔记]]></title>
      <url>http://www.shuang0420.com/2017/08/21/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>16 号在北京举办的，邀请了国内部分被录用论文的作者来报告研究成果，整场报告会分为<strong>文本摘要及情感分析、机器翻译、信息抽取及自动问答、文本分析及表示学习</strong>四个部分。感觉上次的 <a href="http://www.shuang0420.com/2017/07/15/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/">CCF-GAIR 参会笔记</a> 写的像流水账，这次换一种方式做笔记。<br><a id="more"></a></p>
<p>分为四个部分，并没有包含分享的所有论文。第一部分写我最喜欢的论文，第二部分总结一些以模型融合为主要方法的论文，第三部分总结一些对模型组件进行微调的论文，第四部分是类似旧瓶装新酒的 idea。</p>
<h1 id="I-like"><a href="#I-like" class="headerlink" title="I like"></a>I like</h1><h2 id="Multimodal-Summarization-for-Asynchronous-Collection-of-Text-Image-Audio-and-Video"><a href="#Multimodal-Summarization-for-Asynchronous-Collection-of-Text-Image-Audio-and-Video" class="headerlink" title="Multimodal Summarization for Asynchronous Collection of Text, Image, Audio and Video"></a>Multimodal Summarization for Asynchronous Collection of Text, Image, Audio and Video</h2><p>异步的文本、图像、音视频多模态摘要，一般的文本摘要关注的是 <strong>salience, non-redundancy</strong>，这里关注的是 <strong>readability, visual information</strong>，visual information 这里说的就是图片信息，暗示事件的 highlights。考虑一个视频新闻，本身有视觉模态和音频模态，通过 ASR，还可以产生文本模态，问题是<strong>如何将这些模态连接起来，产生一个附带精彩图片的文本摘要呢？</strong> 这篇论文就在讨论这个问题，整个模型<strong>输入</strong>是一个主题的文本以及视频，<strong>输出</strong>是一段附图片的文本摘要。</p>
<p><strong>1. 预处理：</strong><br><strong>视频产生图片：</strong>CV 基本思路，把 Video 切成一个个的 shots(镜头/段落)，每个镜头可以 group(组合) 成一个 story(scene)，每一个镜头还可以细分成 sub-shots，每个 sub-shot 可以用 key-frame 来表示，<strong>选择关键帧作为视觉信息</strong>，同时认为<strong>长镜头的图片相对于短镜头更重要</strong>，基于此对图片重要性进行打分。<br><strong>音频产生文字</strong>：ASR。一方面语音识别结果并不十分准确，另一方面音频模态会有一些音频信号可以暗示我们哪些内容是重要的，基于这两点会产生两个指导策略，稍后提到。</p>
<p><strong>2. 文本重要性打分：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/multimodal_salient.png" class="ful-image" alt="multimodal_salient.png"><br>用 <strong>LexRank</strong>，句子是点，连线是重要性，进行随机游走，针对音频产生文字的两个特性使用两个<strong>指导策略:</strong></p>
<ul>
<li>如果语音识别结果和文本句子语义相同，那么让语音识别结果来推荐文本，反之不然；</li>
<li>如果语音信号比较明显，语音推荐文本，反之不然；</li>
</ul>
<p>这两条指导策略会提升<strong>文本可读性</strong>。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/multimodal_guidance.png" class="ful-image" alt="multimodal_guidance.png"></p>
<p><strong>3. 图文匹配问题：</strong><br>希望摘要能覆盖视觉信息，能解释图片，所以需要做一个文本图片分类器。图像 vcr 解码接两层前向网络，文本做一个高斯分布再求 fisher rank，也是接两层前向网络，最终将两个文本映射到同一个语义空间，计算匹配度。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/visual_match.png" class="ful-image" alt="visual_match.png"></p>
<p>一个问题是<strong>如何在复杂的句子里提出子句</strong>，作者提出了基于传统<strong>语义角色标注</strong>的方法，利用中心谓词提取匹配的 frame 信息(predicate, argument1, argument2)，好处是可以抽取语义相对独立的部分，还可以通过 frame 的设定(只取施、受、谓词)过滤如时间等图片很难反映的信息。</p>
<p><strong>4. 目标函数：</strong><br>提到了三个目标函数：</p>
<ol>
<li><strong>针对文本：</strong>对文本重要性奖励、冗余性惩罚</li>
<li><strong>针对视觉：</strong>图片重要性(镜头时长)，是否被文本摘要覆盖(是否有匹配)</li>
<li><strong>平衡</strong>视觉信息和文本信息</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/multimodal_loss_f.png" class="ful-image" alt="multimodal_loss_f.png">
<p>下面一篇 <strong>Affinity-Preserving Random Walk for Multi-Document Summarization</strong> 多文档摘要也用到了图排序模型，这里略过。</p>
<h2 id="Reasoning-with-Heterogeneous-Knowledge-for-Commonsense-Machine-Comprehension"><a href="#Reasoning-with-Heterogeneous-Knowledge-for-Commonsense-Machine-Comprehension" class="headerlink" title="Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension"></a>Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</h2><p>聚焦两个问题：<strong>如何去获取并且表示常识知识？并且如何应用获取到的常识知识进行推理？</strong> 论文尝试从多个不同来源的异构知识库当中获取了相关的信息，并将这些知识统一表示成了带有推理代价的推理规则的形式，采用一个基于注意力机制的多知识推理模型，综合考虑上述所有的知识完成推理任务。</p>
<p><strong>任务类型：</strong> 在 RocStories 数据集上，在给定一个故事的前 4 句话的情况下，要求系统从两个候选句子当中选出一个作为故事的结尾。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_story.png" class="ful-image" alt="kb_story.png"></p>
<p><strong>推理规则：</strong>统一将知识表示成如下的推理规则的形式，在<strong>关系 f</strong> 下，<strong>元素 Y</strong> 可以由<strong>元素 X</strong> 推出，其<strong>推理代价</strong>是 <strong>s</strong>。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_rule.png" class="ful-image" alt="kb_rule.png"></p>
<h3 id="知识获取"><a href="#知识获取" class="headerlink" title="知识获取"></a>知识获取</h3><p>主要从不同来源获取三类知识，包括：</p>
<ul>
<li><strong>事件序列知识(Event Narrative Knowledge)</strong><br>捕捉事件之间的时间、因果关系(去了餐馆 -&gt; 要点餐)<br>采用两个模型来捕捉这个信息，一种是<strong>基于有序的 PMI 模型</strong>，另外一个<strong>基于Skip-Gram的向量化表示模型</strong>，本质都是基于事件对在文本当中的<strong>有序共现的频繁程度</strong>来计算推理规则的代价的。</li>
<li><strong>实体的语义知识(Entity semantic knowledge)</strong><br>捕捉实体之间的语义关系<br>以星巴克为例，捕捉的第一种关系是<strong>实体间的共指关系(coreference)</strong>，比如说用“咖啡屋”来指代星巴克。从 <u>Wordnet</u> 来获取实体间上下位关系的知识。cost 是 1 当且仅当 X 和 Y 是同义词或者有上下位关系<br>​ 第二种关系是<strong>相关关系(associative)</strong>，比如说出现星巴克时可能会出现“拿铁咖啡”这一类与之相关的实体。通过 <u>Wikipedia</u> 中实体页面的链接关系来得到实体间的相关关系知识，Cost 是<strong>两个实体间的距离</strong>(Milne and Witten(2008).)</li>
<li><strong>情感的一致性知识(Sentiment coherent knowledge)</strong><br>捕捉元素间的情感关系<br>故事的结尾和故事的整体的情感应该基本上保持一致，否则结尾就会显得太突兀，那么这样的结尾就不是一个好的结尾。从 <u>SentiWordnet</u> 上来获得这种不同元素之间的情感一致性的知识。cost 为 1 if both subjective and have opposite sentimental polarity; 为 -1 if both subjective and have same sentimental polarity; 否则为 0</li>
</ul>
<p>上述推理规则代价的计算方式不同，论文使用了一种类似于 Metric Learning的方式，通过在每个类别的推理规则上增加了一个<strong>非线性层</strong>来自动学习对不同类别的推理规则代价的校准。</p>
<p>另外，由于否定的存在会反转事件关系以及情感关系的推理结果，论文对否定进行了特殊处理。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_negation.png" class="ful-image" alt="kb_negation.png"></p>
<h3 id="知识推理"><a href="#知识推理" class="headerlink" title="知识推理"></a>知识推理</h3><p><strong>如何将规则用到阅读理解之中？</strong>换句话说，就是在给定一个文档和候选答案的基础上，如何衡量候选答案是否正确？首先将文档以及候选答案都划分为<strong>元素</strong>，整个推理的过程就被转化成了一个推理规则选择以及对这个推理的合理性进行评估的过程。</p>
<p><strong>重要假设：</strong>一组有效的推理应当要能够覆盖住结尾当中的所有元素。换言之，结尾当中出现的每一个元素，都应当能够在原文当中找到它出现的依据。</p>
<p>对于同样的一个文档和候选答案，我们可以有多种多样不同的推理。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_valid.png" class="ful-image" alt="kb_valid.png"><br>上面一个推理就是一组有效的推理，这组推理是很符合人的认知的。因为我们通常会通过 Mary 和 She 之间的实体共指关系、Restaurant 和 order 之间的序列关系以及 restaurant 和 food 之间的相关关系来判断这个结果是不是成立的。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_invalid.png" class="ful-image" alt="kb_invalid.png">
<p>这个就不怎么合理，因为我们不太会去考虑一个人和一个事件之间是不是有时序关系，以及考虑 walk to 这样一个动作和 food 之间的联系。</p>
<p>采用每一种推理的可能性是不同的，用 $P(R|D, H)$ 来对这种推理的选择建模，基于<strong>元素独立性假设</strong>，得到下面的式子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_form.png" class="ful-image" alt="kb_form.png">
<p>是否选择一条推理规则参与推理一个假设元素 $h_i$，取决于对于原文当中推理得到 $h_i$ 的元素 $d_j$ 的选择，以及对于 $d_j$ 到 $h_i$ 之间推理关系的选择。然后将这个概率分布重新定义了一个重要性函数，与三个因子相关:</p>
<ul>
<li>s(h,d)<br>文档中的元素与候选答案中元素的语义匹配程度</li>
<li>a(h,f) 以及 a(d,f)<br>一个元素与这条推理规则的关系的一个关联程度，使用一个注意力函数来建模这种关联程度</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_form2.png" class="ful-image" alt="kb_form2.png">
<p>将原文到候选的推理代价定义成其所有有效的推理的期望代价<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_form3.png" class="ful-image" alt="kb_form3.png"></p>
<p>使用一个 softmax 函数来归一化所有候选的代价值，并且使用最大后验概率估计来估计模型当中的参数。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>三个 Baseline 进行了比较：</p>
<ul>
<li>Narrative Event Chain (Chambers and Jurafsky, 2008)<br>仅仅考虑是事件与事件之间的关联信息</li>
<li>DSSM (Huang et al., 2013)<br>将文档和候选答案各自表示成了一个语义向量，并且计算它们之间的语义距离</li>
<li>LSTM 模型 (Pichotta and Mooney, 2015)<br>通过对先前的事件进行序列建模来预测后面发生事件的概率。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_res_overall.png" class="ful-image" alt="kb_res_overall.png">
<p><strong>不同知识的影响</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_res_k.png" class="ful-image" alt="kb_res_k.png"><br>每一种知识都能够起到作用，移除任何一种知识都会导致系统的performance显著地降低。</p>
<p><strong>推理规则选择方式加入 attention 机制的影响</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/kb_res_att.png" class="ful-image" alt="kb_res_att.png"></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>一是推理规则怎样产生更多更复杂的推理？二是训练数据，一方面，常识阅读理解数据还是很缺乏，可能需要半监督或远程监督的方法来拓展训练数据；另一方面，可能需要扩展更多的数据源。</p>
<h2 id="Neural-Response-Generation-via-GAN-with-an-Approximate-Embedding-Layer"><a href="#Neural-Response-Generation-via-GAN-with-an-Approximate-Embedding-Layer" class="headerlink" title="Neural Response Generation via GAN with an Approximate Embedding Layer"></a>Neural Response Generation via GAN with an Approximate Embedding Layer</h2><p>生成式聊天系统可以看作是一个特殊的翻译过程，一个 question-answer pair 等价于 SMT 需要处理的一条平行语料，而 SMT 的训练过程实际上也就等价于构建问题和答案当中词语的语义关联过程。NMT 作为 SMT 高级版可以用来实现聊天回复的自动生成。这种新的自动聊天模型架构命名为 <strong>Neural Response Generation(NRG)</strong>。</p>
<p>而现在 NRG 存在问题是生成的答案严重趋同，不具有实际价值，如对于任何的用户 query，生成的结果都有可能是“我也觉得”或“我也是这么认为的”，这种生成结果被称为 <strong>safe response</strong>。safe response 产生原因如下：</p>
<ul>
<li><strong>The data distribution of chat corpus</strong></li>
<li><strong>The fundamental nature of statistical models</strong></li>
</ul>
<p>聊天数据中词语在句子不同位置的概率分布具有非常明显的长尾特性，尤其在句子开头，相当大比例的聊天回复是以“我”“也”作为开头的句子，词语概率分布上的模式会优先被 decoder 的语言模型学到，并在生成过程中严重抑制 query 与 response 之间词语关联模式的作用，也就是说，即便有了 query 的语义向量作为条件，decoder 仍然会挑选概率最大的“我”作为 response 的第一个词语，又由于语言模型的特性，接下来的词语将极有可能是“也”……以此类推，一个 safe response 由此产生。</p>
<p>常见的解决方案包括：<strong>通过引入 attention mechanism 强化 query 中重点的语义信息；削弱 decoder 中语言模型的影响；引入 user modeling 或者外部知识等信息也能够增强生成回复的多样性</strong>。这些其实是对于模型或者数据的局部感知，如果从更加全局的角度考虑 safe response 的问题，就会发现产生 safe response 的 S2S 模型实际上是陷入了一个<strong>局部的最优解</strong>，而我们需要的是给模型<strong>施加一个干扰</strong>，使其跳出局部解，进入更加优化的状态，那么最简单的正向干扰是，告知模型它生成的 safe response 是很差的结果，尽管生成这样的结果的 loss 是较小的。这样就开启了<strong>生成式对抗网络（Generative Adversarial Networks, GAN）</strong>在生成式聊天问题中的曲折探索。</p>
<p>将 GAN 引入聊天回复生成的思路：使用 encoder-decoder 架构搭建一个回复生成器G，负责生成指定 query 的一个 response，同时搭建一个判别器 D 负责判断生成的结果与真正的 response 尚存多大的差距，并根据判别器的输出调整生成器 G，使其跳出产生 safe response 的局部最优局面。</p>
<p>一个重要的问题是<strong>如何实现判别器 D 训练误差向生成器 G 的反向传播(Backpropagation)</strong>。对于文本的生成来说，一个文本样本的生成必然伴随 G 在输出层对词语的采样过程，无论这种采样所遵循的原则是选取最大概率的 greedy思想还是 beam searching，它实际上都引入了离散的操作，这种不可导的过程就像道路上突然出现的断崖，阻挡了反向传播的脚步，使对于 G 的对抗训练无法进行下去。这篇论文就针对<strong>文本生成过程中的采样操作带来的误差无法传导的实际问题</strong>提出了解决方案。</p>
<p>论文为生成器 G 构建了一个 <strong>Approximate Embedding Layer(AEL 如图中红色矩形框中所示，其细节在图右侧部分给出)</strong>，这一层的作用是近似的表达每次采样过程，在每一个 generation step 中不再试图得到具体的词，而是基于词语的概率分布算出一个采样向量。这个操作的具体过程是，在每一个 generation step 里，GRU 输出的隐状态 $h_i$ 在加入一个随机扰动 $z_i$ 之后，经过全连接层和 softmax 之后得到整个词表中每个词语的概率分布，我们将这个概率分布作为权重对词表中所有词语的 embedding 进行加权求和，从而得到一个当前采样的词语的近似向量表示（如图中右侧绿框所示），并令其作为下一个 generation step 的输入。同时，此近似向量同样可以用来拼接组成 fake response 的表示用于 D 的训练。不难看出，这种对于采样结果的近似表示操作是连续可导的，并且引入这种近似表示并不改变模型 G 的训练目标。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/NRG1.png" class="ful-image" alt="NRG1.png">
<p>取得了不错的效果。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/nrg_res.png" class="ful-image" alt="nrg_res.png"></p>
<p>详细戳<a href="https://www.leiphone.com/news/201707/uv3wNCq0MB1HSR56.html" target="_blank" rel="external">首发！三角兽被 EMNLP 录取论文精华导读：基于对抗学习的生成式对话模型浅说</a></p>
<h1 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h1><p>把传统模型和神经网络相结合。</p>
<h2 id="Translating-Phrases-in-Neural-Machine-Translation"><a href="#Translating-Phrases-in-Neural-Machine-Translation" class="headerlink" title="Translating Phrases in Neural Machine Translation"></a>Translating Phrases in Neural Machine Translation</h2><p>目前的 NMT 里 decoder 一次生成一个单词，不能进行 one-many 以及 many-many 的翻译，也就是没法做目标语言 phrase 的翻译，而 SMT 能做，所以想法是把两者结合。结合方法一般来说有两种，一是 shallow，NMT 作为 feature 放到传统框架进行预调；二是 deep，SMT 给 NMT 做推荐，NMT 用神经网络的方式接收 SMT 的东西。这篇论文用的是第二种方式。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/SMT_NMT.png" class="ful-image" alt="SMT_NMT.png"></p>
<p>SMT 先翻译一遍，把 relevant target phrases 扔到 NMT 的 Phrase Memory 里，NMT 从 Phrase Memory 里读取 target phrases 并进行打分，然后系统同时看 target phrase 和 word predictor 的结果，用一个 <strong>balancer</strong> 将 SMT 和 NMT 的优势结合起来，来判断下一个是单词还是短语的概率，来决定选哪个。所以其实产生的翻译 $y={y_1, y_2, …, y_{T_u}}$其实有两个碎片(fragments)组成，NMT 的 word predictor $w={w_1, w_2,…,w_K}$ 以及 phrase memory 里存的相关短语 $p={p_1,p_2,…p_L}$ (这里的<strong>relevant target phrases</strong> 要满足两个条件：<strong>与原文相关(adequacy)；不重复翻译(coverage)</strong>)</p>
<p>另外一点是作者还提出了基于 <strong>chunk</strong> 的翻译，SMT 对 source 提取 Chunk 信息，把布什总统、美国政府这些作为 chunk 让 SMT 预翻，然后把它们写到 phrase memory 里，后续步骤不变。chunk 的实现主要是由 sequence tagging 完成，相同 tag 表示同一个 chunk，开始符号另外标记，比如 “information security” 被标注成 “NP _B NP”，然后新的输入就变成原来的 word embedding 以及 chunking tag embedding。chunk 的好处在于限定了 source-side phrase 的信息，一方面减少了短语间的 overlap，另一方面提高了 decoding 的准确性。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/SMT_NMT_balancer.png" class="ful-image" alt="SMT_NMT_balancer.png">
<p>机器翻译相关戳<br><a href="http://www.shuang0420.com/2017/05/01/NLP 笔记 - Machine Translation/">NLP 笔记 - Machine Translation</a><br><a href="http://www.shuang0420.com/2017/07/10/NLP 笔记 - Machine Translation-Neuron models/">NLP 笔记 - Neural Machine Translation</a></p>
<p>问题是 SMT 没那么强(很难保证准确率)，NMT 也没那么弱(一个单词一个单词的翻译也能把正确的短语翻译出来)</p>
<h2 id="Incorporating-Relation-Paths-in-Neural-Relation-Extraction"><a href="#Incorporating-Relation-Paths-in-Neural-Relation-Extraction" class="headerlink" title="Incorporating Relation Paths in Neural Relation Extraction"></a>Incorporating Relation Paths in Neural Relation Extraction</h2><p>提出了对文本中的<strong>关系路径</strong>进行建模，结合 CNN 模型 (Zeng, et al. (2014). Relation classification via convolutional deep neural network. CGLING) 完成关系抽取任务。<br>传统基于 CNN 的方法，通过 CNN 自动将原始文本映射到特征空间中，以此为依据判断句子所表达的关系<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/re_cnn.png" class="ful-image" alt="re_cnn.png"></p>
<p>这种 CNN 模型存在的问题是难以理解多句话文本上的语义信息。比如说 A is the father of B. B is the father of C. 就没法得出 A 和 C 的关系，基于此，论文提出了在<strong>多样例学习机制</strong>的基础上<strong>引入关系路径编码器</strong>的方法，其实就是原来的 word embedding 输入加上一层 position embedding，position embedding 将当前词与 head entity/tail entity 的相对路径分别用两个 vector 表示。然后用 $\alpha$ 来平衡 text encoder(E) 和 path encoder(G)。<br>$$L(h,r,t)=E(h,r,t|S)+\alpha G(h,r,t|P)$$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/re_model.png" class="ful-image" alt="re_model.png">
<p>Encoder 用的是<strong>多样例学习机制(Multi-instances Learning)</strong>，也就是用一个句子集合联合预测关系，这样可以捕获更广泛的上下文。句子集合的选择方法有<strong>随机方法(rand)，最大化方法(max, 选最具代表性的)，选择-注意力机制(att)</strong>，注意力机制的效果最好。</p>
<p>实验结果：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/re_res.png" class="ful-image" alt="re_res.png"></p>
<p>之后可以继续的两个改进方向，一是对<strong>多步关系路径</strong>进行建模，使得模型可以处理更复杂的语义情况，而是<strong>将文本中的关系路径和知识图谱中的关系路径有机地结合</strong>，更好地完成关系抽取和知识图谱补全的任务。</p>
<h1 id="零件调整"><a href="#零件调整" class="headerlink" title="零件调整"></a>零件调整</h1><p>对已有模型零部件的一些调整改造。</p>
<h2 id="Towards-a-Universal-Sentiment-Classifier-in-Multiple-languages"><a href="#Towards-a-Universal-Sentiment-Classifier-in-Multiple-languages" class="headerlink" title="Towards a Universal Sentiment Classifier in Multiple languages"></a>Towards a Universal Sentiment Classifier in Multiple languages</h2><p>这里我觉得有意思的一点是作者模仿了 skip-gram 模型提出了一种同时训练多语言的 embedding 的方法。一句话解释就是通过中心词来预测自身/其他语言周围的前后词。比如说双语预料中，需要使中文能预测中文自身的周围词，英文能学习英文自身的周围词，还要通过对齐来学习中文来预测英文、英文来预测中文。skip-gram 相关戳 <a href="http://www.shuang0420.com/2016/06/21/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/">词向量总结笔记（简洁版）</a>。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/biskip1.png" class="ful-image" alt="biskip1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/biskip2.png" class="ful-image" alt="biskip2.png">
<p>C 作为 source language S 和 target language T 之间的平行语料，语料库可以分为 $C_S$ 和 $C_T$ 两部分，目标函数如下<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/biskip3.png" class="ful-image" alt="biskip3.png"></p>
<p>然后就用一个 LR 模型进行情感分类。</p>
<h2 id="Neural-Machine-Translation-with-Word-Predictions"><a href="#Neural-Machine-Translation-with-Word-Predictions" class="headerlink" title="Neural Machine Translation with Word Predictions"></a>Neural Machine Translation with Word Predictions</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/nmt_word_pred.png" class="ful-image" alt="nmt_word_pred.png">
<p>我们知道在 NMT 中，训练成本主要来自于输出层在整个 target vocabulary 上的 softmax 计算，为了减小这种 cost，各位学者做出了各种努力，比如说 Devlin et al. (2014) 从计算角度提出了 <strong>self-normalization</strong> 技术，通过改造目标函数把计算整个 matrix 的步骤优化为只计算输出层每一行的值(<a href="http://www.shuang0420.com/2017/07/10/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/">NLP 笔记 - Neural Machine Translation</a>)，而在 <strong>Neural Machine Translation with Word Predictions</strong> 这篇论文中，作者提出了一种减小 target vocabulary 的方法，主要用到了<strong>词预测机制(word predictor)</strong>。</p>
<p>之前 MT 的目标是生成一个<strong>词序列(ordered sequence)</strong>，而现在 word predictor 的目标是生成 y1..yn 的词，但是不考虑词序<strong>(no order)</strong>。</p>
<p>和上图一样的 idea，word prediction 中，initial state($WP_E$)要包含 target sentence 里的所有信息，hidden state(WP_D)要包含没有被翻译的词的所有信息。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/nmt_word_pred_frame.png" class="ful-image" alt="nmt_word_pred_frame.png"></p>
<p>$$P_{WP_E}(y|x)=\prod^{|y|}_{j=1}P_{WP_E}(y_j|x)$$<br>$$P_{WP_D}(y_j,y_{j+1},…,y_{|y|}|y_{&lt;j},x)=\prod^{|y|}_{k=j}P_{WP_D}(y_k|y_{&lt;j},x)$$</p>
<p>这样无论是效果和效率上都有了显著提升<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/nmt_word_pred_res.png" class="ful-image" alt="nmt_word_pred_res.png"></p>
<p>这个方法很好的一点是目标中的词对词预测来说是天然的标注，构造简单。然而要注意的两个点是 <strong>预测要准&amp;预测要快</strong>，否则就失去了意义。还有个问题是，按理来说较大词表质量更好然而翻译效率低，较小的词表，像这篇论文提出的，翻译某句话提前先预测生成一个新的小的词表交给 decoder，效率毫无疑问会提升，但是质量，为啥会更好？不是很理解，坐等论文。</p>
<h2 id="Towards-Bidirectional-Hierarchical-Representations-for-Attention-based-Neural-Machine-Translation"><a href="#Towards-Bidirectional-Hierarchical-Representations-for-Attention-based-Neural-Machine-Translation" class="headerlink" title="Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation"></a>Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation</h2><p>对传统 tree-based encoder 的一个改进。传统的 tree-based encoder 是 bottom-up 的结构，能抓局部信息却捕捉不了全局信息<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/tree_base_encoder.png" class="ful-image" alt="tree_base_encoder.png"></p>
<p>这篇论文对 tree-based encoder 做了改造，让它既能捕捉局部的语义信息，又能捕捉全局的语义信息。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/tree_base_encoder_goal.png" class="ful-image" alt="tree_base_encoder_goal.png"></p>
<p>bottom-up encoding 取得局部信息，top-down encoding 取得全局信息。对于 OOV(out-of-vocabulary) 问题，基于 sub-word 思想，这里单独建立一个二叉词法树并将其融入原来的句法树里。这样如下图所示，模型囊括了句子、短语、词、sub-word 各种全局/局部信息，表达力 max。然而同样带来的问题是会产生重复信息，进而可能会造成重复翻译。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/tree_encoder.png" class="ful-image" alt="tree_encoder.png"></p>
<p>为解决重复翻译的问题，或者说词/短语向量的 balance，这里还引入了 attention 机制<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/tree_decoder_attention.png" class="ful-image" alt="tree_decoder_attention.png"></p>
<p>效果有了一定提升。举个例子说明 tree-based encoder 的优势。用普通的 sequence encoder 翻译 PP 时会产生错误，普通的 tree-based 能翻译好 PP，不过 <em>境外</em> 和 <em>以外的地区</em> 还是有一点差距的，新版 tree-decoder 翻译就无压力。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/tree_encoder_ress.png" class="ful-image" alt="tree_encoder_ress.png"></p>
<h1 id="迁移-idea"><a href="#迁移-idea" class="headerlink" title="迁移 idea"></a>迁移 idea</h1><p>其实就是用已有的但可能用在别的方面的模型/思路解决现在的问题。</p>
<h2 id="A-Question-Answering-Approach-for-Emotion-Cause-Extraction"><a href="#A-Question-Answering-Approach-for-Emotion-Cause-Extraction" class="headerlink" title="A Question Answering Approach for Emotion Cause Extraction"></a>A Question Answering Approach for Emotion Cause Extraction</h2><p>这一部分之前木有研究过，先来看一下什么是 <strong>emotion cause extraction</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Document: 我的手机昨天丢了，我现在很难过。 (I lost my phone yesterday, and  I feel sad now. )</div><div class="line">Emotion：Sad  </div><div class="line">Emotional Expression: 很难过</div><div class="line">Emotion Cause: 我的手机昨天丢了</div></pre></td></tr></table></figure>
<p>任务目标是根据文本信息及其中包含的情感表达抽取出<strong>情感原因</strong>。论文作者之前发过论文，用的是基于 dependency parsing 的方法，把情感原因转化为树的分类任务，但结果依赖 dependency parsing 的准确性，而且只能处理对子句／句子级别的原因，不能处理细粒度的短语级别的原因。所以这一篇转换了思路，把 <strong>emotion cause extraction</strong> 问题转化为 <strong>question-answering</strong> 问题，提出了一种<strong>基于卷积的多层 memory network 方法</strong>，结果比之前基于树的方法提升了 2 个点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Emotional Text =&gt; Reading Text</div><div class="line">Emotional Words =&gt; Question/Query</div><div class="line">Emotion Cause Binary Classification Results =&gt; Answer</div></pre></td></tr></table></figure>
<p>用传统的 memory network 作为基础模型，reading text 用词向量 embedding 表达，存到记忆单元，待判断的情感词的词向量作为注意力单元，将 query 和 text 每个词进行内积操作，softmax 归一化作为词权重，用注意力的加权和作为整个句子的表达。为了引入词语的上下文，用了<strong>类似卷积的注意力加权方法</strong>，每个词的注意力由当前词、前文词、后文词共同决定，加权过程中根据上下文注意力对不同位置的词语进行加权，获得以短语窗口为单位的加权结果，然后进行输出。同时对记忆网络做了多层的堆叠，以学习更深的特征。最后效果得到了提升，并且在短语级别的情感原因抽取上也取得了不错的效果。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/conv_memory_network.png" class="ful-image" alt="conv_memory_network.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/conv_memory_results.png" class="ful-image" alt="conv_memory_results.png">
<p>问题来了，query 是怎么产生的呢？=&gt; <strong>数据集标注好了情感表达词！</strong></p>
<h2 id="Earth-Mover’s-Distance-Minimization-for-Unsupervised-Bilingual-Lexicon-Induction"><a href="#Earth-Mover’s-Distance-Minimization-for-Unsupervised-Bilingual-Lexicon-Induction" class="headerlink" title="Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction"></a>Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction</h2><p>主要研究无监督的双语对齐方法，也就是能无监督地联系两个词向量空间，本质上是需要词向量空间之间，或者说词向量分布之间距离的度量。用的 <strong>EMD 思想</strong>，目标就是寻找一个映射G，使得映射后的源语言词向量分布和目标语言词向量分布的 EMD 或者说 Wasserstein 距离最小化。具体等论文发表再研究了。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/EMD1.png" class="ful-image" alt="EMD1.png"></p>
<h2 id="Chinese-Zero-Pronoun-Resolution-with-Deep-Memory-Network"><a href="#Chinese-Zero-Pronoun-Resolution-with-Deep-Memory-Network" class="headerlink" title="Chinese Zero Pronoun Resolution with Deep Memory Network"></a>Chinese Zero Pronoun Resolution with Deep Memory Network</h2><p>解决中文的零指代消解问题。主要思路，用上下文来表示 ZP，使用两个 LSTM，一个对前文建模(left-to-right)，一个对后文建模(right-to-left)，然后连接两边最后一个隐层的向量作为 AZP 的表达(也可以尝试平均／求和)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/ZP1.png" class="ful-image" alt="ZP1.png"></p>
<p>接着，给定一个 AZP，会有一个 NP 集合被抽出来作为 candidate antecedents，根据每个 candidate antecedent 的重要性产生一个额外的 memory，通过对两个 LSTM(一个前向一个反向) 产生的 hidden vectors 各自进行减法操作然后连接产生最后的 vector 作为最终 candidate antecedents 的表达，并存入外部的 memory 中。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/ZP_memory.png" class="ful-image" alt="ZP_memory.png"></p>
<p>这样我们的 memory 里就有了一堆的候选 candidate antecedents，接着要对 candidate antecedents 的重要性做一个排序，选择合适的 NP 来 fill in the gap (ZP)。这里用了 attention 机制，并加入了一些人工特征(Chen and Ng (2016))，表示为 $v_t^{(feature)}$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/ZP_score.png" class="ful-image" alt="ZP_score.png"></p>
<p>模型用到了人工特征，能不能改进？ZP 位置必须事先指定，能不能自动检测？还有是对 OOV 怎么处理。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>整场报告会听下来，收获还是有很多的，只是不如想象中那么惊艳，替换零部件，加个 attention，融入传统特征，有很多的套路，最大一个收获可能是再次意识到了 <strong>attention</strong> 机制的强大，大部分论文用了 attention 结果都有大幅的改善。anyway，能提高准确率/训练效率的模型就是好模型！大家都是棒棒哒！学习！</p>
<p>周三开完会，周四飞深圳，周五上班，周六日去了澳门浪，好不容易开始笔记，发现好多内容都快忘了，有点难过 TAT，虽然我也有长短期记忆，也禁不起这么折腾…</p>
<p>最后叨叨一句，大部分论文都没发表出来，如果有错误，各位多包容，也热切欢迎大家批评指正～</p>
<p>文末彩蛋(其实还有张 Wu &amp; Tang 喝着喜茶打农药的高清大图，我先犹豫下要不要发～)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/hello1.png" class="ful-image" alt="hello1.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EMNLP%202017%20%E5%8C%97%E4%BA%AC%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E7%AC%94%E8%AE%B0/hello2.png" class="ful-image" alt="hello2.png"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> NLP </tag>
            
            <tag> Machine Translation </tag>
            
            <tag> Relation Extraction </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习-学习优化(Andrew Ng. DL 笔记)]]></title>
      <url>http://www.shuang0420.com/2017/08/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/</url>
      <content type="html"><![CDATA[<p>Andrew Ng. Deep Learning Course 2 Improving Deep Neural Networks 笔记，讲加快学习速度的几种方法，包括 input normalization，batch normalization，mini-batch，Momentum，RMSprop，Adam，learning rate decay 等。<br><a id="more"></a></p>
<h1 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h1><p>神经网络训练开始前，一般需要对输入数据做归一化处理，把数据归一化为 0 均值、方差为 1 的数据，步骤如下：</p>
<ol>
<li>零均值化<br>$u={1\over m} \sum^m_{i=1}x^{(i)}$<br>$x=x-u$</li>
<li>方差归一化<br>$\sigma^2={1\over m}\sum^m_{i=1}x^{(i)}**2$<br>$x/=\sigma^2$</li>
</ol>
<p><strong>归一化后的数据分布：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/normalize_input_0.png" class="ful-image" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/normalize_input_0.png"></p>
<p><strong>问题是为什么要归一化？或者说什么时候需要归一化？</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/normalize_input_1.png" class="ful-image" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/normalize_input_1.png"></p>
<p>一方面是可以提高学习速率，主要考虑 <strong>代价函数</strong>，左图代表未归一化的代价函数，右图代表归一化的代价函数，未归一化的代价函数是狭长的，需要更小的学习率，更多的迭代才能到最小值，而归一化后的函数是偏球形的，无论从哪个位置开始，梯度下降法都能够更直接的找到最小值，这样就可以在梯度下降中使用较大步长，优化代价函数 J 更简单快速。</p>
<p>另一个角度是从数据分布角度来看，如果训练数据和测试数据的分布不同，网络的泛化能力会大大降低，另一方面，如果每个 mini-batch 的数据分布不同，网络就需要去学习适应不同的分布，这也会影响学习速度。</p>
<p>当然有些时候并不需要归一化，比如说如果数据本身特征值就在相似范围内，那么归一化就不是很重要，相反，如果特征值的取值范围差别很大，有些特征值从 0 到 1，有些从 1 到 1000，归一化特征就很重要了。</p>
<p>只有输入特征归一化很多时候是不够的，因为除了输入层，后面隐藏层每一层的输入数据分布是一直在发生变化的，比如第二层输入就是由第一层的参数和原始输入计算得到的(z=wx+t, a=g(z))，而第一层的参数在整个训练过程中一直在变化，这必然会引起后面每一层输入数据分布的改变，这种现象(中间层在训练过程中数据分布的改变)又叫做 <strong>“Internal  Covariate Shift”</strong>。</p>
<p>一个直觉就是把归一化不只应用到输入层，也应用到深度隐藏层，这就有了 batch normalization。不过一个区别是，我们不希望 hidden unit value 必须是均值 0 方差 1 的分布(以 sigmoid 为例，如果 z 的均值为 0 方差为 1，就永远处于 sigmoid 线性部分，相当于把这一层网络学到的特征分布给搞坏了，我们不希望这样)，所以对每个 mini-batch 求均值、方差、归一化、并学习缩放参数：</p>
<p>$$<br>\begin{aligned}<br>u &amp;= {1\over m} \sum^m_{i=1}z^{(i)} \\<br>\sigma^2 &amp;= {1\over m}\sum^m_{i=1}z^{(i)}**2 \\<br>z^{(i)}_{norm} &amp;= z^{(i)}-{\mu \over \sqrt{\sigma^2 + \epsilon} } \\<br>z^{N(i)} &amp;= \gamma z^{(i)}_{norm} + \beta \\<br>  \end{aligned}<br>$$</p>
<p>$\gamma, \ \beta$ 都是需要学习的参数，每一层都不同，可以用 gradient descent 来学习，$\beta^{[i]}=\beta^{[i]}-\alpha d\beta^{[i]}$。在深度学习框架，一般是把 batch norm 应用于 batch norm layer，直接一行代码就搞定啦。</p>
<p>在测试阶段，每次只有一个样本，$\mu, \sigma$ 哪里来？这需要我们进行估算，可以在网络训练完后运行整个训练集得到 $\mu, \sigma$，也可以在训练时做指数加权平均，来粗略估算 $\mu, \sigma$，然后在测试中使用。</p>
<p>最后提一点，由于 Norm 对 z 加了噪音，所以有轻微的 regularization effect 的效果，迫使后面的单元不会过分依赖前面任何一个单元， 当然越大的 mini-batch size 会减小这种效果。</p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h2><p>对整个训练集进行梯度下降时，必须处理完整个训练集后，才能将进行一步梯度下降法，如果把训练集分割为小一点的子训练集(mini-batch)，每次同时处理单个 mini-batch，那么 1个 epoch 虽然只遍历了一次训练集，却能够做 5000 个(如下图)梯度下降</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/mini_batch.png" class="ful-image" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/mini_batch.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/mini_batch2.png" class="ful-image" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/mini_batch2.png">
<p>Mini-batch 梯度下降法比 batch 梯度下降运行更快，那么一个问题是 <strong>怎么选择 mini-batch size?</strong></p>
<ul>
<li>size=m<br>Batch gradient descent(BGD)<br>每一次迭代都对 m 个样本进行计算，计算量大，耗时长</li>
<li>size=1<br>如果 minibatch 的 size 为 1，就称为 <strong>随机梯度下降(stochastic gradient descent, SGD)</strong>，每次迭代仅对一个样本计算梯度，随机梯度下降永远不会收敛，只会在最小值附近波动，但并不会达到并在最小值处停下，也就很容易陷入局部最优解<br>另外，每次处理一个样本，SGD 并没有利用 vectorization 的优势</li>
<li>1 &lt; size &lt; m<br>介于 BSD 和 SGD 之间，每次选取一定量的训练样本将进行迭代，速度比 BGD 快，比 SGD 慢，精度比 BGD 低，比 SGD 高</li>
</ul>
<p>另外还有一种方法是 <strong>带 mini-batch 的 SGD</strong>，用来缓解 SGD  每次用一个样本容易陷入局部最优解的问题，过程是样本分为 mini-batch，然后在对每个 mini-batch 里计算单个样本的梯度然后求和取平均作为该 mini-batch 的梯度来更新参数。</p>
<p>最后还有一种 <strong>Online GD</strong>，应对线上实时的、有不间断的训练数据产生的应用。<strong>在线学习(Online Learning)</strong> 算法就是充分利用实时数据的一个训练算法。与 mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的好处是可以追踪模型的变化趋势。比如搜索广告的点击率(CTR)预估模型，网民的点击行为会随着时间改变。用batch算法(每天对所有历史数据重新训练更新模型)，耗时长，也无法及时反馈用户的点击行为迁移。</p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>比标准的 gradient descent 要快，基本想法是，计算梯度的 <strong>指数加权平均数(exponentially weighted average of gradients)</strong>，并利用该梯度更新权重。</p>
<img class="ful-image" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%8A%A0%E5%BF%AB%E5%AD%A6%E4%B9%A0%E9%80%9F%E5%BA%A6/momentum1.jpeg">
<p>直观上讲，希望横轴学习更快，纵轴学习更慢(不希望有那么剧烈的波动)，Momentum 的过程如下：</p>
<p><strong>On iteration t:</strong><br>  Compute dw, db on current mini-batch<br>$$<br>\begin{aligned}<br>​    v_{dw} &amp;= \beta v_{dw}+(1-\beta)dw \\<br>​    v_{db} &amp;= \beta v_{db}+(1-\beta)db \\<br>​    w &amp;= w-\alpha v_{dw} \\<br>​    b &amp;= b-\alpha v_{db}  \\<br>  \end{aligned}<br>$$<br>​<br>平均这些梯度，纵轴上的波动平均值接近于 0，平均过程中，正负抵消，纵轴方向摆动变小，而所有的的微分都指向横轴，横轴的平均值仍然很大，横轴方向运动更快，相当于走了更直接的路径。把代价函数想象成碗状函数，那么微分项提供了加速度，momentum v 像速度，$\beta$ 表现出摩擦性，所以球不会无限加速下去，原来的 gradient descent 每一步独立于上一步，而现在球可以从向下滚获得动量。</p>
<p>初始化的 $V_{dw}=0，V_{db}=0$，两个矩阵分别和 dw、db 有相同维数。也有做法会把 $1-\beta$ 去掉，这样的话其实 $\alpha$ 就需要根据 $1 \over 1-\beta$ 做相应变化，会影响到学习率 $\alpha$ 的最佳值。</p>
<p>最常见的 $\beta$ 是0.9，平均了前10次迭代的梯度，效果不错。</p>
<p>另外关于指数加权平均，各数值的加权而随时间而指数式递减，越近期的数据加权越重，这个过程实际上是一个递推的过程，不像普通求解平均值需要保留所有的数值然后求和除以n，这种方法只需要保留 n-1 时刻的平均值和 n 时刻的数值就好，这样可以减少内存和空间的做法。</p>
<p>优点是积累的速度v可以让我们越过局部最小点，但也可能造成在全局最优点来回震荡。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop 用的是 moving average of squared gradients，它会联系之前的每一次梯度变化情况来更新学习步长。如果当前得到的梯度为负，那学习步长就会减小一点点；如果当前得到的梯度为正，那学习步长就会增大一点点。</p>
<p><strong>On iteration t:</strong><br>​    Compute dw, db on current mini-batch<br>$$<br>\begin{aligned}<br>​    s_dw &amp;= \beta s_dw + (1-\beta)s_dw^2 \ square, elementwise \\<br>​    s_db &amp;= \beta s_db + (1-\beta)s_db^2 \\<br>​    w &amp;= w-\alpha {dw \over \sqrt{s_{dw}+\epsilon}} \\<br>​    b &amp;= b-\alpha {db \over \sqrt{s_{db}+\epsilon}} \\<br>  \end{aligned}<br>$$</p>
<p>我们希望 $s_dw$ 相对很小，$s_db$ 相对很大，这样就可以减缓纵轴上的变化，就可以用更大的学习率。在实践过程中，分母不能为 0，所以要加上个很小的 $\epsilon$</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>Adam(Adaptive Moment Estimation)，</strong>将 Momentum 和 RMSprop 结合到一起，并加入了 bias correction(指数加权平均刚开始计算时, $v_t$ 与 $\theta_t$ 偏差很大, bias correction 用于解决该问题)，优点是在经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</p>
<p>$v_dw=0, s_db=0, v_db=0, s_db=0$</p>
<p>On iteration t:<br>​    Compute dw, db on current mini-batch<br>$$<br>\begin{aligned}<br>​    v_dw &amp;= \beta_1 v_{dw} + (1-\beta_1)dw, \ \ v_db = \beta_1 v_{db} + (1-\beta_1)db \\<br>​    s_dw &amp;= \beta s_{dw}+(1-\beta_2)dw^2, s_db = \beta s_{db}+(1-\beta_2)db^2 \\<br>​    v^{corrected}_{dw} &amp;= {v_{dw} \over (1-\beta^t_1)}, \ \ v^{corrected}_{db}={v_{db} \over (1-\beta^t_1)} \\<br>​    s^{corrected}_{dw} &amp;= {s_{dw} \over (1-\beta_2^t)}, \ \ s^{corrected}_{db}={s_{db} \over (1-\beta_2^t)} \\<br>​    w &amp;= w - \alpha {v^{corrected}_{dw} \over \sqrt{s_{dw}+\epsilon}}, \ \ b = b - \alpha {v^{corrected}_{dw} \over \sqrt{s_{dw}+\epsilon}} \\<br>  \end{aligned}<br>$$</p>
<p>$\beta_1: 0.9 $ -&gt; dw<br>$\beta_2: 0.999$ -&gt; $dw^2$<br>$\epsilon: 10^{-8}$</p>
<h1 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h1><p>在学习初期，你能承受较大的步伐，但当开始收敛的时候，小的学习率能让你步伐小一点，下面列举了几种 decay 的方法，$\alpha$ 是初始学习率。<br>$$<br>\begin{aligned}<br>\alpha &amp;= {1 \over 1 + decay-rate * epoch-num}\alpha_0 \\<br>\alpha &amp;= 0.95^{epoch-num}\alpha_0 \\<br>\alpha &amp;= {k \over \sqrt{epoch-num}}\alpha_0 \\<br>\alpha &amp;= {k \over \sqrt t}\alpha_0 \\<br>\end{aligned}<br>$$</p>
<p>或者也可以使用离散下降的学习率，开始的 5000 step 用这个学习率，之后的 5000 step 把原来的学习率降低一半……</p>
<p>还有一种更朴实的，人工来控制学习率，像 babysitting 一样，不断观察着训练效果，人工来调整。</p>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - A Neural Attention Model for Abstractive Sentence Summarization]]></title>
      <url>http://www.shuang0420.com/2017/08/08/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/</url>
      <content type="html"><![CDATA[<p>接之前的<a href="http://www.shuang0420.com/2017/05/10/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/">NLP 笔记 - Text Summarization</a>，介绍一种 abstractive summarization 方法 textsum。<br><a id="more"></a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Summarization-Phenomena"><a href="#Summarization-Phenomena" class="headerlink" title="Summarization Phenomena"></a>Summarization Phenomena</h2><p>先来观察下文本摘要的一些现象，一般是通过对源文本进行<strong>泛化(generalization)、删除(deletion)、改写(paraphrase)</strong>等操作来产生目标文本，也就是文本摘要。看一下例子</p>
<ul>
<li><strong>Generalization</strong><br><strong>Source:</strong> <strong>Russian Defense Minister Ivanov</strong> called Sunday for the creation of a joint front for combating global terrorism.<br><strong>Target:</strong> <strong>Russia</strong> calls for joint front against terrorism.</li>
<li><strong>Deletion</strong><br><strong>Source:</strong> Russian Defense Minister Ivanov called <strong>Sunday</strong> for the creation of a joint front for combating global terrorism.<br><strong>Target:</strong> Russia calls for joint front against terrorism.        </li>
<li><strong>Paraphrase</strong><br><strong>Source:</strong> Russian Defense Minister Ivanov called Sunday for the creation of a joint front <strong>for combating</strong> global terrorism.<br><strong>Target:</strong> Russia calls for joint front <strong>against</strong> terrorism.            </li>
</ul>
<h2 id="Types-of-Sentence-Summary"><a href="#Types-of-Sentence-Summary" class="headerlink" title="Types of Sentence Summary"></a>Types of Sentence Summary</h2><p>对于上面等一些操作，产生了文本摘要的几类技术模型。</p>
<ul>
<li><strong>Compressive: deletion-only:</strong><br>压缩，通过对源文本的删除操作产生目标文本。</li>
<li><strong>Extractive: deletion and reordering:</strong><br>抽取式，摘要句子完全从源文档中抽取形成，详细见 <a href="http://www.shuang0420.com/2017/05/10/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/">NLP 笔记 - Text Summarization</a></li>
<li><strong>Abstractive: arbitary transformation</strong><br>合成式，从源文档中抽取句子并进行改写形成摘要。</li>
</ul>
<p>看下几种方法的比较<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/elements.png" class="ful-image" alt="elements.png"></p>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>目前已经有的一些相关工作</p>
<ul>
<li><strong>Syntax-Based</strong><br>Dorr, Zajic, and Schwartz 2003; Cohn and Lapata 2008; Woodsend, Fend, and Lapata 2010</li>
<li><strong>Topic-Based</strong><br>Zajic, Dorr, and Schwartz 2004</li>
<li><strong>Machine Translation-based</strong><br>Banko, Mittal, and Witbrock 2000</li>
<li><strong>Semantics-Based</strong><br>Liu et al 2015</li>
</ul>
<h1 id="Textsum"><a href="#Textsum" class="headerlink" title="Textsum"></a>Textsum</h1><p>Textsum，论文戳 <a href="https://arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="external">A Neural Attention Model for Abstractive Sentence Summarization</a>，tensorflow 代码戳 <a href="https://github.com/tensorflow/models/tree/master/textsum" target="_blank" rel="external">tensorflow/models/textsum</a>。Textsum是一种 abstractive model，主要有下面四个组件构成：</p>
<ul>
<li>Neural language model</li>
<li>Attention-based encoder model</li>
<li>Generation model</li>
<li>Beam-search decoder &amp; additional features model extractive elements</li>
</ul>
<p>下面逐个进行讨论。</p>
<h2 id="Neural-language-model"><a href="#Neural-language-model" class="headerlink" title="Neural language model"></a>Neural language model</h2><p>借鉴了机器翻译的思想，在 <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a> (Bengio et al. 2013)的基础上，加了一个 attention-based encoder(Bahdanau et al. 2014) 来学习输入文本的 latent soft alignment。</p>
<p>先看一下整体逻辑，看图说话，下面左图圈出来的部分是一个<strong>feed-forward neural network language model(NNLM)</strong>，详情戳<a href="http://www.shuang0420.com/2017/07/10/NLP%20笔记%20-%20Machine%20Translation-Neuron%20models/">NLP 笔记 - Machine Translation(Neuron models)</a>，它的输入是当前 output 已经产生的上下文 $y_c$(注意这个 context 窗口的长度是固定的)，与词向量矩阵 E 做个映射，经过线性变化以及激活函数得到得分矩阵 U，再经过一个 softmax 层得到概率分布形式的输出，也就是下一个单词 $y_{i+1}$，现在我们要加上左边的 attention-based encoder，对输入 source text 和当前我们拥有的 context $y_c$ 做一个 encode，放大的话就是右图。对 context embedding 和 source 的每个单词做一个加权的点乘(weighted dot product)得到 attention distribution 也就是 P，对 source text 做一个局部的平滑(local smoothing)，然后对 source 的 smoothing 版本和 attention distribution 做一个点乘，就得到了我们要的 enc，最后把两边的东西一起扔到 softmax 里得到输出。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/abs_graph.png" class="ful-image" alt="abs_graph.png">
<p>看个例子，如下图，行是 source text，列是 output text，假定我们已经产生了 “russia calls”，现在目标是产生下一个单词 for，我们的模型将利用 attention distribution of source 以及 embedding of context，来得到 for 这个 next word。因为我们用了 bag of words 的 smoothing 版本，也就是说我们对 called 周围的单词进行了加权，attention 很大程度上会指向 called，具体逻辑见下一部分 encoders。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/abs_ex1.png" class="ful-image" alt="abs_ex1.png"></p>
<p>上面说到了，这里选择的 encoder 是 attention-based encoder，下面来看一下为什么选这个 encoder。</p>
<h2 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h2><h2 id="Bag-of-words-Encoder"><a href="#Bag-of-words-Encoder" class="headerlink" title="Bag-of-words Encoder"></a>Bag-of-words Encoder</h2><p>BoW encoder 把 encoder 参数估计看作是一个 uniform distribution，赋予了每个词相同的权重，同时忽略了单词词序。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/bow_encoder.png" class="ful-image" alt="bow_encoder.png">
<h3 id="Convolutional-Encoder"><a href="#Convolutional-Encoder" class="headerlink" title="Convolutional Encoder"></a>Convolutional Encoder</h3><p>Convolutional Encoder 通过局部卷积来考虑邻近单词的互动，单词/词组权重由网络训练产生。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/conv%20model2.png" class="ful-image" alt="conv%20model2.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/conv%20model.png" class="ful-image" alt="conv%20model.png"></p>
<h3 id="Attention-Based-Encoder"><a href="#Attention-Based-Encoder" class="headerlink" title="Attention-Based Encoder"></a>Attention-Based Encoder</h3><p>灵感来自 <strong>Bahdanau et al. (2014) attention-based contextual encoder</strong>。和 BoW 相似的一个简单的模型，只不过把 BoW 的 uniform distribution 替换成一个 input 和 summary 之间的 soft alignment P (如下图)，是一个机器翻译的思路。之后用学习到的这个 soft alignment 来给输入的平滑版本进行加权。比如说，如果当前的上下文和位置 i 能很好的对齐，那么单词 $x_{i-Q},…,x_{i+Q}$ 就会被 encoder 赋予更高的权重。与 NNLM 结合的话这个模型可以看作是 attention-based neural machine translation model 的精简版。</p>
<p>$G \in R_{D*V}$: embdding of the context<br>$P \in R_{H*(CD)}$: new weight matrix parameter mapping between the context embedding and input embedding<br>$Q$: smoothing window</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/abs_ex1.png" class="ful-image" alt="abs_ex1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/attention_decoder.png" class="ful-image" alt="attention_decoder.png">
<h2 id="Generating-Summaries"><a href="#Generating-Summaries" class="headerlink" title="Generating Summaries"></a>Generating Summaries</h2><p>用 beam-search decoder 来寻找最好的 summary，这是机器翻译模型的标准化方法(Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015) ，这里做了一点改进。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/beam%20search.png" class="ful-image" alt="beam%20search.png">
<p>算法如上，我们需要维护一个词库 V 以及前 k 个最好的 context，复杂度是 O(KNV)。</p>
<h2 id="Tuning"><a href="#Tuning" class="headerlink" title="Tuning"></a>Tuning</h2><p>最后论文提到了一个 tuning，这其实是在 word embedding 维度太低，语义表示不完全时，去人工添加一些特征，这可以 handle 一些 rare/unseen words 的问题，让系统的 extractive/abstractive 趋势更为平衡。<br>通过修改评分函数来直接估计 summary 概率，使用对数线性模型。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20A%20Neural%20Attention%20Model%20for%20Abstractive%20Sentence%20Summarization/tuning.png" class="ful-image" alt="tuning.png"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> textsum </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[递归神经网络 RNN 笔记]]></title>
      <url>http://www.shuang0420.com/2017/07/21/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>介绍 RNN 及其变种。Stanford cs231n Lecture 10: Recurrent Neural Networks 的部分笔记。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>传统的神经网络中间层每一个神经元和输入的每一个数据进行运算得到一个激励然后产生一个中间层的输出，并没有记忆能力，在输入为序列的情况下的效果有限。而很多东西是受到时域信息影响的，某一时刻产生的激励对下一时刻是有用的，递归神经网络就具备这样的记忆能力，它可以把这一刻的特征与上一刻保存的激励信息相结合(并不只是上一刻 s2 是由 s1、s2 产生的，$s_n$ 不仅有 $s_{n-1}$ 的信息，还包括 $s_{n-2}$、$s_{n-3}$…$s_1$ 的信息。</p>
<p> <strong>CNN vs RNN:</strong></p>
<ol>
<li>CNN 需要固定长度的输入、输出，RNN 的输入可以是不定长的</li>
<li>CNN 只有 one-to-one 一种结构，而 RNN 有多种结构，如下图具体在下一部分再做介绍。</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/one_to_many.png" class="ful-image" alt="one_to_many.png">
<p>   <strong>One-to-one:</strong> Vanilla Neural Networks<br>    最简单的结构，然而效果不怎么好<br>   <strong>One-to-many:</strong> Image Captioning, image -&gt; sequence of works<br>   ​输入一个图片，输出一句描述图片的话<br>   <strong>Many-to-one:</strong> Sentiment Classification, sequence of words -&gt; sentiment<br>   ​输入一句话，判断是正面还是负面情绪<br>   <strong>Many-to-many:</strong> Machine Translation, seq of words -&gt; seq of words<br>   ​有个延时的，譬如<a href="http://lib.csdn.net/base/machinetranslation" target="_blank" rel="external">机器翻译</a>。<br>   <strong>Many-to-many:</strong> Video classification on frame level<br>   ​输入一个视频，判断每帧分类。</p>
<p><strong>Applications:</strong></p>
<ul>
<li>language models</li>
<li>translation</li>
<li>caption generation</li>
<li>program execution</li>
</ul>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>以 (Vanilla) Recurrent Neural Network 为例<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/one_to_one.png" class="ful-image" alt="one_to_one.png"></p>
<p><strong>Forward(前向传播)</strong></p>
<p>$$h_t=f_W(h_{t-1},x_t)$$</p>
<p><strong>Notice: the same function and the same set of parameters are used at every time step.</strong></p>
<p>$$h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t+b)$$<br>$$y_t=Softmax(W_{hy}h_t+c)$$</p>
<p>就是多加上 $h_{t-1}$，activation function 用的是 tanh</p>
<p><strong>Backwards: </strong>Backpropagation Through Time(BPTT)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/bptt.png" class="ful-image" alt="bptt.png"><br>(请原谅我的字～) U 最大特征值(Largest singular value)大于 1 的时候，梯度爆炸，小于 1 的时候，则可能发生梯度消失。&gt; 1 的时候还比较容易察觉，&lt; 1 的时候往往难以发现了。<br>梯度爆炸可以采用<strong>Gradient clipping</strong>的方式(设个天花板)避免，如梯度大于 5 的时候就强制梯度等于5。梯度消散可以通过改变 RNN 结构如采用LSTM的方式抑制。</p>
<p><strong>Loss function(损失函数)</strong><br><strong>Cross-entropy</strong><br>$$E_t(y_t,\hat y_t)=-y_tlog \hat y_t$$<br>$$E(y_t,\hat y_t)=-\sum_t y_tlog \hat y_t$$</p>
<h2 id="其他结构"><a href="#其他结构" class="headerlink" title="其他结构"></a>其他结构</h2><p>经典 RNN 结构，输入和输出序列等长，适用范围比较小，比如 Char RNN，视频每一帧的分类任务等。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/many_to_many.png" class="ful-image" alt="many_to_many.png"></p>
<p>双向 RNN：输入信息一个正向，一个反向，原因是信息等依赖关系顺序不定。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/birnn.png" class="ful-image" alt="birnn.png"></p>
<p>输入是序列，输出是单个值，通常处理序列分类问题，比如句子分类任务。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/many_to_one.png" class="ful-image" alt="many_to_one.png"></p>
<p>输入是值，输出是序列，decoder 时，可以把原始输入信息 X 作为序列输出开始的输入 ，也可以把 X 作为每个阶段的输入，可以处理 <strong>从图像/类别生成文字/语音/音乐</strong> 的问题，输入可以是 CNN 的 FC 层特征。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/one_to_many_.png" class="ful-image" alt="one_to_many_.png"></p>
<p>经典的 Encoder-Decoder 或者说 Seq2Seq 模型，先用一个 Many-to-one 将输入编码成一个上下文向量 c，这个 c 可以是 Encoder 最后一个隐状态，也可以是这个隐状态的变换，还可以是所有隐状态的一个变换。有了 c，另一个 One-to-many 会对其进行解码，把 c 作为初始状态 $h_0$ 输入到 Decoder 中。这一部分也有各种变换，比如说把 c 当做 decoder 时每一步的输入。<br>Seq2Seq 结构应用非常广泛，在机器翻译、文本摘要、问答系统中都很常见。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/sequence_to_sequence.png" class="ful-image" alt="sequence_to_sequence.png"></p>
<h2 id="Example-Character-level-Language-Model-Sampling"><a href="#Example-Character-level-Language-Model-Sampling" class="headerlink" title="Example: Character-level Language Model Sampling"></a>Example: Character-level Language Model Sampling</h2><p><strong>Vocabulary:</strong> [h, e, l, o]<br><strong>Exaple training sequence:</strong> “hello”</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/ch_lm_s.png" class="ful-image" alt="ch_lm_s.png">
<p>At test-time sample characters one at a time, feed back to model<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/ch_lm_s2.png" class="ful-image" alt="ch_lm_s2.png"></p>
<p><strong>Backpropagation through time:</strong><br>Foward through entire sequence to compute loss, then backward through entire sequence to compute gradient.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/btt1.png" class="ful-image" alt="btt1.png"></p>
<p><strong>Truncated Backpropagation through time:</strong><br>Run forward and backward through chunks of the sequence instead of whole sequence.<br>Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/tbtt2.png" class="ful-image" alt="tbtt2.png"></p>
<h2 id="Image-Captioning"><a href="#Image-Captioning" class="headerlink" title="Image Captioning"></a>Image Captioning</h2><blockquote>
<p>可参考的 paper:<br>xplain Images with Multimodal Recurrent Neural Networks, Mao et al.<br>Deep Visual-Semantic Alignments for Generating Image Descriptions, Karpathy and Fei-Fei<br>Show and Tell: A Neural Image Caption Generator, Vinyals et al.<br>Long-term Recurrent Convolutional Networks for Visual Recognition and Description, Donahue et al.<br>Learning a Recurrent Visual Representation for Image Caption Generation, Chen and Zitnick</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/CNN%26RNN.png" class="ful-image" alt="CNN%26RNN.png">
<p>下一篇重点解释。</p>
<h2 id="RNN-局限"><a href="#RNN-局限" class="headerlink" title="RNN 局限"></a>RNN 局限</h2><p>RNN 前后依赖的特性导致两个输入距离比较远的时候作用会非常弱，比如说下面的例子， China 对 Chinese 起决定作用，然而距离太远难以产生关联。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/%E5%B1%80%E9%99%901.png" class="ful-image" alt="%E5%B1%80%E9%99%901.png"></p>
<p>一个解决方案就是设计 Gate，保存重要记忆，也就是下面要讲的 LSTM，LSTM 网络模型解决了 RNN 梯度消散问题，同时保留了长时序列的相关性。</p>
<h1 id="RNN-变种-LSTM"><a href="#RNN-变种-LSTM" class="headerlink" title="RNN 变种: LSTM"></a>RNN 变种: LSTM</h1><p>RNN 中重复的模块只有一个非常既简单的结构 tanh 层，而 LSTM 将重复模块改成了一个相对复杂的结构，这样可以有效避免梯度消失的问题。LSTM 要理解的核心是 GATE，它就是靠这些 gate 的结构让信息有选择性的影响 RNN 中每个时刻的状态。所谓 gate 的结构就是一个使用 sigmoid 神经网络和一个 elementwise multiplication 的操作，这两个操作合在一起就是一个门的结构，sigmoid 作为激活函数的全连接神经网络层会输出一个 0-1 之间的数值，描述当前输入有多少信息量可以通过这个结构，类似于门，门打开时(sigmoid 输出为1时)，全部信息都可以通过；当门关上时(sigmoid 输出为0)，任何信息都无法通过。</p>
<p><strong>Vanilla RNN</strong><br>$$<br>\begin{aligned}<br>  h_t &amp; = tanh(W_{hh}h_{t-1}+W_{xh}x_t) \\<br>  &amp; =  tanh((W_{hh} W_{hx})\binom{h_{t-1}}{x_t}) \\<br>   &amp; = tanh(W \binom{h_{t-1}}{x_t}) \\<br>  \end{aligned}<br>$$</p>
<p>看一下 Vanilla RNN 到 LSTM 前向传播的变化<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/1.png" class="ful-image" alt="1.png"></p>
<p><strong>LSTM 网络结构图</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/LSTMgate2.png" class="ful-image" alt="LSTMgate2.png"></p>
<p><strong>分步：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/LSTMgate.png" class="ful-image" alt="LSTMgate.png"><br><strong>f</strong>: Forget gate, Whether to erase cell<br><strong>i</strong>: Input gate, whether to write to cell<br><strong>g</strong>: Gate gate (?), How much to write to cell<br><strong>o</strong>: Output gate, How much to reveal cell</p>
<p><strong>Step 1:</strong><br>decide what information we’re going to throw away from the cell state<br>新输入 $x_t$ 和前状态 $h_{t-1}$ 通过 sigmoid 变换决定 $c_{t-1}$ 的哪些信息可以舍弃，$f_t$ 与 $C_{t-1}$ 做 elementwise multiplication 运算，对部分信息进行去除</p>
<p><strong>Step 2:</strong><br>decide what new information we’re going to store in the cell state<br>新输入 $x_t$ 前状态 $h_{t-1}$ 通过 sigmoid 变换告诉 $c_t$ 哪些新信息想要保存，通过 tanh 变换建一个新的侯选值向量。<br>$i_t:$ 新信息添加时的系数(对比 $f_t$)，$g_t$ 单独新数据形成的控制参数，用于对 $C_t$ 进行更新。</p>
<p><strong>Step 3:</strong><br>update the old cell state<br>根据旧的控制参数 $C_{t-1}, f_t, i_t, g_t$ 组合生成最终生成该时刻最终控制参数</p>
<p><strong>Step 4:</strong><br>decide what we’re going to output<br>新输入 $x_t$ 和前状态 $h_{t-1}$ 通过 sigmoid 变换决定 cell state 的哪些信息需要输出，与 cell state 通过 tanh 变换后的值相乘，产生此刻的新的 LSTM 输出</p>
<p>核心内容 $C_t$，信息流控制的关键，参数决定了 $h_t$ 传递过程中，哪些被保存或舍弃，参数被 Gate 影响。<strong>怎样实现 Gate 对 C 影响？</strong> Sigmoid 函数系数据定 $C_t$ 参数的变化，而 Sigmoid 函数决定于输入。<br>整个过程：$C_t$ 信息舍弃 =&gt; $C_t$ 局部生成 =&gt; $C_t$ 更新 =&gt; $C_t$ 运算</p>
<p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external"></a></p>
<p><strong>Backpropagation:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/LSTMbp.png" class="ful-image" alt="LSTMbp.png"></p>
<h1 id="LSTM-变种-Peephole-connection"><a href="#LSTM-变种-Peephole-connection" class="headerlink" title="LSTM 变种: Peephole connection"></a>LSTM 变种: Peephole connection</h1><p>$C_t$ 受到 Gate 参数影响 =&gt; 二者相互影响<br>$f_t=\sigma(W_f[C_{t-1},h_{t-1},x_t]+b_f)$<br>$f_t=\sigma(W_i[C_{t-1},h_{t-1},x_t]+b_i)$<br>$f_t=\sigma(W_o[C_{t-1},h_{t-1},x_t]+b_o)$</p>
<h1 id="LSTM-变种-遗忘-更新互为补充"><a href="#LSTM-变种-遗忘-更新互为补充" class="headerlink" title="LSTM 变种: 遗忘/更新互为补充"></a>LSTM 变种: 遗忘/更新互为补充</h1><p>Gate 的 <strong>遗忘/更新</strong> 不再独立，而是互为补充。</p>
<p>$C_t = f_t * C_{t-1} + i_t * g_t$ =&gt; $C_t = f_t * C_{t-1} + (1-f_t) * g_t$</p>
<h1 id="LSTM-变种-GRU-Gated-Recurrent-Unit"><a href="#LSTM-变种-GRU-Gated-Recurrent-Unit" class="headerlink" title="LSTM 变种: GRU(Gated Recurrent Unit)"></a>LSTM 变种: GRU(Gated Recurrent Unit)</h1><ol>
<li>遗忘，更新 Gate 结合(不是独立，也不是互补)形成更新门</li>
<li>控制参数 $C_t$ 与输出 $h_t$ 结合，直接产生带有长短记忆能力的输出 link<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20RNN%20%E7%AC%94%E8%AE%B0/GRU.png" class="ful-image" alt="GRU.png">
</li>
</ol>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul>
<li>RNN 应用在 language model 中，学习的实质是 P(下个词|前面多个词)</li>
<li>Vanilla RNNs 非常简单，但效果不好，通常会使用 LSTM 和 GRU，这两种结构能提高 gradient flow</li>
<li>在一层 RNN 中不同时间序列中激励函数和权值参数都一致</li>
<li>RNN 也可以是多层 RNN，其网络是整个一模型一起训练的</li>
<li>RNN 存在着梯度爆炸和梯度消散的问题。梯度爆炸可以采用<strong>Gradient clipping</strong>的方式避免，梯度消散可以采用 LSTM 的网络结构抑制</li>
<li>中间层的特征带有前后时间特征，对一些任务很有用</li>
<li>额外参数：单双向/梯度上限/梯度计算范围</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CCF-GAIR 参会笔记]]></title>
      <url>http://www.shuang0420.com/2017/07/15/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>回国后参加的第一场大规模的人工智能峰会，感觉收获还是很多的，对部分之前学过的东西做了一遍梳理，对当前工业界的发展现状有了一定了解，对学术界的最新进展有了些体会，最后还结识了一批志同道合的朋友，值回票价。这一篇作为峰会的笔记，记录一些我认为重要的东西，有些零散，参加的场次有限，看官们见谅～<br><a id="more"></a></p>
<h1 id="AI-发展前沿"><a href="#AI-发展前沿" class="headerlink" title="AI 发展前沿"></a>AI 发展前沿</h1><p>这一部分各位演讲嘉宾从宏观角度概括了下 AI 的发展现状。</p>
<h2 id="AI-的典型任务和应用"><a href="#AI-的典型任务和应用" class="headerlink" title="AI 的典型任务和应用"></a>AI 的典型任务和应用</h2><blockquote>
<p><strong>潘云鹤</strong>——中国工程院院士，国务院学位委员会委员、中国科学技术协会顾问、中国图象图形学学会名誉理事长、中国计算机学会理事、中国人工智能学会副理事长。中国智能CAD领域的开拓者，创造性地将人工智能引入CAD技术。</p>
</blockquote>
<p>主要介绍了 AI 行业的一些基本情况，像人工智能的典型任务、应用、重点方向之类，毕竟是刚开场的演讲，听的比较仔细。<br>人工智能应用的 7 个基本领域：</p>
<ul>
<li><strong>机器定理证明(逻辑和推理)</strong> — 仿解题者<br>主要是研究计算机进行逻辑推理</li>
<li><strong>机器翻译(自然语言理解)</strong> — 仿译者<br>研究计算机自然语言理解</li>
<li><strong>专家系统(问题求解和知识表达)</strong> — 仿专家(医生，维修者)<br>研究问题求解和知识表达</li>
<li><strong>博弈(树搜索)</strong> — 仿弈者<br>最早的时候研究搜索，后来逐渐转化为神经网络</li>
<li><strong>模式识别(多媒体认知)</strong> — 仿认知者<br>主要用于视觉、听觉或者各种各样媒体的认知</li>
<li><strong>学习(神经网络)</strong> — 仿初学者<br>主要是研究神经网络</li>
<li><strong>机器人和智能控制(感知和控制)</strong> — 仿生物动作<br>主要是研究和模拟人的感知和控制</li>
</ul>
<p>形成了符号学派、连接学派、行为学派。<br>AI 从萌芽到现在的蓬勃发展期间出现了三次低谷，得到的教训大致是<strong>驱动 AI 的发展主要是靠创新、软件和知识，而非硬件</strong>，<strong>知识不能靠专家手工表达，要靠从环境中自动学习</strong></p>
<p>一些新的技术已经初露端倪，表现在近几年 AI 技术的前沿中，主要有</p>
<ul>
<li><strong>大数据智能</strong><br>DeepMind 已为谷歌挣钱，提高了谷歌 15% 的用电效率</li>
<li><strong>基于网络的群体智能</strong><br>群智计算按难易程度分为三种类型：实现任务分配的<strong>众包模式(Crowd-sourcing)</strong>，较复杂、支持<strong>工作流模式的群体智能(Complex workflows)</strong>，以及最复杂的<strong>协同求解问题的生态系统模式(Problem solving ecosystem)</strong><br>大规模个体通过互联网参与和交互，可以表现出超乎寻常的智慧能力，是解决开放复杂问题的新途径，成功的如 AppStore，Wiki 百科等</li>
<li><strong>跨媒体推理</strong><br>在语言、视觉、图形和听觉之间语义贯通，是实现联想、推理、概括等智能的重要关键</li>
<li><p><strong>无人系统</strong><br>无人系统迅猛发展的速度远快于机器人，因为类人和类动物的机器人，往往不如机械进行智能化和自主化升级来的高效<br>如 (海康)智能分拣机器人，泊车机器人</p>
<p>​<br><strong>AI 的基础和目标:</strong> 模拟人的智能 =&gt; 人机融合 =&gt; 群体智能</p>
</li>
</ul>
<p>直接上图了<br><img class="ful-image" alt="1.1.jpeg"><br><img class="ful-image" alt="1.2.jpeg"><br><img class="ful-image" alt="1.3.jpeg"><br><img class="ful-image" alt="1.4.jpeg"><br><img class="ful-image" alt="1.5.jpeg"><br><img class="ful-image" alt="1.6.jpeg"><br><img class="ful-image" alt="1.7.jpeg"><br><img class="ful-image" alt="1.8.jpeg"></p>
<h2 id="The-Rise-of-AI-And-The-Challenges-of-Human-Aware-AI-Systems"><a href="#The-Rise-of-AI-And-The-Challenges-of-Human-Aware-AI-Systems" class="headerlink" title="The Rise of AI And The Challenges of Human-Aware AI Systems"></a>The Rise of AI And The Challenges of Human-Aware AI Systems</h2><blockquote>
<p><strong>Subbarao Kambhampati</strong>——AAAI主席，亚利桑拿州立大学教授，同时也在很多的国际机构任职，主要研究自动化的决策机制，特别是在人工感知的人工智能领域。</p>
</blockquote>
<p>同样讲了 AI 进展，不过划分方式略有不同。<br><img class="ful-image" alt="2.1.jpeg"></p>
<p>提供了一个很有意思的视角， AI 系统的发展过程和人的学习过程是截然相反的。为什么呢？因为在有 conscious theories 的基础上编程更容易，毕竟 cognitive/reasoning intelligence 一直在发展嘛，还因为我们并没有特别意识到/了解 perceptual intelligence，为什么当今的 AI 能够走入千家万户的视野呢？正是因为 perceptual abilities 让 AI 不再是瞎的聋的，现在的 AI 可以更加的智能，可以存在在各种载体之上，如智能手机，智能音箱，汽车等等。</p>
<img class="ful-image" alt="2.2.jpeg">
<p>教授还指出了几个研究方向：<strong>从小数据中学习、机器的常识、不完整性和交互。</strong></p>
<h2 id="模式识别研究回顾与展望"><a href="#模式识别研究回顾与展望" class="headerlink" title="模式识别研究回顾与展望"></a>模式识别研究回顾与展望</h2><blockquote>
<p><strong>谭铁牛</strong>——中国科学院院士、英国皇家工程院外籍院士、发展中国家科学院院士、巴西科学院外籍院士，发表专著11部、文章500多篇，还有100多项发明。获得了一系列的国家级的奖，现在是中国图象图形学会理事长、中国人工智能学会副理事长。</p>
</blockquote>
<p>主要讲了模式识别的 <strong>基本概念／发展现状／研究方向</strong>。深深的觉得 ppt 做的实在很棒，讲的也很棒，感觉最棒的！直接上图！<br><img class="ful-image" alt="3.1.jpeg"><br><img class="ful-image" alt="3.2.jpeg"><br><img class="ful-image" alt="3.3.jpeg"></p>
<p><strong>模式识别的现状：</strong></p>
<ul>
<li>面向特定任务的模式识别已经取得突破性进展，有的性能已可与人类媲美；</li>
<li>统计与机遇神经网络的模式识别目前占主导地位，深度学习开创了新局面；</li>
<li>通用模式识别系统依然任重道远；</li>
<li>鲁棒性、自适应性和可泛化性是进一步发展的三大瓶颈。</li>
</ul>
<p><strong>现有模式识别的局限性:</strong></p>
<ul>
<li><strong>鲁棒性</strong><br>​    容易收到对抗样本的攻击<br>​    如 CV 中的局部形变／旋转变化／光照变化／遮挡／背景凌乱／多样性／尺度变化</li>
<li><strong>自适应性差</strong><br>​    不能自适应开放环境下数据分布的快速变化</li>
<li><strong>可解释性差</strong><br>​    人在判别过程中可以轻易使用具有逻辑关系的规则；机器特别是深度学习常作为黑箱模型，无法为高风险应用提供具有说服力的决策</li>
</ul>
<p>在人工智能非常火爆的时候，模式识别领域有如下值得<strong>关注的研究方向</strong>：</p>
<ul>
<li><strong>从神经生物学领域获得启发的模式识别</strong><br>神经元(类型、发放特性、突触可塑性)／神经回路(前向、侧向、后向连接)／功能区域(多任务协同、注意、记忆机制)／学习机制(人的学习特性和学习过程)<br>学习过程：发育学习、强化学习<br>学习方法：迁移学习、知识学习<br>学习效果：生成学习、概念学习(e.g., Bayesian Program Learning)</li>
<li><strong>面向大规模多源异构数据的鲁棒特征表达</strong><br>考虑如何在跨景跨媒、多源异质的视觉大数据中找到具有较好泛化性和不变性的表达<br>e.g., 定序测量特征(Ordinal Measure)</li>
<li><strong>结构与统计相结合的模式识别新理论</strong></li>
<li><strong>数据与知识相结合的模式识别</strong><img class="ful-image" alt="3.4.jpeg"></li>
<li><strong>以互联网为中心的模式识别</strong></li>
</ul>
<h1 id="AI-学术前沿"><a href="#AI-学术前沿" class="headerlink" title="AI 学术前沿"></a>AI 学术前沿</h1><p>这一场主要分析了学术界的各位的最新研究进展</p>
<h2 id="Smart-Robotic-Systems-that-Work-in-Real-Outdoor-Environments"><a href="#Smart-Robotic-Systems-that-Work-in-Real-Outdoor-Environments" class="headerlink" title="Smart Robotic Systems that Work in Real Outdoor Environments"></a>Smart Robotic Systems that Work in Real Outdoor Environments</h2><blockquote>
<p><strong>金出武雄</strong>——机器人领域的鼻祖级专家，卡耐基梅隆大学的创始人，同时也是非常著名的荣誉教授，主要研究机器人工、机器人学，享誉全世界的指路者。</p>
</blockquote>
<p>虽然是自家学校的，还是木有认真听。开头主要讲了一个例子，在车上加一个 smart headlight，使得雨滴在图片里面的成像变得很淡，可以毫无障碍在雨雪天的夜晚出行， 因为他一开始就在讲汽车灯光在下雨天各种反射怎么怎么样……然后我就……就没兴趣了TAT……</p>
<img class="ful-image" alt="4.1.jpeg">
<h2 id="A-society-of-AI-Agents-群体智能的社会"><a href="#A-society-of-AI-Agents-群体智能的社会" class="headerlink" title="A society of AI Agents 群体智能的社会"></a>A society of AI Agents 群体智能的社会</h2><blockquote>
<p>汪军， 伦敦大学学院（UCL）计算机系教授、互联网科学与大数据分析专业主任。主要研究智能信息系统，主要包括数据挖掘，计算广告学，推荐系统，机器学习，强化学习，生成模型等等。他发表了100多篇学术论文，多次获得最佳论文奖。是国际公认的计算广告学和智能推荐系统专家。</p>
</blockquote>
<p>讲多智能体怎样竞争怎样协作怎样通讯。从多智体群体的特征切入，介绍多智体的强化学习特性。同一环境下，不同的智体既可以单独处理各自的任务，又可以联合在一起处理优化一个主要的目标方程(一般是长期的 reward 方程)。强化学习的优点就是在没有足够训练数据时，系统会和环境进行交互，得到反馈信息，交互过程中不断学习，在应用上比较灵活</p>
<p><strong>Multi-agent reinforcement learning(MARL)</strong> 的应用有互联网广告的机器招标(Machine Bidding in Online Advertising)，通过对投放广告后的用户的反馈的不断学习，就可以帮助企业精准找到目标用户。再如 AI 玩星际游戏(AI plays StarCraft)等，主要考虑的是智体的通讯问题，多个智体之间怎么合作，达到双向联通。</p>
<img class="ful-image" alt="5.1.jpeg">
<p>其他的应用比如宜家的商场设计，需要模拟人的行为流向，同时让环境跟着用户的变化而变化，把路径安排最优，来优化用户的停留时长(stay)和购买金额(purchase) ，再比如迷宫的设计，一方面给定一个环境，让智体通过强化学习找到最优的策略走出来，另一方面是当智体的智能水平不再提高时，就可以来优化环境，使它更难出去。</p>
<p>多智能强化学习的研究仍然处于非常初步的阶段，主要有两个问题</p>
<ul>
<li><strong>Problem1:</strong> current research is limited to only less than 20 agents<br>许多现实场景中的多智体数量可以达到百万、甚至千万级，比如 uber 的场合怎么办？共享单车怎么办？</li>
<li><strong>Problem2: </strong> the environment is assumed to be given and not designable<br>不是去学环境的设计，而是让人工智能更加适应环境，而很多情况下，很多环境也需要有适应的过程，比如说宜家，强化学习的环境，根据用户的变化，变化环境。用随机 agents 模拟人在店铺走的情况，收集人力图，反馈到铺面设计，来最大用户停留时间/用户消费</li>
</ul>
<p>怎么处理百万级的 agent，一种方法是从自然界中找灵感，可以学习生态学／生物学的 self-organisation(自组织)理论，当一些小的智体遵循这个规则的时候，就会体现一个种群的特质。这些模型可以用宏观的事情解决宏观的问题，但是缺少一种微观的方法去观察这个世界。老虎和羊的模型 Lotka-Volterra model the dynamics of the artificial population 描述了相互竞争的两个种群数量之间的动态关系。汪军教授在此模型上做了一个创新，提出了老虎-羊-兔子模型(Tiger-sheep-rabbit model)，给智体强化学习能力以后，就和 LV 模型中的猞猁抓兔子的动态显现十分相似。当智体之间联合一起优化某一个目标或单独优化自己的目标，作为一个群体，他们就有了内在的规律。如果找到这些规律，对开发智体模型是非常有帮助的。</p>
<h1 id="计算机视觉专场"><a href="#计算机视觉专场" class="headerlink" title="计算机视觉专场"></a>计算机视觉专场</h1><h2 id="Video-Content-3C-Creation-Curation-Consumption"><a href="#Video-Content-3C-Creation-Curation-Consumption" class="headerlink" title="Video Content 3C: Creation, Curation, Consumption"></a>Video Content 3C: Creation, Curation, Consumption</h2><blockquote>
<p>梅涛，微软亚洲研究院资深研究员。</p>
</blockquote>
<p>CV understanding 问题分为几个层次(or 不同粒度)：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/CV%20understanding.png" class="ful-image" alt="CV%20understanding.png"></p>
<p>从图像到视频的变换，实际是从二维到三维的变换，除了要理解每一帧的图片的 object，还要理解帧/物体在 cross, multiple-frame的动态运动。</p>
<p>视频内容的生命周期大致可以分为三个部分，即视频的<strong>创作(creation)、处理(curation)和消费(consumption)</strong>。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/video%20lifecycle.png" class="ful-image" alt="video%20lifecycle.png">
<h3 id="Creation"><a href="#Creation" class="headerlink" title="Creation"></a>Creation</h3><p>先来看一下视频是怎么产生的。首先把 Video 切成一个个的 shots(镜头/段落)，每个镜头 group(组合) 成一个 story(scene)，每一个镜头还可以细分成 sub-shots，每个 sub-shot 可以用 key-frame 来表示。通过这种分层的结构可以把 video 这样一个非线性的东西分成一个 structure data，这就是做 video summarization 的前提。</p>
<p>Video summarization 分两种，static summary 和 dynamic summary。</p>
<ul>
<li><strong>static summary:</strong> automatic selection of representative video keyframes(e.g., 5 keyframes)<br>主要是选择具有高度代表性的 keyframe 来表示视频，一个 5 分钟的 video 给你 5 个 keyframe 你就知道这个视频讲了什么</li>
<li><strong>dynamic summary:</strong> automatic generation of a short clip for fast preview(e.g., 30 second)<br>8 分钟的 video 生成 30 秒的 highlight，概括 video 的所有内容。spots video 知道哪些 segment 是最 hightlight 最精彩的，你最应该看哪部分</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/video%20summ1.png" class="ful-image" alt="video%20summ1.png">
<p><strong>Video Creation</strong> 的涉及的技术还有 <strong>stabilization and photography</strong>，怎么把拍的抖动的 video 变得平稳，怎么分辨 video 中的物体哪些是静止的哪些是动态的，然后产生 animation；另外和文字结合的技术/应用如 <strong>video generation</strong>，给出文字，来产生一个 video，像给一个动作，video 能展示这个动作。</p>
<h3 id="Curation"><a href="#Curation" class="headerlink" title="Curation"></a>Curation</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/caption.png" class="ful-image" alt="action">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/P3D%20res.png" class="ful-image" alt="P3D%20res.png">
<p><strong>Curation</strong> 涉及的技术还有 <strong>pose estimation from RGB video</strong>，应用比如说智能健身教练，通过把动作分解来告诉你哪一个动作是不准确的；还有在 <strong>video captioning</strong> 方面的应用，可以把视频描述做的更生动，比如说原来只能识别一群人(a group of people)，加上 pose estimation 可以识别一群人在跳舞(a group of people are dancing)。还有<strong>auto-commenting</strong>，给视频自动评论，把原有的 caption 变成有人的情感对话的 comments，比如说棒球比赛的动图，可以产生<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">I love baseball</div><div class="line">that&apos;s how to play baseball</div><div class="line">That&apos;s an amazing play</div><div class="line">- [Li, Yao, Mei, MM&apos;16]</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/chat.png" class="ful-image" alt="chat.png">
<p><strong>Video caption</strong> 的经典方法是2D/3D CNN 学一个 video 的表达，然后做一个 embedding，把 embedding 的结果加上 text embedding 放到 rnn 去学。如果加上一个 relevant loss，结果还会与内容相关<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/caption.png" class="ful-image" alt="caption.png"></p>
<h3 id="Consumption"><a href="#Consumption" class="headerlink" title="Consumption"></a>Consumption</h3><p>最后一个环节是 <strong>Video Consumption</strong>，应用像 video instagram，可以给 video 加上 style，或者说滤镜，变成 <strong>stylist video</strong>；也可以做 <strong>style transfer</strong>，比如说把油画的 style transfer 到自拍/风景照中；还有的应用像 <strong>segmentation</strong>，把人抠出来放到另一个场景里，把异地情侣放到一个房间里聊天；更难一点的还可以做 <strong>storytelling</strong>，把很多视频中的图像组合成一个吸引人的故事讲给观众听；另外还有 <strong>video advertising</strong>，来分析广告应该放什么位置，什么时间段选什么广告(是不是和插入点信息相关)，比如说广告和内容可以无缝衔接，也可以在故事高潮的时候/最 boring 的时候放广告。视频广告主要有两种度量方式，一个是 discontinuity，来衡量一个广告插入点的故事情节是否连续；另一个是 attractiveness，来衡量一段原始视频的内容是否精彩。这两者的权衡同事也就是广告商(advertiser)或用户(viewer)需求的权衡。</p>
<h2 id="产业落地和产业化路径"><a href="#产业落地和产业化路径" class="headerlink" title="产业落地和产业化路径"></a>产业落地和产业化路径</h2><p>下一个是演讲者是中山大学教授、商汤科技执行研发总监林倞老师，又是母校，主题是 <strong>深度驱动的人工智能：从学术创新到产业落地</strong>，主要还是介绍了下商汤现在的主要业务。再下一个演讲者是魏京京，图麟科技 CEO，主题是 <strong>计算机视觉的产业化路径</strong>，也就不多说了。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/%E5%95%86%E6%B1%A4.png" class="ful-image" alt="%E5%95%86%E6%B1%A4.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/%E5%9B%BE%E9%BA%9F.png" class="ful-image" alt="%E5%9B%BE%E9%BA%9F.png"></p>
<h3 id="X-数据驱动的-Seeta-视觉引擎与平台"><a href="#X-数据驱动的-Seeta-视觉引擎与平台" class="headerlink" title="X 数据驱动的 Seeta 视觉引擎与平台"></a>X 数据驱动的 Seeta 视觉引擎与平台</h3><blockquote>
<p><strong>山世光</strong>，中科院计算所研究员、博导，基金委优青，CCF青年科学奖获得者，现任中科院智能信息处理重点实验室常务副主任，中科视拓创始人、董事长兼CTO，任IEEE，TIP，CVIU， PRL，Neurocomputing，FCS等国际学术刊物的编委。</p>
</blockquote>
<p>主要记录几个点，原文可以看 <a href="https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&amp;mid=2247486265&amp;idx=1&amp;sn=1fc14df04e82246a5cd9a07057d07942&amp;chksm=ec57bcbedb2035a892af812d7b5c956d2ff2724e7a49106be3ba727324317bda1f5a97c2408c&amp;mpshare=1&amp;scene=1&amp;srcid=0723MBBvTvlpZbp7a3JWqs28&amp;key=188350111ce3d613ac3953993817f8b9270b517e7b0422a23477402db218ce42c4c732a3a525331afcbc526a9eb4caf21181d9e39bc1e66aefddf83addea04865518c9653c66346e4e23146c56d33db5&amp;ascene=0&amp;uin=Nzc1Mjc0MDYw&amp;devicetype=iMac+MacBookPro13%2C3+OSX+OSX+10.12.3+build(16D30" target="_blank" rel="external">中科视拓CTO山世光：如何用X数据驱动AI成长 | CCF-GAIR 2017</a>&amp;version=12020610&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=fYv8LK1dmtHud6qxDzyk0NQoTcNdpkYbWuD9vGD4pepNxlPLVOKnmpPFU3ig4ZlD)</p>
<p><strong>增强学习适合：可以自动判断对错的领域</strong><br>例如：棋类、游戏类<br>视觉、听觉、理解、情感，通通不太行</p>
<p><strong>深度学习适合：好数据肥沃、可以归纳学习的领域</strong></p>
<ol>
<li>数据采集、获取、标注便利的领域，例如视觉，语音，互联网+行业</li>
<li>需要演绎推理的领域非常困难，理解需要演绎和引申，没有太多可作为的地方</li>
</ol>
<p>X 数据有五个含义：</p>
<ul>
<li><strong>大数据</strong><br>大数据驱动的视觉引擎的设计；</li>
<li><strong>小数据</strong><br>在很多场景下，我们获得智能的能力并不是依赖于大量的数据学习，反而是一些小数据，所以要考虑的十四，怎样在小数据的情况下使得我们的算法也能够有效果。通常的思路自然是迁移学习，最简单的是做 finetune 模型，把一个已经训练好的模型，再用小量的数据做调整和优化，使得它适应这些小数据所代表的应用场景。另一个思路是多模态的数据，实现跨模态的比对和融合利用；</li>
<li><strong>脏数据</strong><br>很好理解了，现在的数据都有大量的噪声，要雇人在大量的数据中把它们标注出来太不容易了，干脆就基于有噪声的数据实现机器学习。所以山世光等人在今年提出具有“自纠错学习”能力的深度学习方法，在深度学习的过程中，一边去学习算法，一边去估计哪些样本的标签可能是错误的，把一些可能错误的标签修正过来，从而得到更好的算法。<br>这里的脏可能还有另一层含义，比如说图像识别中有遮挡的情况，山世光等人也提出了一个算法，能够把面部的遮挡部分、脏的部分补出来，补出来之后再去实现感知。把这两个过程迭代起来，形成联合的学习，这个工作发表在去年的 CVPR 上面，取得了非常不错的效果。</li>
<li><strong>无监督数据</strong><br>比如说特定的物体没有标注数据，怎样利用没有标签的数据来训练模型。解决方法如 Bi-shifting 深度模型，实现从源域到无监督目标域到知识迁移。(M.Kan, et al, ICCV2015)</li>
<li><strong>增广数据</strong><br>通过对已有少量数据进行修改的方式，来生成大量数据。有两类方法，一种是模型方法，比如说 3D 重建，另一种是 GAN 方法。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/x5.png" class="ful-image" alt="x5.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/x5.2.png" class="ful-image" alt="x5.2.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/xx.png" class="ful-image" alt="xx.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/xx2.png" class="ful-image" alt="xx2.png">
<p>AI 未来发展需要注意的三个问题：</p>
<ul>
<li><strong>鲁棒性</strong><br>鲁棒性可能是 AI 和视觉智能一个最致命的问题</li>
<li><strong>多模态数据协同</strong><br>对于人来说，除了眼睛之外，我们有很多其它信息来对我们的智力发育提供帮助，包括语音、姿态、动作、以及背后有大量的知识库作支撑。因此，人本身是需要一个多模态系统协同工作的鲁棒AI，这带给我们一个思路，AI的成长和发育也需要多模态</li>
<li><strong>基于小样本的自主学习</strong><br>AI 发育的非常重要的一点，就是如何基于小数据甚至是 0 数据完成智能的发育和后天的学习。比如说我跟大家描述一下某个人长成什么样子，你并没有见过这个人，你并没有见过这个人的照片，我们称为0数据，你如何能够识别这个人，是对AI的一个挑战。类似这样的应用场景，将来会有非常多的研究空间。</li>
</ul>
<h3 id="圆桌对话"><a href="#圆桌对话" class="headerlink" title="圆桌对话"></a>圆桌对话</h3><p>五位圆桌嘉宾包括：中科院计算所研究员、中科视拓董事长兼 CTO 山世光、阅面科技 CEO 赵京雷、图麟科技 CEO 魏京京、瑞为智能 CEO 詹东晖以及臻识科技 CEO 任鹏。<br>感觉最有意义的一个问题是：<strong>你觉得人工智能技术在落地过程中最大的难点是什么</strong>。几位嘉宾一致同意说是<strong>闭环</strong>。最核心的是<strong>市场需求和当前技术达到的闭环</strong>，产品结果和客户需求有差距，要怎么迭代。做产品的时候有很多取舍的东西，要去平衡你的功能、性能，满足客户的指标、期望，最后在产品设计和成本相关的这些方面，其实一个核心就是闭环。创业过程中<strong>最关键的不是你有什么技术，而是你把已有的技术跟他的痛点结合，这个问题不是技术的问题，基本上就是商业问题，需要付出的努力不是做技术的来做的，而是你要接地气，围着客户做讨论、设计和服务，让他慢慢接受你，这是很痛苦的，也是做技术创业需要转换的地方。</strong>我们所谓的技术完美，当然我们希望「快、准、稳」。快是随便找一个很烂的芯片就可以做；准是什么情况下都能工作；稳是不会出现差错，这样落地和闭环就不会出现难题了，<strong>但是我们现在真的做不到。比如刚才说的万分之一，很多时候是达不到的，在这个阶段最难的是怎么去找到客户的需求和技术的边界能够结合的应用，再配合上其他的一些条件，能够满足用户的需求。</strong>刚才几位说得都对，技术不完美还是一个很大的障碍。</p>
<p><a href="http://t.cn/R9h2w8p" target="_blank" rel="external">CV+圆桌对话：算法不是唯一考量，创业公司的商业闭环才是最大难点</a></p>
<h1 id="机器学习专场"><a href="#机器学习专场" class="headerlink" title="机器学习专场"></a>机器学习专场</h1><h2 id="AI-in-games"><a href="#AI-in-games" class="headerlink" title="AI in games"></a>AI in games</h2><blockquote>
<p>田渊栋，Facebook 人工智能研究院研究员</p>
</blockquote>
<p>游戏平台，作为虚拟环境，是非常好的一个数据来源。因为量大，无穷尽，获取速度快，有科学的平台可以提供重复科研的环境。游戏数据的特点：</p>
<ul>
<li>infinite supply of fully labeled data</li>
<li>controllable and replicable</li>
<li>low cost per sample</li>
<li>faster than real-time</li>
<li>less safety and ethical concerns</li>
<li>complicated dynamics with simple rules</li>
</ul>
<p>像围棋，非常简单的规则可以得到一个很有意思的过程，可以从过程中抽取一些概念，得到一些人类推理的知识。然而现实生活中规则太多，不一定是个很好的研究开端。</p>
<p>游戏平台作为数据做研究也存在一些问题：</p>
<ul>
<li>Algorithm is slow and data-inefficient<br>人玩游戏玩几盘就好了，计算机要大量的数据，虽然道最后计算机可以玩的很好</li>
<li>require a lot of resources<br>研究限于比较大的公司</li>
<li>abstract game to real-world<br>游戏能不能扩展到现实世界是个很大的问题<br>让游戏更真实 =&gt; 现在的游戏越来越真实，和现实生活非常接近</li>
<li>hard to benchmark the progress</li>
</ul>
<p>显而易见，解决方案是：<br>=&gt; better algorithm/system<br>=&gt; better environment</p>
<p>这两个都是目前游戏 AI 的研究方向，还有 domain transfer，也是一个研究方向。</p>
<blockquote>
<p>Even with a super-super computer, it is not possible to search the entire space.</p>
</blockquote>
<p>很多人以为机器能够搜索游戏的所有可能，所以能胜过人类，其实这是一个误解。我们能做的是有限搜索，从当前局面出发，哪一步是最好的，extensive search =&gt; evaluation</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.3.png" class="ful-image" alt="6.3.png">
<p>星际这样的游戏，可能的步数是指数级的，怎么做还是个开放性的问题</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.4.png" class="ful-image" alt="6.4.png">
<p>围棋用深度学习的方法 CNN 学出评估函数。怎么估计下一步怎么走？怎么评估？</p>
<p><strong>How to model policy/value function?</strong><br><strong>traditional approach</strong></p>
<ul>
<li>Many manual steps</li>
<li>Conflicting parameters, not scalable<br>有些时候规则是冲突的，大师没办法直观的告诉 ai 什么时候用哪条规则，有的时候是看直觉的，ai 没法学</li>
<li>require strong domain knowledge</li>
</ul>
<p><strong>deep learning</strong></p>
<ul>
<li>End-to-end training<br>lots of data, less tuning</li>
<li>minimal domain knowledge</li>
<li><p>amazing performance</p>
<p>​<br>Case study: AlphaGo   依靠 Monte Carlo Tree Search, aggregate win rates, and search towards the good nodes. 从根节点一路找最大概率走到叶节点，这样的好处是如果发现有些招数很糟糕，那么走了两步就不会继续往下走了，如果发现有些招数不错，可能会一路走下去，走个五六十层，这样可能会发现一些很有意思的招数。</p>
</li>
</ul>
<p>主要策略思想<br><strong>Policy network：</strong> 根据大量人类棋谱学出来<br><strong>Value network: </strong> 网络自己学，把状态中某个步骤拿出来，给每个状态提供一个标定，拿出来训练</p>
<p><strong>future work</strong></p>
<ul>
<li>richer game scenarios<br> multiple base(expand? Rush? Defending?)<br> ​more complicated units 精细控制每个决定，现在智能控制9个</li>
<li>more realistic action space<br>Assign one action per unit</li>
<li>model-based reinforcement learning<br>MCTS with perfect information and perfect dynamics also achieves ~70%<br>现在游戏都是慢慢摸索的，然而对复杂的游戏需要对游戏有大致估计</li>
<li>self-play(Trained AI versus Trained AI)<br>自对弈</li>
</ul>
<h2 id="互联网数据下的模型探索"><a href="#互联网数据下的模型探索" class="headerlink" title="互联网数据下的模型探索"></a>互联网数据下的模型探索</h2><blockquote>
<p>盖坤博士，阿里，P9,阿里妈妈精准展示广告投放</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/1704.05194.pdf" target="_blank" rel="external">Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction</a><br><a href="https://arxiv.org/pdf/1706.06978.pdf" target="_blank" rel="external">Deep Interest Network for Click-Through Rate Prediction</a></p>
<h3 id="互联网数据和经典模型"><a href="#互联网数据和经典模型" class="headerlink" title="互联网数据和经典模型"></a>互联网数据和经典模型</h3><p>互联网数据业界经典的处理方法，典型问题：CTR 预估(点击率预估)</p>
<p><strong>数据特点</strong></p>
<ul>
<li><strong>样本量大</strong> 百亿样本</li>
<li><strong>特征维度大</strong> 无损表示<br>id 特征，原始特征，稀疏的鉴别式特征，轻松超十亿级<br>原始特征，用户特征+物料特征<br>加上交叉特征，笛卡尔积之类的，轻松上千亿</li>
<li><strong>稀疏</strong>数据</li>
</ul>
<p><strong>经典做法</strong></p>
<ul>
<li>简单线性模型 Logistic Regression<br>线性模型+sigmoid一个非线性变换，变成一个概率模式</li>
<li>稀疏正则 L1-Norm 特征筛选<br>压制不太重要的特征</li>
<li>处理非线性：人工特征工程<br>LR 是线性模型，能力有限，要挖掘非线性特征，只能做人工特征，笛卡尔积，做交叉，做种种特征</li>
</ul>
<p><strong>问题</strong></p>
<ul>
<li><strong>人工能力有限</strong>，很难对非线性模式挖掘完全重复</li>
<li>依赖人力和领域经验，方法推广到其他问题的代价大：<strong>不够智能</strong></li>
</ul>
<p><strong>已有的非线性模型</strong></p>
<ul>
<li><strong>Kernel 方法(kernel svm)</strong><br>复杂度太高，一般来说光存储 kernel 矩阵就是数据量平方级</li>
<li><strong>Tree base 方法(如 GBDT)</strong><br>在低维强特征上表现非常好，但在大规模弱特征上(如 id 特征)表现不行<br>​实际上是树的缺点，假如说是 user id, item id 两个信息，建树会带来一个灾难，每个叶子节点在一维特征上做 split，意味着来判断某个特征是不是 id，跟到叶子路径：if(user id == useri &amp;&amp; item id == itemj) 条件判断，变成了根到叶子的组合判断，就变成了一个记忆，判断为记忆历史行为，缺乏推广性</li>
<li><strong>矩阵分解(Topic Model, LDA 等)</strong><br>适用于两种 id 的情况，不适合多种 id 输入</li>
<li><strong>Factorization machines</strong><br>只拟合有限次关系(二次关系)<br>无法拟合其他非线性关系：例如三种特征的交叉，值的高阶变换等</li>
</ul>
<p><strong>我们需要的特性</strong></p>
<ul>
<li>足够强的非线性拟合能力</li>
<li>良好的泛化能力</li>
<li>规模化能力</li>
</ul>
<h3 id="分片线性模型和学习算法-MLR"><a href="#分片线性模型和学习算法-MLR" class="headerlink" title="分片线性模型和学习算法 MLR"></a>分片线性模型和学习算法 MLR</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.5.png" class="ful-image" alt="6.5.png">
<p>分片数为 1 就是一个线性模型，分片数过多会过拟合，实际过程中是用一个搜索的方法来确定分片数。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.6.png" class="ful-image" alt="6.6.png"></p>
<p>分片用 softmax，预测模型用 LR，实际就是一个 MOE 模型，外面再加一个 LR 级联<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.7.png" class="ful-image" alt="6.7.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.8.png" class="ful-image" alt="6.8.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.9.png" class="ful-image" alt="6.9.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.10.png" class="ful-image" alt="6.10.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.11.png" class="ful-image" alt="6.11.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.12.png" class="ful-image" alt="6.12.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.13.png" class="ful-image" alt="6.13.png"></p>
<h3 id="大规模-ID-特征-MLR-实践"><a href="#大规模-ID-特征-MLR-实践" class="headerlink" title="大规模 ID 特征 + MLR 实践"></a>大规模 ID 特征 + MLR 实践</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.14.png" class="ful-image" alt="6.14.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.15.png" class="ful-image" alt="6.15.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.16.png" class="ful-image" alt="6.16.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.17.png" class="ful-image" alt="6.17.png">
<h3 id="深层用户兴趣分布网络"><a href="#深层用户兴趣分布网络" class="headerlink" title="深层用户兴趣分布网络"></a>深层用户兴趣分布网络</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.18.png" class="ful-image" alt="6.18.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.19.png" class="ful-image" alt="6.19.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.20.png" class="ful-image" alt="6.20.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.21.png" class="ful-image" alt="6.21.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.22.png" class="ful-image" alt="6.22.png">
<h2 id="认知分析-透过机器重新审视商业本质"><a href="#认知分析-透过机器重新审视商业本质" class="headerlink" title="认知分析-透过机器重新审视商业本质"></a>认知分析-透过机器重新审视商业本质</h2><blockquote>
<p>杨洋，商业认知分析平台iPIN创始人兼CEO。创办iPIN前，杨洋曾在美国国家旅游与电子商务实验室（NLTeC）从事搜索引擎研究，曾师从世界著名信息科学家Rajiv Banker和Pei-yu Chen从事多年大规模众包集智数据分析和研究，并获得博士学位，曾任哈尔滨工业大学管理学院副教授。此外还有过多次互联网创业经历，并曾在NASDAQ上市公司YY Inc.担任全球化负责人。</p>
</blockquote>
<p>人对信息的处理过程，<strong>感知 -&gt; 认知 -&gt; 分析 -&gt; 决策</strong>。人类对认知的理解还不是很成熟，这也是人工智能的天花板之一，如何让机器去掌握人类常识。</p>
<p>从生物学／物理学／哲学／社会学／经济学／市场心理学／神经科学各个角度阐述了什么是认知。比如说在我们看来，信任/不信任是同一个维度的问题，然而神经科学发现不是，你可以同时信任一个人，不信任一个人，可以爱一个人，同时恨一个人。无论哪个领域，整体的逻辑差不多是：</p>
<blockquote>
<p>认知是<strong>真相</strong>在具体<strong>场景</strong>下的<strong>投影</strong></p>
</blockquote>
<p>实现机器认知的条件</p>
<ul>
<li>信息可靠</li>
<li>信息充分</li>
<li>精细建模<br>如何对小规模数据敏感，dl 很难达到</li>
<li>不懂就“问”<br>机器能主动发问</li>
</ul>
<p>还讲了一些具体应用，个人发展应用类的如完美志愿，涉及到生涯规划，用机器学习的方法来看人是怎么发展的，让机器去学习总结大量的(潜)规则，再如前程导航，SCCT 理论来规划你的人生；企业发展应用如人才分析引擎，把不可矢量的东西都矢量化，从认知层面去做计算，等等。</p>
<h2 id="用人工智能打造教学机器人提升十倍教育效率"><a href="#用人工智能打造教学机器人提升十倍教育效率" class="headerlink" title="用人工智能打造教学机器人提升十倍教育效率"></a>用人工智能打造教学机器人提升十倍教育效率</h2><blockquote>
<p>栗浩洋 「非你莫属」BOSS，社交APP「朋友印象」创始人，国内第一家人工智能自适应网络教育公司「乂学教育」创始人。黑马会上海会长，黑马连营主任导师，央视二套「实战商学院」导师，第一财经《中国经营者》特约嘉宾，《商界时尚》杂志封面人物。曾先后被福布斯、第一财经、i黑马、36Kr等100多家媒体采访报道。</p>
</blockquote>
<p>总体逻辑，实现<strong>有教无类，教无定法，因材施教</strong>。</p>
<p>MOOC &amp; 传统教学无区别对待所有的学生和所有知识点，每个人的起点和终点不一样，然而老师教的是线性的。智适应系统，能精确侦测不同学生的<strong>知识漏洞</strong>，利用知识状态空间+知识空间理论来精准定位学生知识点掌握状态，还可以从大量数据中发现强关联的知识点和弱关联知识点。同时，精准把知识点分拆定位；又由于每个学生掌握一个知识点所需的时间是不同的，逻辑方向是不同的，所以要做个性化的匹配和学习内容推荐</p>
<p>个性化匹配</p>
<ul>
<li>学生画像+内容侧写</li>
<li>机器学习+概率图模型</li>
<li>个性化学习内容和路径匹配</li>
</ul>
<p>哎，感觉把我们当投资人或者顾客了。。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/7.1.png" class="ful-image" alt="7.1.png"></p>
<h1 id="智能助手专场"><a href="#智能助手专场" class="headerlink" title="智能助手专场"></a>智能助手专场</h1><p>这一场本来应该是我最感兴趣的一场，无奈嘉宾讲的都很泛，可能是因为中间设计的技术太多，没法展开吧。</p>
<h2 id="远程语音交互的技术挑战和商业思考"><a href="#远程语音交互的技术挑战和商业思考" class="headerlink" title="远程语音交互的技术挑战和商业思考"></a>远程语音交互的技术挑战和商业思考</h2><blockquote>
<p>陈孝良，北京声智科技有限公司。</p>
</blockquote>
<p>人机交互的升级大致来说分三个时代，最开始是<strong>PC互联网时代</strong>，主要依赖键盘、鼠标，后来是<strong>移动互联网时代</strong>，以智能机为代表，大量依赖触摸屏，到现在的<strong>AI互联网时代</strong>，以远程语音交互为主要交互方式，当然这不是唯一的，还有其他的方式辅助。</p>
<p><strong>技术挑战：</strong> 远场语音交互技术瓶颈在于声学和场景<br>近场可以近似理解为只是实验室的理想环境，远场要考虑更多</p>
<ul>
<li>语音识别率 95%，但用户体验不好<br>行业目标：以语音控制模式打造极致(速度、精度)产品体验<br>核心瓶颈：远场语音识别、远场场景适配、NLP 精确反馈</li>
<li>语义理解技术有本质性突破<br>行业目标：拟人化，全功能型产品，实现听你所言，知你所想<br>核心瓶颈：声光电多种传感的融合，NLP 技术的实质性突破</li>
</ul>
<p><strong>技术挑战：</strong></p>
<ul>
<li>器件<br>需要升级，然而麦克风的性能、精度，核心技术不在国内，标量麦克风=&gt;矢量麦克风，国内相对落后</li>
<li>芯片</li>
<li>算法<br>如离线关键词识别(语音唤醒)、离线声纹识别(Voice ID)，AEC 回声消除，Adaptive Beamforming, Speech Dereverberation，声源定位技术，孤立样本学习，深度强化学习</li>
<li>商业<br>语音交互产业化的风险在于不确定的启动周期，到了启动周期爆发时间就只有三年<br>产品，内容和服务，标准和知识产权，</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.1.png" class="ful-image" alt="6.1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/6.2.png" class="ful-image" alt="6.2.png">
<h2 id="对话即应用：过去仍在，未来已来"><a href="#对话即应用：过去仍在，未来已来" class="headerlink" title="对话即应用：过去仍在，未来已来"></a>对话即应用：过去仍在，未来已来</h2><blockquote>
<p>戴帅湘，蓦然认知 CEO前百度主任架构师，并长期担任百度 Query 理解方向负责人，荣获第一个百度语义技术的最高奖。</p>
</blockquote>
<p>感觉讲的更多是 IoT 的内容，讲了未来发展趋势，几个融合</p>
<ul>
<li>VOI 和 GUI 的融合<br>GUI 人适应机器，处理简单的、特定的任务，VOI，机器适应人，可以处理相对模糊的，复杂的任务</li>
<li>多场景融合<br>关注长尾的需求</li>
<li>设备之间的协同<br>比如说车+家的互联，把家里的空调调到24度</li>
<li>知识和服务的融合</li>
</ul>
<h2 id="设备时代结束，助手时代到来"><a href="#设备时代结束，助手时代到来" class="headerlink" title="设备时代结束，助手时代到来"></a>设备时代结束，助手时代到来</h2><blockquote>
<p>刘耀平，暴风TV CEO，被誉为“互联网电视第一人”。</p>
</blockquote>
<p><strong>听到(DSP) =&gt; 听清(ASR) =&gt; 听懂(NLP) =&gt; 做到(SKILL)</strong></p>
<p>介绍了暴风家庭助手，感觉像 echo show 的中文版，视频看上去不错，不知道真正体验如何。</p>
<p>讲了未来趋势：</p>
<ul>
<li>多设备协同计算(多助手)，未来一定是助手与助手之间的联网和协同，</li>
<li>多屏协同服务，服务在多屏呈现，屏幕无处不在</li>
<li>跨空间场景迁移</li>
</ul>
<p>未来会产生家庭社交平台，人与助手，助手与助手，人与人的交互。</p>
<h1 id="AI-专场"><a href="#AI-专场" class="headerlink" title="AI+专场"></a>AI+专场</h1><h2 id="机器写稿技术与应用"><a href="#机器写稿技术与应用" class="headerlink" title="机器写稿技术与应用"></a>机器写稿技术与应用</h2><blockquote>
<p>万小军，北京大学计算机科学技术研究所研究员。</p>
</blockquote>
<p>运用到机器写稿技术的主要单位有</p>
<ul>
<li>媒体：新华社、头条、南都等</li>
<li>互联网企业：微软、百度、腾讯、头条等</li>
<li>科研院所</li>
</ul>
<p>写稿类型一般有体育、财经、民生、娱乐新闻，以及绝句、诗歌等。</p>
<p><strong>原创 vs 二次创作</strong><br>机器写稿有两种方式，一种是原创，一种是二次创作。原创一般是之前没有稿件，只有结构化的数据，借助结构化的数据去生成新的稿件。比如说写一个天气预报的报道，或者年报、财报都直接可以从数据中生成。而关于一个已经有相关报道的事件，我们可以借助这些报道进行一些拼凑、改写成为新的稿件，这就是二次创作。</p>
<p><strong>基础技术研究</strong></p>
<ul>
<li>自动文摘</li>
<li>自然语言生成</li>
<li>文本推荐</li>
<li>文本复述</li>
</ul>
<p><strong>应用技术研究</strong></p>
<ul>
<li>新闻资讯自动生成</li>
<li>新闻综述自动生成</li>
<li>用户评论自动生成</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/8.1.png" class="ful-image" alt="8.1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/8.2.png" class="ful-image" alt="8.2.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/8.3.png" class="ful-image" alt="8.3.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/8.4.png" class="ful-image" alt="8.4.png">
<p>一些应用：</p>
<ol>
<li>看明星的微博，判断是否有新闻价值，如果有，结合微博、评论信息、背景信息，生成新闻；</li>
<li>新闻资讯的生成，长短可控，几十字、上千字的简讯／资讯；</li>
<li>新闻综述自动生成：根据同一事件的多篇新闻报道，自动生成篇幅较长的事件综述。用 wikinews 做的实验，主要过程：新闻采集 =》(子话题选择)段落划分 =》段落排序 =》段落选择与合并</li>
<li>用户评论自动生成，根据制定的用户观点数据(产品特性+评分)，自动生成对应的产品评论，基于深度学习模型，根据产品特性和评分，生成自然语言的评论<img src="http://ox5l2b8f4.bkt.clouddn.com/images/CCF-GAIR%20%E5%8F%82%E4%BC%9A%E7%AC%94%E8%AE%B0/8.5.png" class="ful-image" alt="8.5.png">
</li>
</ol>
<p>趋势展望</p>
<ul>
<li>让稿件具有态度和立场，更加人性化</li>
<li>通过推理与归纳，撰写深度报道</li>
</ul>
<h2 id="智影：AI-让视频更简单"><a href="#智影：AI-让视频更简单" class="headerlink" title="智影：AI 让视频更简单"></a>智影：AI 让视频更简单</h2><blockquote>
<p>康洪文，慧川智能 CEO</p>
</blockquote>
<p>感觉很酷炫的样子，通过明星识别、场景识别、行为识别以及视频标签化后，可以根据文字来自动生成配套的视频。举个应用的例子，如果微信公众号的很多文章都可以配上配套的短视频，是不是很棒！</p>
<p><a href="http://api.zenvideo.cn/#/" target="_blank" rel="external">智影</a></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>结尾引用某嘉宾的话，未来的世界<strong>无商业不智能，无智能不商业</strong>。从消费和生产两方面来看，<strong>消费互联网解决了高效的流通，产业互联网解决了正确的生产。</strong>看吧，我们赶上了最好的时代，同志们加油～ (P.S.最后和来自各大高校的童鞋以及雷锋网的小伙伴们面了个基，可惜奕欣姐姐嫌弃合照就不上图了。。)</p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> AI </tag>
            
            <tag> chatbot </tag>
            
            <tag> IoT </tag>
            
            <tag> 物联网 </tag>
            
            <tag> CV </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Neural Machine Translation]]></title>
      <url>http://www.shuang0420.com/2017/07/10/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/</url>
      <content type="html"><![CDATA[<p>持续填坑中– <a href="http://www.shuang0420.com/2017/05/01/NLP%20笔记%20-%20Machine%20Translation/">NLP 笔记 - Machine Translation</a>主要讲了机器翻译的传统方法，这一篇介绍基于深度学习的机器翻译方法。<br><a id="more"></a></p>
<p>本文涉及的论文原文：</p>
<ul>
<li>Bengio et al. (2003), <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a></li>
<li>Devlin et al. (2014), <a href="http://www.aclweb.org/anthology/P14-1129" target="_blank" rel="external">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</a></li>
<li>Sutskever et al. (2014), <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li>
<li>Bahdanau et al. (2014), <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li>Xu et al. (2015) <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a></li>
<li>Luong et al. (2015)<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj3qYKMiuDVAhVHXbwKHadFByAQFgg8MAI&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Femnlp15_attn.pdf&amp;usg=AFQjCNGsQHAEQhC6dQuVyiHWcEmajCM-6w" target="_blank" rel="external">Effective approaches to attention based neural machine translation</a></li>
</ul>
<h1 id="Neural-MT"><a href="#Neural-MT" class="headerlink" title="Neural MT"></a>Neural MT</h1><p><strong>Neural MT(NMT)</strong>的定义：</p>
<blockquote>
<p>Neural Machine Translation is the approach of modeling the <strong>entire</strong> MT process via <strong>one</strong> big artificial neural netowrk.</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_base_nmt.png" class="ful-image" alt="phrase_base_nmt.png">
<p>用一个大的神经网络来给整个机器翻译的过程建模，目前主流的结构是 <strong>Neural encoder-decoder architectures</strong>，最<strong>基本的思想</strong>是 encoder 将输入文本转化为一个向量， decoder 根据这个向量生成目标译文，编码解码开始均用 RNN 实现，由于普通 RNN 存在梯度消失/爆炸的问题，通常会引入 LSTM。本篇会介绍 NMT 发展的一些重要节点。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/encodedecode.png" class="ful-image" alt="encodedecode.png">
<blockquote>
<p><strong>一句话解释 encoder-decoder architectures:</strong> Encoder compresses input series into one vector Decoder uses this vector to generate output</p>
</blockquote>
<h2 id="Big-wins-of-Neural-MT"><a href="#Big-wins-of-Neural-MT" class="headerlink" title="Big wins of Neural MT"></a>Big wins of Neural MT</h2><ul>
<li><strong>End-to-end training</strong><br>所有参数同时被训练优化</li>
<li><strong>Distributed representations share strength</strong><br>分布式表达更好的挖掘了单词／词组之间的相似性</li>
<li><strong>Better exploitation of context</strong><br>可以使用更广泛的上下文，无论是 source 还是 target text</li>
<li><strong>More fluent text generation</strong><br>生成的文本质量更高</li>
</ul>
<p>But…</p>
<h2 id="Concerns"><a href="#Concerns" class="headerlink" title="Concerns"></a>Concerns</h2><ul>
<li><strong>Black box component models for reordering, transliteration, etc.</strong></li>
<li><strong>Explicit use of syntactic or semantic structures</strong><br>没有显性的用到句法／语义结构特征</li>
<li><strong>Explicit use of discourse structure, anaphora, etc.</strong><br>没法显性利用指代消解之类的结果</li>
</ul>
<h1 id="NNLM-Bengio-et-al-2003"><a href="#NNLM-Bengio-et-al-2003" class="headerlink" title="NNLM: Bengio et al. (2003)"></a>NNLM: Bengio et al. (2003)</h1><p><a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a>，这篇论文为之后的 NMT 打下了基础。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/nnlm.png" class="ful-image" alt="nnlm.png">
<p>传统的 Language model 有一定缺陷：</p>
<ul>
<li>不能把 ngram 中 n 以外的词考虑进去<br>n 越大句子连贯性越好，但同时数据也就越稀疏，参数也越多</li>
<li>不能考虑单词之间的相似度<br>不能从 the cat is walking in the bedroom 学习到 a dog was running in a room</li>
</ul>
<p><strong>Neural network language model(NNLM)</strong> 可以解决上面的两个问题。主要理解上图，先来定义一些参数：<br><strong>h:</strong> number of hidden units<br><strong>m:</strong> number of features associated with each word<br><strong>b:</strong> output biases, with |V| elements<br><strong>d:</strong> hidden layer biases, with h elements<br><strong>V:</strong> word dictionary $w_1, …w_T \in V$<br><strong>U:</strong> hidden-to-output weights, |V|*h matrix<br><strong>W:</strong> word features to output weights, |V|*(n-1)m matrix<br><strong>H:</strong> hidden layer weights, h*(n-1)m matrix<br><strong>C:</strong> word features C, |V|*m matrix</p>
<p>再来看一下<strong>目标函数:</strong><br> $$f(w_t,…,w_{t-n+1})=\hat P(w_t|w_1^{t-1})$$</p>
<p>目标函数包含了两个部分</p>
<ul>
<li>Matrix C，用来 <strong>map</strong> word_id 和 word feature vector，维度是 |V|*m</li>
<li>g，用来 <strong>map</strong> 这个上下文输入的 feature vector 和下一个单词 $w_t$ 在 V 上的条件概率的分布，g 用来估计 $\hat P(w_t=i|w_1^{t-1})$<br>$$f(i, w_{t-1},…,w_{t-n+1})=g(i,C(w_{t-1}),…,C(w_{t-n+1}))$$</li>
</ul>
<p>也就是说 f 是两个 mapping C 和 g 的组合，词向量矩阵 C 所有单词共享。模型有两个 hidden layer，第一部分的输入是上下文单词 $w_{t-1},…w_{t-n+1}$ 的 word_id，每个 word_id 在 C 中寻找到对应的 word vector，vector 相加得到第二部分的输入， 与 H 相乘加一个 biases 用 tanh 激活，然后与 U 相乘产生一个得分向量，再进入 softmax 把得分向量转化成概率分布形式。</p>
<p>前向传播的式子就是：<br>$$y=b+Wx+Utanh(d+Hx)$$</p>
<p>其中 x 是 word features layer activation vector，由输入词向量拼接而成<br>$$x(C(w_{t-1}), C(w_{t-1}), …, C(w_{t-n+1}))$$</p>
<p>最后的 softmax 层，保证所有概率加和为 1<br>$$\bar P(w_t|w_{t-1},…,w_{t-n+1})={e^{y_{w_t}} \over \sum_ie^{y_i}}$$</p>
<p>通过在训练集上最大化 penalized log-likelihood 来进行训练，其中 $R(\theta)$ 是正则项，如下<br>$$L={1 \over T}\sum_tlogf(w_t,w_{t-1},…,w_{t-n+1;\theta})+R(\theta)$$</p>
<p>用 SGD 梯度下降来更新 U 和 W<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/sgd.png" class="ful-image" alt="sgd.png"></p>
<p>参数集合$\theta = (b,d,W,U,H,C)$，参数量： |V|(1+mn+h)+h(1+(n-1)m)，dominating factor 是|V|(nm+h)</p>
<p>最后提一下两个改进，一个是加上图中的曲线部分，直接把词向量和输出连接起来(direct connections from the word features to the output)，另一个是和 ngram 相结合的 mixture model。看一下在 Brown Corpus 上的评估结果:</p>
<ul>
<li>n-gram model(Kneser-Ney smoothing): 321</li>
<li>neural network language model: 276</li>
<li>neural network + ngram: 252</li>
</ul>
<h1 id="NNJM-Devlin-et-al-2014"><a href="#NNJM-Devlin-et-al-2014" class="headerlink" title="NNJM: Devlin et al. (2014)"></a>NNJM: Devlin et al. (2014)</h1><p><a href="http://www.aclweb.org/anthology/P14-1129" target="_blank" rel="external">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</a>，把 Bengio et al. (2003) 的 model 转化为一个 translation model，简单来说就是把 n-gram target language model 和 m-word source window 相结合，来创建一个 MT decoding feature，可以简单的融入任何的 SMT decoder。</p>
<h2 id="Neural-Network-Joint-Models-NNJM"><a href="#Neural-Network-Joint-Models-NNJM" class="headerlink" title="Neural Network Joint Models(NNJM)"></a>Neural Network Joint Models(NNJM)</h2><p>条件概率模型，generate the next English word <strong>conditioned on</strong></p>
<ul>
<li>The previous n English words you generated $e_{i-1}, e_{i-2}…$</li>
<li>The aligned source word and its m neighbors $…f_{a_i-1}, f_{a_i}, f_{a_i-2}…$</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20et%20al.png" class="ful-image" alt="Devlin%20et%20al.png">
<p>$$P(T|S) \approx \prod^{|T|}_{i=1}P(t_i|t_{i-1},…,t_{i-n+1}, S_i)$$</p>
<p>假设 target word $t_i$ 只和一个 source word $a_i$ 对齐，我们会关注 source 句子里以 $a_i$ 为中心的一个 window $S_i$，也就是和 target $t_i$ 最相关的 source part。</p>
<p>$$S_i = s_{a_i-{m-1 \over 2}},…,s_{a_i},…,s_{a_i+{m-1 \over 2}}$$</p>
<p>1) 如果 $e_i$ 只和一个 source word 对齐，那么 $a_i$ 就是对齐的那个单词的 index<br>2) 如果 $e_i$ 和多个 source word 对齐，那么 $a_i$ 就是在中间的那个单词的 index<br>3) 如果 $e_i$ 没有对齐的 source word，那么就继承最邻近(右边)的 aligned word 的 affiliation</p>
<p>affiliation 用先验的基于规则的 word alignment 来推。</p>
<p>模型的训练过程和 NNLM 相似，不过是多了个 corpus 而已。最大化训练数据的 log-likelihood<br>$$L=\sum_ilog(P(x_i))$$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/NNJM1.png" class="ful-image" alt="NNJM1.png">
<h2 id="Self-normalization"><a href="#Self-normalization" class="headerlink" title="Self-normalization"></a>Self-normalization</h2><p>论文还提出了一种训练神经网络的新方法 – self-normalized 技术。由于训练的 cost 主要来自输出层在整个 target vocabulary 上的一个 softmax 计算，<strong>self-normalization</strong> 用近似的概率替代实际的 softmax 操作，思路很简单，主要改造一下目标函数。先来看一下 softmax log likelihood 的计算：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20formula.png" class="ful-image" alt="Devlin%20formula.png"><br>想象一下，如果 $log(Z(x))=0$，也就是 $Z(x)=1$，那么我们就只用计算输出层第 r 行的值而不用计算整个 matrix 了，所以改造下目标函数，变成<br>$$<br>\begin{aligned} \<br>L =\sum_i[log(P(x_i))-\alpha(log(Z(x_i))-0)^2] \\<br>&amp; =\sum_i[log(P(x_i))-\alpha log^2(Z(x_i))] \\<br> \end{aligned}<br>$$</p>
<p>output layer bias weights 初始化为 log(1/|V|)，这样初始网络就是 self-normalized 的了，decode 时，只用把 $U_r(x)$ 而不是 $log(P(x))$ 作为 feature score，这大大加快了 decoding 时的 lookup 速度(~15x)。其中 $\alpha$ 是一个 trade-off 参数，来平衡网络的 accuracy 以及 mean self-normalization error</p>
<p>其他优化如 pre-computing the (first) hidden layer 等。</p>
<h2 id="Variations"><a href="#Variations" class="headerlink" title="Variations"></a>Variations</h2><p>MT decoder 有两类，一个是 <strong>decoder</strong>方法，用的是 string-to-dependency hierarchical decoder (Shen et al., 2010)，一个是 <strong>1000-best rescoring</strong> 方法，用的 feature 是 5-gram Kneser-Ney LM，Recurrent neural network language model (RNNLM) (Mikolov et al., 2010)</p>
<p>不同模型的影响<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/res1.png" class="ful-image" alt="res1.png"><br>​<br>网络设置的影响<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Devlin%20improve.png" class="ful-image" alt="Devlin%20improve.png"></p>
<p>模型还有一些变种比如说<strong>翻译方向(source-to-target S2T, target-to-source T2S)</strong>，<strong>language model 的方向(left-to-right L2R, right-to-left R2L)</strong>，<strong>NNTM(Neural Network Lexical Translation Model)</strong>，对 many-to-one 问题用 NULL 解决，对 one-to-many 问题用 token concatenated 解决，训练和评估与 NNJM 相似。</p>
<h1 id="Sutskever-et-al-2014"><a href="#Sutskever-et-al-2014" class="headerlink" title="Sutskever et al. (2014)"></a>Sutskever et al. (2014)</h1><p><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/2014_1.png" class="ful-image" alt="2014_1.png">
<p>使用了 encoder-decoder 框架，用多层的 LSTM 来把输入序列映射到一个固定维度的向量，然后用另一个深层的 LSTM 来 decode 这个向量，得到 target sequence，第二个 LSTM 实际上就是一个 NNLM 不过 conditioned on input sequences。主要的改变：</p>
<ul>
<li>fully <strong>end-to-end</strong> RNN-based translation model</li>
<li><strong>two</strong> different RNN<br>encode the source sentence using one LSTM<br>generate the target sentence one word at a time using another LSTM</li>
<li><strong>reverse</strong> the order of the words in all source sentence(but not target sentences)</li>
</ul>
<p><strong>Encoder</strong>可以看作是一个 <strong>conditional language model</strong> (Bengio et al. ,2003) ，对原始句子，每个词用相应向量表示，每个词都有一个隐含状态 h1，代表这个词以及这个词之前的所有词包含的信息，当找到句尾标记的时候，对应的隐状态也就是 encoder 层的最后一个隐状态就就代表了整个句子的信息。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/cat_ex.png" class="ful-image" alt="cat_ex.png">
<p>然后 <strong>decoder</strong> 对其进行解码，<strong>encoder</strong> 最后一层(最后一个时刻)作为 decoder 第一层，LSTM 能保持中期的记忆，那么解码层的每个隐状态，都包含了已经翻译好的状态以及隐状态，然后输出每个词。具体过程如下图：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sutskever%20et%20al.%203.png" class="ful-image" alt="Sutskever%20et%20al.%203.png"></p>
<p>将输入序列倒转喂给 LSTM，能够在 source 和 target 句子间引入许多 short-term dependencies，使得优化问题更加容易。</p>
<p><strong>recurrent activation function</strong> 可以使用：</p>
<ul>
<li>Hyperbolic tangent tanh</li>
<li>Gated recurrent unit [Cho et al., 2014]</li>
<li>Long short-term memory [Sutskever et al., 2014]</li>
<li>Convolutional network [Kalchbrenner &amp; Blunsom, 2013]</li>
</ul>
<p>主要的贡献</p>
<ul>
<li>hard-alignment =&gt; soft-alignment</li>
<li>对长句的泛化能力很好</li>
<li>简单的 decoder</li>
</ul>
<p>关于 <strong>Decoder</strong>，论文 baseline 用了 rescoring 1000-best 的策略，实验用了一个 <strong>left-to-right beam search decoder</strong> 结果有一定提升。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/Sut%20res.png" class="ful-image" alt="Sut%20res.png"></p>
<p>还可以提一句的是，另一种做法是把 encoder 的最后一层喂给 decoder 的每一层，这样就不会担心记忆丢失了。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoder2.png" class="ful-image" alt="decoder2.png">
<h1 id="乱入：Decoders"><a href="#乱入：Decoders" class="headerlink" title="乱入：Decoders"></a>乱入：Decoders</h1><p>在模型能计算 P(T|S) 后，问题来了，怎样才能找出最可能的译文 T 呢？<br>$$\bar T = argmax_T(P(T|S))$$</p>
<h2 id="Exhaustive-Search"><a href="#Exhaustive-Search" class="headerlink" title="Exhaustive Search"></a>Exhaustive Search</h2><p>最开始的想法当然是生成所有翻译，然后用 language model 打分，挑概率最大的了。<strong>BUT!! DO NOT EVEN THINK OF TRYING IT OUT!!!</strong>译文数量是词表的指数级函数，这还用想么？！</p>
<h2 id="Ancestral-Sampling"><a href="#Ancestral-Sampling" class="headerlink" title="Ancestral Sampling"></a>Ancestral Sampling</h2><p>$$x ~ P(x_t|x_1,…,x_n)$$<br>多次取 sample。然而实践中会产生高方差的结果，同一个句子每次翻译结果都不一样，不大好吧？</p>
<h2 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h2><p>$$x_t=argmax\hat x_tP(\hat x_t|x_1,…,x_n)$$<br>每次选取当前最可能的那个单词，然而，这显然不能达到全局最优，每一步都会影响到后面的部分。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/greedySearch.png" class="ful-image" alt="greedySearch.png"></p>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamForm.png" class="ful-image" alt="beamForm.png">
<p>每个时刻记录 k 个最可能的选项，相当于剪枝，然后在这些选项中进行搜索<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/beamSearch.png" class="ful-image" alt="beamSearch.png"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/decoderResults.png" class="ful-image" alt="decoderResults.png">
<h1 id="Attention-Bahdanau-et-al-2014"><a href="#Attention-Bahdanau-et-al-2014" class="headerlink" title="Attention: Bahdanau et al. (2014)"></a>Attention: Bahdanau et al. (2014)</h1><p><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a>，之前的 encoder-decoder 模型是将 source sentence 编码成一个固定长度的 vector，然后 decoder 产生 target sentence，这就要求 neural network 要能够把 source sentence 所有必要信息都压缩到一个 fixed-length vector 里，这对长句并不友好。我们希望在产生一个 target word 的时候只关注部分的 source word。这一篇提出了一个自动搜寻这样一个要关注的 source word window 的方法，换句话说，每次产生一个单词时，模型会从 source sentence 中搜索到相关信息所在的位置，基于包含了这些位置特征的 context vector 和 previous generated words，来生成下一个单词。这也就是<strong>注意力模型</strong>在机器翻译中的应用。</p>
<blockquote>
<p>一句话解释： Attention Mechanism predicts the output $y_t$ with a weighted average context vector $c_t$, not just the last state</p>
</blockquote>
<p>再通俗一点理解，attention 的作用可以看作是一个对齐模型，传统 SMT 我们用 EM 算法来求解对齐，这里做一个隐式的对齐，将 alignment model 用一个 feedforward neural network 参数化，和其他部分一起训练，神经网络会同时来学习 <strong>翻译模型(translation)</strong> 和 <strong>对齐模型(alignment)</strong>。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/phrase_based%20SMT.png" class="ful-image" alt="phrase_based%20SMT.png"><br>attention 效果：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention_al.png" class="ful-image" alt="attention_al.png"></p>
<p><strong>具体过程：</strong><br>用一个感知机公式将 source 和 target 的每个词联系起来，$a(h_{i-1},\bar h_j)=v^T_atanh(W_ah_{i-1}+U_ah_j)$，然后通过 softmax 归一化得到一个概率分布，也就是 attention 矩阵，再进行加权平均得到 context vector(可以看作是 annotation 的期望值)。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention1.JPG" class="ful-image" alt="attention1.JPG"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention2.JPG" class="ful-image" alt="attention2.JPG"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/attention3.JPG" class="ful-image" alt="attention3.JPG"></p>
<p>模型将 input sentence 编码成一系列 vector，然后在 decode 时自适应的选择这些 vector 的一个子集。$a_{ij}$，或者说对应的 $e_{ij}$，反映了 annotation $\bar h_j$ 关于前一个隐状态 $h_{i-1}$ 在决定下一个隐状态 $h_i$ 以及产生 $y_i$ 的重要性。直观的说，这在 decoder 里执行了 attention 机制。decoder 来决定应该对 source sentence 的哪一部分给予关注。通过这个 attention 机制，encoder 不用再将 source 的所有信息压缩到一个定长的 vector 里，信息可以在 annotation 序列中传播，然后由 decoder 来选择性的检索。</p>
<p>Encoder 还可以用双向 RNN 来做，这样每个单词的 annotation 不仅概括了前面单词的信息，还包括了后面单词的信息。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/birnnattention.png" class="ful-image" alt="birnnattention.png">
<h1 id="Attention-优化"><a href="#Attention-优化" class="headerlink" title="Attention 优化"></a>Attention 优化</h1><h2 id="More-Score-Functions"><a href="#More-Score-Functions" class="headerlink" title="More Score Functions"></a>More Score Functions</h2><p>怎么算 alignment score 有下面集中不同的方式，实验表示第二种效果最好。<br><strong>compute alignment weight vector</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/score.png" class="ful-image" alt="score.png"></p>
<h2 id="Global-vs-Local"><a href="#Global-vs-Local" class="headerlink" title="Global vs. Local"></a>Global vs. Local</h2><p>论文<a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Xu et al.2015 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a> 提到了 attention 可以分为 <strong>hard</strong> 和 <strong>soft</strong> 两种模型，简单理解，<strong>hard attention</strong> 就是从 source sentence 里找到一个能与产生单词 $t^{th}$ 对齐的特定单词，把 $s_{t,i}$ 设为 1，source 里的其他单词硬性认为对齐概率为 0；<strong>soft attention</strong> 就是之前 Bahdanau et al. (2014) 提到的，对 source sentence 每个单词都给出一个对齐概率，得到一个概率分布，context vector 每次是这些概率分布的一个加权和，整个模型其实是平滑的且处处可分的。</p>
<p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj3qYKMiuDVAhVHXbwKHadFByAQFgg8MAI&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Femnlp15_attn.pdf&amp;usg=AFQjCNGsQHAEQhC6dQuVyiHWcEmajCM-6w" target="_blank" rel="external">Effective approaches to attention based neural machine translation</a> 提出了一个新的 attention 机制 <strong>local attention</strong>，在得到 context vector 时，我们不想看所有的 source hidden state，而是每次只看一个 hidden state 的子集(subset)，这样的 attention 其实更集中，也会有更好的结果。</p>
<p><strong>Global attention</strong> 其实就是 <strong>soft attention</strong>， <strong>local model</strong> 实际相当于 hard 和 soft attention 的一个混合或者说折中，主要是用来降低 attention 的花费，简单来说就是每次计算先用预测函数得到 source 相关信息的窗口，先预估得到一个 aligned position $p_t$，然后往左往右扩展得到一个 focused window [$p_t-D$,$p_t+D$] 取一个类似于 soft attention 的概率分布。和 global attention 不同，这里 $a_i$ 的维度是固定的。</p>
<p>那么关于 local attention 有两个问题。第一个问题是<strong>怎么产生 aligned position</strong>，之前在 Devlin et al. (2014) 里是用规则，这里用 sigmoid function $p_t = S•sigmoid(v^T_p tanh(W_ph_t))$，其中 S 是 source sentence。第二个问题是<strong>怎么来学习这些 position parameters</strong>，像 $W_p$、$h_t$ 这些参数和网络结构中其他参数没有任何关联，怎么学习呢？方法是像 global attention 一样先计算对齐分数 $score(h_t, \bar h_s)$，然后 normalize，这里有一个 trick 是将得到的 $a_t$ 与一个 truncated Gaussian distribution 结合，也就是 $a_t(s)=align(h_t, \bar h_s)exp(-{(s-p_t)^2 \over 2 \sigma^2})$，$\sigma={D \over 2}$，这样我们会只有一个 peak，现在可以用 BP 来学习预测 position，这个模型这时候几乎是 <strong>处处可分的(differentiable almost everywhere)</strong>，这种对齐称为 <strong>predictive alignment(local-p)</strong></p>
<p><strong>local attention</strong> 的训练花费更少，且几乎处处可分(differentiable)</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/local_attention1.png" class="ful-image" alt="local_attention1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/global_local.png" class="ful-image" alt="global_local.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res.png" class="ful-image" alt="effective_res.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/effective_res2.png" class="ful-image" alt="effective_res2.png">
<h2 id="Coverage-Doubly-attention"><a href="#Coverage-Doubly-attention" class="headerlink" title="Coverage: Doubly attention"></a>Coverage: Doubly attention</h2><p>论文<a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Xu et al.2015 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15</a>提到的思路，用到机器翻译里就是同时注意原文和译文。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/caption.png" class="ful-image" alt="caption.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/doubly%20attention.png" class="ful-image" alt="doubly%20attention.png">
<h2 id="Linguistic-ideas"><a href="#Linguistic-ideas" class="headerlink" title="Linguistic ideas"></a>Linguistic ideas</h2><ul>
<li>[Tu, Lu, Liu, Liu, Li, ACL’16]: NMT model with coverage-based attention</li>
<li>[Cohn, Hoang, Vymolova, Yao, Dyer, Haffari, NAACL’16]: More substantive models of attention<br> using: position (IBM2) + Markov (HMM) + fertility<br>  (IBM3-5) + alignment symmetry (BerkeleyAligner)</li>
</ul>
<p>一般一个单词最多翻译为两三个单词，如果生成了五六个单词，那么模型可能在重复生成。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation-Neuron%20models/ling.png" class="ful-image" alt="ling.png"></p>
<h1 id="Current-Research-Direction-on-Neural-MT"><a href="#Current-Research-Direction-on-Neural-MT" class="headerlink" title="Current Research Direction on Neural MT"></a>Current Research Direction on Neural MT</h1><ul>
<li>Incorporation syntax into Neural MT</li>
<li>Handling of morphologically rich languages</li>
<li>Optimizing translation quality (instead of corpus probability)</li>
<li>Multilingual models</li>
<li>Document-level translation</li>
</ul>
<p>到目前为止，我们都是假设在两种语言 F 和 E 之间训练一个模型。但是，世界上有许多种语言，一些研究已经证明能够利用所有语言的数据去训练一个模型。也可以跨语言执行迁移，先在一个语言对上训练模型，然后将其微调用于其他语言对。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine translation </tag>
            
            <tag> 机器翻译 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Lambda + API Gateway + DynamoDB 添加 slash command 待办清单]]></title>
      <url>http://www.shuang0420.com/2017/07/04/AWS%20Lambda%20+%20API%20Gateway%20+%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/</url>
      <content type="html"><![CDATA[<p>接上一篇<a href="http://www.shuang0420.com/2017/06/19/AWS%20API%20Gateway%20+%20Lambda%20+%20Slack%20App%20-%20新建%20slash%20command/">AWS API Gateway + Lambda + Slack App - 新建 slash command</a>，这一篇介绍怎么用 aws lambda + api gateway + dynamodb 添加 slash command /todo，完成 to-do list 的增、删、查等任务。两个重点，一是连接 dynamodb，二是创建 slack interactive message，具体到这篇的例子，是实现 message button。<br><a id="more"></a></p>
<h1 id="DynamoDB-configuration"><a href="#DynamoDB-configuration" class="headerlink" title="DynamoDB configuration"></a>DynamoDB configuration</h1><p>Amazon DynamoDB 属于 NoSQL 数据库，支持文档和 key-value 存储模型。每一行是一个 <strong>item</strong>，item 时<strong>属性(attributes)</strong> 的集合，每个 attribute 都有各自的<strong>名称(name)</strong>和<strong>值(value)</strong>。DynamoDB 提供了 item 的 4 项基本操作，<strong>创建(PutItem)/读取(GetItem)/更新(UpdateItem)/删除(DeleteItem)</strong>。</p>
<h2 id="Create-table"><a href="#Create-table" class="headerlink" title="Create table"></a>Create table</h2><p>第一步，完成 todolist table 的创建，<a href="https://console.aws.amazon.com/console/home?region=us-east-1" target="_blank" rel="external">aws console</a> 进入<a href="https://console.aws.amazon.com/dynamodb/home?region=us-east-1#gettingStarted:" target="_blank" rel="external">dynamodb</a>，选 create table，做如下设置，primary key 设为 user，一个 user 一个 item，item 有属性 todos，类型是 list，一个 user 当然可以有多个 to-do item，都保存在 todos 的 list 中。当然，之后会有更多的属性，比如说 <strong>channel, priority</strong> 等，后面再做设置。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/dynamodb_create.png" class="ful-image" alt="dynamodb_create.png">
<p>建表完成后在 Items 标签下 Create item，做如下设置，方便后面的测试。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/dynamoDB_editItem.png" class="ful-image" alt="dynamoDB_editItem.png">
<h2 id="Attach-policy-to-IAM-role"><a href="#Attach-policy-to-IAM-role" class="headerlink" title="Attach policy to IAM role"></a>Attach policy to IAM role</h2><p>接下来是测试 lambda function 能否连接数据库，默认情况下，lambda function 是没有这个权限的，我们需要创建一个 role，或者在已有的 role 上 attach 相应的 policy，再对 lambda function 采用这个 role，才可以访问数据库。所以，先从<a href="https://console.aws.amazon.com/console/home?region=us-east-1" target="_blank" rel="external">aws console</a> 进入<a href="https://console.aws.amazon.com/iam/home?region=us-east-1" target="_blank" rel="external">IAM</a>，选择 Roles 以及 lambda configuration 中的 role，比如说 <strong>lambda_basic_execution</strong>(这里以 kmsDecrypt role 为例)，选择 inline policy 来创建并且 attach policy。</p>
<p><strong>Step 1: create inline policy</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/IAM_attach.png" class="ful-image" alt="IAM_attach.png"></p>
<p><strong>Step 2: generate policy</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/AIM_attach2.png" class="ful-image" alt="AIM_attach2.png"></p>
<p><strong>Step 3: edit permissions</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/IAM_attach3.png" class="ful-image" alt="IAM_attach3.png"></p>
<p><strong>ARN</strong> 在 DynamoDB 页面 table overview 下可以找到，这里的 permission 表示使用了这个 role 的 lambda function 只可以对 todolist 这张表进行操作，<strong>Actions</strong> 是操作类型，这里选 All Actions，表示增删改查所有操作都允许。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/ARN.png" class="ful-image" alt="ARN.png">
<p><strong>Step 4: review and apply policy</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/IAM_attach4.png" class="ful-image" alt="IAM_attach4.png"></p>
<p>Policy 的具体内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Sid&quot;: &quot;Stmt1498896886000&quot;,</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: [</div><div class="line">                &quot;dynamodb:*&quot;</div><div class="line">            ],</div><div class="line">            &quot;Resource&quot;: [</div><div class="line">                &quot;arn:aws:dynamodb:us-west-2:XXXXXXXXXXX:table/todolist&quot;</div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Lambda-configuration"><a href="#Lambda-configuration" class="headerlink" title="Lambda configuration"></a>Lambda configuration</h1><p>写一个简单的 lambda function 看看能不能读取表中内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import boto3</div><div class="line"></div><div class="line">todo_table = boto3.resource(&apos;dynamodb&apos;).Table(&apos;todolist&apos;)</div><div class="line"></div><div class="line">def lambda_handler(event, context):</div><div class="line">    print todo_table.get_item(Key=&#123;&apos;user&apos;: &apos;testuser&apos;&#125;)</div></pre></td></tr></table></figure></p>
<p>连接成功<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/succeed.png" class="ful-image" alt="succeed.png"></p>
<p>连接不成功可能是因为地区不一致，可以显性指定 region 来连接。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import boto3</div><div class="line"></div><div class="line">todo_table = boto3.resource(&apos;dynamodb&apos;, &apos;us-west-2&apos;).Table(&apos;todolist&apos;)</div><div class="line"></div><div class="line">def lambda_handler(event, context):</div><div class="line">    print todo_table.get_item(Key=&#123;&apos;user&apos;: &apos;testuser&apos;&#125;)</div></pre></td></tr></table></figure></p>
<p>下一部分附上 debug 过程，提供可能的 debug 思路。</p>
<h2 id="Possible-error-Debug-process"><a href="#Possible-error-Debug-process" class="headerlink" title="Possible error/Debug process"></a>Possible error/Debug process</h2><p>有可能出现 “module initialization error”，具体错误信息是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">module initialization error: An error occurred (AccessDeniedException) when calling the GetItem operation: User: arn:aws:sts::xxxxxxxxxxxxx:assumed-role/kmsDecrypt/todolist is not authorized to perform: dynamodb:GetItem on resource: arn:aws:dynamodb:us-east-1:xxxxxxxxxxxxxx:table/todolist: ClientError</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/not_authorized.png" class="ful-image" alt="not_authorized.png">
<p>把上一步的 policy 改的 less restrict 一些，方便调试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</div><div class="line">    &quot;Statement&quot;: [</div><div class="line">        &#123;</div><div class="line">            &quot;Sid&quot;: &quot;Stmt1498896886000&quot;,</div><div class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</div><div class="line">            &quot;Action&quot;: [</div><div class="line">                &quot;dynamodb:*&quot;</div><div class="line">            ],</div><div class="line">            &quot;Resource&quot;: [</div><div class="line">                &quot;*&quot;</div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>再重新 test 一下 lambda，发现错误变成了 Requested resource not found<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/resource_not_found.png" class="ful-image" alt="resource_not_found.png"></p>
<p>根本没这个 table，怎么办？那干脆把所有 table 列出来看看喽，修改 lambda function，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import boto3</div><div class="line"></div><div class="line">def lambda_handler(event, context):</div><div class="line">    print boto3.client(&apos;dynamodb&apos;).list_tables()</div></pre></td></tr></table></figure></p>
<p>发现 table list 为空。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/log_output_fail.png" class="ful-image" alt="log_output_fail.png"></p>
<p>这时候熟悉 aws 的朋友就知道，<strong>Region! Region! Region! 一定要一致！</strong></p>
<p>再来回顾下 dynamodb table overview，发现 table 所在 region 是 US WEST，而我们的 lambda ARN 是在 US EAST，所以办法是在连接 table 的时候直接指定地区。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/ARN.png" class="ful-image" alt="ARN.png">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import boto3</div><div class="line"></div><div class="line">todo_table = boto3.resource(&apos;dynamodb&apos;, &apos;us-west-2&apos;).Table(&apos;todolist&apos;)</div><div class="line"></div><div class="line">def lambda_handler(event, context):</div><div class="line">    print todo_table.get_item(Key=&#123;&apos;user&apos;: &apos;testuser&apos;&#125;)</div></pre></td></tr></table></figure>
<p>连接成功。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/log_output_success.png" class="ful-image" alt="log_output_success.png"></p>
<h1 id="Slash-command-configuration"><a href="#Slash-command-configuration" class="headerlink" title="Slash command configuration"></a>Slash command configuration</h1><p>在 <a href="https://api.slack.com/apps" target="_blank" rel="external">slack app</a> 页面新建 slash command /todo，command 以及 api gateway 的设置，具体参照<a href="http://www.shuang0420.com/2017/06/19/AWS%20API%20Gateway%20+%20Lambda%20+%20Slack%20App%20-%20新建%20slash%20command/">AWS API Gateway + Lambda + Slack App - 新建 slash command</a>，非常简单的过程。</p>
<h1 id="Slack-Message-Button"><a href="#Slack-Message-Button" class="headerlink" title="Slack Message Button"></a>Slack Message Button</h1><p>先来看一下 message button 的运作流程，slack app 的 interactive messages 下有一个 Request URL，专门用来响应 message button 事件。一个 slack app 只能配置一个 action URL，它会接收所有 channel/team 下的 message button 的所有点击操作，这相当于一个 dispatch station。当用户点击 button 时，action URL 会收到一个 URL encoded request，request 的 body 参数中会包含一个 payload，记录相应的 user action，我们可以通过 payload 来获取用户的点击行为，然后做出响应。</p>
<p>Slack 官方教程 <a href="https://api.slack.com/interactive-messages" target="_blank" rel="external">Making messages interactive</a> 提供了多种响应 message action 的方法，主要说来一是直接对 action URL 的 request 进行 response，要求是 3 秒内必须做出响应；二是通过 response_url 进行对原消息的更新等，三是用 chat.update 来更新原始信息。</p>
<p>经尝试，发现在使用 python 以及 slackclient package 的情况下，并不能用 chat.postMessage, chat.update 来实现 message button。slackclient 作为 slack api 的一个 wrapper，确实可以实现 chat.postMessage，但只能提供 basics 的一些 response，并不能处理请求中加 attachments 的情况。</p>
<p>这里采用的是直接响应的方法。首先需要在 API Gateway 中配置 request url，对应的 integration 还是选 todolist lambda function，mapping templates 还是要改成 x-www-form-urlencoded 的形式。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/api%20gateway1.png" class="ful-image" alt="api%20gateway1.png"></p>
<p>deploy 后记录下 Invoke URL，在 slack app 下修改 request URL<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/request%20url.png" class="ful-image" alt="request%20url.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/request%20url2.png" class="ful-image" alt="request%20url2.png">
<p><strong>message button format:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">def _respond_button(err, res=None):</div><div class="line">    return &#123;</div><div class="line">        &quot;text&quot;: res,</div><div class="line">        &quot;attachments&quot;: [</div><div class="line">            &#123;</div><div class="line">                &quot;callback_id&quot;: &quot;todo&quot;,</div><div class="line">                &quot;color&quot;: &quot;#3AA3E3&quot;,</div><div class="line">                &quot;attachment_type&quot;: &quot;default&quot;,</div><div class="line">                &quot;actions&quot;: [</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Remind me&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;reminder&quot;</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Set priority&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;priority&quot;</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Set project&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;project&quot;,</div><div class="line">                    &#125;</div><div class="line">                ]</div><div class="line">            &#125;</div><div class="line">        ]</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<p>上面的 message format 会产生下图的三个 button<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lambda%20%2B%20API%20Gateway%20%2B%20DynamoDB%20%E6%B7%BB%E5%8A%A0%20slash%20command%20%E5%BE%85%E5%8A%9E%E6%B8%85%E5%8D%95/mes%20button.png" class="ful-image" alt="mes%20button.png"></p>
<p>如果用户点击 Remind me，这个行为对应的信息会发送到 request url，由于 request url 还是绑定了 todolist 这个 lambda function，所以可以在 lambda 中判断 request 是否包含 payload 参数，如果包含，说明这是一个 button 响应事件，就读取 user action 信息，并响应，否则，就响应 command 命令。</p>
<p>完整的 <a href="https://github.com/Shuang0420/slack-todolist" target="_blank" rel="external">lambda function 代码</a>，role 用 kmsDecrypt，环境变量设置好 kmsEncryptedToken，参照<a href="http://www.shuang0420.com/2017/06/19/AWS%20API%20Gateway%20+%20Lambda%20+%20Slack%20App%20-%20新建%20slash%20command/">AWS API Gateway + Lambda + Slack App - 新建 slash command</a>，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div></pre></td><td class="code"><pre><div class="line"># -*- coding: UTF-8 -*-</div><div class="line">import boto3</div><div class="line">import json</div><div class="line">import logging</div><div class="line">import os</div><div class="line"></div><div class="line">from base64 import b64decode</div><div class="line">from urlparse import parse_qs</div><div class="line"></div><div class="line">todo_table = boto3.resource(&apos;dynamodb&apos;, &apos;us-west-2&apos;).Table(&apos;todolist&apos;)</div><div class="line"></div><div class="line">ENCRYPTED_EXPECTED_TOKEN = os.environ[&apos;kmsEncryptedToken&apos;]</div><div class="line"></div><div class="line">kms = boto3.client(&apos;kms&apos;)</div><div class="line">expected_token = kms.decrypt(CiphertextBlob=b64decode(ENCRYPTED_EXPECTED_TOKEN))[&apos;Plaintext&apos;]</div><div class="line"></div><div class="line">logger = logging.getLogger()</div><div class="line">logger.setLevel(logging.INFO)</div><div class="line"></div><div class="line"></div><div class="line">def _respond_text(err, res=None):</div><div class="line">    return &#123;</div><div class="line">        &quot;text&quot;: res</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">def _respond_button(err, res=None):</div><div class="line">    return &#123;</div><div class="line">        &quot;text&quot;: res,</div><div class="line">        &quot;attachments&quot;: [</div><div class="line">            &#123;</div><div class="line">                &quot;callback_id&quot;: &quot;todo&quot;,</div><div class="line">                &quot;color&quot;: &quot;#3AA3E3&quot;,</div><div class="line">                &quot;attachment_type&quot;: &quot;default&quot;,</div><div class="line">                &quot;actions&quot;: [</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Remind me&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;reminder&quot;</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Set priority&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;priority&quot;</div><div class="line">                    &#125;,</div><div class="line">                    &#123;</div><div class="line">                        &quot;name&quot;: &quot;subtask&quot;,</div><div class="line">                        &quot;text&quot;: &quot;Set project&quot;,</div><div class="line">                        &quot;type&quot;: &quot;button&quot;,</div><div class="line">                        &quot;value&quot;: &quot;project&quot;,</div><div class="line">                    &#125;</div><div class="line">                ]</div><div class="line">            &#125;</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def _respond_button_action(params):</div><div class="line">    params = json.loads(params[0])</div><div class="line">    token = params[&apos;token&apos;]</div><div class="line">    user = params[&apos;user&apos;][&apos;name&apos;]</div><div class="line">    channel = params[&apos;channel&apos;][&apos;name&apos;]</div><div class="line">    action = params[&apos;actions&apos;][0][&apos;value&apos;]</div><div class="line">    if action == &apos;reminder&apos;:</div><div class="line">        return &quot;&#123;&#125; set a reminder for this to-do item in &#123;&#125;&quot;.format(user, channel)</div><div class="line">    elif action == &apos;priority&apos;:</div><div class="line">        return &quot;&#123;&#125; set a priority for this to-do item in &#123;&#125;&quot;.format(user, channel)</div><div class="line">    else:</div><div class="line">        return &quot;&#123;&#125; set a project for this to-do item in &#123;&#125;&quot;.foramt(user, channel)</div><div class="line">    return</div><div class="line"></div><div class="line"></div><div class="line">def _get_todo_list(user, todos):</div><div class="line">    if &apos;Item&apos; not in todos or &apos;todos&apos; not in todos[&apos;Item&apos;] or not todos[&apos;Item&apos;][&apos;todos&apos;]:</div><div class="line">        return _respond_text(None, &quot;&#123;&#125; has nothing in to-do list!&quot;.format(user))</div><div class="line">    msg = &apos;&apos;</div><div class="line">    for i, item in enumerate(todos[&apos;Item&apos;][&apos;todos&apos;]):</div><div class="line">        msg += &apos;&#123;&#125; &#123;&#125; \n&apos;.format(i, item)</div><div class="line">    return _respond_text(None, msg)</div><div class="line"></div><div class="line"></div><div class="line">def _remove_from_list(user, index, todos):</div><div class="line">    if not index.isdigit():</div><div class="line">        return _respond_text(None, &quot;Sorry, I didn’t quite get that. This usually works: `/todo done [to-do item index]`. Try `/todo todolist` to get to-do item index.&quot;)</div><div class="line">    todo_item = todos[&apos;Item&apos;][&apos;todos&apos;][int(index)]</div><div class="line">    response = todo_table.update_item(</div><div class="line">            Key=&#123;</div><div class="line">                &apos;user&apos;: user</div><div class="line">            &#125;,</div><div class="line">            UpdateExpression=&quot;REMOVE todos[&quot; + index + &quot;]&quot;,</div><div class="line">            ReturnValues=&quot;UPDATED_NEW&quot;</div><div class="line">        )</div><div class="line">    if response[&apos;ResponseMetadata&apos;][&apos;HTTPStatusCode&apos;] != 200:</div><div class="line">        return response</div><div class="line">    return _respond_text(None, &quot;&#123;&#125; has done &#123;&#125; &#123;&#125;&quot;.format(user, index, todo_item))</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def _clear(user, channel):</div><div class="line">    response = todo_table.delete_item(</div><div class="line">        Key=&#123;</div><div class="line">            &apos;user&apos;: user,</div><div class="line">        &#125;</div><div class="line">    )</div><div class="line">    if response[&apos;ResponseMetadata&apos;][&apos;HTTPStatusCode&apos;] != 200:</div><div class="line">        return response</div><div class="line">    return _respond_text(None, &quot;&#123;&#125; has cleared his/her to-do list in &#123;&#125;&quot;.format(user, channel))</div><div class="line"></div><div class="line"></div><div class="line">def _send_button(user, command, channel, command_text, todos):</div><div class="line">    # insert items into database</div><div class="line">    if &apos;Item&apos; not in todos:</div><div class="line">        response = todo_table.put_item(Item=&#123;</div><div class="line">        &apos;user&apos;: user,</div><div class="line">        &apos;channel&apos;: channel,</div><div class="line">        &apos;todos&apos;: [command_text]</div><div class="line">        &#125;)</div><div class="line">    else:</div><div class="line">        response = todo_table.update_item(</div><div class="line">                Key=&#123;</div><div class="line">                    &apos;user&apos;: user</div><div class="line">                &#125;,</div><div class="line">                UpdateExpression=&quot;SET todos = list_append(todos, :val)&quot;,</div><div class="line">                ExpressionAttributeValues=&#123;&quot;:val&quot; : [command_text]&#125;,</div><div class="line">                ReturnValues=&quot;UPDATED_NEW&quot;</div><div class="line">            )</div><div class="line"></div><div class="line">    if response[&apos;ResponseMetadata&apos;][&apos;HTTPStatusCode&apos;] != 200:</div><div class="line">        return response</div><div class="line"></div><div class="line">    return _respond_button(None, &quot;&#123;&#125; added &apos; &#123;&#125; &apos; to the to-do list in &#123;&#125;&quot;.format(user, command_text, channel))</div><div class="line"></div><div class="line"></div><div class="line">def lambda_handler(event, context):</div><div class="line">    params = parse_qs(event[&apos;body&apos;])</div><div class="line">    # with payload, which means user clicks a button</div><div class="line">    if &apos;payload&apos; in params:</div><div class="line">        return _respond_button_action(params[&apos;payload&apos;])</div><div class="line">    # without payload</div><div class="line">    token = params[&apos;token&apos;][0]</div><div class="line">    if token != expected_token:</div><div class="line">        logger.error(&quot;Request token (%s) does not match expected&quot;, token)</div><div class="line">        return respond(Exception(&apos;Invalid request token&apos;))</div><div class="line"></div><div class="line">    user = params[&apos;user_name&apos;][0]</div><div class="line">    command = params[&apos;command&apos;][0]</div><div class="line">    channel = params[&apos;channel_name&apos;][0]</div><div class="line">    command_text = params[&apos;text&apos;][0]</div><div class="line"></div><div class="line">    # get todo list from database</div><div class="line">    todos = todo_table.get_item(Key=&#123;&apos;user&apos;: user&#125;)</div><div class="line">    if command_text.startswith(&apos;todolist&apos;):</div><div class="line">        return _get_todo_list(user, todos)</div><div class="line"></div><div class="line">    elif command_text.startswith(&apos;done &apos;):</div><div class="line">        return _remove_from_list(user, command_text.split(&apos; &apos;, 1)[1], todos)</div><div class="line"></div><div class="line">    elif command_text.startswith(&apos;clear&apos;):</div><div class="line">        return _clear(user, channel)</div><div class="line"></div><div class="line">    return _send_button(user, command, channel, command_text, todos)</div><div class="line">    # return respond(None, &quot;%s added %s to the to-do list in %s to to-do list %s&quot; % (user, command_text, channel, command_text, response, todo))</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> Slack </tag>
            
            <tag> 物联网 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络 CNN 笔记 - 目标探测]]></title>
      <url>http://www.shuang0420.com/2017/06/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/</url>
      <content type="html"><![CDATA[<p>介绍目标探测的基本方法，传统方法 DPM，神经网络分类 R-CNN 系列方法和神经网络回归 YoLo 系列方法。<br><a id="more"></a></p>
<h1 id="目标探测"><a href="#目标探测" class="headerlink" title="目标探测"></a>目标探测</h1><p>先来看下什么是目标探测，下图<strong>矩形框(running box)</strong>表示的物体都可以作为目标探测的对象。不止矩形框，椭圆形框在某些场合更适合做目标探测，因为它能更好的捕捉对象，并对物体朝向做相应调整，机变性更好。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B.png"></p>
<p>目标探测的任务一般分为<strong>单目标探测</strong>和<strong>多目标探测</strong>。目的一是找到目标的位置坐标，二是判定目标类别。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E4%BB%BB%E5%8A%A1.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E4%BB%BB%E5%8A%A1.png"></p>
<p>目标探测的<strong>应用场景</strong>有安防、自动驾驶等。从技术方面讲，目标探测传统方法用的是<strong>DPM</strong>，虽然目前已经被神经网络超越，但是很多思想可以借鉴。神经网络大体上有两类方法，一类是分类方法主要是<strong>RCNN系列方法</strong> ，先找到若干候选区，然后一个区域一个区域排查，判断有没有要找的物体；另一类是回归方法主要是<strong>YoLo系列方法</strong>，直接找到区域，以及区域有什么物体。</p>
<p>下面来看下目标探测的两个直接思路。</p>
<h2 id="直接思路一：回归问题"><a href="#直接思路一：回归问题" class="headerlink" title="直接思路一：回归问题"></a>直接思路一：回归问题</h2><p>一类思路是把目标探测看作是一个<strong>回归问题</strong>。直接生成 class score，也就是判断是该类别(物品)的 confidence value，和 box coordinates，也就是检测框的坐标值。整个任务的损失函数其实是位置差和分类差的一个组合。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%9B%B4%E6%8E%A5%E6%80%9D%E8%B7%AF.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%9B%B4%E6%8E%A5%E6%80%9D%E8%B7%AF.png"></p>
<h2 id="直接思路二：局部识别问题"><a href="#直接思路二：局部识别问题" class="headerlink" title="直接思路二：局部识别问题"></a>直接思路二：局部识别问题</h2><p>另一类思路是在很多位置上尝试识别，能够完成识别的地方就是目标位置。如下图，我们生成潜在的<strong>候选区域(proposal)</strong>，然后采用分类器逐个判别这些区域内图像是不是目标物体，如果是，可以把候选区域做延展(用 regression)，看有没有更合适的候选框。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/cat1.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/cat1.png"></p>
<p>一个问题是<strong>怎样找到这些候选位置？</strong><br>一种方法是用不同 scale 的 sliding windows 来遍历所有的位置，这种方法代价太高，另一种更有效的方法是直接计算候选区域。现在有很多算法能够有效的产生候选区域，比较常用的是 <strong>EdgeBoxes</strong>（在 RCNN 中使用）。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%E4%BA%A7%E7%94%9F.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%E4%BA%A7%E7%94%9F.png"></p>
<h1 id="传统方法-DPM"><a href="#传统方法-DPM" class="headerlink" title="传统方法-DPM"></a>传统方法-DPM</h1><p>传统方法主要包括 3 个步骤：</p>
<ol>
<li>利用不同尺度的滑动窗口在图像上进行区域搜索，定位候选区域；</li>
<li>对候选区域进行特征提取，如sift，hog，haar等；</li>
<li>利用分类器进行分类识别，svm等</li>
</ol>
<p>主要思路就是提取图像特征，制作出激励模板，在原始图像滑动计算，得到激励效果图，然后根据激励分布确定目标位置。如下图人物识别把人为设计的激励模板和 HOG 特征图结合，如果有人，会得到加强的激励，然而同样的，柱子也会得到激励。</p>
<p><strong>DPM(Deformable Part Model)</strong>可以看做是<strong>HOG(Histograms of Oriented Gradients)+SVM(Surpport Vector Machine) 方法</strong>的扩展，大体思路是一致的 — 先计算梯度方向直方图，然后用 SVM 训练得到物体的梯度模型。有了这样的模板就可以直接用来分类了，简单理解就是模型和目标匹配。DPM 只是在模型上做了很多改进工作。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/dpm1.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/dpm1.png"></p>
<p>由于目标可能会形变，之前模型不能很好的适应复杂场景的探测，所以一个改进是各个部分单独考虑，对物体的不同部分单独进行学习，所以<strong>DPM</strong>把物体看成了多个组成部件(比如说人脸的鼻子，嘴巴等)，用部件间的关系来描述物体，这个特点非常符合自然界许多物体的非刚性特征。基本思路如下:</p>
<ol>
<li>产生多个模板，整体模板(root filter)以及不同局部模板(part filter)<br>root filter 包含目标的整体信息，而 part filter 采用高分辨率的细节建模，看的梯度会更加精细</li>
<li>不同模板同输入图片“卷积”产生特征图</li>
<li>特征图组合形成融合特征</li>
<li>对融合特征进行传统分类，回归，得到目标位置</li>
<li>模型在图像特定位置和尺度的得分， 等于 root filter 的得分加上各个 part filter 得分的总和。每个 part filter 的得分等于该 part 在所有空间位置的得分的最大值，而部件在某位置的得分等于 part filter 在此位置的得分减去此位置的变形代价(也就是 part 偏离其理想位置的程度)</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/dpm2.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/dpm2.jpg">
<p>DPM 的优点是方法比较直观、简单，运算速度快，也可以适应运动物体变形，很好的处理遮挡、非刚性可变和视觉变换问题，到 2012 年前，是最好的方法。然而 DPM 也有一些缺点</p>
<ul>
<li>性能一般</li>
<li>激励特征人为设计，表达能力有限，工作量大，难以进行迁移学习</li>
<li>大幅度旋转无法适应，稳定性差</li>
</ul>
<h1 id="神经网络分类-R-CNN-系列方法"><a href="#神经网络分类-R-CNN-系列方法" class="headerlink" title="神经网络分类: R-CNN 系列方法"></a>神经网络分类: R-CNN 系列方法</h1><h2 id="R-CNN-CVPR2014-TPAMI2015"><a href="#R-CNN-CVPR2014-TPAMI2015" class="headerlink" title="R-CNN(CVPR2014, TPAMI2015)"></a>R-CNN(CVPR2014, TPAMI2015)</h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>神经网络的分类思想是<strong>对多个位置，不同尺寸，用卷积神经网络判断区域内图片是不是某物</strong>，<strong>候选位置(proposal)</strong>提出方法一般用 <strong>EdgeBox</strong>。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn1.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn1.png"></p>
<p>R-CNN 最初提出的时候选择 20 类进行探测，是在 ImageNet 模型的基础上，把 1000 类的分类模型变成能识别 21 类(20类+other)的 Fine-tune 分类模型。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn2.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn2.jpg"><br>=&gt;<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn3.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn3.jpg"></p>
<p><strong>特征的提取过程:</strong> 对图片计算候选区域；对候选区域切分图片，对切分部分进行 resize 变成输入大小；提取相应高级特征；存储特征(大容量，200-300G空间存储图片)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn4.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn4.jpg"></p>
<p><strong>单独目标探测器训练：</strong>对每一类进行单独训练，保证每一类训练数据平衡，每一类都是 binary 分类(yes/no)。比如猫的分类器，可能大部分图片没有一个理想的猫，只有一个耳朵，这不算猫，我们要与真值进行比较，看左上右下区域，如果重合(共有区域)比较多，就认为是猫的图片。每一类都有很多的正例反例(1/0)。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn5.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn5.jpg"></p>
<p><strong>单独目标回归器训练-基于候选区域微调:</strong> 同样的，每一类单独训练，保证每一类训练数据平衡，这里是每一类做 BBOX 回归。目的是在知道是不是猫以及位置的偏移后，用回归对位置进行 offset，离真值(ground truth)更近，最终的探测精度会更高。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn6.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn6.jpg"></p>
<p><strong>总的来说，R-CNN 的测试过程就是</strong></p>
<ol>
<li>对每个图像生成 1k-2k 个候选区域</li>
<li>对每个候选区域，使用深度网络进行特征计算</li>
<li>特征喂给每一类的 svm 分类器，判别是否属于该类分类；同时用回归修正候选框位置</li>
<li>后续处理<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn7.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/rcnn7.jpg">
</li>
</ol>
<h3 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%95%B0%E6%8D%AE%E9%9B%86.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%95%B0%E6%8D%AE%E9%9B%86.jpg">
<h3 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h3><ul>
<li><strong>MAP(mean average precision)</strong></li>
<li><strong>IoU</strong>，真值和预测值多重叠部分与两者的并集的比值，一般大于 0.5 就认为是正确的<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/IoU.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/IoU.jpg">
</li>
</ul>
<h3 id="R-CNN-结果对比"><a href="#R-CNN-结果对比" class="headerlink" title="R-CNN 结果对比"></a>R-CNN 结果对比</h3><p>Regionlets(2013) 并没有经过 fine-tune，R-CNN(2014, AlexNet) 用事先训练好的分类器进行了 fine-tune，R-CNN+bbox reg(AlexNet)，用了 regression，加了 offset 对检测框做了范围调整，R-CNN(vgg-16)把 base model 改成了 vgg</p>
<p>总的来说，主要是从下面三个角度进行了模型的调整</p>
<ol>
<li>Finetune</li>
<li>回归微调</li>
<li>Base 模型<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/crnn9.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/crnn9.jpg">
</li>
</ol>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点:</strong></p>
<ol>
<li>CNN 用于目标探测，利 用了 CNN 高效识别能力， 大大提高性能</li>
<li>摆脱人为设计物品模版， 方法具有通用性</li>
<li>分类＋回归，有了找到精确位置的可能</li>
</ol>
<p><strong>缺陷:</strong></p>
<ol>
<li>为了检测一个目标，所有候选区域计算，大量卷积运算，非常慢<br>对于速度慢这个问题，SPP-NET 给出了解决方案。R-CNN 对图像提完 region proposal(2k 左右)之后将每个 proposal 当成一张图像进行后续处理(CNN提特征+SVM分类)，实际上对一张图像进行了2000次提特征和分类的过程！SPP-NET 对图像提一次卷积层特征，然后将 region proposal 在原图的位置映射到卷积层特征图上，这样对于一张图像只需要提一次卷积层特征，然后将每个 region proposal 的卷积层特征输入到全连接层做后续操作</li>
<li>SVM 训练与CNN 断裂， SVM Loss 没办法用于 CNN Loss，有效信息不能用于优化模型， not end-to-end</li>
<li>每一类单独训练，异常繁琐</li>
</ol>
<h2 id="Fast-R-CNN-ICCV2015"><a href="#Fast-R-CNN-ICCV2015" class="headerlink" title="Fast R-CNN(ICCV2015)"></a>Fast R-CNN(ICCV2015)</h2><p>Fast R-CNN 的三个进步</p>
<ul>
<li>共享卷积计算<br>增加 ROI pooling layer</li>
<li>完整训练(end-to-end)<br>用 softmax 代替 svm 分类，用多目标损失函数加入候选框回归，除 region proposal 提取外实现了 end-to-end</li>
<li>多目标一起学习<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/fast%20rcnn.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/fast%20rcnn.jpg">
</li>
</ul>
<h3 id="共享卷积计算"><a href="#共享卷积计算" class="headerlink" title="共享卷积计算"></a>共享卷积计算</h3><p>Fast R-CNN 在最后一个卷积层后加了一个 ROI pooling layer，实际上就是上面提到的 SPP-NET 的一个精简版，特点是:</p>
<ol>
<li>卷积计算保持空间位置</li>
<li>共同区域的卷积计算只需进行一次</li>
<li>切割候选区+提取特征图=计算完整特征图+切割对应候选区<br>把图片的 region proposal 切割出来，resize，提取特征，其实就等同于在原图特征图里找到 region proposal<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF.jpg">
</li>
</ol>
<p>一个重要的问题是<strong>不同区域的特征如何保持一致？</strong><br>全连接层要求接的区域形状一致；所以要特征图里区域的一致性处理，也就是做一个 pooling<br><strong>特征一致化 - Max Pooling</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%89%B9%E5%BE%81%E4%B8%80%E8%87%B4%E5%8C%96.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E7%89%B9%E5%BE%81%E4%B8%80%E8%87%B4%E5%8C%96.jpg"></p>
<p>局部区域<br>100x50 =&gt;按 4:2 pooling<br>50x100 =&gt; 按 2:4 pooling<br>=&gt; 25x25 feature<br>=&gt; 225 FC</p>
<p>如果 pooling size 不完美，其实也没有问题，pooling 本身就是填充 pooling 后的图的每一个 pixel，只要从 pooling 前某区域选一个 pixel 值即可，不一定要规整</p>
<h3 id="位置-类别联合学习"><a href="#位置-类别联合学习" class="headerlink" title="位置 + 类别联合学习"></a>位置 + 类别联合学习</h3><p><strong>图片 =&gt; cnn feature map计算 =&gt; proposal应用 =&gt; feature map相应区域做 region pooling 得到固定大小的 feature map =&gt; classification &amp; regression</strong><br>用 softmax 代替 svm 分类，使用<strong>多任务损失函数(multi-task loss)</strong>，将候选框回归直接加入到 cnn 网络中训练，除去 region proposal 的提取阶段，这样的训练过程是端到端的(end-to-end)，整个网络的训练和测试十分方便<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/fast%20rcnn.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/fast%20rcnn.png"></p>
<h3 id="性能提升"><a href="#性能提升" class="headerlink" title="性能提升"></a>性能提升</h3><p>看一下性能提升的情况<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%80%A7%E8%83%BD.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%80%A7%E8%83%BD.jpg"><br>然而前提是 不考虑候选区域(proposal)的生成，如果加上候选区域(proposal)的时间<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%80%A7%E8%83%BD2.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/%E6%80%A7%E8%83%BD2.jpg"><br>region proposal 的提取使用 selective search，目标检测时间大多消耗在这上面(提region proposal 2~3s，而提特征分类只需0.32s)，无法满足实时应用，那么，怎么解决候选区域的计算呢？一个方法是也靠神经网络。</p>
<h2 id="Faster-R-CNN-NIPS2015"><a href="#Faster-R-CNN-NIPS2015" class="headerlink" title="Faster R-CNN(NIPS2015)"></a>Faster R-CNN(NIPS2015)</h2><h3 id="RPN-Region-Proposal-Network"><a href="#RPN-Region-Proposal-Network" class="headerlink" title="RPN(Region Proposal Network)"></a>RPN(Region Proposal Network)</h3><p>用神经网络来解决候选区域的生成问题，主要是神经网络特征增加一组输出 <strong>RPN(Region Proposal Network)候选区域网络</strong></p>
<ol>
<li>直接产生候选区域，无需额外生成<br>本质上是 sliding window，RPN 只需在最后的卷积层上滑动一遍，因为 anchor 机制和候选框回归可以得到多尺度多长宽比多 region proposal</li>
<li>直接用于后续特征图切割</li>
</ol>
<p>最后的特征图中有很多个 pixel，每个 pixel 和卷积核进行计算，生成 k 个可能的 prpoposal(实际中 k 往往=9，一个区域可能同时被多个物体占用，所以尽可能把可能分布的形状都生成)，每个 proposal 有个 score 的计算。如图，左边是 3x3 的卷积网络的特征图，右边是 k 个 anchor box(相当于小的候选生成单元)。我们对特征图进行 sliding window 的计算，每个 pixel 生成 256 长的向量(向量长度其实是自己设计的，vgg 建议 512-d)，这个向量用来生成 k 个 proposal 的值，以及对应的 2k score(是/不是目标物体)，4k 个 coordinates(上下左右坐标)。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn1.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn1.jpg"></p>
<p><strong>网络输出的值：</strong></p>
<ol>
<li>是不是一个目标</li>
<li>覆盖范围的相对位置</li>
</ol>
<p><strong>k=9(3种尺寸，3种长宽比)个 anchor，能产生多少个 proposal?</strong><br>特征图 size HxW -&gt; HWx9 in paper 2400x9</p>
<p><strong>如果是 VGG conv5 作为特征图，3x3 区域对应的原始图像区域？</strong><br>经过了 4 个 pooling，往前推，6x6 -&gt; 12x12 -&gt; 24x24 -&gt; 48x48，也就是 16 倍的一个缩放</p>
<p><strong>Anchor的平移不变怎么理解</strong><br>较小的平移 pooling 过程中忽略，3 个 pixel 的移动经过 4 层的 pooling，移动后的位置和原位置相差可以忽略</p>
<p><strong>Anchor 同外接 Proposal 区别</strong><br>数量：1-2个数量级减少；性能：更高效；<br>速度：10x</p>
<p><strong>Anchor 设计的借鉴意义？</strong><br>神经网络有能力找到最终量，也有能力找到很多中间量。只用 Anchor 判断是不是目标，会不会存在大材小用，能够判断更多吗？或者说，能在是不是目标的基础上，判断是什么目标吗，也就是<strong>直接拟合</strong></p>
<p>为了让RPN的网络和Fast R-CNN网络实现卷积层的权值共享，训练 RPN 和 Fast R-CNN的时候用了4阶段的训练方法:</p>
<ol>
<li>使用在 ImageNet 上预训练的模型初始化网络参数，微调 RPN 网络；</li>
<li>使用(1)中RPN网络提取 region proposal 训练 Fast R-CNN网络；</li>
<li>使用(2)的 Fast R-CNN 网络重新初始化 RPN, 固定卷积层进行微调；</li>
<li>固定(2)中 Fast R-CNN 的卷积层，使用 (3) 中 RPN 提取的 region proposal 微调网络<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn2.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn2.jpg">
</li>
</ol>
<p>Faster R-CNN 用了<strong>直接联合学习(joint learning)</strong> 的方法，如上图，一个网络有 4 个损失函数</p>
<ol>
<li>Anchor 是不是目标</li>
<li>Anchor 回归候选区域回归</li>
<li>Fast R-CNN 分类</li>
<li>Fast R-CNN 基于候选位置回归<br>联合学习的方法产生了更少的候选区，但是精度不会受到影响，速度却快了 10 倍，接近于实时处理(@K40 GPU, 12G)。</li>
</ol>
<h3 id="性能提升-1"><a href="#性能提升-1" class="headerlink" title="性能提升"></a>性能提升</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn3.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/faster%20rcnn3.jpg">
<p>接近于实时处理，然而还是很难实时的目标探测，下面的 YOLO 这类方法可以达到实时性。</p>
<h1 id="神经网络回归-YoLo-系列方法"><a href="#神经网络回归-YoLo-系列方法" class="headerlink" title="神经网络回归: YoLo 系列方法"></a>神经网络回归: YoLo 系列方法</h1><h2 id="YoLo"><a href="#YoLo" class="headerlink" title="YoLo"></a>YoLo</h2><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p>YoLo 将目标探测任务看作目标区域预测和类别预测的回归问题，用单个神经网络直接预测物品边界和类别分数，可以<strong>直接找到物体是什么，在哪里</strong>。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/YoLo.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/YoLo.jpg"></p>
<p>把图片分成 SxS 的格子(grid cell)，一般是 7x7 的网络，每个网格生成：</p>
<ol>
<li>B 个 Bbox，4 个 coordinates + 1 个 confidence score</li>
<li>N 个类别分数 $Pr(Class_i|Object)$<br>与 Anchor 不同的是，这里有 N 个分数，表示属于每一类的分数分别是多少</li>
</ol>
<p>S=7, B=2, N=20<br><strong>总共的回归目标：</strong> SxSx(5B+N)<br>​     2x5+20=30 个参数，49x30=1470 个数值，用来回归<br><strong>候选区域个数：</strong> (B=2) 98 个 &lt;&lt; Faster R-CNN<br>每个小区域生成 2 个候选区，一个小的区域就是一个粗糙的 proposal，对小区域进行大范围的 regression，找到目标<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo2.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo2.jpg"></p>
<p><strong>损失函数:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo3.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo3.png"></p>
<h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p><strong>性能：</strong></p>
<ul>
<li>实时运行</li>
<li>精度稍微下降</li>
<li>定位精度较差</li>
</ul>
<p>经过大量的 pooling，位置的响应会有一定弱化<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo4.png" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/yolo4.png"></p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><ol>
<li>YoLo 的每一个网格只预测两个 boxes，一种类别。这导致模型对相邻目标预测准确率下降。因此，YOLO 对成队列的目标（如一群鸟）识别准确率较低。</li>
<li>YoLo 是从数据中学习预测 bounding boxes，因此，对新的或者不常见角度的目标无法识别。</li>
<li>YoLo 的损失函数对small bounding boxes 和 large bounding boxes 的 error 平等对待，影响了模型识别准确率。因为对于小的 bounding boxes，small error影响更大。</li>
</ol>
<h2 id="SSD-The-Single-Shot-Detector"><a href="#SSD-The-Single-Shot-Detector" class="headerlink" title="SSD: The Single Shot Detector"></a>SSD: The Single Shot Detector</h2><p>SSD 分类更细，网络结构有点像 resnet。中间多层特征参与位置、种类计算，在不同 layer 输出的不同尺寸的 feature map 划格子，在格子上提“anchor”，弥补了 Yolo 只在最后一层分 7x7 的框漏掉的部分。和 Yolo 相比，更快更准确。</p>
<ul>
<li>候选区 98 vs 8732</li>
<li>速度 21:46 (vgg base)</li>
<li>精度 66.4:74.3</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/ssd.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9B%AE%E6%A0%87%E6%8E%A2%E6%B5%8B/ssd.jpg">
<blockquote>
<p>参考链接：<br><a href="http://bealin.github.io/2016/10/23/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BB%8ERCNN%E3%80%81Fast-RCNN%E5%88%B0Faster-RCNN/" target="_blank" rel="external">目标检测方法——从RCNN、Fast-RCNN到Faster-RCNN</a><br><a href="https://zhuanlan.zhihu.com/p/25045711" target="_blank" rel="external">YOLO：实时快速目标检测</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS API Gateway + Lambda + Slack App - 新建 slash command]]></title>
      <url>http://www.shuang0420.com/2017/06/19/AWS%20API%20Gateway%20+%20Lambda%20+%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/</url>
      <content type="html"><![CDATA[<p>接之前<a href="http://www.shuang0420.com/2017/06/09/AWS%20Lex%20创建%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/">AWS Lex 创建 Slack Bot - Integrating Lex Bot with Slack</a>，介绍怎么在 slack app 的基础上添加 slash command。<br><a id="more"></a></p>
<p>slash command 的配置页面需要一个 <strong>Request URL</strong>，这个 URL 就由 <strong>AWS API Gateway</strong> 来提供，API Gateway 调用 <strong>AWS Lambda</strong> 来完成具体的动作，而 Lambda 需要一个 slack token 来验证 slack app 的身份，才能够调用 slack api 来接收/回复信息，这个 token 由 <strong>AWS IAM</strong> 进行加密保障安全性。下面的教程分别介绍了怎么加密 token，编写 Lambda，配置 API Gateway，来完成一个 slash command 的创建。</p>
<h1 id="Encrypt-KMS-Key"><a href="#Encrypt-KMS-Key" class="headerlink" title="Encrypt KMS Key"></a>Encrypt KMS Key</h1><p><strong>Step 1:</strong> 在 <a href="https://console.aws.amazon.com/iam/home#encryptionKeys" target="_blank" rel="external">IAM console</a> 创建一个 KMS key，记录下 key-id，后面需要用到。<br><strong>Step 2:</strong> 在 <a href="https://api.slack.com/apps" target="_blank" rel="external">slack apps</a> 选择你的 app，然后在 <code>Basic Information</code> 下的 <code>App Credentials</code> 部分找到 <code>Verification Token</code>，记录下来，这就是我们需要加密的 token。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%2B%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/command-token.png" class="ful-image" alt="command-token.png">
<p><strong>Step 3:</strong> 我们的目的是用 Step 1 创建的 key 来对 Step 2 记录的 token 进行加密。如果电脑已经装了 awscli，那么直接在命令行输入下面的命令即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ aws kms encrypt --key-id &lt;key-id&gt; --plaintext &lt;text&gt;</div></pre></td></tr></table></figure></p>
<p>其中 key-id 就是开始记录下的 kms key id，text 就是 verification token，把产生的 <code>CiphertextBlob</code>记录下来，在下一步 Lambda Configuration 里要用。</p>
<p>如果没有安装 aws-cli，OSX 系统直接用 <code>brew</code> 命令安装一下，最好不要用 <code>pip</code>，很大概率会出问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ brew install awscli</div></pre></td></tr></table></figure>
<p>装好后，<code>aws --version</code> 查看是否安装成功， <code>aws configure</code> 命令来配置账户信息，会要求输入</p>
<ul>
<li><strong>AWS Access Key ID:</strong> 在 aws console 的 <code>My Security Credentials</code> 下</li>
<li><strong>AWS Secret Access Key:</strong> 现在必须创建一个 IAM user 才能得到，戳 <a href="[http://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html">create IAM user</a>)</li>
<li><strong>Default region name:</strong> 根据实际情况填，如果是 US East (N. Virginia)，就填 <code>us-east-1</code>，注意不要在末尾加 abcd，<code>us-east-1a</code> 类似的格式会出错</li>
<li><strong>Default output format:</strong> 可以不填</li>
</ul>
<p>配置完正常进行加密即可。</p>
<h1 id="Lambda-Configuration"><a href="#Lambda-Configuration" class="headerlink" title="Lambda Configuration"></a>Lambda Configuration</h1><p>Lambda blueprint 选 <code>slack-echo-command</code>，创建 lambda function <code>slashTest</code>，这里要实现的是当用户 [user] 调用 /test 这个 [command] 时(之后会创建)并输入文本 [text] 时，返回 <code>[user] invoked [command,e.g., /test] in [channel,eg., directmessage] with the following text: [text]&quot;</code>。代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;;</div><div class="line"></div><div class="line">/*</div><div class="line">This function handles a Slack slash command and echoes the details back to the user.</div><div class="line"></div><div class="line">Follow these steps to configure the slash command in Slack:</div><div class="line"></div><div class="line">  1. Navigate to https://&lt;your-team-domain&gt;.slack.com/services/new</div><div class="line"></div><div class="line">  2. Search for and select &quot;Slash Commands&quot;.</div><div class="line"></div><div class="line">  3. Enter a name for your command and click &quot;Add Slash Command Integration&quot;.</div><div class="line"></div><div class="line">  4. Copy the token string from the integration settings and use it in the next section.</div><div class="line"></div><div class="line">  5. After you complete this blueprint, enter the provided API endpoint URL in the URL field.</div><div class="line"></div><div class="line"></div><div class="line">  To encrypt your secrets use the following steps:</div><div class="line"></div><div class="line">  1. Create or use an existing KMS Key - http://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html</div><div class="line"></div><div class="line">  2. Click the &quot;Enable Encryption Helpers&quot; checkbox</div><div class="line"></div><div class="line">  3. Paste &lt;COMMAND_TOKEN&gt; into the kmsEncryptedToken environment variable and click encrypt</div><div class="line"></div><div class="line">Follow these steps to complete the configuration of your command API endpoint</div><div class="line"></div><div class="line">  1. When completing the blueprint configuration select &quot;Open&quot; for security</div><div class="line">     on the &quot;Configure triggers&quot; page.</div><div class="line"></div><div class="line">  2. Enter a name for your execution role in the &quot;Role name&quot; field.</div><div class="line">     Your function&apos;s execution role needs kms:Decrypt permissions. We have</div><div class="line">     pre-selected the &quot;KMS decryption permissions&quot; policy template that will</div><div class="line">     automatically add these permissions.</div><div class="line"></div><div class="line">  3. Update the URL for your Slack slash command with the invocation URL for the</div><div class="line">     created API resource in the prod stage.</div><div class="line">*/</div><div class="line"></div><div class="line">const AWS = require(&apos;aws-sdk&apos;);</div><div class="line">const qs = require(&apos;querystring&apos;);</div><div class="line"></div><div class="line">const kmsEncryptedToken = process.env.kmsEncryptedToken;</div><div class="line">let token;</div><div class="line"></div><div class="line"></div><div class="line">function processEvent(event, callback) &#123;</div><div class="line">    const params = qs.parse(event.body);</div><div class="line">    const requestToken = params.token;</div><div class="line">    if (requestToken !== token) &#123;</div><div class="line">        console.error(`Request token ($&#123;requestToken&#125;) does not match expected`);</div><div class="line">        return callback(&apos;Invalid request token&apos;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    const user = params.user_name;</div><div class="line">    const command = params.command;</div><div class="line">    const channel = params.channel_name;</div><div class="line">    const commandText = params.text;</div><div class="line"></div><div class="line">    callback(null, `$&#123;user&#125; invoked $&#123;command&#125; in $&#123;channel&#125; with the following text: $&#123;commandText&#125;`);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    const done = (err, res) =&gt; callback(null, &#123;</div><div class="line">        statusCode: err ? &apos;400&apos; : &apos;200&apos;,</div><div class="line">        body: err ? (err.message || err) : JSON.stringify(res),</div><div class="line">        headers: &#123;</div><div class="line">            &apos;Content-Type&apos;: &apos;application/json&apos;,</div><div class="line">        &#125;,</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    if (token) &#123;</div><div class="line">        // Container reuse, simply process the event with the key in memory</div><div class="line">        processEvent(event, done);</div><div class="line">    &#125; else if (kmsEncryptedToken &amp;&amp; kmsEncryptedToken !== &apos;&lt;kmsEncryptedToken&gt;&apos;) &#123;</div><div class="line">        const cipherText = &#123; CiphertextBlob: new Buffer(kmsEncryptedToken, &apos;base64&apos;) &#125;;</div><div class="line">        const kms = new AWS.KMS();</div><div class="line">        kms.decrypt(cipherText, (err, data) =&gt; &#123;</div><div class="line">            if (err) &#123;</div><div class="line">                console.log(&apos;Decrypt error:&apos;, err);</div><div class="line">                return done(err);</div><div class="line">            &#125;</div><div class="line">            token = data.Plaintext.toString(&apos;ascii&apos;);</div><div class="line">            processEvent(event, done);</div><div class="line">        &#125;);</div><div class="line">    &#125; else &#123;</div><div class="line">        done(&quot;Token has not been set.&quot;);</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>配置需要注意的是 <strong>Role</strong> 的选择，默认从 templates 里选 <code>kmsDecrypt</code>，不用修改，加个名字就好。在 <code>Environment variables</code> 里填写上一部分记录下的加密后的 token。但是！代码里请<strong>不要修改！！</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%2B%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/role.png" class="ful-image" alt="role.png">
<h1 id="API-Gateway-Configuration"><a href="#API-Gateway-Configuration" class="headerlink" title="API Gateway Configuration"></a>API Gateway Configuration</h1><p>关于基础的 API Gateway 教程，见 <a href="http://www.shuang0420.com/2017/06/18/AWS%20API%20Gateway%20+%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/">AWS API Gateway + Lambda 教程 - 生成随机数</a></p>
<p>在 <a href="https://console.aws.amazon.com/console/home?region=us-east-1" target="_blank" rel="external">aws console</a> 页面的 <strong>Services -&gt; Application Service -&gt; API Gateway</strong> 下新建 API，然后在 <strong>Action</strong> 下拉框下 <strong>Create Resource</strong>，名称可以写 /test，然后继续 <strong>Create Method</strong> ，选 <strong>POST</strong>，Lambda Function 选择之前我们已经建好的 <code>slashTest</code>，保存后在新页面选择 <strong>Integration Request</strong>，新建一个 <strong>mapping template</strong>，如下：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%2B%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/mapping.png" class="ful-image" alt="mapping.png">
<p><strong>Content-Type:</strong> <code>application/x-www-form-urlencoded</code><br><strong>Template:</strong> <code>{ &quot;body&quot;: $input.json(&quot;$&quot;) }</code></p>
<p>之后部署，<strong>Actions -&gt; Deploy API</strong>，记下完成页面显示的 <strong>Invoke URL</strong>。</p>
<h1 id="Slack-App-Configuration"><a href="#Slack-App-Configuration" class="headerlink" title="Slack App Configuration"></a>Slack App Configuration</h1><p>回到 <a href="https://api.slack.com/apps" target="_blank" rel="external">slack app</a> 页面，选择左侧的 <strong>Slash Commands</strong>，新建一个 Command，<strong>Request URL</strong> 填写上一部分记录下来的地址，注意将 sub resource name 补充完整，这里是 /test。完成后记得 <strong>reinstall app</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%2B%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/command.png" class="ful-image" alt="command.png">
<p>返回结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;statusCode&quot;:&quot;200&quot;,&quot;body&quot;:&quot;\&quot;sxu1 invoked /test in directmessage with the following text: hello world\&quot;&quot;,&quot;headers&quot;:&#123;&quot;Content-Type&quot;:&quot;application/json&quot;&#125;&#125;</div></pre></td></tr></table></figure></p>
<p>如果要返回 plain text，直接修改 lambda function，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    const done = (err, res) =&gt; callback(null, res); //&#123;</div><div class="line">      //     statusCode: err ? &apos;400&apos; : &apos;200&apos;,</div><div class="line">      //     body: err ? (err.message || err) : JSON.stringify(res),</div><div class="line">      //     headers: &#123;</div><div class="line">      //         &apos;Content-Type&apos;: &apos;application/json&apos;,</div><div class="line">      //     &#125;,</div><div class="line">      // &#125;);</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%2B%20Slack%20App%20-%20%E6%96%B0%E5%BB%BA%20slash%20command/test.png" class="ful-image" alt="test.png">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> Slack </tag>
            
            <tag> 物联网 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS API Gateway + Lambda 教程 - 生成随机数]]></title>
      <url>http://www.shuang0420.com/2017/06/18/AWS%20API%20Gateway%20+%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>非常简单的教程，以产生一定范围内的随机数为例，介绍如何用 AWS Lambda + API Gateway 建立一个 serveless API，包括 API 如何传参。<br><a id="more"></a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>如果要用一句话理解 <strong><a href="https://aws.amazon.com/api-gateway/?nc1=h_ls" target="_blank" rel="external">API Gateway</a></strong>，那必须是 <strong>serverless APIs</strong>。<strong>AWS API Gateway</strong> 与 <strong>AWS Lambda</strong> 紧密集成，开发者可以通过 API Gateway 创建基于 REST 风格的 API，各种 app 应用调用这些 API，而这些 API 可以通过 AWS Lambda 中运行的代码来调用公开提供的 AWS 服务(or anything you like)。下面两幅图给出了更直观的逻辑。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/api%20gateway.png" class="ful-image" alt="api%20gateway.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/serveless%20apps.png" class="ful-image" alt="serveless%20apps.png">
<p><strong><a href="https://aws.amazon.com/api-gateway/?nc1=h_ls" target="_blank" rel="external">API Gateway</a></strong> 的优势官方说明说了很多，感觉最大优势除了能够创建完全无服务器的 API 外，值得一提的就是能提供 <strong>安全控制机制(security)</strong> 和 <strong>版本控制(versioning)</strong>，有兴趣还是看文档吧。</p>
<p>下面来一个简单的例子 randomGenerator，产生 0-10 之间的随机数。网上可以找到一些教程，不过有些并不能 work，有很多坑，感觉是版本问题。主要逻辑就是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">API Gateway =&gt;</div><div class="line">handle and validate request</div><div class="line">pass to lambda function</div><div class="line"></div><div class="line">Lambda execution rule =&gt;</div><div class="line">call other aws service OR do something else</div></pre></td></tr></table></figure></p>
<p>相应的，步骤也就是分别配置好 <strong>Lambda function</strong> 和 <strong>API</strong>，然后将两者结合起来，结合方法有两种，一是在 Lambda 界面添加 <strong>Trigger</strong>，连接 API；二是在 <strong>API Gateway</strong> 界面添加 <strong>Lambda function</strong>，绑定 <strong>Lambda</strong>，两种方法都可以。</p>
<h1 id="Example-1-Basic-random-number-generator"><a href="#Example-1-Basic-random-number-generator" class="headerlink" title="Example 1: Basic random-number-generator"></a>Example 1: Basic random-number-generator</h1><h2 id="Lambda-Configuration"><a href="#Lambda-Configuration" class="headerlink" title="Lambda Configuration"></a>Lambda Configuration</h2><p>登录<a href="https://console.aws.amazon.com" target="_blank" rel="external">AWS console</a>在 Service 下选择 Lambda<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/LAMBDA1.png" class="ful-image" alt="LAMBDA1.png"></p>
<p><strong>Step1: Create a Lambda function</strong>，选 Node.js.4.3, Blank Function<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/LAMBDA2.png" class="ful-image" alt="LAMBDA2.png"></p>
<p><strong>Step 2: Configure Triggers</strong>，如果已经配置好了 API Gate，就选择相应的 API name，如果没有，直接默认下一步。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/Add%20trigger.png" class="ful-image" alt="Add%20trigger.png"></p>
<p><strong>Step 3: Configure Function</strong>，进行如下设置，附代码部分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;</div><div class="line">console.log(&apos;Loading function&apos;)</div><div class="line"></div><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    let min = 0;</div><div class="line">    let max = 10;</div><div class="line"></div><div class="line">    let generatedNumber = Math.floor(Math.random() * (max - min)) + min;</div><div class="line"></div><div class="line">    callback(null, generatedNumber);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/LAMBDA4.png" class="ful-image" alt="LAMBDA4.png">
<p>这里我们不需要验证身份，environment variables 留空就好。Role 如果没有 existing role，可以新建一个。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/LAMBDA5.png" class="ful-image" alt="LAMBDA5.png">
<p><strong>Step 4: Submit</strong>，预览一下如果没问题就 submit，等待一会儿 lambda function 就建好啦。</p>
<p><strong>Step 5: Test</strong>，如果在 <strong>Step 2</strong> 里选择了已经建好的 API Gateway 作为 <strong>Trigger</strong>，那么可以直接选择 <strong>Test</strong> 进行测试，也可以通过 url 测试。有可能会遇到 <strong>Internal server error</strong>，官方说明 <strong>response must have statusCode, body, headers</strong>，于是把代码改了下，就成功啦。(后来发现好像不改也没关系。。)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;;</div><div class="line">console.log(&apos;Loading function&apos;);</div><div class="line"></div><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    let min = 0;</div><div class="line">    let max = 10;</div><div class="line"></div><div class="line">    let generatedNumber = Math.floor(Math.random() * (max - min)) + min;</div><div class="line"></div><div class="line">    const response = &#123;</div><div class="line">        statusCode: 200,</div><div class="line">        body: JSON.stringify(generatedNumber)</div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    callback(null, response);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>测试结果如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/lambda%20test.png" class="ful-image" alt="lambda%20test.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/Add%20trigger%20done.png" class="ful-image" alt="Add%20trigger%20done.png">
<p>相反，如果在 <strong>Step 2</strong> 里并没有设置 <strong>Trigger</strong>，我们需要新建 <strong>API</strong>，请看下一部分 <strong>API Gateway Configuration</strong>。</p>
<h2 id="API-Gateway-Configuration"><a href="#API-Gateway-Configuration" class="headerlink" title="API Gateway Configuration"></a>API Gateway Configuration</h2><p>在 <a href="https://console.aws.amazon.com/console/home?region=us-east-1" target="_blank" rel="external">aws console</a> 页面的 <strong>Services -&gt; Application Service -&gt; API Gateway</strong> 下新建 API，然后在 <strong>Action</strong> 下拉框下 <strong>Create Resource</strong>，名称可以写 /number，然后继续 <strong>Create Method</strong> ，选 <strong>GET</strong>，Lambda Function 选择之前我们已经建好的 <code>random-number-generator</code></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/new%20api.png" class="ful-image" alt="new%20api.png">
<p>填写完毕后可以直接测试一下，产生了随机数 1。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/new%20api%20test1.png" class="ful-image" alt="new%20api%20test1.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/new%20api%20test2.png" class="ful-image" alt="new%20api%20test2.png">
<p>测试通过就可以部署，<strong>Actions -&gt; Deploy API</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/new%20api%20deploy.png" class="ful-image" alt="new%20api%20deploy.png">
<p>部署完成后会自动跳转到 Stages 页面，把 <strong>Invoke URL</strong> 记录下来。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/new%20api%20url.png" class="ful-image" alt="new%20api%20url.png">
<p>浏览器测试一下，注意补充 sub-resource，这里是 /number。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-04%20%E4%B8%8A%E5%8D%885.51.27.png" class="ful-image" alt="%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-04%20%E4%B8%8A%E5%8D%885.51.27.png">
<p>然后在其他应用里就可以直接通过 url 调用 API 啦～</p>
<h1 id="Example-2-Passing-information-through-API-Gateway"><a href="#Example-2-Passing-information-through-API-Gateway" class="headerlink" title="Example 2: Passing information through API Gateway"></a>Example 2: Passing information through API Gateway</h1><p>API 经常需要传参，那么怎么通过 <strong>API Gateway</strong> 传递参数呢？比如说我们希望让用户来定义产生随机数的范围，也就是 url 应该是下面这样的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https://[API id].execute-api.us-east-1.amazonaws.com/prod/number?min=1&amp;max=10</div></pre></td></tr></table></figure>
<p>其实做法也很简单，先修改 <strong>Lambda function</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;</div><div class="line">console.log(&apos;Loading function&apos;)</div><div class="line"></div><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    let min = event.min;</div><div class="line">    let max = event.max;</div><div class="line"></div><div class="line">    let generatedNumber = Math.floor(Math.random() * (max - min)) + min;</div><div class="line"></div><div class="line">    callback(null, generatedNumber);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>然后在 method 下选择 <strong>integration request</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/integration%20request1.png" class="ful-image" alt="integration%20request1.png">
<p>修改 <strong>Body Mapping Templates</strong>，新建 mapping template，添加下面的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;min&quot;: $input.params(&apos;min&apos;),</div><div class="line">    &quot;max&quot;: $input.params(&apos;max&apos;)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后 <strong>Deploy api</strong>，浏览器测试下</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/rangetest.png" class="ful-image" alt="rangetest.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20API%20Gateway%20%2B%20Lambda%20%E6%95%99%E7%A8%8B%20-%20%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0/rangetest2.png" class="ful-image" alt="rangetest2.png">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> API Gateway </tag>
            
            <tag> Lambda </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络 CNN 笔记- 目标分类]]></title>
      <url>http://www.shuang0420.com/2017/06/16/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>目标分类的基本框架 + 迁移学习 + 如何设计神经网络 + 实例:基于 VGG 进行人脸表情识别。深度学习的学习笔记。<br><a id="more"></a></p>
<h1 id="目标分类基本框架"><a href="#目标分类基本框架" class="headerlink" title="目标分类基本框架"></a>目标分类基本框架</h1><p>目标分类的<strong>应用场景</strong>有人脸识别、物体识别、场景识别、文字识别等，先看一下目标分类的基本框架。</p>
<ol>
<li>数据准备<br>数据足够？不够怎么增加数据量</li>
<li>模型设计<br>用现有模型？直接用复杂模型？<br>数据少的时候，设计简单网络进行简单学习，还是大网络进行特殊任务学习</li>
<li>训练细节<br>神经网络配件，参数等</li>
</ol>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>数据来源:</strong> 现有数据集的子集; 网络采集; 现有数据人工标注<br>现有数据: <a href="http://deeplearning.net/datasets/" target="_blank" rel="external">http://deeplearning.net/datasets/</a> ，包含各种数据集如<em>自然图片/人工合成图片/人脸/文本/对话…</em></p>
<p><strong>数据扩充:</strong> 原始数据切割; 噪声颜色等像素变化; 旋转平移等姿态变化<br>如下图，一张图片经过了 5x 的像素级变化，包括平均/锐化(unsharp)/动作模糊(motion)等，每种又经过了 6x 的旋转平移，也就是说，原始数据扩充了 30x<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%85%85.png" class="ful-image" alt="%E6%95%B0%E6%8D%AE%E6%89%A9%E5%85%85.png"></p>
<p>旋转平移 R, T 的矩阵变换<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%97%8B%E8%BD%AC%E5%B9%B3%E7%A7%BB.png" class="ful-image" alt="%E6%97%8B%E8%BD%AC%E5%B9%B3%E7%A7%BB.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%97%8B%E8%BD%AC%E5%B9%B3%E7%A7%BBmatlab.png" class="ful-image" alt="%E6%97%8B%E8%BD%AC%E5%B9%B3%E7%A7%BBmatlab.png"></p>
<p><strong>数据规范:</strong> 均值处理;归一化;大小调整<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># 均值处理</div><div class="line">tr_data = tr_data - MEAN_IMAGE</div><div class="line"></div><div class="line"># 归一化</div><div class="line">trdata/=256</div><div class="line"></div><div class="line"># 大小调整</div><div class="line">for t in range(3):</div><div class="line">	im224[t,:,:] = cv2.resize(imi[:,:,t], (224,224))</div><div class="line">datablob[i,:,:,:] = im224</div></pre></td></tr></table></figure></p>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><h3 id="任务类型"><a href="#任务类型" class="headerlink" title="任务类型"></a>任务类型</h3><p><strong>分类：</strong>表情分类，属于什么种类，人群分类<br><strong>分类+回归：</strong>表情+程度，种类+信心，什么人+人数<br><strong>多目标分类：</strong>面部行为，群体行为，车流预测</p>
<h3 id="模型选取"><a href="#模型选取" class="headerlink" title="模型选取"></a>模型选取</h3><p>看<strong>现有模型(the-state-of-the-art)</strong>能否借鉴</p>
<ul>
<li>偏图像处理，CV：ICCV, ECCV, CVPR</li>
<li>偏理论，机器学习相关：ICML NIPS</li>
<li>偏语言处理，信息挖掘：ACL, KDD</li>
</ul>
<p>如果能借鉴，是否要做局部更改，从哪里改变；<br>如果不能借鉴，就需要从头设计，那么新结构特点是什么，为什么可行</p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>要考虑的问题有</p>
<ul>
<li><strong>GPU-Batch size，是否并行</strong><br>注意 GPU内存-Batch Size 的关系，batch size 设置太小，速度慢，batch 更新效果没那么好，如果设置过大，程序会崩掉</li>
<li><strong>数据循环方式／平衡性考虑</strong><ol>
<li>数量较少的类别，数据是否需要补偿</li>
<li>从头到尾多次循环(不利于类别不平衡的情况)</li>
<li>每次随机选取部分数据(更容易处理平衡性)</li>
</ol>
</li>
<li><strong>网络深度宽度确定</strong><br>直接答案当然是深度优先，主要原因是层数变多，能用更少的参数更有效的学习特征<br>如 5x5 一层卷积核相当于两层 3x3 卷积核，然而两层 3x3 只要 18 个参数，而一层 5x5 要 25 个参数</li>
<li><strong>损失函数设计</strong><br>比如分类用 SOFTMAX，还是直接拟合</li>
<li><strong>学习率变化方式</strong><br>模型各层学习率是否一致</li>
<li><strong>评价方式</strong><br>准确率，F1 score<br>比如在 0/1 分类中，评价方式的设计可能会偏向正例，因为很多情况下 0 会 overweight 1，假设分类结果全是 0，precision=90%，看起来很高，然而什么都没学到，所以要用 F1 score。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">F1 score： 2*(Recall*Precision)/(Recall+Precison)</div><div class="line">Recall: 正确的1识别／真值所有1个数</div><div class="line">Precision:正确的1的识别／所有认为是1的个数</div></pre></td></tr></table></figure>
</li>
</ul>
<p>更多见 <a href="http://www.shuang0420.com/2017/01/20/卷积神经网络%20CNN%20笔记/">卷积神经网络 CNN 笔记</a> 功能层部分。</p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p><strong>问题：</strong>ImageNet 上亿参数，数据量百万，是不是参数多的模型都需要大量数据？<br>当然不是啦，我们可以用别人训练好的模型(<strong>基础模型</strong>)，在训练好的部分参数基础上进行训练。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A01.png" class="ful-image" alt="%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A01.png"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.png" class="ful-image" alt="%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.png">
<p>基础模型的选择往往看是否已有特定任务的模型，另外关于 <strong>学习率(learning rate)</strong> 的处理，最低卷积层的学习率基本不变，中间卷积层看情况(数据是否类似等)，最后全连接，结构和参数都需要变化。</p>
<h1 id="如何设计神经网络"><a href="#如何设计神经网络" class="headerlink" title="如何设计神经网络"></a>如何设计神经网络</h1><p><strong>研究问题：</strong>如何进行面部行为识别(AU detection)<br><img class="ful-image" alt="%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3%EF%BC%8C%E6%AF%94%E5%A6%82%E5%BE%AE%E7%AC%91%EF%BC%8C%E8%87%B3%E5%B0%91%E9%9C%80%E8%A6%81%E5%98%B4%E8%A7%92%E4%B8%8A%E6%89%AC%2B%E7%9C%BC%E8%A7%92%E4%B8%8B%E5%9E%82%E4%B8%A4%E4%B8%AA%E6%9D%A1%E4%BB%B6%E7%BB%84%E6%88%90"></p>
<p>面部行为识别有很多的应用，比如测试疲劳驾驶等，一个很有意思的应用场景是推荐系统，想象看电视的时候有一个前置摄像头观察观众的反应，通过表情识别可以知道观众喜欢哪个节目，然后可以针对性的给更多高质量的推送，再比如应用到教育上面，如果能自动通过疑惑的表情判别出哪一部分学生不理解，可以针对性的给学生多解释几遍做巩固加强，当然这些都涉及到隐私问题，在这里不讨论。</p>
<h2 id="现有模型"><a href="#现有模型" class="headerlink" title="现有模型"></a>现有模型</h2><p>看一下已有方法/模型。<br><strong>Deepface</strong>，<a href="http://www.shuang0420.com/2017/04/25/卷积神经网络%20CNN%20笔记%28高级篇%29/">卷积神经网络 CNN 笔记(高级篇)</a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/deepface.jpg" class="ful-image" alt="deepface.jpg">
<p>传统 CNN 用同一个卷积核对整张图片进行卷积运算，卷积核参数共享，不同局部特性对参数影响相互削弱，达不到最优的效果，对应的解决方法是局部卷积，不同的区域用不同参数。Deepface 对每个 pixel 都用单独一个卷积核来学习，这种<strong>全局部卷积连接</strong>有主要有下面几个缺陷</p>
<ul>
<li>预处理：大量对准，对对准要求高，原始信息可能丢失</li>
<li>卷积参数数量很大，模型收敛难度大，需要大量数据</li>
<li>模型可扩展性差，基本限于人脸计算</li>
</ul>
<p>也有人把特征图分成 8x8=64 小份，一小份一个卷积核，但是这并不能彻底解决上面的问题，我们的改进目标是：</p>
<ul>
<li>不需要预处理，自动进行局部探测</li>
<li>不要所有区域都处理，更多关注在有意义的区域<br>比如额头的信息就比较少，眼睛眉毛嘴巴的信息相对重要的多</li>
<li>重要区域之间不会影响削弱学习效果</li>
</ul>
<h2 id="注意力网络-attention-layer"><a href="#注意力网络-attention-layer" class="headerlink" title="注意力网络-attention layer"></a>注意力网络-attention layer</h2><p>一个想法是<strong>注意力网络-attention layer</strong>，通过权重来聚焦，如下图，我们的目标是看篮子里有什么，所以篮子给大的权重，其他地方不重要，就给小的权重，极端情况就是篮子给 1，其他部分给 0。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/attention%20layer.png" class="ful-image" alt="attention%20layer.png"></p>
<p>再来看一下注意力网络在面部行为识别上的应用<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/attention%20layer%20for%20face.png" class="ful-image" alt="attention%20layer%20for%20face.png"></p>
<p>左图是人脸的肌肉分布，中间的图绿色的点是特征点分布，蓝点是行为单元中心(action unit)，蓝点通过平移变换找到绿点，生成右图的 attention layer。步骤如下：</p>
<ol>
<li>Dlib(或原始数据集)找到人脸关键点</li>
<li>人脸关键点 -&gt; 行为单元中心</li>
<li>由中心生成注意力图<br>中心为 1，往外扩散</li>
</ol>
<p>这样在 CNN 结构下的注意力网络对误差的容忍度其实是很高的，原来 10 个 pixel 的误差经过几层 pooling 可能就到了 1 个甚至零点几个 pixel。</p>
<p>得到注意力网络后，我们需要对原始模型进行修改，一个问题是<strong>添加在哪里？什么方式添加？</strong>一个初始想法自然是放到中间做一个大的滤波，但是这样会完全丢掉不重要的区域，而我们希望保留原始结果，只是多加强下注意力，一个想法是采用 <strong>Residual net</strong> 的思想，将注意力层和之前的特征图层进行融合。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%B7%BB%E5%8A%A0attention%20layer.png" class="ful-image" alt="%E6%B7%BB%E5%8A%A0attention%20layer.png">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C%E6%B7%BB%E5%8A%A0.png" class="ful-image" alt="%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C%E6%B7%BB%E5%8A%A0.png">
<p>为什么添加在 3,4 层而不是 1,2 层呢？因为在 3,4 层表达会更强一些，1,2 层相对太底层。</p>
<p><strong>注意力网络的效果图：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C%E6%95%88%E6%9E%9C.png" class="ful-image" alt="%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C%E6%95%88%E6%9E%9C.png"></p>
<h2 id="局部学习网络"><a href="#局部学习网络" class="headerlink" title="局部学习网络"></a>局部学习网络</h2><p>另一个想法是<strong>局部学习网络：针对不同的区域进行针对性学习，不同的区域的学习不会相互干扰，对区域的分布能够自动适应</strong>。方法也就是切割局部，形成局部神经网络，中间层可以做 upscaling，也就是反向 pooling，之后也可以做下 deconvolution，如下图：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E5%B1%80%E9%83%A8%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C.png" class="ful-image" alt="%E5%B1%80%E9%83%A8%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C.png">
<h2 id="网络合并"><a href="#网络合并" class="headerlink" title="网络合并"></a>网络合并</h2><p><strong>网络合并：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E7%BD%91%E7%BB%9C%E5%90%88%E5%B9%B6.png" class="ful-image" alt="%E7%BD%91%E7%BB%9C%E5%90%88%E5%B9%B6.png"></p>
<p>这种结构的效果还是非常不错的<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/%E6%95%88%E6%9E%9C.png" class="ful-image" alt="%E6%95%88%E6%9E%9C.png"></p>
<p>再总结下上面这种网络结构的作用：</p>
<ul>
<li>无需提前进行面部对准就可以对面部行为识别</li>
<li>脸部各个行为单元局部针对学习，局部信息 可以单独用于某个行为单元识别</li>
<li>根据控制肌肉的分布以及人脸特征点检测结 果确定区域，更具有合理性以及可操作性</li>
</ul>
<p>具体可以看论文<a href="https://arxiv.org/abs/1702.02925" target="_blank" rel="external">EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection</a></p>
<h1 id="实例：基于-VGG-进行人脸表情识别"><a href="#实例：基于-VGG-进行人脸表情识别" class="headerlink" title="实例：基于 VGG 进行人脸表情识别"></a>实例：基于 VGG 进行人脸表情识别</h1><p><strong>数据集：</strong><a href="https://drive.google.com/open?id=0B3ANX1iL124qbmxOc2cyQzhvUFE" target="_blank" rel="external">CIFE:Candid image for facial expression</a><br>在 <a href="https://www.dropbox.com/s/9li9mi4105jf45v/vgg16.tflearn?dl=0" target="_blank" rel="external">vgg16</a> 基础上调整模型。</p>
<p><strong>vgg16:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/vgg16.png" class="ful-image" alt="vgg16.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20-%20%E7%9B%AE%E6%A0%87%E5%88%86%E7%B1%BB/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>这里选择在中高层更新参数(最后一个卷积群+全连接层)，模型代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">def vgg16(input=None, classes=1000):</div><div class="line">    x = tflearn.conv_2d(input, 64, 3, activation=&apos;relu&apos;, scope=&apos;conv1_1&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 64, 3, activation=&apos;relu&apos;, scope=&apos;conv1_2&apos;, trainable=False)</div><div class="line">    x = tflearn.max_pool_2d(x, 2, strides=2, name=&apos;maxpool1&apos;)</div><div class="line"></div><div class="line">    x = tflearn.conv_2d(x, 128, 3, activation=&apos;relu&apos;, scope=&apos;conv2_1&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 128, 3, activation=&apos;relu&apos;, scope=&apos;conv2_2&apos;, trainable=False)</div><div class="line">    x = tflearn.max_pool_2d(x, 2, strides=2, name=&apos;maxpool2&apos;)</div><div class="line"></div><div class="line">    x = tflearn.conv_2d(x, 256, 3, activation=&apos;relu&apos;, scope=&apos;conv3_1&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 256, 3, activation=&apos;relu&apos;, scope=&apos;conv3_2&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 256, 3, activation=&apos;relu&apos;, scope=&apos;conv3_3&apos;, trainable=False)</div><div class="line">    x = tflearn.max_pool_2d(x, 2, strides=2, name=&apos;maxpool3&apos;)</div><div class="line"></div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv4_1&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv4_2&apos;, trainable=False)</div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv4_3&apos;, trainable=False)</div><div class="line">    x = tflearn.max_pool_2d(x, 2, strides=2, name=&apos;maxpool4&apos;)</div><div class="line"></div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv5_1&apos;)</div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv5_2&apos;)</div><div class="line">    x = tflearn.conv_2d(x, 512, 3, activation=&apos;relu&apos;, scope=&apos;conv5_3&apos;)</div><div class="line">    x = tflearn.max_pool_2d(x, 2, strides=2, name=&apos;maxpool5&apos;)</div><div class="line"></div><div class="line">    x = tflearn.fully_connected(x, 4096, activation=&apos;relu&apos;, scope=&apos;fc6&apos;)</div><div class="line">    x = tflearn.dropout(x, 0.5, name=&apos;dropout1&apos;)</div><div class="line"></div><div class="line">    # change the structure, now fc only has 2048, leass parameters, which is enough for this task</div><div class="line">    x = tflearn.fully_connected(x, 2048, activation=&apos;relu&apos;, scope=&apos;fc7&apos;, restore=False)</div><div class="line">    x = tflearn.dropout(x, 0.5, name=&apos;dropout2&apos;)</div><div class="line"></div><div class="line">    x = tflearn.fully_connected(x, classes, activation=&apos;softmax&apos;, scope=&apos;fc8&apos;, restore=False)</div><div class="line"></div><div class="line">    return x</div></pre></td></tr></table></figure></p>
<p>完整代码<a href="https://github.com/Shuang0420/TensorFlow_Study/tree/master/emotion_vgg_finetune" target="_blank" rel="external">emotion_vgg_finetune</a></p>
<p>环境配置，docker 获取镜像 <a href="https://hub.docker.com/r/shuang0420/tensorflow-tflearn-python3-jupyter/" target="_blank" rel="external">shuang0420/tensorflow-tflearn-python3-jupyter</a></p>
<p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ docker run  --name notebooks -d -v /$(pwd):/notebooks -v /$(pwd)/tensorflow/logs:/logs -p 8888:8888 shuang0420/tensorflow-tflearn-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=&apos;&apos;</div><div class="line">$</div><div class="line">$ docker run  --name board -d -v /$(pwd)/tensorflow/logs:/logs -p 6006:6006 shuang0420/tensorflow-tflearn-python3-jupyter tensorboard --logdir /logs</div></pre></td></tr></table></figure></p>
<p><code>/$(pwd):</code> 和 <code>/$(pwd)/tensorflow/logs</code> 是本机目录，它把 container 中的 Jupyter notebooks 以及 logs 匹配到了本机目录，使得 container 和本机可以共享资源。当然首先要保证你的 docker 和 local host 有共享这些目录的权限，在 Docker Preferences 里可以设置。</p>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用 Docker 快速配置深度学习(Tensorflow)环境]]></title>
      <url>http://www.shuang0420.com/2017/06/15/%E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/</url>
      <content type="html"><![CDATA[<p>用 docker 配置 tensorflow 环境(Tensorflow + Python3 + Jupyter Notebook + tflearn)，在 <a href="https://hub.docker.com/r/dash00/tensorflow-python3-jupyter/" target="_blank" rel="external">dash00/tensorflow-python3-jupyter</a> 基础上，添加 tflearn package，创建新的 docker image <a href="https://hub.docker.com/r/shuang0420/tensorflow-tflearn-python3-jupyter/" target="_blank" rel="external">shuang0420/tensorflow-tflearn-python3-jupyter</a></p>
<a id="more"></a>
<h1 id="Use-Other’s-Image"><a href="#Use-Other’s-Image" class="headerlink" title="Use Other’s Image"></a>Use Other’s Image</h1><p>我们在 <a href="https://hub.docker.com/r/dash00/tensorflow-python3-jupyter/" target="_blank" rel="external">dash00/tensorflow-python3-jupyter</a> 基础上创建自己的新镜像。</p>
<h2 id="Download-Image"><a href="#Download-Image" class="headerlink" title="Download Image"></a>Download Image</h2><p>首先获取镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker pull dash00/tensorflow-python3-jupyter</div></pre></td></tr></table></figure>
<p>原镜像 <code>dash00/tensorflow-python3-jupyter</code> 包含了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">- Jupyter Notebook</div><div class="line">- TensorFlow</div><div class="line">- scikit-learn</div><div class="line">- pandas</div><div class="line">- matplotlib</div><div class="line">- numpy</div><div class="line">- scipy</div><div class="line">- Pillow</div><div class="line">- Python 2 and 3</div></pre></td></tr></table></figure>
<h2 id="Start-Container"><a href="#Start-Container" class="headerlink" title="Start Container"></a>Start Container</h2><h3 id="Use-basic-container"><a href="#Use-basic-container" class="headerlink" title="Use basic container"></a>Use basic container</h3><p>如果用下面的启动方式，当结束 container 的时候，jupyter notebook 里的内容也会随之消失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -it -p 8888:8888 dash00/tensorflow-python3-jupyter</div></pre></td></tr></table></figure>
<h3 id="Use-persistent-folder"><a href="#Use-persistent-folder" class="headerlink" title="Use persistent folder"></a>Use persistent folder</h3><p>这种启动方式将 notebook 内容存到了本地，本质上是一个 mapping。<code>/$(pwd)/notebooks</code> 就是本机 notebook 目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -it -p 8888:8888 -v /$(pwd)/notebooks:/notebooks dash00/tensorflow-python3-jupyter</div></pre></td></tr></table></figure>
<h3 id="Use-Jupyter-Notebook-and-Tensorboard-in-the-same-time"><a href="#Use-Jupyter-Notebook-and-Tensorboard-in-the-same-time" class="headerlink" title="Use Jupyter Notebook and Tensorboard in the same time"></a>Use Jupyter Notebook and Tensorboard in the same time</h3><p>同时运行 jupyter notebook 和 tensorboard</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ docker run  --name notebooks -d -v /$(pwd)/notebooks:/notebooks -v /$(pwd)/logs:/logs -p 8888:8888 dash00/tensorflow-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=&apos;&apos;</div><div class="line">$</div><div class="line">$ docker run  --name board -d -v /$(pwd)/logs:/logs -p 6006:6006 dash00/tensorflow-python3-jupyter tensorboard --logdir /logs</div></pre></td></tr></table></figure>
<p>打开浏览器输入 <code>http://&lt;CONTAINER_IP&gt;:8888/</code> 打开 jupyter notebook，输入 <code>http://&lt;CONTAINER_IP&gt;:6006/</code> 打开 tensorboard</p>
<h1 id="Modify-and-Create-New-Image"><a href="#Modify-and-Create-New-Image" class="headerlink" title="Modify and Create New Image"></a>Modify and Create New Image</h1><h2 id="Modify-Old-Image"><a href="#Modify-Old-Image" class="headerlink" title="Modify Old Image"></a>Modify Old Image</h2><p>进入 docker image，注意跟在 root@ 后面的 <code>97748739b45d</code> 就是新的 docker image id。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ docker run -it dash00/tensorflow-python3-jupyter /bin/bash</div><div class="line">root@97748739b45d:/notebooks#</div></pre></td></tr></table></figure>
<p>先看一下是什么系统<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root@97748739b45d:/notebooks# lsb_release -a</div><div class="line">No LSB modules are available.</div><div class="line">Distributor ID:	Ubuntu</div><div class="line">Description:	Ubuntu 16.04.2 LTS</div><div class="line">Release:	16.04</div><div class="line">Codename:	xenial</div></pre></td></tr></table></figure></p>
<p><a href="https://hub.docker.com/r/dash00/tensorflow-python3-jupyter/" target="_blank" rel="external">dash00/tensorflow-python3-jupyter</a> 提到装了 python2 和 python3，tf 是装在 python3 下，所以 tflearn 也要装在 python3 下。发现默认 python 进入的是 python2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># python</div><div class="line">Python 2.7.12 (default, Nov 19 2016, 06:48:10)</div><div class="line">[GCC 5.4.0 20160609] on linux2</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; exit()</div><div class="line"># python3</div><div class="line">Python 3.5.2 (default, Nov 17 2016, 17:05:23)</div><div class="line">[GCC 5.4.0 20160609] on linux</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; exit()</div></pre></td></tr></table></figure></p>
<p>pip install 要在 python3 下，为了使用稳定版本的 tflearn，需要用到 git，尝试下以下命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># python3 -m pip install git+https://github.com/tflearn/tflearn.git</div><div class="line">Collecting git+https://github.com/tflearn/tflearn.git</div><div class="line">  Cloning https://github.com/tflearn/tflearn.git to /tmp/pip-u0c73_t1-build</div><div class="line">  Error [Errno 2] No such file or directory: &apos;git&apos; while executing command git clone -q https://github.com/tflearn/tflearn.git /tmp/pip-u0c73_t1-build</div><div class="line">Cannot find command &apos;git&apos;</div><div class="line">You are using pip version 8.1.1, however version 9.0.1 is available.</div><div class="line">You should consider upgrading via the &apos;pip install --upgrade pip&apos; command.</div></pre></td></tr></table></figure></p>
<p>发现没有装 git，就先装一下喽<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># apt-get update</div><div class="line"># apt-get install git</div></pre></td></tr></table></figure></p>
<p>再次 pip 下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># python3 -m pip install git+https://github.com/tflearn/tflearn.git</div><div class="line"># python3</div><div class="line">Python 3.5.2 (default, Nov 17 2016, 17:05:23)</div><div class="line">[GCC 5.4.0 20160609] on linux</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import tflearn</div><div class="line">hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># python3 -m pip install --upgrade pip</div><div class="line"># python3 -m pip install h5py</div></pre></td></tr></table></figure>
<p>成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># python3</div><div class="line">Python 3.5.2 (default, Nov 17 2016, 17:05:23)</div><div class="line">[GCC 5.4.0 20160609] on linux</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import tflearn</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure></p>
<h2 id="Commit-test-and-upload"><a href="#Commit-test-and-upload" class="headerlink" title="Commit, test, and upload"></a>Commit, test, and upload</h2><p>然后退出当前容器，通过命令 <code>docker commit</code> 来提交容器副本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># exit</div><div class="line">$ docker commit -m=&quot;install git and tflearn&quot; -a=&quot;shuang0420&quot; 97748739b45d shuang0420/tensorflow-tflearn-python3-jupyter:latest</div><div class="line">sha256:97748739b45dc8ce994521fa11d7ad6349bc83762e76139086789e0416560710</div></pre></td></tr></table></figure>
<p>各个参数说明：</p>
<ul>
<li><strong>-m:</strong>提交的描述信息</li>
<li><strong>-a:</strong>指定镜像作者</li>
<li><strong>e218edb10161：</strong>容器ID</li>
<li><strong>runoob/ubuntu:v2:</strong>指定要创建的目标镜像名</li>
</ul>
<p>使用 <code>docker images</code> 命令来查看我们的新镜像 <code>shuang0420/tensorflow-tflearn-python3-jupyter</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ docker images</div><div class="line">REPOSITORY                                  TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">shuang0420/tensorflow-tflearn-python3-jupyter   latest              97748739b45d        20 hours ago        1.28 GB</div><div class="line">dash00/tensorflow-python3-jupyter           latest              34eeac184315        4 weeks ago         1.17 GB</div><div class="line">hello-world                                 latest              48b5124b2768        5 months ago        1.84 kB</div></pre></td></tr></table></figure>
<p>现在的镜像包含了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">- git</div><div class="line">- Jupyter Notebook</div><div class="line">- TensorFlow</div><div class="line">- tflearn</div><div class="line">- scikit-learn</div><div class="line">- pandas</div><div class="line">- matplotlib</div><div class="line">- numpy</div><div class="line">- scipy</div><div class="line">- Pillow</div><div class="line">- Python 2 and 3</div></pre></td></tr></table></figure>
<p>然后使用新镜像 <code>shuang0420/tensorflow-tflearn-python3-jupyter</code> 来启动一个容器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run  --name notebooks -d -v /$(pwd)/notebooks:/notebooks -v /$(pwd)/logs:/logs -p 8888:8888 shuang0420/tensorflow-tflearn-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=&apos;&apos;</div></pre></td></tr></table></figure>
<p>如果出现下面的错误，说明之前已经启动了一个名为 notebooks 的 container，我们可以直接启动该容器，或者退出并删除原容器，新建一个。通过 <code>docker ps -a</code> 命令查看 container id 并删除该 container，再重新运行命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">docker: Error response from daemon: Conflict. The container name &quot;/notebooks&quot; is already in use by container 4602dc6d7f0b8b7756fa31d63a0ecb19bd37147c2af80710294a480587f9eb08. You have to remove (or rename) that container to be able to reuse that name..</div><div class="line">See &apos;docker run --help&apos;.</div><div class="line"></div><div class="line">$ docker ps -a</div><div class="line">CONTAINER ID        IMAGE                                              COMMAND                  CREATED             STATUS                           PORTS               NAMES</div><div class="line">&#123;% imgurl %E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/kitematic.png ful-image alt:kitematic.png %&#125;</div><div class="line">$</div><div class="line">$ docker rm 4602dc6d7f0b</div><div class="line">4602dc6d7f0b</div><div class="line">$ docker run  --name notebooks -d -v /$(pwd)/notebooks:/notebooks -v /$(pwd)/logs:/logs -p 8888:8888 shuang0420/tensorflow-tflearn-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=&apos;&apos;</div><div class="line">$ docker run  --name board -d -v /$(pwd)/logs:/logs -p 6006:6006 shuang0420/tensorflow-tflearn-python3-jupyter tensorboard --logdir /logs</div><div class="line">$</div></pre></td></tr></table></figure>
<p><strong>浏览器输入 <code>localhost:8888</code> 打开 jupyter notebook</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/tflearn.png" class="ful-image" alt="tflearn.png">
<p><strong>浏览器输入 <code>localhost:6006</code> 打开 jupyter notebook</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/tensorboard.png" class="ful-image" alt="tensorboard.png">
<p>当然也可以通过 <code>kitematic</code> 来直接控制 container 啦～～</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/kitematic.png" class="ful-image" alt="kitematic.png">
<p>用 <code>push</code> 命令将 image 上传到 docker hub<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker push shuang0420/tensorflow-tflearn-python3-jupyter:latest</div></pre></td></tr></table></figure></p>
<p>已上传至 docker hub，见 <a href="https://hub.docker.com/r/shuang0420/tensorflow-tflearn-python3-jupyter/" target="_blank" rel="external">shuang0420/tensorflow-tflearn-python3-jupyter</a></p>
<h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><h2 id="Run-jupyter-and-tensorboard"><a href="#Run-jupyter-and-tensorboard" class="headerlink" title="Run jupyter and tensorboard"></a>Run jupyter and tensorboard</h2><p><a href="https://hub.docker.com/r/shuang0420/tensorflow-tflearn-python3-jupyter/" target="_blank" rel="external">shuang0420/tensorflow-tflearn-python3-jupyter</a> 的使用方法，基本用法和 <a href="https://hub.docker.com/r/dash00/tensorflow-python3-jupyter/" target="_blank" rel="external">dash00/tensorflow-python3-jupyter</a> 相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ docker run  --name notebooks -d -v /$(pwd):/notebooks -v /$(pwd)/tensorflow/logs:/logs -p 8888:8888 shuang0420/tensorflow-tflearn-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=&apos;&apos;</div><div class="line">$</div><div class="line">$ docker run  --name board -d -v /$(pwd)/tensorflow/logs:/logs -p 6006:6006 shuang0420/tensorflow-tflearn-python3-jupyter tensorboard --logdir /logs</div></pre></td></tr></table></figure>
<p><code>/$(pwd):</code> 和 <code>/$(pwd)/tensorflow/logs</code> 是本机目录，它把 container 中的 Jupyter notebooks 以及 logs 匹配到了本机目录，使得 container 和本机可以共享资源。当然首先要保证你的 docker 和 local host 有共享这些目录的权限，在 Docker Preferences 里可以设置。</p>
<h2 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h2><p>用 <code>docker stats</code> 来查看 container 的资源使用状况。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O           PIDS</div><div class="line">cb7ef0a4afc2        0.02%               8.438 MiB / 1.952 GiB   0.42%               219 kB / 1.2 MB     207 MB / 38.3 MB    2</div><div class="line">0e6a9a715cbd        0.00%               19.51 MiB / 1.952 GiB   0.98%               189 kB / 285 kB     1.24 GB / 2.23 GB   16</div></pre></td></tr></table></figure></p>
<p>或者进入 docker 用 <code>top</code> 查看。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Last login: Mon Jun 19 16:11:32 on ttys000</div><div class="line">top - 13:38:02 up 1 day, 19:58,  0 users,  load average: 0.09, 0.17, 0.11</div><div class="line">Tasks:   6 total,   1 running,   5 sleeping,   0 stopped,   0 zombie</div><div class="line">%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</div><div class="line">KiB Mem :  2046768 total,  1923504 free,    66636 used,    56628 buff/cache</div><div class="line">KiB Swap:  1048572 total,   683580 free,   364992 used.  1861204 avail Mem</div><div class="line"></div><div class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</div><div class="line">    1 root      20   0   18036      0      0 S   0.0  0.0   0:00.04 bash</div><div class="line">    7 root      20   0  300448  12820   5268 S   0.0  0.6   0:02.41 jupyter-noteboo</div><div class="line">   31 root      20   0   18248   1828   1648 S   0.0  0.1   0:00.07 bash</div><div class="line">  130 root      20   0  591052    688    688 S   0.0  0.0   0:00.78 python3</div><div class="line">  148 root      20   0   18244     12     12 S   0.0  0.0   0:00.02 bash</div><div class="line">  170 root      20   0   36644   1252   1032 R   0.0  0.1   0:00.46 top</div></pre></td></tr></table></figure></p>
<h2 id="Memory-and-CPU"><a href="#Memory-and-CPU" class="headerlink" title="Memory and CPU"></a>Memory and CPU</h2><p>Mac OS 默认给 docker 分配 4 个 CPU 和 2 GB 的内存，因此不管怎么用 <code>docker update</code> 和 <code>docker run</code> 命令来调整 container 的 CPU 和 memory，始终不能超过 docker 的限制，想要用更多的 cpu 和 memory 资源，只用在 Docker Preferences -&gt; Advanced 中调整即可。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BD%BF%E7%94%A8%20Docker%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Tensorflow%E7%8E%AF%E5%A2%83/docker%20preference.png" class="ful-image" alt="docker%20preference.png"></p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Tensorflow </tag>
            
            <tag> docker </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS Lex 创建 Slack Bot - Integrating Lex Bot with Slack]]></title>
      <url>http://www.shuang0420.com/2017/06/09/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/</url>
      <content type="html"><![CDATA[<p>非常简单的教程，新建一个 Slack Bot 支持购买房/车的功能。主要流程: <strong>编写 Lambda Function =&gt; 调用 Lambda function 创建 Lex bot =&gt; 创建 Slack Application 并与 Lex bot 关联 =&gt; 测试并发布</strong>。<br><a id="more"></a></p>
<h1 id="Create-Lambda-Function"><a href="#Create-Lambda-Function" class="headerlink" title="Create Lambda Function"></a>Create Lambda Function</h1><p>Lambda Function 部分的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">exports.handler = (event, context, callback) =&gt; &#123;</div><div class="line">    var purchase = event.currentIntent.slots.purchase,</div><div class="line">    price = &quot;free&quot;;</div><div class="line"></div><div class="line">    if (purchase === &quot;home&quot;) &#123;</div><div class="line">        price = &quot;200000 dollars&quot;;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    callback(null, &#123;</div><div class="line">        &quot;dialogAction&quot;: &#123;</div><div class="line">            &quot;type&quot;: &quot;Close&quot;,</div><div class="line">            &quot;fulfillmentState&quot;: &quot;Fulfilled&quot;,</div><div class="line">            &quot;message&quot;: &#123;</div><div class="line">                &quot;contentType&quot;: &quot;PlainText&quot;,</div><div class="line">                &quot;content&quot;: &quot;You have purchased a &quot; + event.currentIntent.slots.purchase + &quot; for &quot; + price</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>有一个 purchase 的 slot，这段代码返回的是，如果买房，价格 200000 刀，如果买其他东西(slot value 限制下)，都免费。</p>
<p>Lambda 的配置不多说了，和 <a href="http://www.shuang0420.com/2017/06/05/Alexa%20开发新技能%20-%20Lambda/">Alexa 开发新技能 - Lambda</a>的配置相同。</p>
<p>供测试的 <strong>input event example</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;currentIntent&quot;: &#123;</div><div class="line">    &quot;name&quot;: &quot;PurchaseIntent&quot;,</div><div class="line">    &quot;slots&quot;: &#123;</div><div class="line">      &quot;purchase&quot;: &quot;home&quot;</div><div class="line">    &#125;,</div><div class="line">    &quot;confirmationStatus&quot;: &quot;Confirmed&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;bot&quot;: &#123;</div><div class="line">    &quot;name&quot;: &quot;bot-name&quot;,</div><div class="line">    &quot;alias&quot;: &quot;bot-alias&quot;,</div><div class="line">    &quot;version&quot;: &quot;bot-version&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;userId&quot;: &quot;User ID specified in the POST request to Amazon Lex.&quot;,</div><div class="line">  &quot;inputTranscript&quot;: &quot;Text used to process the request&quot;,</div><div class="line">  &quot;invocationSource&quot;: &quot;FulfillmentCodeHook or DialogCodeHook&quot;,</div><div class="line">  &quot;outputDialogMode&quot;: &quot;Text or Voice, based on ContentType request header in runtime API request&quot;,</div><div class="line">  &quot;messageVersion&quot;: &quot;1.0&quot;,</div><div class="line">  &quot;sessionAttributes&quot;: &#123;</div><div class="line">     &quot;key1&quot;: &quot;value1&quot;,</div><div class="line">     &quot;key2&quot;: &quot;value2&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>Expect Result:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/expectRes.png" class="ful-image" alt="expectRes.png"></p>
<h1 id="Create-Amazon-Lex-Bot"><a href="#Create-Amazon-Lex-Bot" class="headerlink" title="Create Amazon Lex Bot"></a>Create Amazon Lex Bot</h1><p>登录 <a href="https://console.aws.amazon.com/lex/" target="_blank" rel="external">AWS Lex</a>，新建一个 bot(get started =&gt; custom bot)，过程非常简单，设置基本信息包括 bot name, output voice, session timeout 等，然后创建 slot type，设置 intent，基本流程和 <a href="http://www.shuang0420.com/2017/06/05/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/">Alexa add new skill</a> (尤其是 Skill Builder Beta 界面)差不多，直接上截图了。</p>
<p><strong>Create Lex Bot:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/lex1.png" class="ful-image" alt="lex1.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/lex2.png" class="ful-image" alt="lex2.png"></p>
<p><strong>Add slot type:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/AddSlot.png" class="ful-image" alt="AddSlot.png"><br>保存的 slot type 在之后所有的 bot 设置中都可以重复使用。</p>
<p><strong>Add intent:</strong><br>提供一些 <strong>Sample utterances</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">I would like to purchase a &#123;purchase&#125;​</div><div class="line">Buy me a &#123;purchase&#125;​</div><div class="line">I&apos;m going to buy a &#123;purchase&#125;​</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/lex3.png" class="ful-image" alt="lex3.png">
<p><strong>Call lambda function:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/lex4.png" class="ful-image" alt="lex4.png"></p>
<p>如果用户输入了除 <strong>slot value (这里是 home, car)</strong>以外的东西，就会触发 <strong>Prompt</strong>。</p>
<p>设置好之后选择 save intent，然后选右上角的 build，创建完成后就可以开始测试 bot，这里的测试结果有点不如人意，prompt 并没有起到作用，然而没关系，最后在 Slack 界面是 work 的<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/lex5.png" class="ful-image" alt="lex5.png"></p>
<h1 id="Create-Slack-Application"><a href="#Create-Slack-Application" class="headerlink" title="Create Slack Application"></a>Create Slack Application</h1><p>如果没有 Slack 账号，需要先注册<a href="https://get.slack.help/hc/en-us/articles/212675257-Creating-a-Slack-account" target="_blank" rel="external">注册</a><br>登录<a href="http://api.slack.com/" target="_blank" rel="external">Slack API</a>，选择 start building =&gt; create an app，然后进行相关设置:</p>
<ol>
<li>In the left menu, choose <strong>Bot Users</strong>.<ul>
<li>Provide a user name.</li>
<li>For <strong>Always Show My Bot as Online</strong>, choose <strong>On</strong>.<br>Save the changes.</li>
</ul>
</li>
<li>Choose <strong>Interactive Messages</strong> from the left menu.<ul>
<li>Choose <strong>Enable Interactive Messages</strong>.</li>
<li>Specify any valid URL in the <strong>Request URL</strong> box. For example, you can use <a href="https://slack.com" target="_blank" rel="external">https://slack.com</a>.<br>Note<br>For now, enter any valid URL so that you get the verification token that you need in the next step. You will update this URL after you add the bot channel association in the Amazon Lex console.</li>
<li>Choose <strong>Enable Interactive Messages</strong>.</li>
</ul>
</li>
<li>In the <strong>Settings</strong> section in the left menu, choose <strong>Basic Information</strong>. Record the following application credentials:<ul>
<li>Client ID</li>
<li>Client Secret</li>
<li>Verification Token</li>
</ul>
</li>
</ol>
<h1 id="Integrate-the-Slack-Application-with-the-Amazon-Lex-Bot"><a href="#Integrate-the-Slack-Application-with-the-Amazon-Lex-Bot" class="headerlink" title="Integrate the Slack Application with the Amazon Lex Bot"></a>Integrate the Slack Application with the Amazon Lex Bot</h1><p>找到之前创建的 <a href="https://console.aws.amazon.com/lex/" target="_blank" rel="external">lex bot</a>，到 <strong>Channels</strong> 标签页，在左边菜单栏选择 <strong>Slack</strong>，并提供下面的信息：</p>
<ul>
<li>Type a name. For example, <code>BotSlackIntegration</code>.</li>
<li>Choose “aws/lex” from the <strong>KMS key</strong> drop-down.</li>
<li>For <strong>Alias</strong>, choose the bot alias.</li>
<li>Type the <strong>Client Id</strong>, <strong>Client secret</strong>, and <strong>Verification Token</strong>, which you recorded in the preceding step. These are the credentials of the Slack application.</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/Channels.png" class="ful-image" alt="Channels.png">
<p>关于 Aliases 的问题，可以在 <strong>Settings</strong> 的 <strong>Aliases</strong> 部分进行设置，发现不设置 Alias 的话 Activate Bot 的时候可能会出错，所以还是设置下吧~<br>设置好后  <strong>Activate</strong>，记录下 <strong>Postback URL</strong> 和 <strong>OAuth URL</strong>，然后回到 <a href="http://api.slack.com/" target="_blank" rel="external">Slack Application</a> 页面，做如下设置：</p>
<ol>
<li>Update the <strong>OAuth &amp; Permissions</strong> feature as follows:<ol>
<li>In the <strong>Redirect URLs</strong> section, add the OAuth URL that Amazon Lex provided in the preceding step. Choose <strong>Add a new Redirect URL</strong>, and then choose <strong>Save URLs</strong>.</li>
<li>In the <strong>Permission Scopes</strong> section, choose two permissions in the <strong>Select Permission Scopes</strong> drop down. Filter the list with the following text:<ul>
<li><strong>chat:write:bot</strong></li>
<li><strong>team:read</strong><br>Choose <strong>Save Changes</strong>.</li>
</ul>
</li>
</ol>
</li>
<li>Update the <strong>Interactive Messages</strong> feature by updating the <strong>Request URL</strong> value to the Postback URL that Amazon Lex provided in the preceding step. Choose <strong>Add</strong>, and then choose <strong>Save URLs</strong>.</li>
<li>Subscribe to the <strong>Event Subscriptions</strong> feature as follows:<ul>
<li>Enable events by choosing the <strong>On</strong> option.</li>
<li>Set the <strong>Request URL</strong> value to the Postback URL that Amazon Lex provided in the preceding step.</li>
<li>Subscribe to the <code>message.im</code> bot event to enable direct messaging between the end user and the Slack bot.</li>
<li>Save the changes.</li>
</ul>
</li>
</ol>
<p>这里没什么 tricky 的地方，要注意的是每一步都要记得 <strong>Save</strong></p>
<h1 id="Test-the-Integration"><a href="#Test-the-Integration" class="headerlink" title="Test the Integration"></a>Test the Integration</h1><ol>
<li>Choose <strong>Manage Distribution</strong> under <strong>Settings</strong>. Choose <strong>Add to Slack</strong> to install the application. Authorize the bot to respond to messsages.</li>
<li>You are redirected to your Slack team. Choose your bot from the <strong>Direct Messages</strong> section in the left menu. If you don’t see your bot, choose the plus icon (+) next to <strong>Direct Messages</strong> to search for your bot.</li>
<li>Engage in a chat with your Slack application, which is linked to the Amazon Lex bot. Your bot now responds to messages.<img src="http://ox5l2b8f4.bkt.clouddn.com/images/AWS%20Lex%20%E5%88%9B%E5%BB%BA%20Slack%20Bot%20-%20Integrating%20Lex%20Bot%20with%20Slack/Slack%20test.png" class="ful-image" alt="Slack%20test.png">
</li>
</ol>
<blockquote>
<p>参考链接：<br><a href="http://docs.aws.amazon.com/lex/latest/dg/slack-bot-assoc-create-bot.html" target="_blank" rel="external">Integrating with Slack</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> chatbot </tag>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> Slack </tag>
            
            <tag> 物联网 </tag>
            
            <tag> Lex </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Alexa 开发新技能 - Lambda]]></title>
      <url>http://www.shuang0420.com/2017/06/05/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/</url>
      <content type="html"><![CDATA[<p>非常简单的教程，讲怎么给 Alexa 添加新的 skill，让你的 Echo 更个性化。本篇添加的 skill 是让 Alexa 从 reddit 上读前 10 条热点。<br><a id="more"></a></p>
<p>代码戳<a href="https://github.com/Shuang0420/Alexa-Starter-RedditReader/tree/master/reddit_reader_lambda" target="_blank" rel="external">Alexa-Starter-RedditReader</a>，其他版本如用 python flask 实现，见<a href="http://www.shuang0420.com/2017/05/02/Alexa%20开发新技能/">Alexa 开发新技能 - python flask</a>。这一篇截图比较细，一方面是因为 AWS 版本迭代太快，网上之前的教程可能会过时，另一方面实在是因为一步步做下来踩了很多坑，争取这篇教程可以让大家少走一些弯路。</p>
<p>这一篇会用到 AWS Lambda，Lambda 的优势官方说明描述的很清楚，简单来说最显著的优点就是，与 <a href="http://www.shuang0420.com/2017/05/02/Alexa%20开发新技能/">Alexa 开发新技能 - python flask</a> 相比，我们不再需要后端运行代码并通过 ngrok 等工具将代码部署到公开网络。</p>
<blockquote>
<p>通过 AWS Lambda，无需配置或管理服务器即可运行代码。您只需按消耗的计算时间付费 – 代码未运行时不产生费用。借助 Lambda，您几乎可以为任何类型的应用程序或后端服务运行代码，而且全部无需管理。只需上传您的代码，Lambda 会处理运行和扩展高可用性代码所需的一切工作。您可以将您的代码设置为自动从其他 AWS 服务触发，或者直接从任何 Web 或移动应用程序调用。</p>
</blockquote>
<p>总结下来 AWS Lambda 有以下几个特点:</p>
<ul>
<li>run code in response to events</li>
<li>no maintenance of server, no worry about infrastructure</li>
<li>scale automatically</li>
<li>never pay for idle</li>
</ul>
<p>下面介绍怎么来用 Nodejs 和 Lambda 实现 Reddit Reader。</p>
<h1 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h1><ul>
<li>Nodejs 4.3</li>
<li><a href="https://developer.amazon.com/edw/home.html" target="_blank" rel="external">Amazon Developer Account</a></li>
</ul>
<h1 id="Code-for-new-skill-Lambda-Function"><a href="#Code-for-new-skill-Lambda-Function" class="headerlink" title="Code for new skill (Lambda Function)"></a>Code for new skill (Lambda Function)</h1><p>这里我们使用 Lambda function，get_headlines() 是我们主要的 method，从 Reddit 里返回 10 条热点。逻辑是这样的：</p>
<ul>
<li>(用户呼唤 Reddit Reader)</li>
<li>Alexa 问用户 ‘Hello there, would you like the news?’</li>
<li>用户回答<br>肯定回复: 读 10 headlines<br>否定回复: 回答 ‘I am not sure why you asked me to run then, but okay… bye’</li>
</ul>
<p>首先建好 project 框架<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ mkdir alexaproject</div><div class="line">$ cd alexaproject/</div><div class="line">$ npm init</div><div class="line">$ mkdir src</div><div class="line">$ touch src/index.js</div></pre></td></tr></table></figure></p>
<p>设置相关依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ npm install alexa-sdk --save</div><div class="line">$ npm install request --save</div></pre></td></tr></table></figure></p>
<p>版本：</p>
<ul>
<li>“alexa-sdk”: “^1.0.9”,</li>
<li>“request”: “^2.81.0”</li>
</ul>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;;</div><div class="line"></div><div class="line">var Alexa = require(&quot;alexa-sdk&quot;);</div><div class="line">var request = require(&apos;request&apos;);</div><div class="line"></div><div class="line">exports.handler = function(event, context, callback) &#123;</div><div class="line">  var alexa = Alexa.handler(event, context, callback);</div><div class="line">  alexa.appId = &quot;amzn1.ask.skill.[YOUR_APP_ID]&quot;;</div><div class="line">  alexa.registerHandlers(handlers);</div><div class="line">  alexa.execute();</div><div class="line">&#125;</div><div class="line"></div><div class="line">var handlers = &#123;</div><div class="line">  &quot;LaunchRequest&quot;: function() &#123;</div><div class="line">    var speechOutput = &quot;Hello there, would you like the news?&quot;;</div><div class="line">    var reprompt = speechOutput;</div><div class="line">    this.emit(&apos;:ask&apos;, speechOutput, reprompt);</div><div class="line">  &#125;,</div><div class="line">  &quot;YesIntent&quot;: function() &#123;</div><div class="line">    var self = this;</div><div class="line">    get_headlines(function(headlines) &#123;</div><div class="line">      var speechOutput = &apos;The current world news headlines are &apos; + headlines;</div><div class="line">      self.emit(&apos;:tell&apos;, speechOutput);</div><div class="line">    &#125;);</div><div class="line">  &#125;,</div><div class="line">  &quot;NoIntent&quot;: function() &#123;</div><div class="line">    var speechOutput = &apos;I am not sure why you asked me to run then, but okay... bye&apos;</div><div class="line">    this.emit(&apos;:tell&apos;, speechOutput);</div><div class="line">  &#125;,</div><div class="line">  &quot;AMAZON.StopIntent&quot;: function() &#123;</div><div class="line">    var speechOutput = &quot;Good bye! Thank you for using Reddit Reader&quot;;</div><div class="line">    this.emit(&apos;:tell&apos;, speechOutput);</div><div class="line">  &#125;,</div><div class="line">  &quot;AMAZON.CancelIntent&quot;: function() &#123;</div><div class="line">    var speechOutput = &quot;Good bye! Thank you for using Reddit Reader&quot;;</div><div class="line">    this.emit(&apos;:tell&apos;, speechOutput);</div><div class="line">  &#125;,</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function get_headlines(callback) &#123;</div><div class="line">  request(&apos;https://reddit.com/r/worldnews/.json?limit=10&apos;, function (error, response, body) &#123;</div><div class="line">    if (!error &amp;&amp; response.statusCode == 200) &#123;</div><div class="line">      var body=JSON.parse(body)[&apos;data&apos;][&apos;children&apos;];</div><div class="line">      var res = &quot;&quot;;</div><div class="line">      body.forEach(function(ele)&#123;</div><div class="line">        res = res + ele[&apos;data&apos;][&apos;title&apos;] + &quot; &quot;;</div><div class="line">      &#125;);</div><div class="line">    &#125;</div><div class="line">    return callback(res);</div><div class="line">  &#125;);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果不用 alexa-sdk，也可以自己搭一个框架出来，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div></pre></td><td class="code"><pre><div class="line">&apos;use strict&apos;;</div><div class="line"></div><div class="line">var request = require(&apos;request&apos;);</div><div class="line">exports.handler = function(event, context) &#123;</div><div class="line">  try &#123;</div><div class="line">    console.log(&quot;event.session.application.applicationId&quot;) + event.session.application.applicationId;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * Uncomment this if statement and populate with your skill&apos;s application ID to</div><div class="line">     * prevent someone else from configuring a skill that sends requests to this function.</div><div class="line">     */</div><div class="line"></div><div class="line">    if (event.session.application.applicationId !== &quot;[YOUR_APP_ID]&quot;) &#123;</div><div class="line">         context.fail(&quot;Invalid Application ID&quot;);</div><div class="line">     &#125;</div><div class="line"></div><div class="line"></div><div class="line">    if (event.session.new) &#123;</div><div class="line">      onSessionStarted(&#123;requestId: event.request.requestId&#125;, event.session);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    if (event.request.type === &quot;LaunchRequest&quot;) &#123;</div><div class="line">      onLaunch(event.request,</div><div class="line">          event.session,</div><div class="line">          function callback(sessionAttributes, speechletResponse) &#123;</div><div class="line">            context.succeed(buildResponse(sessionAttributes, speechletResponse));</div><div class="line">          &#125;);</div><div class="line">    &#125; else if (event.request.type === &quot;IntentRequest&quot;) &#123;</div><div class="line">      onIntent(event.request,</div><div class="line">          event.session,</div><div class="line">          function callback(sessionAttributes, speechletResponse) &#123;</div><div class="line">            context.succeed(buildResponse(sessionAttributes, speechletResponse));</div><div class="line">          &#125;);</div><div class="line">    &#125; else if (event.request.type === &quot;SessionEndedRequest&quot;) &#123;</div><div class="line">      onSessionEnded(event.request, event.session);</div><div class="line">      context.succeed();</div><div class="line">    &#125;</div><div class="line">  &#125; catch (e) &#123;</div><div class="line">    context.fail(&quot;Exception: &quot; + e);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">/**</div><div class="line"> * called when the user invokes the skill without specifying what they want.</div><div class="line"> */</div><div class="line">function onLaunch(launchRequest, session, callback) &#123;</div><div class="line">  getWelcomeResponse(callback)</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">/**</div><div class="line"> * Called when the user specifies an intent for this skill.</div><div class="line"> */</div><div class="line">function onIntent(intentRequest, session, callback) &#123;</div><div class="line">  var intent = intentRequest.intent</div><div class="line">  var intentName = intentRequest.intent.name;</div><div class="line"></div><div class="line">  // dispatch custom intents to handlers here</div><div class="line">  if (intentName == &quot;&quot;) &#123;</div><div class="line">    handleResponse(intent, session, callback)</div><div class="line">  &#125; else if (intentName == &quot;YesIntent&quot;) &#123;</div><div class="line">    handleYesResponse(intent, session, callback)</div><div class="line">  &#125; else if (intentName == &quot;NoIntent&quot;) &#123;</div><div class="line">    handleNoResponse(intent, session, callback)</div><div class="line">  &#125; else if (intentName == &quot;AMAZON.StopIntent&quot;) &#123;</div><div class="line">    // not used here</div><div class="line">    // handleGetHelpRequest(intent, session, callback)</div><div class="line">  &#125; else if (intentName == &quot;AMAZON.CancelIntent&quot;) &#123;</div><div class="line">    handleFinishSessionRequest(intent, session, callback)</div><div class="line">  &#125; else if (intentName == &quot;AMAZON.HelpIntent&quot;) &#123;</div><div class="line">    handleFinishSessionRequest(intent, session, callback)</div><div class="line">  &#125; else &#123;</div><div class="line">    throw &quot;Invalid intent&quot;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">/**</div><div class="line"> * Called when the session starts.</div><div class="line"> */</div><div class="line">function onSessionStarted(sessionStartedRequest, session) &#123;</div><div class="line">    console.log(&quot;onSessionStarted requestId=&quot; + sessionStartedRequest.requestId</div><div class="line">                + &quot;, sessionId=&quot; + session.sessionId);</div><div class="line">&#125;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Called when the user ends the session.</div><div class="line"> * Is not called when the skill returns shouldEndSession=true.</div><div class="line"> */</div><div class="line">function onSessionEnded(sessionEndedRequest, session) &#123;</div><div class="line">    console.log(&quot;onSessionEnded requestId=&quot; + sessionEndedRequest.requestId</div><div class="line">                + &quot;, sessionId=&quot; + session.sessionId);</div><div class="line">    // Add cleanup logic here</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function getWelcomeResponse(callback) &#123;</div><div class="line">  var speechOutput = &quot;Hello there, would you like the news?&quot;</div><div class="line"></div><div class="line">  var reprompt = speechOutput</div><div class="line"></div><div class="line">  var header = &quot;news&quot;</div><div class="line"></div><div class="line">  var shouldEndSession = false</div><div class="line"></div><div class="line">  var sessionAttributes = &#123;</div><div class="line">    &quot;speechOutput&quot;: speechOutput,</div><div class="line">    &quot;repromptText&quot;: reprompt</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  callback(sessionAttributes, buildSpeechletResponse(header, speechOutput, reprompt, shouldEndSession))</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function handleYesResponse(intent, session, callback) &#123;</div><div class="line">  get_headlines(function(headlines) &#123;</div><div class="line">    var speechOutput = &apos;The current world news headlines are &apos; + headlines;</div><div class="line">    var shouldEndSession = true</div><div class="line">    callback(session.attributes, buildSpeechletResponseWithoutCard(speechOutput, &quot;&quot;, shouldEndSession))</div><div class="line">  &#125;);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function handleNoResponse(intent, session, callback) &#123;</div><div class="line">  var speechOutput = &apos;I am not sure why you asked me to run then, but okay... bye&apos;</div><div class="line">  var shouldEndSession = true</div><div class="line">  callback(session.attributes, buildSpeechletResponseWithoutCard(speechOutput, &quot;&quot;, shouldEndSession))</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">function buildSpeechletResponse(title, output, repromptText, shouldEndSession) &#123;</div><div class="line">  return &#123;</div><div class="line">    outputSpeech: &#123;</div><div class="line">      type: &quot;PlainText&quot;,</div><div class="line">      text: output</div><div class="line">    &#125;,</div><div class="line">    card: &#123;</div><div class="line">      type: &quot;Simple&quot;,</div><div class="line">      title: title,</div><div class="line">      content: output</div><div class="line">    &#125;,</div><div class="line">    reprompt: &#123;</div><div class="line">      outputSpeech: &#123;</div><div class="line">        type: &quot;PlainText&quot;,</div><div class="line">        text: repromptText</div><div class="line">      &#125;</div><div class="line">    &#125;,</div><div class="line">    shouldEndSession: shouldEndSession</div><div class="line">  &#125;;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function buildSpeechletResponseWithoutCard(output, repromptText, shouldEndSession) &#123;</div><div class="line">  return &#123;</div><div class="line">    outputSpeech: &#123;</div><div class="line">      type: &quot;PlainText&quot;,</div><div class="line">      text: output</div><div class="line">    &#125;,</div><div class="line">    reprompt: &#123;</div><div class="line">      outputSpeech: &#123;</div><div class="line">        type: &quot;PlainText&quot;,</div><div class="line">        text: repromptText</div><div class="line">      &#125;</div><div class="line">    &#125;,</div><div class="line">    shouldEndSession: shouldEndSession</div><div class="line">  &#125;;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function buildResponse(sessionAttributes, speechletResponse) &#123;</div><div class="line">  return &#123;</div><div class="line">    version: &quot;1.0&quot;,</div><div class="line">    sessionAttributes: sessionAttributes,</div><div class="line">    response: speechletResponse</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function handleFinishSessionRequest(intent, session, callback) &#123;</div><div class="line">  callback(session.attributes, buildSpeechletResponseWithoutCard(&quot;Good bye! Thank you for using Reddit Reader&quot;,&quot;&quot;,true))</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">function get_headlines(callback) &#123;</div><div class="line">  request(&apos;https://reddit.com/r/worldnews/.json?limit=10&apos;, function (error, response, body) &#123;</div><div class="line">    if (!error &amp;&amp; response.statusCode == 200) &#123;</div><div class="line">      var body=JSON.parse(body)[&apos;data&apos;][&apos;children&apos;];</div><div class="line">      var res = &quot;&quot;;</div><div class="line">      body.forEach(function(ele)&#123;</div><div class="line">        res = res + ele[&apos;data&apos;][&apos;title&apos;] + &quot; &quot;;</div><div class="line">      &#125;);</div><div class="line">    &#125;</div><div class="line">    return callback(res);</div><div class="line">  &#125;);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>更多方法戳<a href="https://github.com/alexa/alexa-skills-kit-sdk-for-nodejs" target="_blank" rel="external">alexa-skills-kit-sdk-for-nodejs</a></strong></p>
<h1 id="Lambda-Configuration"><a href="#Lambda-Configuration" class="headerlink" title="Lambda Configuration"></a>Lambda Configuration</h1><p>登录<a href="https://console.aws.amazon.com" target="_blank" rel="external">AWS console</a>在 Service 下选择 Lambda<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA1.png" class="ful-image" alt="LAMBDA1.png"></p>
<p>Create a Lambda function，选 Node.js.4.3, Blank Function<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA2.png" class="ful-image" alt="LAMBDA2.png"></p>
<p>Configure Triggers，选 Alexa Skills Kit<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA3.png" class="ful-image" alt="LAMBDA3.png"></p>
<p>Configure Function，如下设置<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA4.png" class="ful-image" alt="LAMBDA4.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA5.png" class="ful-image" alt="LAMBDA5.png"></p>
<p>注意两个点，代码上传后，Handler 的路径设置必须和代码中 handler 文件的路径相一致，建议对文件内所有文件打包，而不是直接对文件夹打包。Role 如果没有 existing role，可以新建一个。</p>
<p>完成后选择新建的 function，在 Action 里选择 Configure test event<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA6.png" class="ful-image" alt="LAMBDA6.png"></p>
<p>选择 Alexa Start Session，修改 Applicatio ID，即 Alexa console 里的 Application ID，在 Deploy 下会讲到。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/DEVELOPERCONSOLE.png" class="ful-image" alt="DEVELOPERCONSOLE.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA7.png" class="ful-image" alt="LAMBDA7.png"></p>
<p>选择 Save and Test，返回结果<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/LAMBDA8.png" class="ful-image" alt="LAMBDA8.png"></p>
<h1 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h1><p>去<a href="https://developer.amazon.com" target="_blank" rel="external">Amazon developer</a> 网站上注册用户并登陆，注意这里的用户名和你的 Echo 用户名是一致的。点开 Alexa tab，选择 add skill，开始部署。</p>
<p><strong>Step1:</strong><br>填写 Name 和 Invocation Name，Invocation Name 用来 invoke app，Application ID 可以在保存页面后找到，<strong>需要添加到代码以及 Configure test event 里</strong>，如果需要对 Lambda 页面进行测试的话。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/1.jpg" class="ful-image" alt="1.jpg"></p>
<p><strong>Step2:</strong><br>主要填写 Intent Schema 和 Sample Utterances</p>
<p><strong>Intent Schema: </strong>how alexa will traverse your application</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123; &quot;intents&quot;: [&#123; &quot;intent&quot;: &quot;YesIntent&quot; &#125;,</div><div class="line">			  &#123; &quot;intent&quot;: &quot;NoIntent&quot; &#125;]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>Sample Utterances:</strong> the words people say to trigger intent<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">YesIntent yes</div><div class="line">YesIntent sure</div><div class="line"></div><div class="line">NoIntent no</div><div class="line">NoIntent go away</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/2.jpg" class="ful-image" alt="2.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/3.jpg" class="ful-image" alt="3.jpg">
<p><strong>Step3:</strong><br>选择并填写 Endpoint，即 AWS Lambda 下 function 的 ARN<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/6.png" class="ful-image" alt="6.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/4.png" class="ful-image" alt="4.png"></p>
<p><strong>Step4:</strong><br>就可以用 Echo 来 test 啦~ 如果没有 Echo，可以用 <strong>Service Simulator</strong> 来模拟，可以输入 text，也可以输入 json<br>先给 Alexa 一个关于 Reddit Reader 的指令，然后按照我们的代码，Alexa 会问你要不要读新闻<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/7.jpg" class="ful-image" alt="7.jpg"></p>
<p>然后我们回答 yes，Alexa 就开始读新闻啦~<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD%20-%20Lambda/8.jpg" class="ful-image" alt="8.jpg"></p>
]]></content>
      
        <categories>
            
            <category> IoT </category>
            
        </categories>
        
        
        <tags>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> 物联网 </tag>
            
            <tag> Echo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Sentiment Analysis]]></title>
      <url>http://www.shuang0420.com/2017/06/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/</url>
      <content type="html"><![CDATA[<p>Stanford Dan Jurafsky &amp; Chris Manning: Natural Language Processing 课程笔记。<br><a id="more"></a></p>
<p><strong>Sentiment Analysis</strong> 有许多别称，如 Opinion extraction/Opinion mining/Sentiment mining/Subjectivity analysis，都是同一个意思，不过隐含着不同的应用场景。大致来说，情感分析有以下的应用:</p>
<p><strong>Products:</strong> 产品评价，不仅仅是简单的好评差评，情感分析还能分析人们对具体产品的具体属性的具体评价，如下图，对 product review 抽取 aspects/attributes，判断 sentiment，最后 aggregate 得出结果<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/product%20sentiment.png" class="ful-image" alt="product%20sentiment.png"><br><strong>Public sentiment:</strong> 公众意见(public opinion)，比如说分析消费者信息指数，股票指数等。之前就有人做过用 CALM 来预测道琼斯指数(Bollen et al. 2011 Twitter mood predicts the stock market)，算法也应用到了工业场景<br><strong>Politics:</strong> 公共政策，看公众对候选人/政治议题的看法<br><strong>Prediction:</strong> 预测选举结果，预测市场趋势等等。</p>
<p>一个成熟的产品<a href="https://www.csc2.ncsu.edu/faculty/healey/tweet_viz/tweet_app/" target="_blank" rel="external">Twitter Sentiment App</a>，能够通过 Twitter 数据来分析人们对某个品牌/话题的情感<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/Twitter%20sentiment%20app.jpg" class="ful-image" alt="Twitter%20sentiment%20app.jpg"></p>
<blockquote>
<p><strong>Attitudes</strong>: “enduring, affectively colored beliefs, dispositions towards objects or persons”</p>
</blockquote>
<p>Sentiment analysis 说白了就是来分析人们对一个事物的<strong>态度(attitudes)</strong>，包含下面几个元素(以 Mary likes the movie 为例)</p>
<ul>
<li>Holder (source) of attitude<br>持有态度的人: Mary</li>
<li>Target (aspect) of attitude<br>对象: the movie</li>
<li>Type of attitude<br>态度类型: like<br><strong>From a set of types:</strong> Like, love, hate, value, desire, etc.<br><strong>Or (more commonly) simple weighted polarity:</strong> positive, negative, neutral, together with strength</li>
<li>Text containing the attitude<br>文本: Mary likes the movie<br>Sentence or entire document</li>
</ul>
<p>最简单的情感分析任务，或者说在情感分析方向的 baseline model，是分析/预测电影评论是 positive 还是 negative 的。</p>
<h1 id="Baseline-Algorithm"><a href="#Baseline-Algorithm" class="headerlink" title="Baseline Algorithm"></a>Baseline Algorithm</h1><p>常用到的语料库 <a href="hXp://www.cs.cornell.edu/people/pabo/movie-­‐review-­‐data" target="_blank" rel="external">IMDB Polarity Data 2.0</a>，<br><strong>目的：</strong> polarity detection: is this review positive or negative?<br><strong>步骤：</strong></p>
<ol>
<li>Tokenization</li>
<li>Feature Extraction</li>
<li>Classification using different classifiers<ul>
<li>Naive Bayes</li>
<li>MaxEnt</li>
<li>SVM</li>
</ul>
</li>
</ol>
<h2 id="Sentiment-Tokenization"><a href="#Sentiment-Tokenization" class="headerlink" title="Sentiment Tokenization"></a>Sentiment Tokenization</h2><p>除了正常 tokenization 要注意的问题如处理 HTML/XML markup 外，情感分析还可能需要处理</p>
<ul>
<li><strong>twitter markup</strong>(hashtag 等)</li>
<li><strong>Capitalization:</strong> 大小写通常会保留，大写字母往往反映强烈的情感</li>
<li><strong>Emotions</strong>(表情符号)</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/Emoticons.png" class="ful-image" alt="Emoticons.png">
<p>有用的 Tokenizer 代码</p>
<ul>
<li><a href="https://github.com/dlatk/happierfuntokenizing" target="_blank" rel="external">Christopher Potts sentiment tokenizer</a></li>
<li><a href="https://github.com/brendano/tweetmotif" target="_blank" rel="external">Brendan O’Connor twitter tokenizer</a></li>
</ul>
<h2 id="Extracting-Features"><a href="#Extracting-Features" class="headerlink" title="Extracting Features"></a>Extracting Features</h2><p>关于特征提取，两个重要的问题，一是怎么来<strong>处理否定词(negation)</strong>，二是<strong>选什么词作为特征</strong>。</p>
<h3 id="Negation"><a href="#Negation" class="headerlink" title="Negation"></a>Negation</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">I didn&apos;t like this movie</div><div class="line">I really like this movie</div></pre></td></tr></table></figure>
<p>如果对否定词不做处理，那么上面两条评论的结果都是 positive，这显然不对。一种有效的处理否定词的方案是<strong>对否定词后、下一个标点符号前的每个词都加上 NOT_ 的前缀来作为标识</strong>，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">didn&apos;t like this movie, but I</div><div class="line">=&gt;</div><div class="line">didn&apos;t NOT_like NOT_this NOT_movie but I</div></pre></td></tr></table></figure></p>
<p>具体见<br>Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Paciﬁc Finance Association Annual Conference (APFA).<br>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79—86.</p>
<h3 id="Words-to-use"><a href="#Words-to-use" class="headerlink" title="Words to use"></a>Words to use</h3><p>一般两种方案，一是仅仅<strong>使用形容词(adjectives)</strong>，而是使用<strong>所有的单词(all words)</strong>，通常而言，使用所有的词的效果会更好些，因为动词(verbs)、名词(nouns)会提供更多有用的信息。</p>
<h2 id="Classifier"><a href="#Classifier" class="headerlink" title="Classifier"></a>Classifier</h2><p>作为 Baseline model，这里会使用 Naive Bayes，没啥悬念，计算如下</p>
<p>$$c_{NB}=argmax_{c_j \in C} P(c_j) \prod_{i \in positions} P(w_i | c_j)$$</p>
<ul>
<li><strong>Prior:</strong> how likely we see a positive movie review</li>
<li><strong>Likelihood Function:</strong> for every review, how likely every word is expressed by a positive movie review</li>
</ul>
<p>采用 <strong>Laplace/Add-one Smoothing</strong></p>
<p>$$\hat P(w|c)={count(w,c)+1 \over count(c)+|V|}$$</p>
<p>一个变种或者改进版是<strong>Binarized(Boolean feature) Multinomial Naive Bayes</strong>，它基于这样一个直觉，对情感分析而言，单词是否出现(word occurrence)这个特征比单词出现了几次(word frequency)更为重要，举个例子，出现一次 fantastic 提供了 positive 的信息，而出现 5 次 fantastic 并没有给我们提供更多信息。boolean multinomial Naive Bayes 就是把所有大于 1 的 word counts 压缩为 1。</p>
<p>算法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/boolean%20multinomial%20naive%20bayes.jpg" class="ful-image" alt="boolean%20multinomial%20naive%20bayes.jpg"></p>
<p>也有研究认为取中间值 log(freq(w)) 效果更好一些，相关论文如下：<br>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sen+ment Classiﬁcation using Machine Learning Techniques. EMNLP-­‐2002, 79—86.<br>V. Metsis, I. Androutsopoulos, G. Paliouras. 2006. Spam Filtering with Naive Bayes – Which Naive Bayes? CEAS 2006 -­‐ Third Conference on Email and Anti‐Spam.<br>K.-­‐M. Schneider. 2004. On word frequency informa+on and negative evidence in Naive Bayes text classiﬁca+on. ICANLP, 474-­‐485.<br>JD Rennie, L Shih, J Teevan. 2003. Tackling the poor assumptions of naive bayes text classiﬁers. ICML 2003</p>
<p><em>当然在实践中，MaxEnt 和 SVM 的效果要比 Naive Bayes 好的多</em></p>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>有些句子里并<strong>不包含情感词(sentiment word)</strong>，如下面一句是 negative 的态度，然而并不能通过情感词来得出</p>
<p>“If you are reading this because it is your darling fragrance, please wear it at home exclusively, and tape the windows shut.”</p>
<p>还有一个问题是<strong>排序问题(Order effect)</strong>，尽管前面堆砌了很多情感词，但最后来个全盘否定，显然 Naive Bayes 没法处理这种问题<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/Order%20effects.png" class="ful-image" alt="Order%20effects.png"></p>
<h1 id="Sentiment-Lexicons"><a href="#Sentiment-Lexicons" class="headerlink" title="Sentiment Lexicons"></a>Sentiment Lexicons</h1><p>看一下目前已经有的 Lexicons，</p>
<ul>
<li><a href="http://www.wjh.harvard.edu/~inquirer" target="_blank" rel="external">The General Inquirer</a><ul>
<li><a href="http://www.wjh.harvard.edu/~inquirer/homecat.htm" target="_blank" rel="external">List of Categories</a></li>
<li><a href="http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xls" target="_blank" rel="external">Spreadsheet</a></li>
</ul>
</li>
<li><a href="http://www.liwc.net/" target="_blank" rel="external">LIWC(Linguistic Inquiry and Word Count)</a></li>
<li><a href="http://www.cs.pitt.edu/mpqa/subj_lexicon.html" target="_blank" rel="external">MPQA Subjectivity Cues Lexicon</a></li>
<li><a href="http://www.cs.uic.edu/~liub/FBS/opinion-­‐lexicon-­‐English.rar" target="_blank" rel="external">Bing Liu Opinion Lexicon</a></li>
<li><a href="http://sentiwordnet.isti.cnr.it/" target="_blank" rel="external">SentiWordNet</a></li>
</ul>
<p>看下各个词库的 disagreements between polarity lexicons<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/disagreement%20between%20polarity.png" class="ful-image" alt="disagreement%20between%20polarity.png"></p>
<p>那么怎么来分析 IMDB 里每个单词的 polarity 呢？<br><strong>How likely is each word to appear in each sentiment class?</strong><br><strong>likelihood:</strong> $$P(w|c)={f(w,c) \over \sum_{w \in c} f(w,c)}$$<br><strong>Make them comparable between words - Scaled likelihood:</strong><br>$${P(w|c)\over P(w)}$$</p>
<p>更多见 Potts, Christopher. 2011. On the negativity of negation. SALT 20, 636-­‐659.</p>
<h1 id="Learning-Sentiment-Lexicons"><a href="#Learning-Sentiment-Lexicons" class="headerlink" title="Learning Sentiment Lexicons"></a>Learning Sentiment Lexicons</h1><p>除了目前已有的 lexicon，我们还可以根据自己的语料库来训练自己的 sentiment lexicon。</p>
<h2 id="Semi-supervised-learning-of-lexicons"><a href="#Semi-supervised-learning-of-lexicons" class="headerlink" title="Semi-supervised learning of lexicons"></a>Semi-supervised learning of lexicons</h2><p>基于<strong>少量的有标注的数据+人工建立的规则</strong>，采用 <strong>bootstrap</strong> 方法来学习 lexicon</p>
<h3 id="Hatzivassiloglou-and-McKeown-intuition-for-identifying-word-polarity"><a href="#Hatzivassiloglou-and-McKeown-intuition-for-identifying-word-polarity" class="headerlink" title="Hatzivassiloglou and McKeown intuition for identifying word polarity"></a>Hatzivassiloglou and McKeown intuition for identifying word polarity</h3><p><strong>论文:</strong> Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the Semantic Orientation of Adjectives. ACL, 174–181</p>
<p>基于这样的<strong>假设</strong>:</p>
<ul>
<li>用 AND 连起来的形容词有着相同的 polarity<br>fair <strong>and</strong> legitimate, corrupt <strong>and</strong> brutal</li>
<li>用 BUT 连起来的形容词则相反<br>fair <strong>but</strong> brutal</li>
</ul>
<p><strong>论文方法：</strong></p>
<ol>
<li>对 1336 个形容词形成的种子集合进行标注，657 个 positive，679 个 negative</li>
<li>通过 google 搜索来查询 conjoined 形容词，eg. “was nice and”<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/STEP2.png" class="ful-image" alt="STEP2.png"></li>
<li>Supervised classifier 通过 count(AND), count(BUT) 来给每个词对(word pair)计算 polarity similarity<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/STEP3.png" class="ful-image" alt="STEP3.png"></li>
<li>将 graph 分区<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/STEP4.png" class="ful-image" alt="STEP4.png">
</li>
</ol>
<p>这种方法难以处理短语</p>
<h3 id="Turnev-Algorithm"><a href="#Turnev-Algorithm" class="headerlink" title="Turnev Algorithm"></a>Turnev Algorithm</h3><p><strong>论文:</strong> Turney (2002):Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews</p>
<p><strong>步骤:</strong></p>
<ol>
<li>从评论中抽取形容词短语(two-word phrase)<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/phrase1.png" class="ful-image" alt="phrase1.png"></li>
<li>学习短语的 polarity<br>如何衡量短语的 polarity 呢？<br>基于下面的假设<ul>
<li>Positive phrases co-‐occur more with <em>“excellent”</em></li>
<li>Negative phrases co-­‐occur more with <em>“poor”</em><br>用 PMI(Pointwise Mutual Information) 来计算 co-occurrence<br><strong>Mutual information</strong> between 2 random variables X and Y<br>$$I(X,Y)=\sum_x \sum_y P(x,y)log_2{P(x,y) \over P(x)P(y)}$$<br><strong>Pointwise mutual information:</strong> how much more do events x and y co-occur than if they were independent<br>$$PMI(X,Y)=log_2{P(x,y) \over P(x)P(y)}$$<br>同样通过搜索引擎(Altavista)查询得到概率<br>P(word) = hits(word)/N<br>$P(word_1,word_2)=hits(word_1 \ NEAR \ word_2)/N^2$<br>$$P(word_1,word_2)=log_2 {hits(word_1 \ NEAR \ word_2) \over hits(word_1)hits(word_2)}$$<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/Polarity%20phrase.png" class="ful-image" alt="Polarity%20phrase.png"></li>
</ul>
</li>
<li>Rate a review by the average polarity of its phrases</li>
</ol>
<p>一般来说 baseline 的准确率是 59%, Turney algorithm 可以提高到 74%</p>
<h3 id="Using-WordNet-to-learn-polarity"><a href="#Using-WordNet-to-learn-polarity" class="headerlink" title="Using WordNet to learn polarity"></a>Using WordNet to learn polarity</h3><p><strong>论文:</strong><br>S.M. Kim and E. Hovy. 2004. Determining the sentiment of opinions. COLING 2004<br>M. Hu and B. Liu. Mining and summarizing customer reviews. In Proceedings of KDD, 2004</p>
<p><strong>步骤:</strong></p>
<ol>
<li>有一小部分 positive/negative seed-words</li>
<li>从 WordNet 中找到 seed-words 的同义词(synonyms)和反义词(antonyms)<br><strong>Positive Set:</strong> positive words 的同义词 + negative words 的反义词<br><strong>Negative Set:</strong> negative words 的同义词 + positive words 的反义词</li>
<li>重复 2 直到达到终止条件</li>
<li>过滤不合适的词</li>
</ol>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>采用半监督方法来引入 lexicons，好处是:</p>
<ul>
<li>can be domain-specific</li>
<li>can be more robust(for more/new words)</li>
</ul>
<p><strong>Intuition:</strong></p>
<ol>
<li>starts with a seed set of words(good,poor)</li>
<li>find other words that have similar polarity:<br>•  Using “and” and “but”<br>•  Using words that occur nearby in the same document<br>•  Using WordNet synonyms and antonyms</li>
</ol>
<h1 id="Other-Sentiment-Tasks"><a href="#Other-Sentiment-Tasks" class="headerlink" title="Other Sentiment Tasks"></a>Other Sentiment Tasks</h1><h2 id="Finding-aspects-attributes-target"><a href="#Finding-aspects-attributes-target" class="headerlink" title="Finding aspects/attributes/target"></a>Finding aspects/attributes/target</h2><p><strong>论文:</strong><br>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD.<br>S. Blair-­‐Goldensohn, K. Hannan, R. McDonald, T. Neylon, G. Reis, and J. Reynar. 2008. Building a Sen+ment Summarizer for Local Service Reviews. WWW Workshop.</p>
<p>很多时候，一条评论并不能简单的被归为 positive/negative，它可能讨论了多个维度，既有肯定又有否定，如下面这个句子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The food was great but the service was awful!</div></pre></td></tr></table></figure></p>
<p>这条评论就是对食物(food)持肯定态度(positive)，对服务(service)持否定态度(negative)，在这种情况下，我们不能简单的对这条评论进行 positive/negative 的分类，而要对其在 food，service 这两个维度上的态度进行分类。<strong>food，service 这些维度，或者说 attributes/aspects/target 从哪里来？</strong> 有两种方法，一种是从文本中抽取常用短语+规则来作为 attributes/aspects，另一种是预先定义好 attributes/aspects</p>
<h3 id="Frequent-phrases-rules"><a href="#Frequent-phrases-rules" class="headerlink" title="Frequent phrases + rules"></a>Frequent phrases + rules</h3><p>首先找到产品评论里的高频短语，然后按规则进行过滤，可用的规则如找紧跟在 sentiment word 后面的短语，”…great fish tacos” 表示 fish tacos 是一个可能的 aspect</p>
<h3 id="Supervised-classification"><a href="#Supervised-classification" class="headerlink" title="Supervised classification"></a>Supervised classification</h3><p>对一些领域如 restaurants/hotels 来说，aspects 比较规范，所以事实上可以人工给一些产品评论标注 aspect(aspects 如 food, décor, service, value, NONE)，然后再给每个句子/短语分类看它属于哪个 aspect</p>
<p>具体步骤:</p>
<ol>
<li>从评论中抽取句子/短语</li>
<li>对句子/短语进行情感分类</li>
<li>得到句子/短语的 aspects</li>
<li>汇总得到 summary<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Sentiment%20Analysis/sentiment%20for%20aspects.png" class="ful-image" alt="sentiment%20for%20aspects.png">
</li>
</ol>
<p>值得注意的是，baseline method 的假设是<strong>所有类别出现的概率是相同的</strong>。如果类别不平衡(在现实中往往如此)，我们不能用 accuracy 来评估，而是需要用 F-scores。而类别不平衡的现象越严重，分类器的表现可能就越差。有两个办法来解决这个问题</p>
<ol>
<li>Resampling in training<br>就是说如果 pos 有10^6 条数据，neg 有 10^4 的数据，那么我们都从 10^4 的数据中来划分训练数据</li>
<li>Cost-sensitive learning<br>对较少出现的那个类别的 misclassification 加大惩罚(penalize SVM more for misclassification of the rare thing)</li>
</ol>
<h2 id="How-to-deal-with-7-stars"><a href="#How-to-deal-with-7-stars" class="headerlink" title="How to deal with 7 stars"></a>How to deal with 7 stars</h2><p><strong>论文:</strong> Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. ACL, 115–124</p>
<p>怎样来处理评分型的评论？</p>
<ol>
<li>Map to binary<br>压缩到 positive/negative。比如说大于 3.5 的作为 negative，其他作为 positive</li>
<li>Use linear or ordinal regression<br>or specialized models like metric labeling</li>
</ol>
<h2 id="Summary-on-Sentiment"><a href="#Summary-on-Sentiment" class="headerlink" title="Summary on Sentiment"></a>Summary on Sentiment</h2><p>通常被建立分类/回归模型来预测 binary/ordinal 类别<br><strong>关于特征提取:</strong></p>
<ul>
<li>negation 很重要</li>
<li>对某些任务，在 Naive bayes 里使用所有的词汇表现更好</li>
<li>对其他任务，可能用部分词汇更好<br>Hand-built polarity lexicons<br>Use seeds and semi-supervised learning to induce lexicons</li>
</ul>
<h1 id="Computational-work-on-other-affective-states"><a href="#Computational-work-on-other-affective-states" class="headerlink" title="Computational work on other affective states"></a>Computational work on other affective states</h1><p>对其他任务也可以用相似手段</p>
<ul>
<li>Emotion:<br>•  Detecting annoyed callers to dialogue system<br>•  Detecting confused/frustrated versus conﬁdent students</li>
<li>Mood:<br>•  Finding traumatized or depressed writers</li>
<li>Interpersonal stances:<br>•  Detection of flirtation or friendliness in conversations</li>
<li>  Personality traits:<br>•  Detection of extroverts</li>
</ul>
<p>E.g., Detection of Friendliness</p>
<p>Friendly speakers use collaborative conversational style<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- Laughter</div><div class="line">- Less use of negative emotional words</div><div class="line">- More sympathy</div><div class="line">​  That’s too bad I’m sorry to hear that!</div><div class="line">- More agreement</div><div class="line">​	 I think so too!</div><div class="line">- Less hedges</div><div class="line">​	 kind of sort of a little …</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> sentiment analysis </tag>
            
            <tag> 情感分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Learning to Extract Conditional Knowledge for Question Answering using Dialogue]]></title>
      <url>http://www.shuang0420.com/2017/05/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/</url>
      <content type="html"><![CDATA[<p>论文<a href="http://dl.acm.org/citation.cfm?id=2983777" target="_blank" rel="external">Learning to Extract Conditional Knowledge for Question Answering using Dialogue</a>提出了 conditional knowledge base(CKB)，存储的信息格式为 (subject, predicate, object|condition)。当用户问句缺少必要条件(condition)时，自动用 dialogue model 来向用户提问获取必要信息，再进行回答。<br><a id="more"></a><br>简化版过程，从训练数据的用户问句里抽取实体，频率最高的 50% 作为 subject，剩余的作为 candidate condition。对于每一个 subject，学习用户问句的 pattern 和 condition (类似于关系抽取)，然后学习 pattern 和 condition 的 embedding，并对其进行聚类得到 pattern cluster 和 condition cluster，再从聚类信息和 QA 对中抽取信息组成 (subject, predicate, object|condition) 作为 CKB。</p>
<p>当用户提问并没有清楚的指定条件时，就可以用  dialogue model 向用户提问获取 condition，具体过程如下</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/structure.jpg" class="ful-image" alt="structure.jpg">
<p>用户界面演示<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/example.jpg" class="ful-image" alt="example.jpg"></p>
<h1 id="Pattern-mining"><a href="#Pattern-mining" class="headerlink" title="Pattern mining"></a>Pattern mining</h1><p>这一步的目的是学习 pattern 和 condition，用 bootstrapping 方法。</p>
<p><strong>input:</strong> all questions with the same subject<br><strong>output:</strong> question patterns; conditions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Eg.,</div><div class="line"></div><div class="line">input: windows_xp free upgrade windows_10</div><div class="line">subject: windows_10</div><div class="line">candidate condition: windows_xp</div><div class="line">pattern: SLOT0 free upgrade windows_10</div></pre></td></tr></table></figure>
<p>输入有两个 entities, windows_xp 和 windows_10，windows_10 被选为 subject，那么 windows_xp 就是 candidate condition，然后我们产生了 pattern “SLOT0 free upgrade windows_10”，当遇到下面新的输入时，win7 就会被抽取作为 candidate condition，因为输入和 pattern 正好匹配</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">new input: win7 free upgrade windows_10</div><div class="line">pattern: SLOT0 free upgrade windows_10</div><div class="line">new candidate condition: win7</div></pre></td></tr></table></figure>
<p>一个问题是<strong>怎么来产生初始的种子</strong>，方法是</p>
<ul>
<li>remove question words</li>
<li>use special type of words for question chucking<br>special type of words: prepositions, copulas, interrogatives, conjunctions, modal verbs, personal pronouns, verbs, some stop words</li>
<li>add remaining parts into seed dictionary</li>
</ul>
<h1 id="Pattern-Aggregation"><a href="#Pattern-Aggregation" class="headerlink" title="Pattern Aggregation"></a>Pattern Aggregation</h1><p>这一步非常简单，就是做一个 groupby，把上一步产生的相同 pattern 不同 condition 的输出按 pattern 分组，见 Step2</p>
<h1 id="Condition-and-Pattern-Representation-Learning"><a href="#Condition-and-Pattern-Representation-Learning" class="headerlink" title="Condition and Pattern Representation Learning"></a>Condition and Pattern Representation Learning</h1><p>不同的 pattern 可能反应了相同的 user intent，这一步的目的就是对 pattern 进行聚类，目的是希望每一个类别代表一个 user intent。同时，对聚类后的每一个 pattern cluster 的 condition 进行聚类，聚类标准是在当前 condition 下的问题是否拥有相似的 answer。</p>
<p>首先学习 pattern 和 condition 的 embedding。对此论文提出了一种新的算法 <strong>patterns and conditions jointly embedding algorithm(PCJE)</strong>，由 <strong>condition embedding model, pattern embedding model 和 alignment model</strong> 三个 model 组成，目标函数是三个 model 的目标函数之和。</p>
<p><strong>Condition embedding model</strong> 主要看 $p(c_k, c_m)$，通过 Skip-gram 来学习 question, answer, condition pairs 的嵌入向量，要注意的是，这里 Skip-gram 的目标函数最大化 $J_c = J(\theta) + \beta E_c$，其中 $ J(\theta) $ 是 Skip-gram 原本的目标函数, $E_c$ 是正则化后的 condition pair($c_k,c_m$) 的联合概率，如下</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/skipgram.jpg" class="ful-image" alt="skipgram.jpg">
<p>$$p(c_k,c_m) = {1 \over 1+exp(-c^T_kc_m)}$$<br>$$E_c=\sum_{(k,m) \in P_c} w^c_{km}logp(c_k,c_m)$$</p>
<p><strong>Pattern embedding model</strong> 主要看 $p(v_k, v_m)$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/patternEmbedding.jpg" class="ful-image" alt="patternEmbedding.jpg"></p>
<p><strong>Alignment model</strong> 主要看 $p(c_k, v_m)$，通过 pattern 和 condition 的共现关系来对齐两个向量空间<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/alignmentModel.jpg" class="ful-image" alt="alignmentModel.jpg"></p>
<p>整体需要优化的目标函数<br>$$J=J_c+J_p+J_\alpha$$</p>
<h1 id="Conditions-and-Patterns-Clustering"><a href="#Conditions-and-Patterns-Clustering" class="headerlink" title="Conditions and Patterns Clustering"></a>Conditions and Patterns Clustering</h1><p><strong>input:</strong> patterns, conditions, embedding representations<br><strong>output:</strong> pattern clusters, condition clusters</p>
<blockquote>
<p><strong>patterns in the same cluster will share the same intent, which is predicate in classical KB.</strong></p>
</blockquote>
<p>用了下面的层次聚类算法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/co-clustering%20algorithm.jpg" class="ful-image" alt="co-clustering%20algorithm.jpg"></p>
<h1 id="Conditional-Knowledge-Base-Construction"><a href="#Conditional-Knowledge-Base-Construction" class="headerlink" title="Conditional Knowledge Base Construction"></a>Conditional Knowledge Base Construction</h1><p><strong>input:</strong> pattern clusters, condition clusters<br><strong>output:</strong> (Subject, Predicate, Object | Condition) triples</p>
<p>要知道并不是所有的 condition 都是重要的，这一步骤会过滤一些不重要的 condition。这里提到了一个概念 missing percentage of slots，指的是能够匹配 pattern 然而却没有 slot 的情况，比如下面的例子，对输入而言，尽管匹配了 pattern，但 SLOT0 是缺失的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pattern: SLOT0 free upgrade windows_10</div><div class="line">input: free upgrade windows_10</div></pre></td></tr></table></figure></p>
<p>对每个 slot 计算 missing percentage (of slots)，然后把 slot 分为下面三种类型</p>
<ol>
<li>只有一个 cluster 的 slots（没有询问的必要）</li>
<li>基本上不会被忽略的 slots</li>
<li>几乎没有用户会在意的 slots</li>
</ol>
<p>第一种直接过滤，第二种也就是过滤 missing percentage 大于 0.7 的 slot，第三种也就是过滤 missing percentage 小于 0.3 的 slot，剩下的 slots 才是重要的，组成 (Subject, Predicate, Object | Condition) 格式存到 CKB 中。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Extract%20Conditional%20Knowledge%20for%20Question%20Answering%20using%20Dialogue/map.jpg" class="ful-image" alt="map.jpg">
<p><strong>Subject:</strong> 选定的 entity<br><strong>Predicate:</strong> 同一 cluster 的若干 pattern，用频率最高的若干个单词/短语作为代表性的 predicate<br><strong>Object:</strong> 根据 answer set 与 pattern cluster 的 average embedding 的 cosine similarity 来选择 top answer 作为 object<br><strong>Condition:</strong> 同一 cluster 的若干 condition</p>
<h1 id="Dialogue-Model-Construction"><a href="#Dialogue-Model-Construction" class="headerlink" title="Dialogue Model Construction"></a>Dialogue Model Construction</h1><p>两个任务，看 input question 是否匹配 pattern，缺失的 condition 是否重要，如果重要，那么，提问并提供候选项来让用户选择，填充 slot，返回答案</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Question Answering </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Text Summarization]]></title>
      <url>http://www.shuang0420.com/2017/05/10/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/</url>
      <content type="html"><![CDATA[<p>Stanford Dan Jurafsky &amp; Chris Manning: Natural Language Processing 课程笔记。文档/会议/邮件摘要，QA 系统回答 what/how 等复杂问题，都需要自动摘要技术。本篇主要讲基于查询的摘要。<br><a id="more"></a></p>
<h1 id="Types-of-Summarization-Task"><a href="#Types-of-Summarization-Task" class="headerlink" title="Types of Summarization Task"></a>Types of Summarization Task</h1><p><strong>单文档 vs 多文档</strong></p>
<ul>
<li>Single-­‐document summarization<br>给定一个文档，产生 abstract/outline/headline</li>
<li>Multiple-­‐document summarization<br>给定主题相关的一组文档，通过摘要来概况同一事件/主题的信息</li>
</ul>
<p>与单文档相比，多文档任务面临的<strong>减小句子冗余度/确定句子顺序/确定压缩比率(从每个文档中抽取句子的比例)/指代消解问题</strong>都更加的突出</p>
<p><strong>查询无关 vs 查询相关</strong></p>
<ul>
<li>Generic summarization<br>对一个文档的内容做整体性的摘要</li>
<li>Query-­‐focused summarization<br>根据用户查询语句表达的信息需求(information need)来对一篇文档做出摘要总结，如 Google snippets<br>用于 QA 系统，根据提问产生文档摘要来回答一个复杂的问题</li>
</ul>
<p>查询相关的文本摘要对句子重要性的衡量需要同时考虑主题性以及查询相关性</p>
<p><strong>抽取式 vs 合成式</strong></p>
<ul>
<li>Extractive summarization<br>摘要句子完全从源文档中抽取形成</li>
<li>Abstractive summarization: our own words<br>从源文档中抽取句子并进行改写形成摘要</li>
</ul>
<p>目前来看，大多数的系统是抽取式，合成式的技术还不够成熟。</p>
<p>本章主要讨论 <strong>Extractive summarization</strong></p>
<h1 id="Baseline-Model"><a href="#Baseline-Model" class="headerlink" title="Baseline Model"></a>Baseline Model</h1><p>好的作者常常会在标题和第一句话就表达主题，因此最简单的 baseline 就是抽取文档中的首句作为摘要。</p>
<h1 id="Generating-snippets-query-focused-summaries"><a href="#Generating-snippets-query-focused-summaries" class="headerlink" title="Generating snippets: query-focused summaries"></a>Generating snippets: query-focused summaries</h1><p>看一下 Google snippets</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/google%20snippets.png" class="ful-image" alt="google%20snippets.png">
<ul>
<li>Single-­‐document summarization</li>
<li>Query-­‐focused summarization</li>
<li>Extractive summarization</li>
</ul>
<h2 id="Main-Stages"><a href="#Main-Stages" class="headerlink" title="Main Stages"></a>Main Stages</h2><p>产生 snippets 的<strong>主要步骤(Stages):</strong></p>
<ul>
<li><strong>content selection</strong><br>选择需要抽取的句子(segment/moving window)</li>
<li><strong>information ordering</strong><br>对抽取的句子进行排序</li>
<li><strong>sentence realization</strong><br>形成摘要</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/summarization%20stages.png" class="ful-image" alt="summarization%20stages.png">
<h2 id="Base-Summarization-Algorithm"><a href="#Base-Summarization-Algorithm" class="headerlink" title="Base Summarization Algorithm"></a>Base Summarization Algorithm</h2><p>对一个 base summarization algorithm 而言，其实只需要做第一步 conent selection，之后的 information ordering 即保留句子在源文档的位置，sentence realization 即保留原句。</p>
<h3 id="Unsupervised-content-selection"><a href="#Unsupervised-content-selection" class="headerlink" title="Unsupervised content selection"></a>Unsupervised content selection</h3><p>我们需要选择的是<strong>salient or informative</strong>的句子，一般来说，salient words 有两种选择方法</p>
<ul>
<li>tf-idf<br>也就是找在该文档中经常出现，并且在其他文章中很少出现的单词</li>
<li>topic signature<br>通过计算 log-likelihood ratio(LLR) 并设置 threhold 来过滤并选择重要的单词<br>$weight(w_i)=1 \ if -2log \lambda(w_i)&gt;10 \ else \ 0$</li>
</ul>
<p>这里主要介绍下<strong>Topic signature-based content selection with queries</strong></p>
<p><strong>Step1: choose words</strong><br>salient words 有下面两个来源:</p>
<ul>
<li>计算每个单词的 log-likelihood ratio(LLR) ，根据 threshold 进行选择</li>
<li>选择所有出现在 query 里的单词</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/topic%20signature.png" class="ful-image" alt="topic%20signature.png">
<p><strong>Step2: weigh a sentence(or window)</strong><br>在上一步计算的单词分数基础上，计算每个句子/短语窗口的分数<br>$$weight(s)={1 \over |S|} \sum_{w \in S} weight(w)$$<br>选择 top k 个句子。</p>
<h3 id="Supervised-content-selection"><a href="#Supervised-content-selection" class="headerlink" title="Supervised content selection"></a>Supervised content selection</h3><p>其实是一个二分类问题，对文档中的每一个句子，用分类器进行二值分类，1 代表这个句子可以作为摘要输出句子，0 代表不能。</p>
<p>监督学习方法难点是获得训练集，很有可能摘要句子并不是文档中的完整句子，所以需要事先把文档句子和摘要句子对齐，才能得到分类标签。然后对文档句子抽取特征将句子映射为特征向量，再训练分类器，可以用的算法如 <strong>Naive Bayes/Decision Tree/HMM/CRF/LR/SVM/SVM-HMM等</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/supervised%20content%20selection.png" class="ful-image" alt="supervised%20content%20selection.png">
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>上面提到的只是最基础的方法，对非监督方法而言，还有下面的方法</p>
<ul>
<li>线性组合方法：利用手工构建的评分函数，采取若干重要特征并手工设定特征权重，以此来对句子重要性进行得分计算。</li>
<li>词汇链方法：通过文章中相邻句子的语义相似性来判断文章主题，引入Wordnet等语言资源中的同义词和近义词信息，分析文章中相邻句子的语义相似性。寻找若干最长的词汇链来确定文章包含主题，并依此来构建文摘句子集合；[6,7]  </li>
<li>图模型方法：将文章中每个句子作为图中的节点，利用句子之间内容相似性构建图中节点之间的边。构建好文章图后，利用PageRank或者HITS<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="external">算法</a>来迭代计算图中节点的权值，按照权值大小作为句子重要性的评分依据来对文摘句子进行抽取。[3,4]</li>
<li>子主题分析方法：通过聚类或者语义块分析等手段，发现文章包含的子主题，并从不同的子主题中抽取句子来构造摘要句子集合。LSA，PLSA等方法属于这一类[8,10,12]。</li>
</ul>
<p>一些研究工作 <a href="https://pdfs.semanticscholar.org/8ddf/5baeeab2e2fd401c0959a2d70e4c2ba68a33.pdf" target="_blank" rel="external">Document Summarization using Conditional Random Fields</a> 和 <a href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwjC5bWohpzUAhWBsY8KHUnHDlQQFggkMAA&amp;url=http%3a%2f%2fwww2009%2eorg%2fproceedings%2fpdf%2fp71%2epdf&amp;usg=AFQjCNHZgHD2NVdqCJ9xONGboyNI0oyNog" target="_blank" rel="external">Enhancing Diversity, Coverage and Balance for  Summarization through Structure Learning</a>对主流的一些自动文摘方法做了对比，对于非监督方法来说，基于 HITS 的图模型方法明显优于其他方法，对于监督方法来说，SVM-HMM 和 CRF 方法效果最好，其中 SVM-HMM 方法在一般<a href="http://lib.csdn.net/base/softwaretest" target="_blank" rel="external">测试</a>集合上稍微优于CRF，在难度高的测试集合上效果明显好于CRF方法。这两个方法优于HITS图模型方法，不过优势并非特别明显；从测试结果来看，方法效果排序如下 SVM-HMM&gt;CRF&gt;HITS&gt;HMM&gt;SVM&gt;LR&gt;NB&gt;LSA</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">1. 简单特征线性组合方法(非监督方法)</div><div class="line">	即确定一些主要特征，然后设定特征权重后根据线性组合方式来进行句子打分和排序输出；</div><div class="line">	优点：</div><div class="line">		方法简单；</div><div class="line">		无需训练数据；</div><div class="line">		执行速度快；</div><div class="line">	缺点：</div><div class="line">		由于手工拟合评分函数，只能采取部分主要特征；</div><div class="line">		权重设定需要手工设置并不断调试；</div><div class="line">		效果一般；</div><div class="line">2. 基于HITS的图模型方法(非监督方法)</div><div class="line">	考虑到目前的研究表明，基于 HITS 的图模型方法是非监督方法中效果最好的，如果采取非监督方法，则优先考虑 HITS 的图模型方法；</div><div class="line">	优点:</div><div class="line">		无需训练集合；</div><div class="line">		基本与语言和领域无关；</div><div class="line">		效果好；</div><div class="line">	缺点：</div><div class="line">		由于存在任意句子相似性计算和迭代计算，所以运行速度相对比较慢；需要改进速度提出改进方法；</div><div class="line">		该方法没有考虑信息冗余的问题，可能需要有针对性的改进；</div><div class="line">3. 基于 CRF 或者 SVM-HMM 的监督学习方法</div><div class="line">	目前研究表明，CRF 和 SVM-HMM 在所有监督和非监督方法中是效果最好的，其中 SVM-HMM 效果略好于 CRF，CRF 略好于 HITS 图模型方法；所以如果采取监督学习思路，可以考虑CRF或者SVM-HMM的方法；</div><div class="line">	优点：</div><div class="line">		效果好；</div><div class="line">	缺点：</div><div class="line">		需要训练数据；</div><div class="line">		效果依赖于训练数据质量和领域等方面的情况；</div><div class="line">		执行速度慢；尤其是融合HITS模型等复杂特征，需要首先计算复杂特征，所以速度应该是最慢的；</div></pre></td></tr></table></figure>
<p>这部分的总结来自<a href="http://blog.csdn.net/malefactor/article/details/8312838" target="_blank" rel="external">文本摘要技术调研</a></p>
<p>[1] .Jie Tang, Limin Yao, and Dewei Chen . Multi-topic based Query-oriented Summarization.<br>W.-T.Yih, J. Goodman, L. Vanderwende, and H. Suzuki. Multi-document summarization by maximizing informative content-words.In Proceedingsof IJCAI’07, 2007.<br>[2]  Dou Shen1,Jian-Tao Sun.etc    DocumentSummarization using Conditional Random Fields.  In<em>Proceedingsof IJCAI’07</em>, 2007.<br>[3] GunesErkan.  Dragomir R. Radev.  LexRank: Graph-based LexicalCentrality as Salience in   Text Summarization.  Journal of ArtificialIntelligence Research 22 (2004) 457-479<br>[4] Rada Mihalcea.  Language Independent Extractive Summarization.<br>[5] Liangda Li, Ke Zhou†,Gui-Rong Xue etc  Enhancing Diversity, Coverage and Balance for  Summarization through Structure Learning.  WWW 2009.<br>[6] Gregory Silber and Kathleen F. McCoy  Efficient Text Summarization Using Lexical Chains.<br>[7] Barzilay,Regina and Michael Elhadad. Using Lexical Chainsfor Text Summarization. in Proceedings of the IntelligentScalable Text Summarization Workshop(ISTS’97), 1997.<br>[8] Shanmugasundaram Hariharan   Extraction Based Multi Document Summarization using Single Document  Summary Cluster   <em>Int. J.Advance. Soft Comput. Appl., Vol. 2, No. 1, March 2010</em><br>[9] ShanmugasundaramHariharan, “Merging Multi-Document Text Summaries-A Case Study”, <em>Journal of Scienceand Technology</em>, Vol.5, No.4,pp.63-74, December 2009.<br>[10] JinZhang etc  AdaSum: An Adaptive Model for Summarization.  CIKM 2008.<br>[11] Varadarajan and Hristidis. A System forQuery-Specific Document Summarization CIKM2006.<br>[12] LeonhardHennig  Topic-based Multi-Document Summarization with Probabilistic Latent Semantic Analysis</p>
<h1 id="Complex-Questions-Summarizing-Multiple-Documents"><a href="#Complex-Questions-Summarizing-Multiple-Documents" class="headerlink" title="Complex Questions: Summarizing Multiple Documents"></a>Complex Questions: Summarizing Multiple Documents</h1><p>自动摘要还可以用于回答复杂的问句(如 how/what)，有两大类方法，自底向上的 snippet 方法，以及自上而下的信息抽取方法。</p>
<p><strong>Bottom-up snippet method:</strong></p>
<ol>
<li>找到相关文档集合</li>
<li>从文档集合中抽取 informative sentences</li>
<li>对句子进行排序并形成回答</li>
</ol>
<p><strong>Top-down information extraction method:</strong></p>
<ol>
<li>根据不同的问题类型建立特定的信息抽取框架</li>
<li>抽取信息</li>
<li>形成回答</li>
</ol>
<p>属于研究热点，有很大提升空间。</p>
<h2 id="Bottom-up-snippet-method"><a href="#Bottom-up-snippet-method" class="headerlink" title="Bottom-up snippet method"></a>Bottom-up snippet method</h2><p>处理框架：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/query-focused%20multi-document%20summarization.png" class="ful-image" alt="query-focused%20multi-document%20summarization.png">
<h3 id="Sentence-Simplification"><a href="#Sentence-Simplification" class="headerlink" title="Sentence Simplification"></a>Sentence Simplification</h3><p>首先简化句子，可以删除 <strong>同位语/定语从句/没有命名实体的介词短语/句子开头的状语</strong> 等</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/Simplifying%20sentences.png" class="ful-image" alt="Simplifying%20sentences.png">
<h3 id="Sentence-Extraction"><a href="#Sentence-Extraction" class="headerlink" title="Sentence Extraction"></a>Sentence Extraction</h3><h4 id="Maximal-Marginal-Relevance-MMR"><a href="#Maximal-Marginal-Relevance-MMR" class="headerlink" title="Maximal Marginal Relevance(MMR)"></a>Maximal Marginal Relevance(MMR)</h4><p>MMR 是一种从多个文档中进行选择的递归(iterative)方法，递归的从文档中选取最合适的句子插入到 summary/answer 中，两个指标是<strong>相关性(relevant)</strong>，以及 <strong>新颖度(novel)</strong>。这在<a href="http://www.shuang0420.com/2016/12/07/Search%20Engines笔记%20-%20Diversity/">Search Engines笔记 - Diversity</a>也提到过。</p>
<ul>
<li><strong>Relevant</strong><br>与 query 最相关的句子，high cosine similarity to the query</li>
<li><strong>Novel</strong><br>减少 summary/answer 的冗余程度，low cosine similariy to the summary<br>$\hat S_{MMR}=max s \in D \lambda sim(s,Q) - (1-\lambda)max s \in S sim(s,S)$</li>
<li>不断添加句子到 summary 中，直到 summary length 到达预期要求</li>
</ul>
<h4 id="LLR-MMR-choosing-informative-yet-non-redundant-sentences"><a href="#LLR-MMR-choosing-informative-yet-non-redundant-sentences" class="headerlink" title="LLR + MMR: choosing informative yet non-redundant sentences"></a>LLR + MMR: choosing informative yet non-redundant sentences</h4><p>1.  根据 LLR 对每个句子进行打分<br>2.  选择 top k 个句子作为候选的摘要句<br>3.  迭代的从候选集里选取高分并且还不在当前摘要里的句子，添加进摘要</p>
<h3 id="Information-Ordering"><a href="#Information-Ordering" class="headerlink" title="Information Ordering"></a>Information Ordering</h3><ul>
<li>时间顺序(Chronological ordering)<br>根据时间对句子进行排序，主要用于新闻类的摘要(Barzilay, Elhadad, and McKeown 2002)</li>
<li>一致性(Coherence)<br>根据 cosine similarity 对句子排序，使得摘要中相邻的句子更相似，或者相邻句子讨论同一个实体(Barzilay and Lapata 2007)</li>
<li>专题排序(Topical ordering)<br>从源文档中学习主题排序</li>
</ul>
<h2 id="Information-Extraction-Method"><a href="#Information-Extraction-Method" class="headerlink" title="Information Extraction Method"></a>Information Extraction Method</h2><p>用信息抽取(IE)的方法来回答，比如说一个人物传记类的问题答案通常包含人物的 <strong>生卒年月，国籍，教育程度，名望/荣誉等</strong>，一个定义类问题(definition)通常包括 <strong>属(genus)或者上位词(hypernym)</strong>，如 The Hajj is a type of ritual，一个关于用药的医学类问题通常包括 <strong>问题(medical condition)、治疗(intervention, the drug or procedure)和结果(outcome)</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/de%EF%AC%81nition%20questions.png" class="ful-image" alt="de%EF%AC%81nition%20questions.png">
<p>上图是课程提到的 IE 方法框架，暂时不是很理解 predicate identification 这一步，感觉 <a href="http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d3/data/pdf/anthology-PDF/H/H01/H01-1054.pdf" target="_blank" rel="external">Multidocument Summarization via Information Extraction</a>这篇文档的框架更加 straight-forward 一些。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/dIE%20METHOD.png" class="ful-image" alt="IE%20METHOD.png">
<h1 id="Evaluating-summaries-ROUGE"><a href="#Evaluating-summaries-ROUGE" class="headerlink" title="Evaluating summaries: ROUGE"></a>Evaluating summaries: ROUGE</h1><p>ROUGE 是内部评价指标，以 BLEU 为基础，虽然比不上人工评价，但是用起来很方便</p>
<p>给定一个文档 D，以及一个自动生成的文本摘要 X：</p>
<ol>
<li>由 N 个人产生 D 的 reference summaries</li>
<li>运行系统，产生自动文本摘要 X</li>
<li>计算 reference summaries 中的 bigram 在 X 里出现的比例</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/ROUGE-2.png" class="ful-image" alt="ROUGE-2.png">
<p>举个例子，问句是 “What is water spinach?”，Human 1, Human 2, Human 3 是人工产生的 reference summaries，System answer 是自动摘要，计算如下</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Text%20Summarization/ROUGE%20EXAMPLE.png" class="ful-image" alt="ROUGE%20EXAMPLE.png">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> text summarization </tag>
            
            <tag> 文本摘要 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Alexa 开发新技能 - python flask]]></title>
      <url>http://www.shuang0420.com/2017/05/02/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/</url>
      <content type="html"><![CDATA[<p>非常简单的教程，讲怎么给 Alexa 添加新的 skill，让你的 Echo 更个性化。本篇添加的 skill 是让 Alexa 从 reddit 上读前 10 条热点。<br><a id="more"></a></p>
<p>代码戳<a href="https://github.com/Shuang0420/Alexa-Starter-RedditReader/tree/master/reddit_reader_flask" target="_blank" rel="external">Alexa-Starter-RedditReader</a>，其他版本如用 Lambda Function 实现，见<a href="http://www.shuang0420.com/2017/06/05/Alexa%20开发新技能%20-%20Lambda/">Alexa 开发新技能 - Lambda</a></p>
<h1 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h1><ul>
<li>Python</li>
<li><a href="https://developer.amazon.com/edw/home.html" target="_blank" rel="external">Amazon Developer Account</a></li>
<li><a href="https://ngrok.com/download" target="_blank" rel="external">ngrok</a></li>
</ul>
<h1 id="Code-for-new-skill"><a href="#Code-for-new-skill" class="headerlink" title="Code for new skill"></a>Code for new skill</h1><p>这里我们用 Flask 来建一个简单的 web application，代码如下，get_headlines() 是我们主要的 method，从 Reddit 里返回 10 条热点。逻辑是这样的：</p>
<ul>
<li>(用户呼唤 Reddit Reader)</li>
<li>Alexa 问用户 ‘Hello there, would you like the news?’</li>
<li>用户回答<br>肯定回复: 读 10 headlines<br>否定回复: 回答 ‘I am not sure why you asked me to run then, but okay… bye’</li>
</ul>
<p>注意要确保有上面的 python package，如果没有，先安装</p>
<ul>
<li>flask == 0.12.1</li>
<li>flask_ask == 0.9.3</li>
<li>unidecode == 0.4.20</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ pip install flask</div><div class="line">$ pip install flask_ask</div><div class="line">$ pip install unidecode</div></pre></td></tr></table></figure>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">from flask import Flask, render_template</div><div class="line">from flask_ask import Ask, statement, question, session</div><div class="line">import json</div><div class="line">import requests</div><div class="line">import time</div><div class="line">import unidecode</div><div class="line"></div><div class="line"></div><div class="line"># define Flask app</div><div class="line">app = Flask(__name__)</div><div class="line"># give basic endpoint, can be flask skill/program endpoint</div><div class="line">ask = Ask(app, &quot;/reddit_reader&quot;)</div><div class="line"></div><div class="line"></div><div class="line">def get_headlines():</div><div class="line">    sess = requests.Session()</div><div class="line">    # just change user-agent</div><div class="line">    sess.headers.update(&#123;&apos;User-Agent&apos;: &apos;I am testing Alexa&apos;&#125;)</div><div class="line">    time.sleep(1)</div><div class="line">    # get first 10 headlines</div><div class="line">    url = &apos;https://reddit.com/r/worldnews/.json?limit=10&apos;</div><div class="line">    html = sess.get(url)</div><div class="line">    data = json.loads(html.content.decode(&apos;utf-8&apos;))</div><div class="line">    titles = [unidecode.unidecode(listing[&apos;data&apos;][&apos;title&apos;]) for listing in data[&apos;data&apos;][&apos;children&apos;]]</div><div class="line">    titles = &apos;...&apos;.join([i for i in titles])</div><div class="line">    return titles</div><div class="line"></div><div class="line"></div><div class="line"># set home url path</div><div class="line">@app.route(&apos;/&apos;)</div><div class="line">def homepage():</div><div class="line">    return &apos;hi there, how ya doin?&apos;</div><div class="line"></div><div class="line"></div><div class="line">@ask.launch</div><div class="line">def start_skill():</div><div class="line">    # it will say</div><div class="line">    welcome_message = &apos;Hello there, would you like the news?&apos;</div><div class="line">    # question expect response</div><div class="line">    return question(welcome_message)</div><div class="line"></div><div class="line"></div><div class="line"># handle user input yes or no response</div><div class="line"># user input is intent</div><div class="line">@ask.intent(&quot;YesIntent&quot;)</div><div class="line">def share_headlines():</div><div class="line">    # grab the headline</div><div class="line">    headlines = get_headlines()</div><div class="line">    headline_msg = &apos;The current world news headlines are &#123;&#125;&apos;.format(headlines)</div><div class="line">    # statement tell you sth</div><div class="line">    return statement(headline_msg)</div><div class="line"></div><div class="line"></div><div class="line">@ask.intent(&quot;NoIntent&quot;)</div><div class="line">def no_intent():</div><div class="line">    bye_text = &apos;I am not sure why you asked me to run then, but okay... bye&apos;</div><div class="line">    return statement(bye_text)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    app.run(debug=True)</div></pre></td></tr></table></figure></p>
<h1 id="Set-endpoint"><a href="#Set-endpoint" class="headerlink" title="Set endpoint"></a>Set endpoint</h1><p>这里我们用 <a href="https://ngrok.com/download" target="_blank" rel="external">ngrok</a>，将本地 web 服务部署到公共网络。如果没有下载，先从<a href="https://ngrok.com/download" target="_blank" rel="external">这</a>下载并解压，然后运行，端口就是 Flask 运行的端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./ngrok http 5000</div></pre></td></tr></table></figure></p>
<h1 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h1><p>去<a href="https://developer.amazon.com" target="_blank" rel="external">Amazon developer</a> 网站上注册用户并登陆，注意这里的用户名和你的 Echo 用户名是一致的。点开 Alexa tab，选择 add skill，开始部署。</p>
<p><strong>Step1:</strong><br>填写 Name 和 Invocation Name，Invocation Name 用来 invoke app<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/1.jpg" class="ful-image" alt="1.jpg"></p>
<p><strong>Step2:</strong><br>主要填写 Intent Schema 和 Sample Utterances</p>
<p><strong>Intent Schema: </strong>how alexa will traverse your application</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123; &quot;intents&quot;: [&#123; &quot;intent&quot;: &quot;YesIntent&quot; &#125;,</div><div class="line">			  &#123; &quot;intent&quot;: &quot;NoIntent&quot; &#125;]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>Sample Utterances:</strong> the words people say to trigger intent<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">YesIntent yes</div><div class="line">YesIntent sure</div><div class="line"></div><div class="line">NoIntent no</div><div class="line">NoIntent go away</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/2.jpg" class="ful-image" alt="2.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/3.jpg" class="ful-image" alt="3.jpg">
<p><strong>Step3:</strong><br>选择并填写 Endpoint，这里我们用的是 HTTPS，可以在 ngrok 的命令行找到 endpoint url<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/6.jpg" class="ful-image" alt="6.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/4.jpg" class="ful-image" alt="4.jpg"></p>
<p><strong>Step4:</strong><br>这里我们就选 My development endpoint is a sub-domain of a domain that has a wildcard certificate from a certificate authority，作为测试的话不选也没有关系<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/5.jpg" class="ful-image" alt="5.jpg"></p>
<p><strong>Step5:</strong><br>就可以用 Echo 来 test 啦~ 如果没有 Echo，可以用 <strong>Service Simulator</strong> 来模拟<br>先给 Alexa 一个关于 Reddit Reader 的指令，然后按照我们的代码，Alexa 会问你要不要读新闻</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/7.jpg" class="ful-image" alt="7.jpg">
<p>然后我们回答 yes，Alexa 就开始读新闻啦~<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Alexa%20%E5%BC%80%E5%8F%91%E6%96%B0%E6%8A%80%E8%83%BD/8.jpg" class="ful-image" alt="8.jpg"></p>
<p><strong>一个简单的实例<a href="https://github.com/Shuang0420/PNC_Virtual_Assistant" target="_blank" rel="external">PNC_Virtual_Assistant</a></strong></p>
<blockquote>
<p>参考链接<br><a href="https://www.youtube.com/watch?v=DFiCsMcipr4" target="_blank" rel="external">Intro and Skill Logic - Alexa Skills w/ Python and Flask-Ask Part 1</a><br><a href="http://flask-ask.readthedocs.io/en/latest/responses.html#automatic-handling-of-plaintext-and-ssml" target="_blank" rel="external">Flask Building Responses</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> IoT </category>
            
        </categories>
        
        
        <tags>
            
            <tag> IoT </tag>
            
            <tag> Alexa </tag>
            
            <tag> 物联网 </tag>
            
            <tag> Echo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Machine Translation]]></title>
      <url>http://www.shuang0420.com/2017/05/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的 guest lecture 笔记。重点讲机器翻译传统 Noise Channel 模型以及目前流行的几个神经网络模型。<br><a id="more"></a></p>
<p>一张图看机器翻译历史。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/history.jpg" class="ful-image" alt="history.jpg"></p>
<h1 id="Noise-Channel"><a href="#Noise-Channel" class="headerlink" title="Noise Channel"></a>Noise Channel</h1><p>我们想要一个 p(e|f) 的模型，f 代表源语言(confusing foreign sentence)，e 代表目标语言(possible English translation)，noise channel 前面讲过了，如下图<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/noisy%20channel%20MT.png" class="ful-image" alt="noisy%20channel%20MT.png"></p>
<p>$$<br>  \begin{aligned}<br>  \hat e &amp; = argmax_e p(e|f) \\<br>   &amp; = argmax_e {p(e) p(f|e) \over p(f)} \\<br>   &amp; = argmax_e p(e) p(f|e) \\<br>  \end{aligned}<br>$$</p>
<p>其中</p>
<ul>
<li>p(e): language model<br>判断产生的句子是否通顺，是否符合语法，是否地道(fluent, grammatical, idiomatic)<br>一般会用 ngram 模型</li>
<li>p(f|e): translation model/channel model<br>其实是一个翻转的翻译概率(reverse translation probability)，看把目标语言(English sentence)翻译成源语言(foreign sentence)的概率，通常用 MLE 估计，需要注意泛化问题<br>保证足够多的翻译可能</li>
</ul>
<p>p(e) language model 的问题很好解决，见<a href="http://www.shuang0420.com/2017/02/24/NLP%20笔记%20-%20Language%20models%20and%20smoothing/">NLP 笔记 - Language models and smoothing</a>，难点是怎样确定 p(f|e)，第一个想法自然是直接用 MLE，然而问题是句子实在太多，这种方法很难泛化。</p>
<p>$$P(f|e)={count(f,e) \over count(e)}$$</p>
<p>所以想到的是能不能从 lexical 层面着手，先进行词对词的翻译，然后再进行词的排序，也就是下面要讲的 Lexical Translation Model。</p>
<h2 id="Lexical-Translation"><a href="#Lexical-Translation" class="headerlink" title="Lexical Translation"></a>Lexical Translation</h2><p>我们怎样翻译一个单词？最简单粗暴的方法当然是从字典(dictionary)里找它！然而一个单词可能有不同的 sense/registers/inflections，也就会有不同的翻译，怎么办？用单词频率来计算 MLE p(e|f,m)<br>其中，</p>
<ul>
<li>e: 完整的目标句(English sentence)<br>e = {$e_1, e_2, … ,e_m$}</li>
<li>f: 完整的源句(Foreign sentence)<br>f = {$f_1, f_2, …, f_n$}</li>
</ul>
<p>我们作出以下假设</p>
<ul>
<li>e 的每个单词 $e_i$ 从 f 的某个单词(exactly one word)产生</li>
<li>我们需要找词与词之间的对应关系，才能进行词的翻译，把对应关系看做语料里的隐变量(latent alignment)，用 $a_i$ 表示 $e_i$ 来自于 $f_{e_i}$ 这个对应关系</li>
<li>给定对齐 a，翻译决策是条件独立(conditionally independent)的，仅依赖于 aligned source word $f_{e_i}$</li>
</ul>
<p>于是就有下面这个公式<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/lexical%20translation.png" class="ful-image" alt="lexical%20translation.png"></p>
<h2 id="Representing-Word-Alignment"><a href="#Representing-Word-Alignment" class="headerlink" title="Representing Word Alignment"></a>Representing Word Alignment</h2><p><strong>如何表示单词的对齐？</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/alignment1.png" class="ful-image" alt="alignment1.png"></p>
<p>上图的表示就是 $a = (1,2,3,4)^T$</p>
<h3 id="Reorder"><a href="#Reorder" class="headerlink" title="Reorder"></a>Reorder</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/reorder.png" class="ful-image" alt="reorder.png">
<p>当然在翻译过程中单词的顺序可能会被打乱，上图的表示就是 $a = (3,4,2,1)^T$</p>
<h3 id="Word-Dropping"><a href="#Word-Dropping" class="headerlink" title="Word Dropping"></a>Word Dropping</h3><p>有些源语言的单词可能根本不会被翻译</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/word%20dropping.png" class="ful-image" alt="word%20dropping.png">
<p>=&gt; $a = (2,3,4)^T$</p>
<h3 id="Word-Insertion"><a href="#Word-Insertion" class="headerlink" title="Word Insertion"></a>Word Insertion</h3><p>有些时候，翻译过程中我们需要添加单词，然而这些单词必须是可以被解释的，所以通常我们会再源语句上加一个 NULL token</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/word%20insertion.png" class="ful-image" alt="word%20insertion.png">
<p>=&gt; $a = (1,2,3,0,4)^T$</p>
<h3 id="One-to-many-Translation"><a href="#One-to-many-Translation" class="headerlink" title="One-to-many Translation"></a>One-to-many Translation</h3><p>一个源单词可能被翻译成多个目标单词的组合</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/one-to-many%20translation.png" class="ful-image" alt="one-to-many%20translation.png">
<p>=&gt; $a = (1,2,3,4,4)^T$</p>
<h3 id="Many-to-one-Translation"><a href="#Many-to-one-Translation" class="headerlink" title="Many-to-one Translation"></a>Many-to-one Translation</h3><p>多个源单词的组合也可以被翻译成一个目标单词，<strong>然而！</strong> 在 lexical translation 中，并不支持这种翻译</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/many-to-one%20translation.png" class="ful-image" alt="many-to-one%20translation.png">
<p>=&gt; $a = ??? a = (1,2,(3,4)^T)^T ?$</p>
<h2 id="Learning-EM-algorithm"><a href="#Learning-EM-algorithm" class="headerlink" title="Learning - EM algorithm"></a>Learning - EM algorithm</h2><p>怎样来训练得到词对齐呢 P(e|f)？可以用 EM 算法</p>
<ul>
<li>选择 random(or uniform) 的初始参数对模型进行初始化</li>
<li>用现有参数，计算训练数据中每个 target word 的 alignments $p(a_i|e,f)$ 的期望值</li>
<li>计算 MLE，得到更好的参数，对模型进行更新</li>
<li>反复迭代直至收敛</li>
</ul>
<p><strong>1. 假设对齐概率分布是均匀的</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/EM1.png" class="ful-image" alt="EM1.png"><br><strong>2.发现 la 和 the 的对应关系更明显</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/EM2.png" class="ful-image" alt="EM2.png"><br><strong>3.更新参数</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/EM4.png" class="ful-image" alt="EM4.png"><br><strong>4.反复迭代直至收敛</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/EM5.png" class="ful-image" alt="EM5.png"></p>
<p>举一个简单的中译英的例子(pls forgive my poor handwriting : ( )</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/EM_ch.jpg" class="ful-image" alt="EM_ch.jpg">
<h1 id="IBM-Model1"><a href="#IBM-Model1" class="headerlink" title="IBM Model1"></a>IBM Model1</h1><p>IBM Model 1 是最为基本的翻译模型，也是一个最简单的基于词汇的翻译模型，除了上面提到的假设外，该模型还做出了其他假设</p>
<ul>
<li>m 个词的对齐决策是相互独立的</li>
<li>每个 $a_i$ 在所有源单词和 NULL 上的对齐分布是 uniform 的</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/IBM%20Model1.png" class="ful-image" alt="IBM%20Model1.png">
<p>得到翻译概率后<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/IBM%20Model1%202.png" class="ful-image" alt="IBM%20Model1%202.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/IBM%20Model1%203.png" class="ful-image" alt="IBM%20Model1%203.png"></p>
<h1 id="Log-linear-model"><a href="#Log-linear-model" class="headerlink" title="Log-linear model"></a>Log-linear model</h1><p>还有一种模型是把上面提到的各种概率/模型作为特征，用对数线性模型(如逻辑回归，最大熵模型等)将各种特征结合起来，利用最小错误率调参等方法照到合适的特征权重。其实就是基于各种特征，看哪种翻译组合更好的问题。这是后来整理阿里巴巴骆卫华讲座内容时看到的方法。除了 noisy model 提到的对齐模型和语言模型，讲座还讲到了调序模型，调序模型也就是把两个词/短语之间是否要换序的问题看做二分类问题，如下<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/reordermodel.jpg" class="ful-image" alt="reordermodel.jpg"></p>
<h1 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h1><h2 id="Phrase-based-MT"><a href="#Phrase-based-MT" class="headerlink" title="Phrase-based MT"></a>Phrase-based MT</h2><p>基于词的翻译模型没有考虑上下文，也不支持 many-to-one 的对齐，基于短语的翻译模型可以解决这个问题。短语模型需要引入多一层的隐含变量，叫做 source segmentation。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/phrase_based_MT.png" class="ful-image" alt="phrase_based_MT.png"></p>
<img class="ful-image" alt="phrase_based%20SMT">
<p>短语模型首先需要确定短语，注意短语既不能超过源语言的范围也不能超过目标语言的范围，可以用 MLE 计算源短语与目标短语的共现关系，同时要考虑的是源端到目标端之间是多对多的关系，源语言 -&gt; 目标语言可能有多种翻译，目标语言 -&gt; 源语言也可能有多种翻译，所以需要计算一个双向的概率。由于有些短语出现的概率非常低，所以可以把短语概率退化成词的概率来进行计算，如下<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/phrase%20model.jpg" class="ful-image" alt="phrase%20model.jpg"></p>
<p>也就是 <strong>big house 大房子</strong> 的概率可以用 lex=p(大|big)p(房子|house) 来进行计算。</p>
<p>之后将正向概率和反向概率合并，就可以得到短语表。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/phrase%20model2.jpg" class="ful-image" alt="phrase%20model2.jpg"></p>
<h2 id="Alignment-Priors"><a href="#Alignment-Priors" class="headerlink" title="Alignment Priors"></a>Alignment Priors</h2><p>初始化参数时我们假定对齐概率的分布是 uniform 的，然而事实上我们可以加个先验概率<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/alignment%20priors.png" class="ful-image" alt="alignment%20priors.png"></p>
<h2 id="Syntactic-structure"><a href="#Syntactic-structure" class="headerlink" title="Syntactic structure"></a>Syntactic structure</h2><p>加入句法结构，也可以改善结果<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Machine%20Translation/syntactic%20structure.png" class="ful-image" alt="syntactic%20structure.png"></p>
<h1 id="Neuron-models"><a href="#Neuron-models" class="headerlink" title="Neuron models"></a>Neuron models</h1><p>用神经网络来做翻译模型需要考虑的问题</p>
<ul>
<li><strong>How to represent inputs and outputs?</strong><br>input 的表示可以用 one-hot vector 或者 distributed representations(通常 word vector)</li>
<li><strong>Neural architecture?</strong><br>How many layers? (Requires non-linearities to improve capacity!)<br>How many neurons?<br>Recurrent or not?<br>What kind of non-linearities?</li>
</ul>
<p>详细戳 <a href="http://www.shuang0420.com/2017/07/10/NLP%20笔记%20-%20Machine%20Translation-Neuron%20models/">NLP 笔记 - Neural Machine Translation</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine translation </tag>
            
            <tag> 机器翻译 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP笔记 - NLU之意图分类]]></title>
      <url>http://www.shuang0420.com/2017/04/27/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>阿里巴巴水德的关于意图分类的讲座笔记。<br><a id="more"></a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="自然语言理解-NLU"><a href="#自然语言理解-NLU" class="headerlink" title="自然语言理解(NLU)"></a>自然语言理解(NLU)</h2><blockquote>
<p><strong>Natural language understanding (NLU)</strong> is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem. [1]</p>
</blockquote>
<p>从概念定义可以得出的信息</p>
<ul>
<li>属于 NLP 的一个分支</li>
<li>属于人工智能的一个部分</li>
<li>用来解决机器理解人类语言的问题</li>
<li>属于人工智能的核心难题</li>
</ul>
<p>补充一句，什么是 AI-complete 或 AI-hard 问题？其实就是说解决这个问题的难度等同于让计算机和人类一样智能，或者说实现它就相当于实现了 strong AI，基本标志 AI 革命的完美结束。</p>
<h2 id="语义表示-Semantic-representation"><a href="#语义表示-Semantic-representation" class="headerlink" title="语义表示(Semantic representation)"></a>语义表示(Semantic representation)</h2><p><strong>语义表示(Semantic representation)</strong> 有三种典型形式:</p>
<ul>
<li><strong>分布语义表示（distributional semantics representation)</strong><br>​    把语义表示成一个向量，如 word2vec、LSA、LDA 及各种神经网络模型(如 LSTM)<br>​    基于Harris的分布假设：semantically similar words occur in similar contexts<br>​    对人机交互而言，这种表示方法缺少一个细节性/可解释的表示，不能说出第 n 维表示什么<br>​    <a href="http://www.shuang0420.com/2017/03/21/NLP%20笔记%20-%20再谈词向量/">NLP 笔记 - 再谈词向量</a></li>
<li><strong>模型论语义表示（model-theoretic semantics representation）</strong><br>​    把自然语言映射成逻辑表达式(logic form)<br>​    在计算方法上，典型的就是构建一个semantic parser<br>​    难度比较大，见<a href="http://www.shuang0420.com/2017/04/07/NLP%20笔记%20-%20Meaning%20Representation%20Languages/">NLP 笔记 - Meaning Representation Languages</a>的一阶谓词演算和 lambda reduction。<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/1.jpg" class="ful-image" alt="1.jpg"></li>
<li><strong>框架语义表示（frame semantics representation）</strong><br>​    这种表示方法对人机交互非常有帮助<br>​    比如说 “订一张上海飞北京的头等舱，下午5点出发，国航的”，把语义用一个frame表示出来，如图所示：<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/2.jpg" class="ful-image" alt="2.jpg">
</li>
</ul>
<p>在智能语音交互中，普遍采用<strong>frame</strong>语义表示，比如飞机票，第一层是 domain，确定是 flight_ticket 这一领域，下一层是这一领域下的 intent，比如说 search_flight_ticket，最下面一层是 intent 下的 slots。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/3.jpg" class="ful-image" alt="3.jpg"><br>自然语言理解的核心过程，第一步就是对 domain/intent 分类，然后接着对 slot 进行填充。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/4.jpg" class="ful-image" alt="4.jpg"></p>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>来理解一下上面所说到的 domain 和 intent 的概念。</p>
<ul>
<li>domain<br>领域，同一类型的数据或者资源，以及围绕这些数据或资源提供的服务；<br>比如“地图”，“酒店”，“飞机票”、“火车票”等；<br>领域的目的其实是为了界定要解的 intent 范围，因为泛领域的 NLU 目前还做不到</li>
<li>intent<br>意图，对于领域数据的操作，表示用户想要完成的任务，一般以动宾短语来命名；<br>比如飞机票领域中，有“查询机票”、“退机票”等意图；</li>
</ul>
<p>然后就到了本篇重点，<strong>意图分类(intent classification)</strong>，意图分类是一个典型的文本分类问题，所有传统的分类方法都可以使用，比如SVM/Decision Tree/Maximum Entropy 等等，数学形式来表示就是给定一个标注数据集合，$U={(u_1,c_1),…,(u_n,c_n)}$，其中，$c_i \in C$ 是具体的 intent，$u_i$ 是输入的 utterance，求解目标是<br>$$c_k = argmax_{c \in C}p(c|u_k)$$</p>
<h1 id="主要难点"><a href="#主要难点" class="headerlink" title="主要难点"></a>主要难点</h1><h2 id="语言多样性"><a href="#语言多样性" class="headerlink" title="语言多样性"></a>语言多样性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">• 我要听大王叫我来巡山</div><div class="line">• 给我播大王叫我来巡山</div><div class="line">• 我想听歌大王叫我来巡山</div><div class="line">• 放首大王叫我来巡山</div><div class="line">• 给唱一首大王叫我来巡山</div><div class="line">• 放音乐大王叫我来巡山</div><div class="line">• 放首歌大王叫我来巡山</div><div class="line">• 给大爷来首大王叫我来巡山</div></pre></td></tr></table></figure>
<h2 id="语言歧义性"><a href="#语言歧义性" class="headerlink" title="语言歧义性"></a>语言歧义性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">• 我要去拉萨</div><div class="line">  – 火车票？</div><div class="line">  – 飞机票？</div><div class="line">  – 音乐？</div><div class="line">  – 还是查找景点？</div></pre></td></tr></table></figure>
<p>解决语言歧义性的方法一般有下面 3 种</p>
<ol>
<li>选一个 top intent，通常可能就选最常见的 intent</li>
<li>给一个 intent list 让用户来选</li>
<li>综合上下文信息来确定一个最优的 intent，这是最好的方法</li>
</ol>
<h2 id="语言鲁棒性"><a href="#语言鲁棒性" class="headerlink" title="语言鲁棒性"></a>语言鲁棒性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">• 错字</div><div class="line">  – 大王叫我来新山</div><div class="line">• 多字</div><div class="line">  – 大王叫让我来巡山</div><div class="line">• 少字</div><div class="line">  – 大王叫我巡山</div><div class="line">• 别称</div><div class="line">  – 熊大熊二（指熊出没）</div><div class="line">• 不连贯</div><div class="line">  – 我要看那个恩花千骨</div><div class="line">• 噪音</div><div class="line">  – 全家只有大王叫我去巡山咯</div><div class="line">• …</div></pre></td></tr></table></figure>
<h2 id="知识依赖"><a href="#知识依赖" class="headerlink" title="知识依赖"></a>知识依赖</h2><p>要知道语言是对现实世界的描述，很多词是有多种含义的，如果没有对现实世界的知识会难以分类。看下面的例子，大鸭梨可以是水果也可以是餐厅，七天可以是天数也可以是酒店。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">– 大鸭梨</div><div class="line">– 七天</div><div class="line">– 总参</div><div class="line">– 天气预报</div><div class="line">– 晚安</div></pre></td></tr></table></figure></p>
<h2 id="上下文依赖-context"><a href="#上下文依赖-context" class="headerlink" title="上下文依赖(context)"></a>上下文依赖(context)</h2><p>上下文的概念包含了很多内容，比如说</p>
<ul>
<li>对话上下文<br>​    多轮对话</li>
<li>设备上下文<br>​    指硬件设备，如手机/电视/汽车/…</li>
<li>应用上下文<br>​    用户在哪个 app 里进行对话</li>
<li>用户画像<br>​    用户的个性化信息，尤其是地理位置等</li>
<li>……</li>
</ul>
<p>如下面两段对话，上下文不同，宁夏的含义也就不同。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">U：买张火车票</div><div class="line">A：请问你要去哪里？</div><div class="line">U：宁夏</div><div class="line">U：来首歌听</div><div class="line">A：请问你想听什么歌？</div><div class="line">U：宁夏</div></pre></td></tr></table></figure></p>
<h1 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h1><ul>
<li>基于规则（rule-based）<br>CFG<br>JSGF<br>……</li>
<li>传统机器学习方法<br>SVM<br>ME<br>……</li>
<li>深度学习方法<br>CNN<br>RNN/LSTM<br>……</li>
<li>融合规则和深度学习的方法</li>
</ul>
<h2 id="基于规则的意图分类"><a href="#基于规则的意图分类" class="headerlink" title="基于规则的意图分类"></a>基于规则的意图分类</h2><p>实际上是<a href="http://www.shuang0420.com/2017/02/28/NLP%20笔记%20-%20Syntax%20and%20Parsing/">基于上下文无关语法(CFG)</a>，以上面提到的飞机票领域为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">FRAME: flight_ticket</div><div class="line">NETS:</div><div class="line">[search_flight_ticket]</div><div class="line">[refund_flight)ticket]</div><div class="line"></div><div class="line">[search_flight_ticket]</div><div class="line">	[flight_range]						[flight_ticket]</div><div class="line">	[from]			[to]				  (的 (飞机票))</div><div class="line">		(从 [city]) (去 [city])</div><div class="line"></div><div class="line"></div><div class="line">[city]</div><div class="line">(北京)</div><div class="line">(上海)</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>从北京到上海的飞机票<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/5.jpg" class="ful-image" alt="5.jpg"></p>
<h2 id="基于统计模型的意图分类"><a href="#基于统计模型的意图分类" class="headerlink" title="基于统计模型的意图分类"></a>基于统计模型的意图分类</h2><p>给定输入 utterance u 和类别 c，我们要求的是 P(c|u)，核心问题就是:</p>
<ul>
<li>如何表示 u，也就是 text representation</li>
<li>如何学习 P(c|u)，也就是 classifer</li>
</ul>
<h3 id="Text-Representation"><a href="#Text-Representation" class="headerlink" title="Text Representation"></a>Text Representation</h3><ul>
<li>Bag of Words (BOW)<br>– 优点：简单<br>– 缺点：没有考虑语言结构，相似关系等</li>
<li>Hand-crafted features<br>– 优点：精准<br>– 缺点：领域依赖，扩展性差</li>
<li>Learned feature representation<br>– 优点：能够学到所有相关的信息<br>– 缺点：需要学习</li>
</ul>
<h3 id="Classifier"><a href="#Classifier" class="headerlink" title="Classifier"></a>Classifier</h3><p>两类模型任君选择</p>
<ul>
<li>生成式模型Generative (joint) models，计算 P(c,u)<br>– Naive Bayes<br>– HMM<br>– …</li>
<li>判别式模型Discriminative (conditional) models, 计算 P(c|u)<br>– logistic regression<br>– maximum entropy<br>– conditional random fields<br>– support vector machines<br>– …</li>
</ul>
<h3 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h3><p>一种简单的实现方案，首先用 bag of words 提取基本特征，接着人工定义规则来提取一些高质量的特征(提取意图词，过滤 stopwords 等)，然后将这些特征用 one-hot 或者 tf-idf 方式表示为向量，再将特征向量喂给 svm 分类器。如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">1. Text Representation</div><div class="line">	– Bag of Words (BOW)</div><div class="line">	 	&gt;&gt; unigram</div><div class="line">		&gt;&gt; bigram</div><div class="line">		&gt;&gt; pos</div><div class="line">	– Hand-crafted features</div><div class="line">		&gt;&gt; 意图词，用来表达意图的有限词集合，比如“飞机票/机票/餐厅/美食/火车…”</div><div class="line">		&gt;&gt; 无效词比例，定义一个有效词的集合，然后计算 utterance 中无效词的比例</div><div class="line">		&gt;&gt; pattern特征，比如“从[city]”，“去[city]”，…</div><div class="line">		&gt;&gt; …</div><div class="line">	– feature vector</div><div class="line">		&gt;&gt; One-hot</div><div class="line">		&gt;&gt; TF-IDF</div><div class="line"></div><div class="line">2. Classifier</div><div class="line">	– svm, 比如可以采用开源的libsvm/liblinear实现</div></pre></td></tr></table></figure></p>
<h2 id="基于深度学习的意图分类"><a href="#基于深度学习的意图分类" class="headerlink" title="基于深度学习的意图分类"></a>基于深度学习的意图分类</h2><p>两种典型策略</p>
<ul>
<li>RNN（recurrent neutral network）<br>序列化、有记忆模型</li>
<li>CNN（convolutional neutral network）<br>非序列化、无记忆模型</li>
</ul>
<p>一般我们把 RNN/CNN 是特征学习方法，分类器可以有多种选择。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/6.jpg" class="ful-image" alt="6.jpg"></p>
<h3 id="RNN-Recurrent-Neutral-Network"><a href="#RNN-Recurrent-Neutral-Network" class="headerlink" title="RNN(Recurrent Neutral Network)"></a>RNN(Recurrent Neutral Network)</h3><p>从 language model 理解，<br>$$<br>\begin{aligned}<br>p(W_1=w_1, W_2=w_2,…,W_{L+1}=stop)  &amp; = p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)…p(w_n|w_1,w_2,…w_{n-1}) \\<br>   &amp; = (\prod^L_{l=1}p(W_l=w_l|W_{1:l-1}=w_{1:l-1}))p(W_{L+1}=stop|W_{1:L}=w_{1:L}) \\<br>   &amp; = (\prod^L_{l=1}p(w_l|history_l))p(stop|history_L)<br>  \end{aligned}<br>  $$</p>
<p>其实也就是下图的<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/7.jpg" class="ful-image" alt="7.jpg"></p>
<p>RNN 最后要加一层，一般是 softmax 分类。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/8.jpg" class="ful-image" alt="8.jpg"></p>
<p>由于 RNN 的训练存在梯度消失/梯度爆炸问题，实际中往往采用 LSTM/GRU 等结构，Classifier 采用 softmax 函数，和网络一起训练</p>
<h3 id="CNN-Convolutional-neutral-network"><a href="#CNN-Convolutional-neutral-network" class="headerlink" title="CNN(Convolutional neutral network)"></a>CNN(Convolutional neutral network)</h3><p>之前讲过啦，见<a href="http://www.shuang0420.com/2016/08/05/实习总结之%20sentence%20embedding/">实习总结之 sentence embedding</a>，一张图简单回顾。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/9.jpg" class="ful-image" alt="9.jpg"><br>这里要说的是怎么将其用于意图分类。如果能够融入知识(如下图的语义标签)，将 word vector 和 knowledge vector 结合起来，效果可能会有所提升。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/10.jpg" class="ful-image" alt="10.jpg"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20NLU%E4%B9%8B%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB/11.jpg" class="ful-image" alt="11.jpg">
<p>阿里巴巴做了实验，发现在他们自己 4k+ 的测试集上能带来 3% 的效果提升，并且，越是知识依赖严重的领域，效果越是明显，比如说在音乐、地图领域的提升比天气领域的提升明显很多。</p>
<h2 id="融合规则和深度学习的系统"><a href="#融合规则和深度学习的系统" class="headerlink" title="融合规则和深度学习的系统"></a>融合规则和深度学习的系统</h2><ul>
<li>规则解决哪些问题？<br>冷启动<br>解bug<br>解业务特有意图</li>
<li>模型解决哪些问题？<br>海量数据下把问题解决的更加深入和彻底<br>解通用的意图</li>
</ul>
<h1 id="产品"><a href="#产品" class="headerlink" title="产品"></a>产品</h1><p>意图分类在阿里产品中的使用，如汽车/阿里云/YunOS手机助理/支付宝/天猫魔盒/机器人等等。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Information Extraction </tag>
            
            <tag> Intent Classification </tag>
            
            <tag> NLU </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络 CNN 笔记(高级篇)]]></title>
      <url>http://www.shuang0420.com/2017/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0(%E9%AB%98%E7%BA%A7%E7%AF%87)/</url>
      <content type="html"><![CDATA[<p>对应 <a href="http://www.shuang0420.com/2017/03/10/深度学习知识框架/">深度学习知识框架</a>，学习更先进的 CNN，包括 AlexNet, VGG, GooLeNet, ResNet, DeepFace, U-Net。<br><a id="more"></a></p>
<h1 id="AlexNet-现代深度卷积网络起源"><a href="#AlexNet-现代深度卷积网络起源" class="headerlink" title="AlexNet: 现代深度卷积网络起源"></a>AlexNet: 现代深度卷积网络起源</h1><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.</a></p>
<p>AlexNet 在 2012 年提出，是 LeNet 更深更宽的版本。AlexNet 由 5 个卷积层，以及部分卷积层后跟着的 max-pooling 层，和 3 个全连接层，还有最后的 1000-way 的 softmax 层组成，共有 6000 万个参数(用全连接层的参数来估算)和 650,000 个神经元。为了加快训练速度，使用了线性修正单元 <strong>ReLU</strong> 和 <strong>多 GPU 加速并行</strong>；为了减少全连接层的过拟合，采用了 <strong>dropout, data augmentation 和 LRN</strong>。网上可能会看到两个版本的 AlexNet，因为在 ImageNet LSVRC-2010 大赛后，论文作者又输入了该模型的一个变体，参加了 ILSVRC-2012。AlexNet 确立了深度卷积网络在计算机视觉的统治地位。</p>
<p>来看一下总体架构：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/1.jpg" class="ful-image" alt="1.jpg">
<p>结构及参数计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">[227x227x3] INPUT</div><div class="line">[55x55x96] CONV1: 96 11x11 filters at stride 4, pad 0</div><div class="line">--- OUTPUT VOLUME SIZE = (227-11)/4+1 = 55 ---</div><div class="line">--- TOTAL NUMBER OF PARAMETERS = 11*11*3*96 = 35K ---</div><div class="line">[27x27x96] MAX POOL1: 3x3 filters at stride 2</div><div class="line">--- OUTPUT VOLUME SIZE = (55-3)/2+1 = 27 ---</div><div class="line">[27x27x96] NORM1: Normalization layer</div><div class="line">[27x27x256] CONV2: 256 5x5 filters at stride 1, pad 2</div><div class="line">[13x13x256] MAX POOL2: 3x3 filters at stride 2</div><div class="line">[13x13x256] NORM2: Normalization layer</div><div class="line">[13x13x384] CONV3: 384 3x3 filters at stride 1, pad 1</div><div class="line">[13x13x384] CONV4: 384 3x3 filters at stride 1, pad 1</div><div class="line">[13x13x256] CONV5: 256 3x3 filters at stride 1, pad 1</div><div class="line">[6x6x256] MAX POOL3: 3x3 filters at stride 2</div><div class="line">--- TOTAL NUMBER OF PARAMETERS = 6*6*256 ---</div><div class="line">[4096] FC6: 4096 neurons</div><div class="line">--- TOTAL NUMBER OF PARAMETERS = 4096*36*256=37,748,736 ---</div><div class="line">[4096] FC7: 4096 neurons</div><div class="line">--- TOTAL NUMBER OF PARAMETERS = 4096*4096=16,777,216 ---</div><div class="line">[1000] FC8: 1000 neurons (class scores)</div><div class="line">--- TOTAL NUMBER OF PARAMETERS = 1000*4096=4,096,000 ---</div><div class="line">=== TOTAL NUMBER OF PARAMETERS =&gt; 37,748,736 + 16,777,216 + 4,096,000 =&gt; 6000W ===</div></pre></td></tr></table></figure></p>
<p>上图明确显示了两个GPU之间的职责划分。一个 GPU 运行图中顶部的层次部分，另一个 GPU 运行图中底部的层次部分。GPU 之间仅在某些层互相通信。</p>
<p>为了防止过拟合，AlexNet 用了以下三种技术</p>
<ul>
<li>数据增强<br>a. 由生成图像转化和水平反射组成 如从 256×256 的图像中随机提取 224×224 的碎片(以及水平反射的镜像)，并在这些提取的碎片上训练网络，相当于增加了 ((256-224)^2)*2=2048 倍的数据量<br>b. 改变训练图像中RGB通道的强度 在整个 ImageNet 训练集的 RGB 像素值集合中执行PCA。对每个训练图像，成倍增加已有主成分，比例大小为对应特征值乘以一个从均值为 0，标准差为 0.1 的高斯分布中提取的随机变量</li>
<li>Dropout<br>以一定的概率(如 0.5)将每个隐层神经元的输出设置为零。AlexNet 主要是最后几个全连接层使用了 dropout</li>
<li>LRN 层<br>LRN (Local Response Normalization 局部响应归一化)，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</li>
</ul>
<p>一些细节：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Details/Retrospectives:</div><div class="line">- first use of ReLU</div><div class="line">- used Norm layers (not common anymore)</div><div class="line">- heavy data augmentation</div><div class="line">- dropout 0.5</div><div class="line">- batch size 128</div><div class="line">- SGD Momentum 0.9</div><div class="line">- Learning rate 1e-2, reduced by 10</div><div class="line">manually when val accuracy plateaus</div><div class="line">- L2 weight decay 5e-4</div><div class="line">- 7 CNN ensemble: 18.2% -&gt; 15.4%</div></pre></td></tr></table></figure></p>
<p>AlexNet 的学习过程，用随机梯度下降法和一批大小为 128、动力为 0.9、权重衰减为 0.0005 的样例来训练网络。用一个均值为 0、标准差为 0.01 的高斯分布初始化每一层的权重，用常数 1 初始化第 2、第 4 和第 5 个卷积层以及全连接隐层的神经元偏差，在其余层用常数 0 初始化神经元偏差。对所有层都使用了相等的 learning rate，并在整个训练过程中手动调整。当 validation error 在当前 learning rate 下不再提高时，就将 learning rate 除以 10。learning rate 初始化为 0.01，在终止前降低三次。训练该网络时大致将这 120 万张图像的训练集循环了 90 次，在两个 NVIDIA GTX 580 3GB GPU 上花了五到六天。</p>
<p>AlexNet 在 ILSVRC-2010 大赛上实现了top-1测试集误差率 <strong>37.5%</strong>，top-5测试集误差率 <strong>17.0%</strong> ，在 ILSVRC-2012大赛上实现了测试集误差率 top-5 <strong>15.3%</strong>。</p>
<h1 id="VGG-AlexNet增强版"><a href="#VGG-AlexNet增强版" class="headerlink" title="VGG: AlexNet增强版"></a>VGG: AlexNet增强版</h1><p><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="external">Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).</a></p>
<p>VGG 相当于 AlexNet 的增强版，有着结构简单且深度、精度增强的优势。这里一个 tricky 的地方是，VGG 不是模型的缩写，而是 Visual Geomety Group，也就是牛津大学计算机视觉组(Department of Engineering Science, University of Oxford)的简称。</p>
<p>VGG 与 AlexNet 最鲜明的对比是卷积层、卷积核设计的变化。VGGNet 探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠 3x3 的小型卷积核和 2x2 的最大池化层，成功构筑了 16~19 层深的卷积神经网络。</p>
<ul>
<li>结构变化: <strong>卷积层</strong>转化为<strong>卷积群</strong>(如下图)</li>
<li>=&gt; 参数变化: 60m =&gt; 138m</li>
<li>=&gt; 识别率变化(TOP5): 15.3% =&gt; 7.3%</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/2.jpg" class="ful-image" alt="2.jpg">
<p>VGGNet 有 5 个卷积群，每一群内有 2~3 个卷积层，每个群连接一个 max-pooling 层来缩小图片尺寸。每个卷积群内的卷积核数量一样，越靠后的卷积群的卷积核数量越多：64 – 128 – 256 – 512 – 512。其中经常出现多个完全一样的 3x3 的卷积层堆叠在一起的情况，为什么呢？看下图，可以发现两个 3x3 的卷积层串联其实相当于 1 个 5x5 的卷积层，也就是说一个像素会跟周围 5x5 的像素产生关联。类似的，3 个 3x3 的卷积层串联的效果则相当于 1 个 7x7 的卷积层。<strong>为什么要堆叠卷积层而不直接用 7x7 的呢？</strong>一是因为 3 个串联的 3x3 的卷积层的参数比 1 个 7x7 的卷积层更少，二是因为 3 个 3x3 的卷积层比 1 个 7x7 的卷积层有更多的非线性变换（前者可以用三次 ReLU 激活函数，而后者只有一次），对特征的学习能力更强。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/11.jpg" class="ful-image" alt="11.jpg">
<p><strong>网络结构及参数计算:</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/3.jpg" class="ful-image" alt="3.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/6.jpg" class="ful-image" alt="6.jpg">
<p><strong>VGG的作用：</strong></p>
<ul>
<li>结构简单：同 AlexNet 结构类似，都是卷积层、池化层、全连接层的组合</li>
<li>性能优异：同 AlexNet 提升明显，同 GoogleNet, ResNet 相比，表现相近</li>
<li>选择最多的基本模型，方便进行结构的优化、设计，SSD, RCNN，等其他任务的基本模型(base model)</li>
</ul>
<h1 id="GooLeNet-多维度识别"><a href="#GooLeNet-多维度识别" class="headerlink" title="GooLeNet: 多维度识别"></a>GooLeNet: 多维度识别</h1><p><a href="https://arxiv.org/pdf/1409.4842v1.pdf" target="_blank" rel="external">Szegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.</a></p>
<p>GoogLeNet，ILSVRC 2014 winner，TOP5 的错误率为 <strong>6.7%</strong>。特点是结构复杂、多分辨率融合。GooLeNet 主要目标是找到最优的稀疏结构单元，也就是 Inception module。Inception 结构是将不同的卷积层通过并联的方式结合在一起，它主要改进了网络内部计算资源的利用率，让我们能在固定计算资源下增加神经网络深度和宽度，另外，Inception 结构还遵循了 Hebbian 原则并增加了多尺度处理。</p>
<p>先看一下 GoogLeNet 的<strong>整体结构</strong>，其实是由 9 个相似的 Inception module 构成的。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/4.jpg" class="ful-image" alt="4.jpg">
<p><strong>Inception 结构发展</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/5.jpg" class="ful-image" alt="5.jpg">
<blockquote>
<p> All we need is to find the optimal local construction and to repeat it spatially.</p>
</blockquote>
<p>GooLeNet 的 Inception 对特征图进行了三种不同的卷积(1x1, 3x3, 5x5)来提取多个尺度的信息，也就是提取更多的特征。举个例子，一张图片有两个人，近处一个远处一个，如果只用 5x5，可能对近处的人的学习比较好，而对远处那个人，由于尺寸的不匹配，达不到理想的学习效果，而采用不同卷积核来学习，相当于融合了不同的分辨率，可以较好的解决这个问题。把这些卷积核卷积后提取的 feature map (再加多一个 max pooling 的结果)进行聚合操作合并(在输出通道数这个维度上聚合)作为输出，也就是左图的结构，会发现这样结构下的参数暴增，耗费大量的计算资源。</p>
<p>所以有了右图的改进方案，在 3x3，5x5 之前，以及 pooling 以后都跟上一个 1x1 的卷积用以降维，就可以在提取更多特征的同时，大量减少参数，降低计算量。1x1 的卷积核性价比很高，很小的计算量就能增加一层特征变换和非线性化(如果后面接 ReLU 等 activation layer)，另外，这也是一种降维(dimension reductionality)的方式，可以减少过拟合。具体的作用见<a href="http://www.notehub.cn/2016/10/15/algo/ml/OnebyOne_Convolution/" target="_blank" rel="external">1x1大小的卷积层的作用</a></p>
<p>Inception v1 有 22 层，但只有 500w 的参数量，是 AlexNet 的 1/12。<strong>为什么要减少参数量？</strong>一是因为参数越多，需要喂给模型的数据量就越大，二是因为参数越多，耗费的计算资源就越大。<strong>InceptionNet 为什么参数少而且效果好？</strong>一是因为用平均池化层代替了最后的全连接层，二是因为上面解释的 Inception Module 的作用。</p>
<p>Inception v2 学习了 VGG，用两个 3x3 的卷积替代了 5x5 的大卷积(降低参数&amp;减少过拟合)，并提出了 Batch Normalization 方法，另外根据 BN 对其他部分做了一些调整，比如增大 learning rate，加快学习衰减速度，去除 dropout 减轻L2 正则，去除 LRN，更彻底的对训练样本进行 shuffle 等等。</p>
<p>Inception v3 优化了 Inception Module 的结构，同时引入了 Factorization into small convolutions 思想，把一个较大的二维卷积拆分成两个较小的一维卷积，如将 7x7 卷积拆成 1x7 卷积核 7x1 卷积，进一步节约参数减轻过拟合，此外，Inception v3 还增加了一层非线性扩展模型表达能力。</p>
<p>Inception v4 则结合了微软的 ResNet，在 TOP5 error 上反超了 ResNet 3.1%。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/8.jpg" class="ful-image" alt="8.jpg">
<p>Inception 网络就是由多个上面所说的 inception model 堆叠起来的，Inception model 之间可能再通过 max pulling 减小 feature map，论文作者提出，为了 memory efficiency，最好前几层按正常 CNN 套路来，在深层的时候使用 Inception model。</p>
<p><strong>网络结构图:</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/7.jpg" class="ful-image" alt="7.jpg">
<p>参数总数约为 5m，最后用 average pooling 层代替了全连接层。顺带讲一下<strong>全卷积结构(FCN)</strong>，一般的神经网络是<strong>卷积层(CNN)+全连接层(FC)</strong>，全卷积网络没有全连接层(全连接层一会需要大量的参数，二会引起过拟合)，这样的特点是:</p>
<ul>
<li>输入图片大小无限制</li>
<li>空间信息有丢失</li>
<li>参数更少，表达力更强</li>
<li>不适合分类，适合做数据生成</li>
</ul>
<h1 id="ResNet-机器超越人类识别"><a href="#ResNet-机器超越人类识别" class="headerlink" title="ResNet: 机器超越人类识别"></a>ResNet: 机器超越人类识别</h1><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the…arxiv.org</a></p>
<p>由微软提出，最初灵感出自神经网络层数不断加深导致的训练集上误差增大的问题(Degradation)，也就是随着网络层数增加，准确率会先上升然后达到饱和，再持续增加深度会导致准确率下降的现象。相比于 AlexNet 和 VGG，ResNet 层数更多，训练用了 8GPU，三周完成。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/9.jpg" class="ful-image" alt="9.jpg">
<p>下图描述了 ResNet 的结构特性，ResNet 允许原始输入信息传输到后面的层中。特征图经过卷积层和非线性层后和之前的特征图进行数据融合，融合结果再往后面的网络推进。看最右边的部分，发现网络有很多“支线”来将输入直接连到后面的层，使得后面的层可以直接学习残差，这种结构也被称为 shortcut 或者 skip connections，它能够保护信息的完整性，整个网络只需要学习输入、输出差别的那一部分，简化了学习目标和难度。具体来说，假定某段神经网络的输入是 x，期望输出是 H(x)，如果直接把输入 x 传到输出作为初始结果，那么此时需要学习的目标就是 F(x)=H(x)-x，这就是一个 ResNet 的残差学习单元(Residual Unit)，ResNet 的学习目标不再是学习一个完整的输出，而是学习输出和输入的差别，也就是残差。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/10.jpg" class="ful-image" alt="10.jpg">
<p><strong>为什么 ResNet 有效？</strong></p>
<ol>
<li>前向计算：低层卷积网络高层卷积网络信息融合；层数越深，模型的表现力越强</li>
<li>反向计算：导数传递更直接，越过模型，直达各层</li>
</ol>
<p><a href="https://arxiv.org/abs/1602.04485" target="_blank" rel="external">Benefits of depth in neural networks by Matus Telgarsky</a></p>
<h1 id="DeepFace-结构化图片的特殊处理"><a href="#DeepFace-结构化图片的特殊处理" class="headerlink" title="DeepFace: 结构化图片的特殊处理"></a>DeepFace: 结构化图片的特殊处理</h1><p><a href="https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf" target="_blank" rel="external">Taigman Y, Yang M, Ranzato M A, et al. Deepface: Closing the gap to human-level performance in face verification</a></p>
<p>由 Facebook 提出。广义的人脸识别是说看到这个人脸，确定它是谁，而落在应用/产品层面，更多的是 verification，先拿到别人的照片，做一个 model，再拿一张新的照片，来判断是不是这个人，也就是人脸身份确认。</p>
<p>先来看一下人脸图片的数据特点：</p>
<ul>
<li>结构化：所有人脸的组成相似，理论上可以实现对齐</li>
<li>差异化：相同的位置，形貌(appearance)不同</li>
</ul>
<p>DeepFace 在训练神经网络前，使用了<strong>对齐(frontalization)</strong>，论文认为神经网络能够 work 的原因在于一旦人脸经过对齐后，人脸区域的特征就固定在某些像素上了，所以就可以用卷积神经网络来学习特征。之后的 DeepID 和 FaceNet 并没有对齐这个步骤。</p>
<p>传统 CNN 用同一个卷积核对整张图片进行卷积运算，卷积核参数共享，不同局部特性对参数影响相互削弱，达不到最优的效果，对应的解决方法是局部卷积，不同的区域用不同参数。</p>
<p>人脸识别的<strong>基本流程</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">detect -&gt; aligh -&gt; represent -&gt; classify</div></pre></td></tr></table></figure></p>
<p><strong>结构:</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/14.jpg" class="ful-image" alt="14.jpg">
<p>经过 3D 对齐后，形成的图像都是 152×152 的图像，输入到上述网络结构中，该结构的参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- Conv：32 个 11×11×3 的卷积核</div><div class="line">- max-pooling: 3×3， stride=2</div><div class="line">- Conv: 16个9×9 的卷积核</div><div class="line">- Local-Conv: 16 个 9×9 的卷积核，参数不共享</div><div class="line">- Local-Conv: 16 个 7×7 的卷积核，参数不共享</div><div class="line">- Local-Conv: 16 个 5×5 的卷积核，参数不共享</div><div class="line">- Fully-connected: 4096 维</div><div class="line">- Softmax: 4030 维</div></pre></td></tr></table></figure></p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中 Max-pooling 层使得卷积的输出对微小的偏移情况更加鲁棒。但没有用太多的 Max-pooling 层，因为太多的 Max-pooling 层会使得网络损失图像信息。</p>
<p>后面三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<ul>
<li>对齐的人脸图片中，不同的区域会有不同的统计特征，卷积的局部稳定性假设并不存在，所以使用相同的卷积核会导致信息的丢失</li>
<li>不共享的卷积核并不增加抽取特征时的计算量，而会增加训练时的计算量</li>
<li>使用不共享的卷积核，需要训练的参数量大大增加，因而需要很大的数据量，然而这个条件本文刚好满足。</li>
</ul>
<p>全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置的特征之间的相关性。其中，第7层（4096-d）被用来表示人脸。</p>
<p>全连接层的输出可以用于Softmax的输入，Softmax层用于分类。</p>
<p><strong>全局部卷积连接的缺陷：</strong></p>
<ul>
<li>预处理：大量对准，对对准要求高，原始信息可能丢失</li>
<li>卷积参数数量很大，模型收敛难度大，需要大量数据</li>
<li>模型可扩展性差，基本限于人脸计算</li>
</ul>
<h1 id="U-Net-图片生成网络"><a href="#U-Net-图片生成网络" class="headerlink" title="U-Net: 图片生成网络"></a>U-Net: 图片生成网络</h1><p><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf" target="_blank" rel="external">Noh, H., Hong, S. and Han, B., 2015. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1520-1528).</a></p>
<p>通过卷积神经网络生成特殊类型的图片，基本结构 <strong>CONV-FC-CONV</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/15.jpg" class="ful-image" alt="15.jpg">
<p><strong>VGG U-Net:</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/12.jpg" class="ful-image" alt="12.jpg">
<p>主要要理解的概念是<strong>池化-反池化(Pooling-Unpooling)</strong>,<strong>卷积-逆卷积(Convolution-Deconvolution)</strong>。</p>
<p><strong>上采样/反池化</strong>实际是在池化的时候记住原本的位置，然后在上采样的时候对应回去放回原本的位置，前后空间位置保持一致，其他的地方可以直接补 0。而<strong>逆卷积</strong>实际上是有学习能力的上采样，它在生成图像有更好的连贯性，更好的空间表达能力。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/13.jpg" class="ful-image" alt="13.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0%28%E9%AB%98%E7%BA%A7%E7%AF%87%29/16.jpg" class="ful-image" alt="16.jpg">
<blockquote>
<p>参考链接：<br> <a href="http://www.cnblogs.com/payton/articles/6732130.html" target="_blank" rel="external">CNN浅析和历年ImageNet冠军模型解析</a><br> <a href="http://blog.csdn.net/stdcoutzyx/article/details/46776415" target="_blank" rel="external">DeepFace–Facebook的人脸识别</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
            <category> CNN </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Compositional Semantics]]></title>
      <url>http://www.shuang0420.com/2017/04/13/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。主要讲了两种语义分析的方法: Lexicalized CFG 及 CCG，补充介绍了词汇语义学的一些概念。比较基础，网上参考资料较少，具体需要研究相关论文。<br><a id="more"></a></p>
<p><strong>Semantics Road Map</strong></p>
<ol>
<li>Lexical semantics</li>
<li>Vector semantics</li>
<li>Meaning representation languages and semantic roles</li>
<li><strong>Compositional semantics, semantic parsing</strong></li>
<li>Discourse and pragmatics</li>
</ol>
<p>上一章讲了 FOL，它是定义良好表现良好易于理解，然而也存在一些 <strong>Issues</strong></p>
<ul>
<li>“Meanings” of sentences are truth values.</li>
<li>Only <strong>first-order</strong> (no quantifying over predicates).</li>
<li>Not very good for <strong>“fluents”</strong> (time-varying things, real-valued quantities, etc.)</li>
<li>Brittle: <strong>anything</strong> follows from <em>any</em> contradiction(!)</li>
<li><strong>Goedel incompleteness</strong>: “This statement has no proof”!<br>• (Finite axiom sets are incomplete w.r.t. the real world.)</li>
<li><strong>So:</strong> Most systems use its descriptive apparatus (with extensions) but not its inference mechanisms.</li>
</ul>
<h1 id="Stages-of-Semantic-Parsing"><a href="#Stages-of-Semantic-Parsing" class="headerlink" title="Stages of Semantic Parsing"></a>Stages of Semantic Parsing</h1><p>Semantic Analysis/Parsing 的目的是把自然语言转化成机器可以理解的逻辑语言如 FOL</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Input</div><div class="line">- Sentence</div><div class="line"></div><div class="line">Syntactic Analysis</div><div class="line">- Syntactic structure</div><div class="line"></div><div class="line">Semantic Analysis</div><div class="line">- Semantic representation</div></pre></td></tr></table></figure>
<p><strong>Goal:</strong><br>Learn f: sentence -&gt; logical form</p>
<p>Example<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input</div><div class="line">- Javier likes pizza</div><div class="line"></div><div class="line">Output</div><div class="line">- like(Javier, pizza)</div></pre></td></tr></table></figure></p>
<h1 id="Syntax-driven-semantic-analysis"><a href="#Syntax-driven-semantic-analysis" class="headerlink" title="Syntax-driven semantic analysis"></a>Syntax-driven semantic analysis</h1><p><strong>句法驱动的语义分析(Syntax-driven semantic analysis)</strong>仅仅以词典和语法中的静态知识为输入语句指派意义表示(meaning representation)，这是一个<strong>贫瘠的</strong>表示，<strong>上下文独立(context-independent)，推理无关(inference-free)，视野有限</strong>。然而，在某些领域，这样的表示足以产生有用的结果，另一方面，这些表示也可以作为后续处理的输入，进而产生更丰富、更有用的意义表示。</p>
<p><strong>基于组合性原则的语义(Compostitional Semantics)</strong>假定一个句子的意义可以由它的几个部分(subparts)的意义组成(这显然有些例外，如 hot dog, straw man, New York 等)。</p>
<p>要注意的是，这个思想并不是说句子的意义仅仅依赖于句子中的词汇，而是还依赖于句中词汇的<strong>顺序</strong>、词汇所形成的<strong>群组</strong>和词汇之间的<strong>关系</strong>。也就是说，在句法驱动的语义分析中，意义表示的组成是由句法成分和关系来引导的。</p>
<h2 id="给上下文无关语法规则扩充语义"><a href="#给上下文无关语法规则扩充语义" class="headerlink" title="给上下文无关语法规则扩充语义"></a>给上下文无关语法规则扩充语义</h2><p><strong>基本思路</strong>是给 CFG 规则添加语义附着。从 parse tree 开始，使用 FOL 和 lambda 表达式来建立语义，过程</p>
<ol>
<li>Parse the sentence syntactically</li>
<li>Associate some semantics to each word</li>
<li>Combine the semantics of word and non-terminals recursively</li>
<li>Until the root of the sentence</li>
</ol>
<p>最简单的例子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/semantic%20parsing%20eg..png" class="ful-image" alt="semantic%20parsing%20eg..png">
<p>稍复杂的例子，bottom-up</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/5.jpg" class="ful-image" alt="5.jpg">
<p>当然还有一种对应的是从上往下，follow SLP</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/6.jpg" class="ful-image" alt="6.jpg">
<h3 id="歧义"><a href="#歧义" class="headerlink" title="歧义"></a>歧义</h3><p>由于句法和词汇都存在歧义，因此句法驱动的语义分析将产生成倍的歧义意义表示，解决这些歧义并不是语义分析器的工作，而是通过使用领域知识和上下文知识在候选项中进行选择的后续理解处理。当然也可以通过使用鲁棒的词性标注器、介词短语附着机制以及词义排歧原理，以减少歧义表示的数量。</p>
<h2 id="Using-CCG-Steedman-1996"><a href="#Using-CCG-Steedman-1996" class="headerlink" title="Using CCG(Steedman 1996)"></a>Using CCG(Steedman 1996)</h2><p>CCG(Combinatorial Categorial Grammar)的<strong>基本思路</strong>是先将词/短语和类别(categories)配对，然后逐步的对类别进行组合，通过 logical form 来捕捉句子的完整含义。</p>
<blockquote>
<p><strong>CCG</strong> assigns a category to each word and constructs a parse by combining pairs of categories to form an S. Not all pairs of categories can combine. A pair is allowed to combine if one category(e.g. A) is contained within the category next to it(e.g. B/A) and lies on the side indicated by the slash(\ for left, / for right). When two categories combine, the result is a new category, taken from the left of the slash(B in this example)</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/CCG1.jpg" class="ful-image" alt="CCG1.jpg">
<p>CCG Category 由两部分组成，左边是句法(syntax)，右边是语义(semantics)<br>Syntax 包含基本符号和运算符</p>
<ul>
<li>Primitive symbols: N, S, NP, ADJ and PP</li>
<li>Syntactic combination operator (/,)<br>Slashes specify argument order and direction</li>
</ul>
<p>CCG: Five (5) grammar rules!</p>
<ul>
<li>Application:<br>– Forward: A/B + B = A    A/B:S + B:T = A:S.T<br>– Backward: B + A\B = A   B:T + A\B:S = A:S.T</li>
<li>Composition:<br>– Forward: A/B + B/C = A/C<br>– Backward: B\C + A\B = A\C</li>
<li>Coordination:<br>– X CONJ X’ = X”</li>
<li>Type raising:<br>– A = X/(X\A)</li>
</ul>
<p>Semantics 包含逻辑语言，如 lambda calculus，将 syntax 和 semantics 匹配起来就形成了 CCG Category，如<br>(S\NP)/ADJ: $\lambda f.\lambda x.f(x)$</p>
<p>一个简单的例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/semantic%20parsing%20eg.2.png" class="ful-image" alt="semantic%20parsing%20eg.2.png"></p>
<p>CCG 的优点</p>
<ul>
<li>简化了组合规则(combinatory rules)</li>
<li>将复杂性从 rule 身上转移到了 (categorial) lexical entries 上</li>
<li>更紧密的与语义融合</li>
<li>句法和语义成分之间是一对一的关系</li>
</ul>
<h3 id="Handling-Coordination"><a href="#Handling-Coordination" class="headerlink" title="Handling Coordination"></a>Handling Coordination</h3><h4 id="Constituent-Coordination"><a href="#Constituent-Coordination" class="headerlink" title="Constituent Coordination"></a>Constituent Coordination</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- John likes Mary and dislikes Bob.</div><div class="line">  NP (VP and VP)</div></pre></td></tr></table></figure>
<p>如上面的例子可以直接通过 coordination rule 来 parse<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/coor3.jpg" class="ful-image" alt="coor3.jpg"></p>
<h4 id="Non-constituent-coordination"><a href="#Non-constituent-coordination" class="headerlink" title="Non-constituent coordination"></a>Non-constituent coordination</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- John likes and Mary dislikes Bob.</div></pre></td></tr></table></figure>
<p>然而这个例子就只能通过 Type raising 来解决<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/TR1.jpg" class="ful-image" alt="TR1.jpg"></p>
<p>完整的例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/TR2.jpg" class="ful-image" alt="TR2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/TR3.jpg" class="ful-image" alt="TR3.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/TR3.jpg" class="ful-image" alt="TR3.jpg"></p>
<h3 id="UW-SPF"><a href="#UW-SPF" class="headerlink" title="UW SPF"></a>UW SPF</h3><p>UW 有一个开源项目 <a href="https://github.com/cornell-lic/spf" target="_blank" rel="external">University of Washington Semantic Parsing Framework (UW SPF)</a>，将语义分析的任务分成了三个子问题 <strong>Parsing -&gt; Learning -&gt; Modeling</strong></p>
<p><strong>Parsing choice</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/parsing%20choice.png" class="ful-image" alt="parsing%20choice.png"><br><strong>Note:</strong> 项目中用了 Combinatory Categorial Grammars</p>
<p><strong>Learning</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/learning.jpg" class="ful-image" alt="learning.jpg"><br><strong>Note</strong>: 项目中用了 Unified learning algorithm</p>
<p><strong>Semantic Modeling</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/semantic%20modeling.jpg" class="ful-image" alt="semantic%20modeling.jpg"></p>
<p>更多见 <a href="http://yoavartzi.com/tutorial/" target="_blank" rel="external">Semantic Parsing with Combinatory Categorial Grammars</a></p>
<h2 id="CCGs-vs-CFGs"><a href="#CCGs-vs-CFGs" class="headerlink" title="CCGs vs. CFGs"></a>CCGs vs. CFGs</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/CFG%20vs%20CCG.jpg" class="ful-image" alt="CFG%20vs%20CCG.jpg">
<h1 id="补充：词汇语义学-Lexical-Semantics"><a href="#补充：词汇语义学-Lexical-Semantics" class="headerlink" title="补充：词汇语义学(Lexical Semantics)"></a>补充：词汇语义学(Lexical Semantics)</h1><p>词汇语义学方面的知识可以用于基于 CFG 的语义分析。先看一下词位之间以及词位的含义之间的各种关系</p>
<ul>
<li>同形关系(homonymy) 指词形(包括字形和音形)相同但意义不同的词之间的关系</li>
<li>多义关系(polysemy) 指单个词位具有多个相关含义的概念</li>
<li>同义关系(synonymy) 指具有相同意义的不同词位之间的关系</li>
<li>上下位关系(hyponymy) 指具有类别包含(class-inclusion)关系的词位之间的关系</li>
</ul>
<p>我们知道，意义表示是以基本的谓词-论元结构为基础的，在组成这样的表示时，我们假定词位的某些类倾向于促成谓词和谓词-论元结构，而其他的倾向于促成论元。这里探讨的是，词位的各个意义表示之间具有可分析的内在结构，正是这个内在结构与语法相结合，才确定了良构句子中词位之间的关系。</p>
<h2 id="Thematic-Roles"><a href="#Thematic-Roles" class="headerlink" title="Thematic Roles"></a>Thematic Roles</h2><p><strong>题元角色(Thematic Roles)</strong>是一组范畴，为刻画动词的某些 argument 的特征提供了浅层语义语言，比如下面两个句子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Houston&apos;s Billy Hatcher broke a bat.</div><div class="line">He opened a rawer.</div></pre></td></tr></table></figure>
<p>通过 FOPC 可以得到下面的部分表示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">∃e,x,y ISA(e,Breaking) ∧ Breaker(e,BillyHatcher) ∧ BrokenThing(e,y) ∧ ISA(y,BaseballBat)</div><div class="line">∃e,x,y ISA(e,Opening) ∧ Opener(e,he) ∧ OpenedThing(e,y) ∧ ISA(y,Door)</div></pre></td></tr></table></figure></p>
<p>可以发现，动词 break 和 open 的主语的角色分别为 Breaker 和 Opener，然而每种可能发生事的深层角色(deep role)都是特定的，比如 Breaking 事件中的 Breaker，Opening 事件中的 Opener，Eating 事件中的 Eater 等，但 Breaking 和 Opener 有一些共同点，它们都是有意志的行为人，常常是有生命的，并且对它们的事件负有直接因果责任，题元角色(Thematic Roles)就是表示这种共性的一种方式，比如说我们把这两个动词的主语称为 agent。</p>
<p>常用的 Thematic roles 及其定义<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/thematic%20roles.jpg" class="ful-image" alt="thematic%20roles.jpg"></p>
<p>The Thematic Grid or Case Frame shows<br>• How many arguments the verb has<br>• What roles the arguments have<br>• Where to find each argument<br>  For example, you can find the agent in the subject position</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/7.jpg" class="ful-image" alt="7.jpg">
<p>Thematic roles 的另一个用途是，通过<strong>题元层级(thematic hierarchy)</strong>来决定主语和宾语，如上，如果一个动词的题元描述包括一个 AGENT，一个 INSTRUMENT，和一个 THEME，那么 AGENT 被实现为主语，如果题元描述值包括一个 INSTRUMENT 和一个 THEME，那么 INSTRUMENT 成为主语。</p>
<h2 id="Verb-Subcategorization"><a href="#Verb-Subcategorization" class="headerlink" title="Verb Subcategorization"></a>Verb Subcategorization</h2><p>动词可以有不同数量的 argument，以及不同的 VP 规则，应用 SUBCAT 特征，可以创建 allowed arguments 的集合。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/Verb%20Subcategorization.jpg" class="ful-image" alt="Verb%20Subcategorization.jpg"></p>
<h2 id="Selectional-Restriction"><a href="#Selectional-Restriction" class="headerlink" title="Selectional Restriction"></a>Selectional Restriction</h2><p><strong>选择限制(selectional restriction)</strong>是一种语义约束，通过容许词位对那些能够与它们在一个句子中出现的词位和短语设置某些语义限制，来增加语义角色。举个例子，看下面四个句子，前两个句子很正常，我们可以说 dog, child 是开心的，因为他们是动物，有情绪，所以可以把他们和 happy 联系起来，然而后两个句子就非常的诡异，事实上它们违反了选择限制(selectional restriction)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1. The dog was happy.</div><div class="line">2. The child was happy.</div><div class="line">3. The rock was happy.</div><div class="line">4. The napkin was happy.</div></pre></td></tr></table></figure>
<p>我们可以通过面向事件的意义表示，来捕捉选择限制的语义，像 eat 这样的动词的语义贡献可以表示为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">∃e,x,y Eating(e) ∧ Agent(e, x) ∧ Theme(e,y)</div></pre></td></tr></table></figure></p>
<p>所有关于 y(theme 角色的填充者)的信息是：通过 THEME 关系，与 Eating 事件相关联，为加入 y 必须是可食用的东西这一选择限制，我们可以通过简单添加一个新的 term 来实现，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">∃e,x,y Eating(e) ∧ Agent(e, x) ∧ Theme(e,y) ∧ ISA(y, EdibleThing)</div></pre></td></tr></table></figure></p>
<p>当遇到像 ate a hamburger 这样的短语，语义分析器能够形成如下的表示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">∃e,x,y Eating(e) ∧ Agent(e, x) ∧ Theme(e,y) ∧ ISA(y, EdibleThing) ∧ ISA(y, Hamburger)</div></pre></td></tr></table></figure></p>
<p>这个表示是完全合理的，假如在知识库中有一个合理的事实集，y 在范畴 Hamburger 中的成员属性与它再范畴 EdibleThing 中的成员属性是一致的。相反，ate a takeoff 这样的短语是不合理的，因为 takeoff 中的成员属性与范畴 EdibleThing 中的成员属性不一致。然而这种方法预先假定存在一个大规模的关于组成选择限制的概念的事实逻辑库，遗憾的是，尽管这类知识库正在建设，但还没有普遍使用，而且几乎没有达到任务所需规模的知识库。</p>
<p>一个更实际的方法是利用 WordNet 信息库中的上下位关系，这种方法中，语义角色的选择限制是用 WordNet 的同义集而不是逻辑概念来表述的，如果填充语义角色的词位是由谓词给语义角色指定的同义集的上位词中的一个，那么这个给定的意义表示可以被判断为良构的。</p>
<p>如在 WordNet 的 60，000 个同义集中，有一个 {food, nutrient} 的同义集，我们可以将它指定为对动词 eat 的 THEME 角色的选择限制，也就是将这个橘色的填充者限定为再这个同义集或其下位词中，看下图，hamburger 的上位关系链显示它确实为 food<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP-%E7%AC%94%E8%AE%B0---Compositional-Semantics/selection%20restriction.jpg" class="ful-image" alt="selection%20restriction.jpg"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> semantic analysis </tag>
            
            <tag> 语义分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP笔记 - Relation Extraction]]></title>
      <url>http://www.shuang0420.com/2017/04/10/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/</url>
      <content type="html"><![CDATA[<p>Stanford NLP 关于关系抽取的笔记，仅探讨如何提取关系的三元组(triple)，即一个谓词(predicate)带 2 个形参(argument)，比如说 Founding-location(IBM,New York) 这类。<br><a id="more"></a></p>
<p>关系抽取的<strong>应用领域</strong>还是很广泛的，如</p>
<ul>
<li>建立新的结构化的知识库(knowledge bases)<br>几乎各个领域都会用到</li>
<li>扩大现有知识库<br>将更多的单词添加到 WordNet 词典(thesaurus)<br>将更多事实(facts) 添加到 FreeBase 或者 DBPedia中</li>
<li>支持 QA 系统<br>The granddaughter of which actor starred in the movie “E.T.”?<br> (acted-in ?x “E.T.”)(is-a ?y actor)(granddaughter-of ?x ?y)</li>
</ul>
<p>先来看下关系抽取的数据集以及现有的关系定义。比较有名的<strong>ACE(Automated Content Extraction)</strong>有 6 大类关系 17 个子类，用于医疗的<strong>UMLS</strong> 有 134 中 entity type，54 种关系……</p>
<p>还有的一些世界范围内知名的高质量大规模开放知识图谱，如包括 DBpedia、Yago、Wikidata、BabelNet、ConceptNet 以及 Microsoft Concept Graph等，中文的有开放知识图谱平台 OpenKG……</p>
<p>ACE 的 17 类关系<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>具体的应用实例</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/3.jpg" class="ful-image" alt="3.jpg">
<p>Wikipedia 的 info box</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/4.jpg" class="ful-image" alt="4.jpg">
<p>info box 信息可以转换成 RDF 三元组 $(subject \ predicate \ object)$ 如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Golden Gate Park location San Francisco  </div><div class="line">=&gt;</div><div class="line">dbpedia:Golden_Gate_Park dbpedia-owl:location dbpedia:San_Francisco</div></pre></td></tr></table></figure>
<p>常用的 Freebase relations</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">people/person/nationality,</div><div class="line">people/person/profession,</div><div class="line">biology/organism_higher_classification,</div><div class="line">location/location/contains</div><div class="line">people/person/place-of-birth</div><div class="line">film/film/genre</div></pre></td></tr></table></figure>
<p>RDF 是一种本体语言，关于本体，貌似是本科时的东西了，就不多说了。</p>
<h1 id="Relation-extractors"><a href="#Relation-extractors" class="headerlink" title="Relation extractors"></a>Relation extractors</h1><ol>
<li>手写规则(hand-written patterns)</li>
<li>监督学习(supervised machine learning)</li>
<li>半监督/无监督学习(semi-supervised and unsupervised)<ul>
<li>Bootstrapping(using seeds)</li>
<li>Distant supervision</li>
<li>Unsupervised learning from the web</li>
</ul>
</li>
</ol>
<h1 id="Hand-written-patterns"><a href="#Hand-written-patterns" class="headerlink" title="Hand-written patterns"></a>Hand-written patterns</h1><p>首先是基于字符串的 pattern，举一个 IS-A 的关系</p>
<p>Agar is a substance prepared from a mixture of red algae, <strong>such as</strong> Gelidium, for laboratory or industrial use</p>
<p>通过 such as 可以判断这是一种 IS-A 的关系，可以写的规则是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">“Y such as X ((, X)* (, and|or) X)”</div><div class="line">“such Y as X”</div><div class="line">“X or other Y”</div><div class="line">“X and other Y”</div><div class="line">“Y including X”</div><div class="line">“Y, especially X”</div></pre></td></tr></table></figure>
<p>另一个直觉是，更多的关系是在特定是实体之间的，所以我们可以用 Named entity tag 来帮助关系抽取，比如说</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">•  located-in (ORGANIZATION, LOCATION)</div><div class="line">•  founded (PERSON, ORGANIZATION)</div><div class="line">•  cures (DRUG, DISEASE)</div></pre></td></tr></table></figure>
<p>然后想到了我们可以把基于字符串的 pattern 和基于 ner 的 pattern 结合起来，就有了下面的例子。                </p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/5.jpg" class="ful-image" alt="5.jpg">
<p>对应的工具有 Stanford CoreNLP 的 tokensRegex。</p>
<p>手写规则的<strong>优点</strong>是：</p>
<ul>
<li>人工规则有高准确率(high-precision)</li>
<li>可以为特定领域定制(tailor)</li>
</ul>
<p><strong>缺点</strong>是：</p>
<ul>
<li>低召回率(low-recall)</li>
<li>要考虑周全所有可能的 pattern 很难，也很费时间精力</li>
<li>需要为每条关系来定义 pattern</li>
</ul>
<h1 id="Supervised-relation-extraction"><a href="#Supervised-relation-extraction" class="headerlink" title="Supervised relation extraction"></a>Supervised relation extraction</h1><h2 id="研究综述"><a href="#研究综述" class="headerlink" title="研究综述"></a>研究综述</h2><p>漆桂林,高桓,吴天星.知识图谱研究进展[J].情报工程,2017,3(1):004-025</p>
<blockquote>
<p>Zhou[13] 在 Kambhatla 的基础上加入了基本词组块信息和 WordNet，使用 SVM 作为分类器，在实体关系识别的准确率达到了 55.5%，实验表明实体类别信息的特征有助于提高关系抽取性能； Zelenko[14] 等人使用浅层句法分析树上最小公共子树来表达关系实例，计算两颗子树之间的核函数，通过训练例如 SVM 模型的分类器来对实例进行分。但基于核函数的方法的问题是召回率普遍较低，这是由于相似度计算过程匹配约束比较严格，因此在后续研究对基于核函数改进中，大部分是围绕改进召回率。但随着时间的推移，语料的增多、深度学习在图像和语音领域获得成功，信息抽取逐渐转向了基于神经模型的研究，相关的语料被提出作为测试标准，如 SemEval-2010 task 8[15]。基于神经网络方法的研究有，Hashimoto[16] 等人利用 Word Embedding 方法从标注语料中学习特定的名词对的上下文特征，然后将该特征加入到神经网络分类器中，在 SemEval-2010 task 8 上取得了 F1 值 82.8% 的效果。基于神经网络模型显著的特点是不需要加入太多的特征，一般可用的特征有词向量、位置等，因此有人提出利用基于联合抽取模型，这种模型可以同时抽取实体和其之间的关系。联合抽取模型的优点是可以避免流水线模型存在的错误累积[17-22]。其中比较有代表性的工作是[20]，该方法通过提出全新的全局特征作为算法的软约束，进而同时提高关系抽取和实体抽取的准确率，该方法在 ACE 语料上比传统的流水线方法 F1 提高了 1.5%，；另一项工作是 [22]，利用双层的 LSTM-RNN 模型训练分类模型，第一层 LSTM 输入的是词向量、位置特征和词性来识别实体的类型。训练得到的 LSTM 中隐藏层的分布式表达和实体的分类标签信息作为第二层 RNN 模型的输入，第二层的输入实体之间的依存路径，第二层训练对关系的分类，通过神经网络同时优化 LSTM 和 RNN 的模型参数，实验与另一个采用神经网络的联合抽取模型[21]相比在关系分类上有一定的提升。但无论是流水线方法还是联合抽取方法，都属于有监督学习，因此需要大量的训练语料，尤其是对基于神经网络的方法，需要大量的语料进行模型训练，因此这些方法都不适用于构建大规模的 Knowledge Base。</p>
</blockquote>
<p>[13] Guodong Z, Jian S, Jie Z, et al. ExploringVarious Knowledge in relation Extraction.[c]// acl2005, Meeting of the Association for ComputationalLinguistics, Proceedings of the Conference, 25-30 June, 2005, University of Michigan, USA. DBLP.2005:419-444.</p>
<p>[14] Zelenko D, Aone C, Richardella A. KernelMethods for relation Extraction[J]. the Journal ofMachine Learning Research, 2003, 1083-1106.</p>
<p>[15] Hendrickx I, Kim S N, Kozareva Z, et al.semEval-2010 task 8: Multi-way classification ofsemantic relations between Pairs of nominals[c]//the Workshop on semantic Evaluations: recentachievements and Future Directions. association forComputational Linguistics, 2009:94-99.</p>
<p>[16] Hashimoto K, Stenetorp P, Miwa M, et al. Task-oriented learning of Word Embeddings for semanticRelation Classification[J], Computer Science,2015:268-278.</p>
<p>[17] Singh S, Riedel S, Martin B, et al. JointInference of Entities, Relations, and Coreference[C]//the Workshop on automated Knowledge baseConstruction ,San Francisco, CA, USA, October27-november 1. 2013:1-6.</p>
<p>[18] Miwa M, Sasaki Y. Modeling Joint Entity andrelation Extraction with table representation[c]//conference on Empirical Methods in naturalLanguage Processing. 2014:944-948.</p>
<p>[19] Lu W, Dan R. Joint Mention Extraction andclassification with Mention Hypergraphs[c]//conference on Empirical Methods in naturallanguage Processing. 2015:857-867.</p>
<p>[20] Li Q, Ji H. Incremental Joint Extraction of EntityMentions and relations[c]// annual Meeting of theAssociation for Computational Linguistics. 2014:402-412.</p>
<p>[21] Kate R J, Mooney R J. Joint Entity andrelation Extraction using card-pyramid Parsing[c]//conference on computational natural languagelearning. 2010:203-212.</p>
<p>[22] Miwa M, Bansal M. End-to-End Relation Extraction using lstMs on sequences and tree structures[c]// annual Meeting of the association for computational linguistics. 2016:1105-1116.</p>
<h2 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h2><p>非常传统的监督学习思路</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- 选择我们想提取的关系集合</div><div class="line">- 选择相关的命名实体集合</div><div class="line">- 寻找并标注数据</div><div class="line">	选择有代表性的语料库</div><div class="line">	标记命名实体</div><div class="line">	人工标注实体间的关系</div><div class="line">	分成训练、开发、测试集</div><div class="line">- 训练分类器</div></pre></td></tr></table></figure>
<p>为了提高 efficiency，通常我们会训练两个分类器，第一个分类器是 yes/no 的二分类，判断命名实体间是否有关系，如果有关系，再送到第二个分类器，给实体分配关系类别。这样做的好处是通过排除大多数的实体对来加快分类器的训练过程，另一方面，对每个任务可以使用 task-specific feature-set。</p>
<p>可以采用的分类器有</p>
<ul>
<li>MaxEnt</li>
<li>Naive Bayes</li>
<li>SVM</li>
<li>…</li>
</ul>
<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>E.g., <strong>American Airlines</strong>, a unit of AMR, immediately matched the move, spokesman <strong>Tim Wagner</strong> said<br><strong>Mention 1:</strong> American Airlines<br><strong>Mention 2:</strong> Tim Wagner</p>
<p><strong>Word features</strong></p>
<ul>
<li>Headwords of M1 and M2, and combination<ul>
<li>M1: Airlines,  M2: Wagner, Combination: Airlines-Wagner</li>
</ul>
</li>
<li>Bag of words and bigrams in M1 and M2<ul>
<li>{American, Airlines, Tim, Wagner, American Airlines, Tim Wagner}</li>
</ul>
</li>
<li>Words or bigrams in particular positions left and right of M1/M2<ul>
<li>M2: -1 spokesman</li>
<li>M2: +1 said</li>
</ul>
</li>
<li>Bag of words or bigrams between the two entities<ul>
<li>{a, AMR, of, immediately, matched, move, spokesman, the, unit}</li>
</ul>
</li>
</ul>
<p><strong>Named Entities Type and Mention Level Features</strong></p>
<ul>
<li>Named-entities types<br>M1: ORG<br>M2: PERSON</li>
<li>Concatenation of the two named-entities types<br>ORG-PERSON</li>
<li>Entity Level of M1 and M2 (NAME, NOMINAL, PRONOUN)<br>M1: NAME     [it or he would be PRONOUN]<br>M2: NAME     [the company would be NOMINAL]</li>
</ul>
<p><strong>Parse Features</strong></p>
<ul>
<li>Base syntactic chunk sequence from one to the other<br>NP NP PP VP NP NP</li>
<li>Constituent path through the tree from one to the other<br>NP ↑ NP ↑ S ↑ S ↓ NP</li>
<li>Dependency path<br>Airlines matched Wagner said</li>
</ul>
<p><strong>Gazetteer and trigger word features</strong></p>
<ul>
<li>Trigger list for family: kinship terms<br>parent, wife, husband, grandparent, etc. [from WordNet]</li>
<li>Gazetteer:<br>List of useful geo or geopolitical words<br>  Country name list<br>  Other sub-entities</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/7.jpg" class="ful-image" alt="7.jpg">
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>最常用的 Precision, Recall, F1</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/8.jpg" class="ful-image" alt="8.jpg">
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如果测试集和训练集很相似，那么监督学习的准确率会很高，然而，它对不同 genre 的泛化能力有限，模型比较脆弱，另一方面，获取这么大的训练集代价也是昂贵的。</p>
<h1 id="Semi-supervised-realation-extraction"><a href="#Semi-supervised-realation-extraction" class="headerlink" title="Semi-supervised realation extraction"></a>Semi-supervised realation extraction</h1><h2 id="Seed-based-or-bootstrapping-approaches"><a href="#Seed-based-or-bootstrapping-approaches" class="headerlink" title="Seed-based or bootstrapping approaches"></a>Seed-based or bootstrapping approaches</h2><p>半监督学习主要是利用少量的标注信息进行学习，这方面的工作主要是基于 Bootstrap 的方法。基于 Bootstrap 的方法主要是利用少量的实例作为初始种子(seed tuples)的集合，然后利用 pattern 学习方法进行学习，通过不断的迭代，从非结构化数据中抽取实例，然后从新学到的实例中学习新的 pattern 并扩充 pattern 集合。</p>
<p>漆桂林,高桓,吴天星.知识图谱研究进展[J].情报工程,2017,3(1):004-025</p>
<blockquote>
<p>Brin[23]等人通过少量的实例学习种子模板，从网络上大量非结构化文本中抽取新的实例，同时学习新的抽取模板，其主要贡献是构建了 DIPRE 系统；Agichtein[24]在 Brin 的基础上对新抽取的实例进行可信度的评分和完善关系描述的模式，设计实现了 Snowball 抽取系统；此后的一些系统都沿着 Bootstrap 的方法，但会加入更合理的对 pattern 描述、更加合理的限制条件和评分策略，或者基于先前系统抽取结果上构建大规模 pattern；如 NELL（Never-EndingLanguage Learner）系统[25-26]，NELL 初始化一个本体和种子 pattern，从大规模的 Web 文本中学习，通过对学习到的内容进行打分来提高准确率，目前已经获得了 280 万个事实。</p>
</blockquote>
<p>[23] brin s. Extracting Patterns and relations fromthe World Wide Web[J]. lecture notes in computerScience, 1998, 1590:172-183.</p>
<p>[24] Agichtein E, Gravano L. Snowball : Extractingrelations from large Plain-text collections[c]// acMConference on Digital Libraries. ACM, 2000:85-94.</p>
<p>[25] Carlson A, Betteridge J, Kisiel B, et al. Toward anarchitecture for never-Ending language learning.[c]// twenty-Fourth aaai conference on artificialIntelligence, AAAI 2010, Atlanta, Georgia, Usa, July.DBLP, 2010:529-573.</p>
<p>[26] Mitchell T, Fredkin E. Never-ending Languagelearning[M]// never-Ending language learning.Alphascript Publishing, 2014.</p>
<p><strong>Relation Bootstrapping</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">•  Gather a set of seed pairs that have relation R</div><div class="line">•  Iterate:</div><div class="line">1.  Find sentences with these pairs</div><div class="line">2.  Look at the context between or around the pair and generalize the context to create patterns</div><div class="line">3.  Use the patterns for grep for more pairs</div></pre></td></tr></table></figure>
<p>看一个完整的例子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/6.jpg" class="ful-image" alt="6.jpg">
<p>从 5 对种子开始，找到包含种子的实例，替换关键词，形成 pattern，迭代匹配，就为 $(authoer, book)$ 抽取到了 relation pattern，<strong><em>x, by y</em></strong>, 和 <strong><em>x, one of y’s</em></strong></p>
<h3 id="Snowball"><a href="#Snowball" class="headerlink" title="Snowball"></a>Snowball</h3><p>对 Dipre 算法的改进。Snowball 也是一种相似的迭代算法，Dipre 的 X,Y 可以是任何字符串，而 Snowball 要求 X,Y 必须是命名实体，并且 Snowball 对每个 pattern 计算了 confidence value</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Group instances w/similar prefix, middle, suffix, extract patterns</div><div class="line">	•  But require that X and Y be named entites</div><div class="line">	•  And compute a confidence for each pattern</div><div class="line"></div><div class="line">.69 ORGANIZATION &#123;&apos;s, in, headquaters&#125; LOCATION</div><div class="line">.75 LOCATION &#123;in, based&#125; ORGANIZATION</div></pre></td></tr></table></figure>
<h2 id="Distant-Supervision"><a href="#Distant-Supervision" class="headerlink" title="Distant Supervision"></a>Distant Supervision</h2><p>Distant Supervision 其实结合了 bootstrapping 和监督学习的长处，它使用一个大的数据库来得到海量的 seed example，然后从这些 example 中创建许多的 feature，最后与有监督的分类器相结合。</p>
<p>与监督学习相似的是这种方法用许多的 feature 训练了一个分类器，通过详细的人工创造的知识进行监督，不需要用迭代的方法来扩充 pattern。</p>
<p>与无监督学习相似的是这种方法采用了大量没有标注的数据，对训练语料库中的 genre 并不敏感，适合泛化。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%E7%AC%94%E8%AE%B0%20-%20Relation%20Extraction/9.jpg" class="ful-image" alt="9.jpg">
<p>​        </p>
<h1 id="Unsupervised-relation-extraction"><a href="#Unsupervised-relation-extraction" class="headerlink" title="Unsupervised relation extraction"></a>Unsupervised relation extraction</h1><blockquote>
<p>Bollegala[27]从搜索引擎摘要中获取和聚合抽取模板，将模板聚类后发现由实体对代表的隐含语义关系; Bollegala[28]使用联合聚类(Co-clustering)算法，利用关系实例和关系模板的对偶性，提高了关系模板聚类效果，同时使用 L1 正则化 Logistics 回归模型，在关系模板聚类结果中筛选出代表性的抽取模板，使得关系抽取在准确率和召回率上都有所提高。</p>
<p>无监督学习一般利用语料中存在的大量冗余信息做聚类，在聚类结果的基础上给定关系，但由于聚类方法本身就存在难以描述关系和低频实例召回率低的问题，因此无监督学习一般难以得很好的抽取效果。</p>
</blockquote>
<p>[27] Bollegala D T, Matsuo Y, Ishizuka M. Measuringthe similarity between implicit semantic relationsfrom the Web[J]. Www Madrid! track semantic/dataWeb, 2009:651-660.</p>
<p>[28] Bollegala D T, Matsuo Y, Ishizuka M. RelationalDuality: Unsupervised Extraction of semantic relations between Entities on the Web[c]//International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, Usa, April. DBLP, 2010:151-160.</p>
<p>​<br><strong>Open Information Extraction</strong> 从网络中抽取关系，没有训练数据，没有关系列表。过程如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">1. Use parsed data to train a “trustworthy tuple” classifier</div><div class="line">2. Single-pass extract all relations between NPs, keep if trustworthy</div><div class="line">3. Assessor ranks relations based on text redundancy</div><div class="line"></div><div class="line">E.g.,</div><div class="line">(FCI, specializes in, sobware development)</div><div class="line">(Tesla, invented, coil transformer)</div></pre></td></tr></table></figure></p>
<h1 id="Evaluation-of-Semi-supervised-and-Unsupervised-Relation-Extraction"><a href="#Evaluation-of-Semi-supervised-and-Unsupervised-Relation-Extraction" class="headerlink" title="Evaluation of Semi-supervised and Unsupervised Relation Extraction"></a>Evaluation of Semi-supervised and Unsupervised Relation Extraction</h1><p>因为抽取的是新的关系，并不能准确的计算 precision 和 recall。然而我们可以估计，从结果集中随机抽取一个关系的 sample，然后人工来检验准确率</p>
<p>​    $$\hat P = {\text {Number of correctly extracted relations in the sample} \over \text {Total number of extracted relations in the sample}}$$</p>
<p>也可以计算不同 recall level 上的 precision，比如说分别计算在前 1000，10,000，100,000 个新的关系中的 precision，在各个情况下随机取样。</p>
<p>然而，并没有方法来计算 recall。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Relation Extraction </tag>
            
            <tag> Information Extraction </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Knowledge Representation]]></title>
      <url>http://www.shuang0420.com/2017/04/07/NLP%20%E7%AC%94%E8%AE%B0%20-%20Knowledge%20Representation/</url>
      <content type="html"><![CDATA[<p>作为 <a href="http://www.shuang0420.com/2017/04/07/NLP%20笔记%20-%20Meaning%20Representation%20Languages/">NLP 笔记 - Meaning Representation Languages</a> 这一篇的补充，简单介绍了<strong>事件(event)，时间(time)，信念(beliefs)</strong> 的意义表示。<br><a id="more"></a></p>
<h1 id="Knowledge-Representation"><a href="#Knowledge-Representation" class="headerlink" title="Knowledge Representation"></a>Knowledge Representation</h1><ul>
<li>Ontologies<br>Mammal includes Cat,Dog,Whale<br>Cat includes PersianCat, ManxCat</li>
<li>Categories and Object<br>Categories: Cat;    Object: Martin the cat<br>Categories is a set of object, object is part of categories</li>
<li>Events</li>
<li>Times</li>
<li><p>Beliefs</p>
<p>​<br>一些常见关系，ISA, AKO, HASA</p>
</li>
<li>ISA relation<br>is a ISA(Martin,Cat)</li>
<li>AKO relation<br>a kind of AKO(PersianCat,Cat)</li>
<li>HASA relation<br>has a HASA(Cat, tail)</li>
</ul>
<h1 id="Categories-and-Subsumption"><a href="#Categories-and-Subsumption" class="headerlink" title="Categories and Subsumption"></a>Categories and Subsumption</h1><p><strong>范畴(Category)</strong>表示的是一种所属关系(ISA)，一个范畴的所有成员共享一套相关的特征，表示范畴的最普通方法是为每个范畴造出一个一元的谓词，这样的谓词可以对每个有关范畴进行说明。如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Maharani is an Indian restaurant.</div><div class="line">IndianRestaurant(Maharani)</div></pre></td></tr></table></figure></p>
<p>左边的项是 category，括号里是 domain element。</p>
<p>Subsumption 表示的是一种包含关系(AKO)，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">All Indian restaurants are restaurants.</div><div class="line">IndianRestaurant ⊑ Restaurant</div></pre></td></tr></table></figure></p>
<p>注意的是，IndianRestaurant(Maharani) 这种方法里范畴表示的是关系，而不是实实在在的客体，只能对构成关系的各个成分有所说明，而很难对于范畴本身有所说明，比如我们想表达<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">MostPopular(Maharani, VegetarianRestaurant)</div></pre></td></tr></table></figure></p>
<p>这就不是一个合格的 FOPC 公式，因为 predicate 必须是 term，而不能是其他的 predicate。解决这个问题的方法是<strong>“具体化”(reification)</strong>，把我们想表述的所有概念都表示为实实在在的客体，这样就可以把 VegetarianRestaurant 这个范畴表示为诸如 Maharani 这样的客体啦，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ISA(Maharani, IndianRestaurant)</div><div class="line">AKO(IndianRestaurant, Restaurant)</div></pre></td></tr></table></figure></p>
<h1 id="Representing-Events"><a href="#Representing-Events" class="headerlink" title="Representing Events"></a>Representing Events</h1><p><strong>事件(event)</strong> 表示包括一个单独的谓词(predicate)以及与给定的例子想联系的角色所需的多个论元(argument)。以下面四个句子为例，动词 eat 这样的 predicate 的 argument 个数是可变的，然而这些例子都表示同一类事件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- Martin ate</div><div class="line">- Martin ate in the morning</div><div class="line">- Martin ate fish</div><div class="line">- Martin ate fish in the morning</div></pre></td></tr></table></figure>
<p>用 FOL 来表达，<strong>第一种方式</strong>是为动词所允许的每种 argument 格式建立一个次范畴化框架(subcategorization)，也就是为 eating 建立不同的谓词，来处理 eat 的各种可能的行为方式，这样就会比了 eating 究竟有多少个 argument 的问题，然而这种方法代价太高了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- Eating1(Martin)</div><div class="line">- Eating2(Martin, Morning)</div><div class="line">- Eating3(Martin, Fish)</div><div class="line">- Eating4(Martin, Fish, Morning)</div></pre></td></tr></table></figure></p>
<p>可以看到，事件之间在逻辑上存在着明显的关系，如如果 4 为真，3,2,1 也为真，然而这种方法并不能提供这些关系，一种解决方法是使用 <strong>意义假设(meaning postulate)</strong>，把谓词中的不同语义联系在一起<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">- ∀x,y,z Eating4(x,y,z) =&gt; Eating3(x,y)</div><div class="line">- ∀x,y,z Eating4(x,y,z) =&gt; Eating2(x,z)</div><div class="line">- ∀x,y,z Eating4(x,y,z) =&gt; Eating1(x)</div></pre></td></tr></table></figure></p>
<p>很明显，这种方法负担太重，存在 scalability 的问题，也不适合处理个性化的事件。</p>
<p><strong>第二种方式</strong>是只用一个框架来定义所有可能出现的 arguments，这种方法假定谓词的 argument 数目与该谓词在它的次范畴化框架中所表现出来的 argument 数目是相同的，然而在实际应用中，我们很难确定一个给定时间的正确角色数目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Eating4(x,y,z)</div><div class="line">	with some arguments unspecified</div></pre></td></tr></table></figure></p>
<p>造成的问题是</p>
<ul>
<li>Too many commitments</li>
<li>Hard to combine Eating4(Martin, Fish, z) with Eating4(Martin,y,Morning)</li>
</ul>
<p><strong>第三种方式 Reification(具体化)</strong>，使事件成为能够量词化的客体，并且能够通过定义好的关系与其它客体联系起来，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">∃ e: ISA(e, Eating) ∧ Eater(e, Martin) ∧ Eaten(e,Fish)</div></pre></td></tr></table></figure></p>
<p>表示存在一个吃饭的事件，Martin 是这个事件的行为者，Fish 是被吃的东西，这样的表示方法不需要说明量词的确定数目，无论出现多少角色和填充项都可以结合到谓词中，也不需要对角色进行意义假设。</p>
<h1 id="Representing-Time"><a href="#Representing-Time" class="headerlink" title="Representing Time"></a>Representing Time</h1><p>很多句子包含了时间的信息，上面的语义表达并没有讨论时间表示的问题，这里介绍下时间表达的基本概念<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- Martin went from the kitchen to the yard</div><div class="line">- ISA(e,Going) ∧ Goer(e, Martin) ∧ Origin(e,kitchen) ∧ Target(e,yard)</div><div class="line"></div><div class="line">Issue</div><div class="line">- no tense information: past? present? future?</div><div class="line"></div><div class="line">Fluents</div><div class="line">- A predicate that is true at a given time: T(f,t)</div></pre></td></tr></table></figure></p>
<p>我们可以增加时间变量<strong>事件的时间段(e.g., IntervalOf(w,i))、事件的终点(e.g., EndPoint(i,e))以及由动词时态(e.g., Precedes(e,Now))</strong>来说明关于这个终点到当前时间的时间谓词，然而简单动词的关系并不是直截了当的，现在时可以用于说明将来事件，将来时也可以用于说明一个过去事件，为了处理这种现象，Reichenbach(1947)提出了参照点(reference point)的概念，如下</p>
<p>E: event, R: reference, U: utterance<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Knowledge%20Representation/time2.png" class="ful-image" alt="time2.png"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Knowledge%20Representation/time3.png" class="ful-image" alt="time3.png"></p>
<p>下面的例子中，departed 这个事件就是参照点(reference time)，吃饭事件在参照点之前<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">When Mary&apos;s flight departed, I had eaten lunch.</div></pre></td></tr></table></figure></p>
<h1 id="Representing-Beliefs"><a href="#Representing-Beliefs" class="headerlink" title="Representing Beliefs"></a>Representing Beliefs</h1><p>有一些单词和词语的意义表示包含的逻辑公式并不一定在现实世界中真实存在，而是某个假设世界中的东西，这些单词具有“创造世界的能力”，如 believe, want, imagine, know 等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Example</div><div class="line">- Milo believes that Martin ate fish</div><div class="line"></div><div class="line">One possible representation</div><div class="line">- ∃ e,b: ISA(e,Eating) ∧ Eater(e,Martin) ∧ Eaten(e,Fish) ∧ ISA(b,Believing) ∧ Believer(b,Milo) ∧ Believed(b,e)</div></pre></td></tr></table></figure>
<p>然而这种表示中，所有的连接部分都必须为真，这导致的结果是 “Martin ate fish”，漏掉了信念这个事件，然而，Speaker 的信念并不能使这个命题在现实世界中成为真命题。处理这种情况的标准方法是使用 operator 来增强 FOPC，我们可以引入一个称为 Believes 的 operator，取两个 FOPC 公式作为它的 argument，一个公式指派一个 believer，另一个公式指派所相信的命题，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Believes(Milo, ∃ e: ISA(e,Eating) ∧ Eater(e,Martin) ∧ Eaten(e,Fish))</div></pre></td></tr></table></figure></p>
<p>Believes 这样的 operator 又称为 <strong>模态算符(modal operator)</strong>，相应地，用 modal operator 来增强的逻辑，称为 <strong>模态逻辑(modal logic)</strong></p>
<h1 id="Other-concepts"><a href="#Other-concepts" class="headerlink" title="Other concepts"></a>Other concepts</h1><h2 id="TBox-and-ABox"><a href="#TBox-and-ABox" class="headerlink" title="TBox and ABox"></a>TBox and ABox</h2><p><strong>TBox</strong>包含了应用领域关于类别或概念的知识，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">All bistros are restaurants</div><div class="line">All restaurants are businesses</div></pre></td></tr></table></figure></p>
<p><strong>ABox</strong>包含了领域里关于个体的事实，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">India Garden is an Indian restaurant</div></pre></td></tr></table></figure></p>
<h2 id="Negation-and-Disjunciton"><a href="#Negation-and-Disjunciton" class="headerlink" title="Negation and Disjunciton"></a>Negation and Disjunciton</h2><h3 id="Negation"><a href="#Negation" class="headerlink" title="Negation"></a>Negation</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">IndianRestaurant ⊑ not ItalianRestaurant</div><div class="line">Indian restaurants can’t also be Italian restaurants.</div></pre></td></tr></table></figure>
<h3 id="Disjunciton"><a href="#Disjunciton" class="headerlink" title="Disjunciton"></a>Disjunciton</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Restaurant ⊑ (or ItalianRestaurant IndianRestaurant MexicanRestaurant)</div><div class="line">Restaurants are Italian restaurants, Indian restaurants, or Mexican restaurant.</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> semantic analysis </tag>
            
            <tag> 语义分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Meaning Representation Languages]]></title>
      <url>http://www.shuang0420.com/2017/04/07/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。主要是关于意义表示的一些理论性知识，重点掌握<strong>一阶谓词演算(First Order Predicate Calculus)</strong><br><a id="more"></a></p>
<p><strong>Semantics Road Map</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/1.jpg" class="ful-image" alt="1.jpg">
<ol>
<li>Lexical semantics</li>
<li>Vector semantics</li>
<li><strong>Meaning representation languages and semantic roles</strong></li>
<li>Compositional semantics, semantic parsing</li>
<li>Discourse and pragmatics</li>
</ol>
<p><strong>语义分析常用的知识源</strong></p>
<ul>
<li>词本身的意义<br>• Word-sense identifier, like bass7<br>• Morphological info<br>• Grammatical info (POS, etc.)<br>• Phonetic info, if speech system<br>• Semantic info<br>• Comments, etc.</li>
<li>语法结构所带的意义</li>
<li>话语的结构知识</li>
<li><p>发生话语的上下文知识以及与话题相关的常识</p>
<p>​<br>这一篇讲 meaning representation，它常见的应用场景有：</p>
</li>
<li>在考试中回答文章问题<br>  具有背景知识，如关于问题主题的背景知识、关于学生知识水平的背景知识以及关于如何正常回答(answered normally)这些问题的背景知识</li>
<li>在饭店里阅读菜单点菜<br>  阅读菜单、点什么菜、到哪里吃饭、根据菜谱做菜、编写新的菜谱等等这些都要求对食品有深入的知识，知道怎样做菜，知道人们喜欢吃什么以及喜欢到什么饭店等</li>
<li>通过阅读说明书学习使用新软件<br>  要求对当前计算机、有关软件及使用有深入知识，对用户有一般知识</li>
</ul>
<p>注意的是这一篇的重点是表示句子的<strong>字面意义(literal meaning)</strong>，关注的是与单词的常规意义紧密联系的表示，而不反映它们出现的上下文环境，这种表示在涉及到<strong>俗语(idioms)</strong>和<strong>比喻(metaphor)</strong>时会显得不足。</p>
<h1 id="Desirable-qualities"><a href="#Desirable-qualities" class="headerlink" title="Desirable qualities"></a>Desirable qualities</h1><p>探讨 Meaning representation 的最基本要求。</p>
<h2 id="Verifiability"><a href="#Verifiability" class="headerlink" title="Verifiability"></a>Verifiability</h2><p>我们需要确定 meaning representation 的真实性，最直接的办法就是把 representation 和知识库(knowledge base)的表示相比较或匹配。<br>比如下面这个句子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Does Hello Bistro serve vegetarian food?</div></pre></td></tr></table></figure>
<p>假定这个问题包含了 Maharani serves vegetarian food 的意义，将其简单注释为 Serves(Maharani, VegetarianFood)，那么我们就要看这个输入的 representation 是否能和关于饭店(restaurant)的事实知识库(knowledge base of facts)相匹配，返回结果如下</p>
<ul>
<li>If 在知识库中找到一个表示与之匹配 -&gt; 返回一个肯定回答</li>
<li>Elif 知识库是完全的 -&gt; 返回否定</li>
<li>Else -&gt; 返回不知道</li>
</ul>
<p>这个概念就是 <strong>可能性验证(verifiability)</strong>，也就是系统把意义表示所描述的情况与在知识库中所模拟的某个世界的情况进行比较的一种能力。</p>
<h2 id="Unambiguous-Representation"><a href="#Unambiguous-Representation" class="headerlink" title="Unambiguous Representation"></a>Unambiguous Representation</h2><p><strong>无歧义表示(Unambiguous Representation)</strong> 要求最终的 meaning representation 只能有一个无歧义的表示，如下面的例子，我们只能准确的捕捉其中一种意义，而不能同时表示两种。有一些方法可以来决定某种解释比另外的解释更优先(或者更不优先)，这一篇暂且不讨论这类技术。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/2.jpg" class="ful-image" alt="2.jpg">
<h2 id="Canonical-Form"><a href="#Canonical-Form" class="headerlink" title="Canonical Form"></a>Canonical Form</h2><p><strong>规范形式(Canonical Form)</strong>要求的是表达相同事情的输入必须有相同的意义表示(input that mean the same thing should have the same meaning representation)。</p>
<p>比如下面四句话</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">• “Mad Mex has vegetarian dishes.”</div><div class="line">• “They have vegetarian food at Mad Mex.”</div><div class="line">• “Vegetarian dishes are served at Mad Mex.”</div><div class="line">• “Mad Mex serves vegetarian fare.”</div></pre></td></tr></table></figure>
<p>系统必须把 vegetarian dishes, vegetarian food, vegetarian fare 归入在这种上下文环境中的同一个事物，同时，这里 have 和 serve 的用法也是等价的，我们希望 meaning representation 能做到这一点。有一种思路是，查下字典，会发现这些单词都具有许多不同词义(word sense)，同时也可以看出，至少有一个意义是这些单词都共享的(可能是同义词synonymous)，如果有能力从不同用法中筛选出这个共享的意义，就可以把同样的意义表示指派给不同的单词/短语。</p>
<h2 id="Inference-Variables-and-Expressiveness"><a href="#Inference-Variables-and-Expressiveness" class="headerlink" title="Inference, Variables, and Expressiveness"></a>Inference, Variables, and Expressiveness</h2><p><strong>推论(Inference)</strong>说明了系统根据输入的意义表示以及存储的背景知识做出可靠结论的能力。</p>
<p>看下面两个例子，对第一个问句而言，基于规范形式的方法和简单的匹配不能使系统对这个问题做出合适回答，我们需要把这个问题的意义表示和在知识库中表示的事实联系起来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">• “Can vegetarians eat at Mad Mex?”</div><div class="line">• “I’d like to find a restaurant where I can get vegetarian food.”</div></pre></td></tr></table></figure>
<p>对于第二个句子而言，它并没有对任何特定的饭店进行推论，用户想要的是关于某个出售素食的饭店的信息，并没有提到特定的饭店，所以也不能进行简单匹配，需要引入 <strong>variable</strong> 来注释这句话，变成</p>
<p>$$Serves(x, VegetarianFood)$$</p>
<p>只有当 x 能够被知识库中某个已知的课题来替换，使整个命题得到匹配时，我们才可以说这个命题被成功匹配了，匹配完成后才可以用来实现用户的提问。</p>
<p>最后，意义表示方法应该具有足够的<strong>表达能力(expressiveness)</strong>，最理想的情况当然是只用一个单独的意义表示语言(meaning representation language)就可以恰当表达任何自然语言的意义。当然这个期望可能太高了，下面会讲到一般的 <strong>一阶谓词演算(FOL)</strong>就具有足够的表达能力来处理多数问题。</p>
<h1 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h1><p>下图说明了 I have a car 这个句子使用的四种常见的意义表示语言，第一行是 <strong>一阶谓词演算(First Order Predicate Calculus)</strong>，中间部分是一个<strong>语义网络(Semantic Network)</strong>，第三行包含一个<strong>概念依存(Conceptual Dependency)</strong>的图示和一个<strong>基于框架的表示(Frame-Based)</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/3.jpg" class="ful-image" alt="3.jpg">
<p>尽管四种方式各有差别，但是都表达了下面三个特征：</p>
<ul>
<li>实体(Objects)<ul>
<li>e.g., people, restaurants, cuisines</li>
</ul>
</li>
<li>实体性质(Properties of objects)<ul>
<li>e.g., pickiness, noisiness, spiciness</li>
</ul>
</li>
<li>实体之间的关系(Relations between objects)<ul>
<li>e.g., serves(Oishii Bento, Japanese)</li>
</ul>
</li>
</ul>
<h1 id="First-Order-Predicate-Calculus"><a href="#First-Order-Predicate-Calculus" class="headerlink" title="First Order Predicate Calculus"></a>First Order Predicate Calculus</h1><p>FOL(First Order Logic)可以用来表示 <strong>Objects, Relations, Functions</strong></p>
<ul>
<li>Objects<br>Martin the cat</li>
<li>Relations<br>Martin and Moses are brothers</li>
<li>Functions<br>Martin’s age</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/4.jpg" class="ful-image" alt="4.jpg">
<p>上图是一个用于说明 EOPC 表示句法的一个上下文无关语法。逐个来看一下。</p>
<h2 id="Term"><a href="#Term" class="headerlink" title="Term"></a>Term</h2><p>先来看一下<strong>项(term)</strong>这个概念，项是 EOPC 表示客体的一种设置，包含三个信息块，<strong>常量(constant)</strong>、<strong>函数(function)</strong>和<strong>变量(variable)</strong>，常量引用所描述的世界中的特定客体，函数在英语中经常表示为所属格(genitive)的概念，如 location of Maharani 或 Maharani’s location，翻译成 EOPC 可以表示为 LocationOf(Maharani)，函数在局上上相当于一个单独的 argument predicate，但尽管外表上它很像 predicate，而实际上只涉及一个单独客体的“项”，这样的好处是，用函数来引用客体时，不用与命名它的常量相联系，当存在像饭店这样的很多命名客体时，如果使用函数，只需要像一个 location 这样的函数就可以与各种名字的饭店联系起来。变量之前提到过，就是方便我们对客体做出判断，进行推论，而不必参照任何特定命名客体。</p>
<h2 id="Quantifier"><a href="#Quantifier" class="headerlink" title="Quantifier"></a>Quantifier</h2><p>另外，引用特定的匿名客体，以及一般性的引用在一个集合中的全部客体，都可以通过 <strong>量词(quantifier)</strong>来实现，<strong>存在量词(existential quantifier, ∃)</strong>，读作 there exists，<strong>全称量词(universal quantifier, ∀)</strong>，读作 for all。看一下解释</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/13.jpg" class="ful-image" alt="13.jpg">
<h2 id="Predicates"><a href="#Predicates" class="headerlink" title="Predicates"></a>Predicates</h2><p>再看一下 predicates，predicates 是一种符号，用于引用名称以及在给定领域内的一定数量的客体之间的关系，如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Serves(UnionGrill, AmericanFood)</div><div class="line">Restaurant(UnionGrill)</div></pre></td></tr></table></figure>
<p>第一个表示的意思是，serve 是具有两个位置的 predicate，由常量 Maharani 和 VegetarianFood 标出的客体之间的关系，第二个表示略有不同，原句是 Maharani is a restaurant，predicate 只有一个位置，不涉及多个客体，只是确认一个单独客体的某个性质，这种情况下，predicate 对 Maharani 的范畴所属关系进行编码。</p>
<h2 id="Connectives"><a href="#Connectives" class="headerlink" title="Connectives"></a>Connectives</h2><p>Anyway，有了引用客体的能力(refer to objects)、确认关于客体事实的能力(assert facts about objects)、把一些客体与另外的客体相互联系的能力(relate objects to one another)，就能够构造出初步的组合表示。而更大的组合表示需要<strong>逻辑连词(logical connective)</strong>把不同的意义表示结合到一起。如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Have(Speaker, FiveDollars) ∧ ¬ Have(Speaker, LotOfTime)</div><div class="line">∀x Person(x) ⇒ Have(x, FiveDollars)</div><div class="line">∃x,y Person(x) ∧ Restaurant(y) ∧ ¬HasVisited(x,y)</div></pre></td></tr></table></figure>
<p>常用的连接词和解释</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/12.jpg" class="ful-image" alt="12.jpg">
<p>常见的两个错误：</p>
<ol>
<li><p>=&gt; 是 ∀ 的主要连词，∧ 和 ∀ 搭配往往会出错，如我们要表达 All cats eat fish，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Wrong:</div><div class="line">∀x Cat(x) ∧ EatsFish(x)</div><div class="line">Everyone is a cat and everyone eats fish.</div><div class="line"></div><div class="line">Correct:</div><div class="line">∀x Cat(x) =&gt; EatsFish(x)</div></pre></td></tr></table></figure>
<p>当然也有例外，如下面这句 ∧ 和 ∀ 搭配，就是对的<br>​    ∀x Person(x) ∧ Know(I, x) =&gt; Smartness(x) &lt; Smartness(Max)<br>​    Max is the smartest person I know.</p>
</li>
<li><p>∧ 是 ∃ 的主要连词，=&gt; 和 ∃ 搭配往往会出错，如我们要表达 There’s a cat that eats fish.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Wrong:</div><div class="line">∃x Cat(x) =&gt; EatsFish(x)</div><div class="line">is true if anyone who is not a cat (if leftside is False, then anything can go into right side)</div><div class="line"></div><div class="line">Correct:</div><div class="line">∃x Cat(x) ∧ EatsFish(x)</div></pre></td></tr></table></figure>
<p>​</p>
</li>
</ol>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>具体来个例子，下面这个句子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ay Caramba is near ICSI.</div></pre></td></tr></table></figure>
<p>通过辨认与句子中的各种语法成分对应的 term 和 predicate，并构造逻辑公式，可以得到下面的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Near(LocationOf(AyCaramba),LocationOf(ICSI))</div></pre></td></tr></table></figure>
<p>这个公式的意义是根据 LocationOf(AyCaramba) 和 LocationOf(ICSI) 两项之间的关系、谓词 near 以及它们所模拟的世界中相应的客体和关系等而获得。这个句子可以根据在显示世界中 Ay Caramba 是不是真的和 ICSI 相近而被指派 True 或者 False 值。</p>
<p>两道题</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/14.jpg" class="ful-image" alt="14.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/15.jpg" class="ful-image" alt="15.jpg">
<p>第 1 题答案 DJBAE，第 2 题答案 ECDAH</p>
<h2 id="Lambda-Expression"><a href="#Lambda-Expression" class="headerlink" title="Lambda Expression"></a>Lambda Expression</h2><p>lambda 符号(Church, 1940)提供了形式化参数的功能，对 FOPC 进行了扩充，它引入了下面的表达式：</p>
<p>$$\lambda x P(x)$$</p>
<p>$x$是变量，$P(x)$是使用这些变量的 FOPC 表达式。lambda表达式的意义在于可以生成新的 FOPC 表达式，在这些新的表达式中，形参变量由指定项来绑定，也就是 lambda 变量由指定的 FOPC 项来进行简单的字面替换，去掉 lambda，这个过程也就是 lambda 化简(<strong>$\lambda$ -reduction</strong>)，如下面将 lambda 表达式用于常量 A，对表达式进行化简</p>
<p>$$\lambda x P(x)(A)$$</p>
<p>$$P(A)$$</p>
<p>lambda符号提供了动词语义中需要的两种能力：形参表使我们获得变量集，lambda化简实现用项来替换变量的处理。</p>
<p>最简单的例子来理解：<br>if $inc(x) = \lambda x \ x+1$<br>then $inc(4) = (\lambda x \ x+1)(4)=5$</p>
<p>if $add(x,y) = \lambda x, \lambda y(x+y)$<br>then $add(3,4) = (\lambda x,\lambda y(x+y))(3)(4)=(\lambda y 3+y)(4)=3+4=7$</p>
<h2 id="Semantics-of-FOL"><a href="#Semantics-of-FOL" class="headerlink" title="Semantics of FOL"></a>Semantics of FOL</h2><p>FOL 句子可以被赋 true/false 值，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Eg.</div><div class="line">Milo is younger than Martin</div><div class="line">&lt;(AgeOf(Milo),AgeOf(Martin))=true</div><div class="line">=(AgeOf(Milo),AgeOf(Martin))=false</div></pre></td></tr></table></figure></p>
<p>一些补充见 <a href="">NLP 笔记 - Knowledge Representation</a></p>
<h1 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h1><h2 id="Modus-Ponens"><a href="#Modus-Ponens" class="headerlink" title="Modus Ponens"></a>Modus Ponens</h2><p><strong>取式推理(Modus Ponens)</strong>相当于非形式化的 if-then 推理，比如说 $\alpha$ 和 $\beta$ 都是 EOPC 的公式，那么，可以把取式推理定义如下：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/5.jpg" class="ful-image" alt="5.jpg">
<p>我们把 implication rule 的左边的项作为 <strong>前提(antecedent)</strong>，右边的项作为 <strong>结论(consequent)</strong>，如果前提在知识库中出现，那么就可以推论出结论。举个例子</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/6.jpg" class="ful-image" alt="6.jpg">
<p>公式 VegetarianRestaurant(Rudys) 与前提相匹配，就可以用取式推理，得出 Serves(Rudys, VegetarianFood) 这个结论。</p>
<p>取式推理有两种应用方式，<strong>向前链接(Forward Chaining)</strong>和<strong>向后链接(Backward Chaining)</strong>，向前链接就是刚才所描述的方法，优点是有关事实必须在知识库中表现出来，因为在向前链接中所有的推论都必须实现进行，这样可以充分减少回答下一个问题所需时间，因为这时只需要进行简单查询，而缺点是，所引用或存储的事实可能是以后永远都用不上的。<strong>产生式系统(production system)</strong>大量使用认知模型的研究成果，通过增加控制知识的方法来决定所要激发的规则，从而提升了向前链接系统。</p>
<blockquote>
<p><strong>Forward chaining</strong> as individual facts are added to the database, all derived inferences are generated<br><strong>Backward chaining</strong> starts from queries, e.g., the Prolog programming language</p>
</blockquote>
<p>关于<strong>向后链接(Backward Chaining)</strong>，第一步是根据提问是否存储在知识库来判定提问公式是否为真，如果提问不在知识库中，下一步就搜索在知识库中有没有可以应用的 implication rule，如果一条规则的结果部分与提问公式相匹配，那么这条规则就是可应用的规则。如果存在任意这样的规则，并且规则的前提为真，那么提问就被证明了。然后把前提作为一个新的提问，就可以递归向后链接。还是以上面的例子为例，我们首先看 Serves(Rudys, VegetarianFood)在不在知识库中，发现它不在，就开始搜索一个可应用的规则可以让我们得到横线上面给定的规则。也就是用常量替换 x 后，证明 VegetarianRestaurant(Rudys)。Prolog就是一个向后链接的语言。如下面的例子，知识库中存在前 5 条知识，我们要证明最后一条 ?- father(M, bill) 是否为真，首先找到第一条，知道我们需要 parent(M, bill), male(M)，才能证明 father(M, bill)  为真，然后发现，parent(john, bill) + male(john)，即 M=john 时，father(M, bill) 为真，得证。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">father(X,Y):  -parent(X, Y), male(X).</div><div class="line">parent(john, bill)</div><div class="line">parent(jane, bill)</div><div class="line">female(jane)</div><div class="line">male(john)</div><div class="line">?- father(M, bill)</div></pre></td></tr></table></figure>
<p>这里要注意的一个概念是<strong>向后链接(Backward Chaining) vs. 向后推理(Backward Inference)</strong></p>
<ul>
<li>向后链接是从提问到已知事实的推理<ul>
<li>可靠的推理方法</li>
</ul>
</li>
<li>向后推理是从已知结果到未知前提的推理<ul>
<li>不可靠的推理方法</li>
<li>如果一个规则的结果为真，就假定其前提也为真</li>
<li>如果知道 Serves(Rudys, VegetarianFood) 为真，那么VegetarianRestaurant(Rudys)也为真</li>
</ul>
</li>
</ul>
<p>一些原则<br><strong>Universal Instantiation</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/9.jpg" class="ful-image" alt="9.jpg"></p>
<p><strong>Existential Instantiation</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/10.jpg" class="ful-image" alt="10.jpg"></p>
<p><strong>Unification</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/11.jpg" class="ful-image" alt="11.jpg"></p>
<h1 id="Description-Logics"><a href="#Description-Logics" class="headerlink" title="Description Logics"></a>Description Logics</h1><p>除了 FOPC 外，常用的意义表示方法还有<strong>语义网络(Semantic network)</strong>、<strong>框架(frame)方法</strong>。其中框架方法又叫做<strong>槽填充(slot-filter)</strong>表示法，特征称为槽(slot)，而这些槽的值用填充者(filter)来表示。这种表示方法比FOL的限制性更强。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Meaning%20Representation%20Languages/7.jpg" class="ful-image" alt="7.jpg">
<p>​</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> semantic analysis </tag>
            
            <tag> 语义分析 </tag>
            
            <tag> FOL </tag>
            
            <tag> FOPC </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[QA system - Question Generation]]></title>
      <url>http://www.shuang0420.com/2017/04/06/QA%20system%20-%20Question%20Generation/</url>
      <content type="html"><![CDATA[<p>关于如何从给定文本中产生合适的问句。–持续更新中–<br><a id="more"></a></p>
<p>做了一段时间的 Asking System，尝试了各种包，也踩了很多的坑，从全无头绪到现在小有心得，中间经历了很多，这里做一些记录。</p>
<p>总的思路是找出简单的陈述句，然后对其进行变形(如下)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">X is Y =&gt; Is X Y?  /   What is Y? (what/why/who/when)</div><div class="line">The X verbs Y =&gt; Does X verb Y  /  What does the X verb?</div></pre></td></tr></table></figure>
<p>第一大问题是怎么找候选陈述句，过于复杂的句子我们需要进行简化，我们要找的句型是 S &lt; (NP $.. VP)，然而就是这样的 NP VP 下接的诸多介词短语或从句也够我们烦恼的了，定义更加限制性的规则还是很有必要的。另外非常适合用来产生问题的句子结构还有 <strong>同位语结构</strong>，<strong>动词修饰名词结构</strong>等等，下文会提到。</p>
<p>找出的这些候选陈述句应当提前做好 <strong>指代消解</strong>，方便进一步处理。</p>
<p>无论是对一般疑问句还是特殊疑问句，关键的都是找 <strong>谓词(predicate)</strong>。</p>
<p>对于一般疑问句而言，如果 predicate 是 auxiliary verb 或者 modal，比如 is, was, are, did, have, etc.，那么直接将 predicate 提前，其余部分按原顺序拼接，而 predicate 不是 auxiliary verb 也不是 modal，那么可能需要将动词还原成 do+动词原形的形式，然后将助动词提前，其余照抄(此时句子中的动词已经还原)。</p>
<p>而对特殊疑问句而言，需要利用 NER 来确定对哪个词进行提问以及用哪种疑问词，NER 的质量在这种场景下显得尤其重要。在实践过程中发现部分的 NER 对时间和地点的识别能力不是很强，这时候可能需要人工规则来进行补充，比如给介词短语添加限制来确定地点等。特殊疑问句另一个问题是一些不可能作为 answer phrase 的词，在下文的 Marking Unmovable Phrases 中也会提到，最为简单粗暴的方法是在 what/who 的问句中只对直接宾语来进行提问。</p>
<p>最后，关于问句的泛化，可以利用 WordNet 来替换一些关键词。</p>
<p>顺便提一下句子类型，在论文里经常会出现 declarative sentence 这类专有名词，这里稍稍解释翻译下。</p>
<ul>
<li>Declarative Sentences<br>  陈述句，陈述一个事实，以句号结尾，是最常见的类型<br>  e.g. There are five million people at risk.</li>
<li>Imperative Sentence<br>  祈使句，以感叹号或者句号结尾<br>  e.g. Fetch my umbrella!/Please bring my umbrella.</li>
<li>Interrogative Sentence<br>  疑问句，以问号结尾<br>  e.g. Can you find my umbrella?</li>
<li>Exclamatory Sentence<br>  感叹句，表达兴奋或者其他情感，感叹号结尾<br>  e.g. You’ve broken my umbrella!</li>
</ul>
<p>在 QG 系统中，我们需要的是 Declarative Sentences</p>
<h1 id="QG-Framework"><a href="#QG-Framework" class="headerlink" title="QG Framework"></a>QG Framework</h1><p>问句如何产生？一般是从文章中提取出合适的可以产生问句的 candidate，然后对其进行变形来得到。很多有用的问题可以被看做是把一个陈述句通过 <strong>词汇(lexical)、句法(syntactic)、语义(semantic)</strong> 层面的转化得到的。因此，在产生问句的阶段，我们需要把原有的句子转化为陈述句。</p>
<p>看一下 question generation 的框架。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/1.jpg" class="ful-image" alt="1.jpg"></p>
<p><strong>Stage 1</strong>，通过改变词汇来将原有语句转化为陈述句。有些句子很复杂，有些句子我们只需要某些成分，所以可以通过一些方法将原有的句子化简，形成能够产生问句的陈述句。在这个阶段，很多 NLP transformation 的方法都会被用到，包括 extractive summarization, sentence compression, sentence splitting, sentence fusion, paraphrase, textual entailment, lexical semantics for word substitution 等。</p>
<p><strong>Stage 2</strong>，通过进行一些句法层面的转化(如 WH-movement, subject-auxiliary inversion 等)将陈述句转换为问句，这个模块又叫 question transducer。</p>
<p><strong>Stage 3</strong>，对上一阶段产生的问题进行排序，特征可以是 source sentence, input sentences, question, transformation 各部分的特征集合。</p>
<p>明确一些概念：</p>
<ul>
<li>source sentence<br>  输入文档中的原始句子</li>
<li>input sentence<br>  question transducer 的输入，可能是原始句子也可能是经过转化的陈述句</li>
<li>answer phrase<br>  陈述句里可能被作为问句成分的目标词(targets for WH-movement)</li>
<li>question phrase<br>  替代 answer phrase 的疑问词</li>
</ul>
<h2 id="Stage-1"><a href="#Stage-1" class="headerlink" title="Stage 1"></a>Stage 1</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>输入：</strong> 原始文本<br><strong>输出：</strong> 陈述句</p>
<p>过于复杂的句子往往会导致不自然或者没有意义的问句，因此我们首先需要进行预处理以便简化 transformation。可以做的比如移除一些短语类型，像是句子开始的连词，句子层次的修饰语等，在论文 headline generation (Dorr and Zajic, 2003) and summarization (Toutanova et al., 2007) 中有具体介绍。一些规则如下</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/2.jpg" class="ful-image" alt="2.jpg">
<h3 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h3><p>实际项目中为了得到更快的速度，我们大大简化了这一流程，首先，忽略所有过长的句子(&gt;=50个单词)，其次，只选择 <strong>同位语(APPOSITION)、动词修饰短语(VERB_MODIFIER)、名词动词结构的短语(NP_VP)</strong> 三种类型的句子成分作为产生问句的 candidate。</p>
<p>规则<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NP_VP = &quot;S &lt; (NP=np ?$PP=pp1 $.. (VP=vp &lt; (/VB.?/=tensed ?$.. SBAR=reason)))&quot;</div><div class="line">APPOSITION = &quot;NP !&lt; CC !&lt; CONJP &lt; (NP=np1 $.. (/,/ $.. (NP=app $.. /,/)))&quot;</div><div class="line">VERB_MODIFIER = &quot;NP=noun &gt; NP $.. VP=modifier&quot;</div></pre></td></tr></table></figure></p>
<p>操作这些规则还是用 tregex，由于是 python，选择 Stanford coreNLP，具体见 <a href="http://www.shuang0420.com/2017/03/24/ParseTree操作若干-Tregex%20and%20Stanford%20CoreNLP/">ParseTree操作若干-Tregex and Stanford CoreNLP</a>。</p>
<p>对标注好的部分进行重组，形成规范化的简单句子。以 NP_VP 规则为例，就是形成 NP + VP + PP1的句子。</p>
<h2 id="Stage-2"><a href="#Stage-2" class="headerlink" title="Stage 2"></a>Stage 2</h2><p><strong>输入：</strong> 上一步产生的陈述句<br><strong>输出：</strong> 一个问句集合</p>
<p>先来看一下流程图，下图描述了如何从给定文本产生一个问句，带*的表示特殊疑问句必须的步骤，一般疑问句不需要。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/0.jpg" class="ful-image" alt="0.jpg"></p>
<p>answer type 可以是名词短语或者是介词短语，能够产生 who, what, where, when, how much 这类的问句。系统可以进行扩展来产生其他的类似 how, why, what kind of 这类的问句。</p>
<p>一个句子可能存在多个可能的 answer phrase，而一个 answer phrase 可能产生多个 question phrase，举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Declarative sentence:</div><div class="line">Francium was discovered by Marguerite Perey in France in 1939</div><div class="line"></div><div class="line">Possible questions:</div><div class="line">• Where was francium discovered by Marguerite Perey in 1939?</div><div class="line">• When was francium discovered by Marguerite Perey in France?</div><div class="line">• Was francium discovered by Marguerite Perey in France in 1939?</div><div class="line">• By what was francium discovered in France in 1939?</div></pre></td></tr></table></figure>
<p>当然，最后一个句子是一个错误，Marguerite Perey 应该是个人。Question transducer 会 overgenerate 很多不相关、不重要的问句。这一阶段会用到的技术：</p>
<ul>
<li>mark phrases that cannot be answer phrases, due, for example, to island constraints;</li>
<li>remove each answer phrase and generate possible question phrases for it;</li>
<li>decompose the main verb;</li>
<li>invert the subject and auxiliary verb;</li>
<li>insert one of the question phrases</li>
</ul>
<p>一个完整的例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/3.jpg" class="ful-image" alt="3.jpg"></p>
<h3 id="Marking-Unmovable-Phrases"><a href="#Marking-Unmovable-Phrases" class="headerlink" title="Marking Unmovable Phrases"></a>Marking Unmovable Phrases</h3><p>input tree 中有些短语由于 WH-movement 的限制，是不可能作为 answer phrases 的，所以我们可以预先把这些成分用 UNMV- 标记给替换掉，这部分的操作可以是并行的，因为它们的顺序不重要。当然这里有两条规则是例外的，它们用在限制 input tree 的下一步传播，适用于所有其他规则，第一条规则把 UNMV- 节点下的所有节点都标记为 UNMV-。第二条规则把 movable 节点下的所有节点标记为 UNMV-，因为名词短语是可以移动的 islands，名词短语是可以移动的这一事实限制很有用，比如说，在关系从句中的短语是不能移动的，如下面产生的问句是不合理的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input sentence:</div><div class="line">I bought the book that inspired Bob.</div><div class="line"></div><div class="line">Question:</div><div class="line">Who did I buy the book that inspired?</div></pre></td></tr></table></figure>
<p>具体规则如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/4.jpg" class="ful-image" alt="4.jpg"></p>
<h3 id="Generating-Possible-Question-Phrases"><a href="#Generating-Possible-Question-Phrases" class="headerlink" title="Generating Possible Question Phrases"></a>Generating Possible Question Phrases</h3><p>transducer 查看所有可能的 answer phrase，对每一个可能，复制 input tree，移除 answer phrase，产生可能的 question phrase(这个过程在 yes-no 问句中省略)。</p>
<p>给定一个 answer phrase，question phrase 包含一个 question word(e.g., who,what,where,when)，也可能是个介词短语如 whose car，跟着 answer phrase 的 head。</p>
<p>对一个给定的 answer phrase，系统使用这个短语的 entity label 和句法结构来产生一系列可能的 question phrase，每一个都用来产生最后的问句。下图描述了产生 question phrase 的一些限制。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/5.jpg" class="ful-image" alt="5.jpg">
<h3 id="Practice-1"><a href="#Practice-1" class="headerlink" title="Practice"></a>Practice</h3><p>实践中我们用 spacy 来检测 named entity，主要是因为 spacy 的速度很快，</p>
<p>先对所有 word 进行遍历，找到 PRP 或者 NUM，判断是否能形成 whose 以及 how many/how much 的问题，同时记录是否有 named entity，如果有，再来看 named entity</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">if word.tag_.startswith(&apos;PRP&apos;) and is_possessive(word):</div><div class="line">	question_phrase = &apos;whose&apos;</div><div class="line">if word.pos_.startswith(&apos;NUM&apos;) and word.ent_type_ != &apos;DATE&apos;::</div><div class="line">	question_phrase = &apos;how many_how much&apos;</div></pre></td></tr></table></figure>
<p>之所以要分两步，是因为上面的 whose 和 how many/how much 其实是对单词提问的，而剩下的 who/where/when/how/why 是对短语进行提问的。对于 who/where/when，可以用 named entity 来进行识别。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">if ne.label_ == &apos;PERSON&apos;:</div><div class="line">	question_phrase = &apos;who&apos;</div><div class="line">if ne.label_ in [&apos;LOC&apos;,&apos;GPE&apos;]:</div><div class="line">	question_phrase = &apos;where&apos;</div><div class="line">if ne.label_ in [&apos;DATE&apos;,&apos;TIME&apos;]:</div><div class="line">	question_phrase = &apos;when&apos;</div></pre></td></tr></table></figure>
<p>对于 why 和 how，基本是基于规则，如果在 reason 部分有 because, since, as 等表示原因的单词，那么就可以产生 why 的问句，如果在 pp2 部分有 using, by, through, with, via 等表示方式的单词，就可以产生 how 的问句。</p>
<p>值得注意的是，有些数字后面跟的是速度的单位，比如说 km/s, mph 等，这时候问 how many(much) km/s 就有点诡异了，所以可以加个判断，如果量词后面跟这些单词，就产生 how fast 的问句；另外有些数字后面如果跟的是 %，那么就变成 how many percent 这种问句。至于用 how many 还是 how much，就看下一个名词是单数还是复数了(注意如果量词是 one，下一个名词是单数，然而提问方式得变成 how many noun_plural)。</p>
<h3 id="Decomposition-of-the-Main-Verb"><a href="#Decomposition-of-the-Main-Verb" class="headerlink" title="Decomposition of the Main Verb"></a>Decomposition of the Main Verb</h3><h4 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h4><p>为了实现 subject-auxiliary inversion，如果一个 auxiliary verb 或者 modal 没有出现，question transducer 将会把主要的动词还原成 do(合适形态) + 动词原形，然后修改动词短语的树的结构。</p>
<p>如 John saw Mary 会变成 John did see Mary。</p>
<p>如果 auxiliary verb 已经存在了，那么并不需要 decomposition，如 John has seen Mary 会变成 Who has John seen。</p>
<p>下图识别了需要 decompose 的动词</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/6.jpg" class="ful-image" alt="6.jpg">
<p>另外，可以通过 WordNet 来 lemmatize each verb first by checking morphological variants，每次从动词的最右边去掉一个字母直到在 WordNet 里找到匹配的单词，这种方法非常适合英文因为大多数动词都可以从 lemma 添加几个字母(如 ed)派生得到，或者从 WordNet 里的 lemma 匹配得到。</p>
<h4 id="Practice-2"><a href="#Practice-2" class="headerlink" title="Practice"></a>Practice</h4><p>这里用到了 patterns.en 的包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from patterns.en import lemma</div><div class="line"># check if contains auxiliary</div><div class="line">def has_auxiliary(head_verb_tag, verb):</div><div class="line">    if head_verb_tag==&apos;MD&apos; or lemma(verb[0]) in [&apos;be&apos;,&apos;do&apos;,&apos;have&apos;] and len(verb)&gt;1:</div><div class="line">        return True</div><div class="line">    return False</div><div class="line"></div><div class="line"></div><div class="line"># decomposition of verb</div><div class="line">def decompose_verb(verb, verb_tag):</div><div class="line">    tense = verb_tense_dict[verb_tag]</div><div class="line">    return conjugate(&apos;do&apos;, tense), lemma(verb)</div></pre></td></tr></table></figure></p>
<h3 id="Subject-Auxiliary-Inversion"><a href="#Subject-Auxiliary-Inversion" class="headerlink" title="Subject-Auxiliary Inversion"></a>Subject-Auxiliary Inversion</h3><p>这一部分仍然是对句子的操作，目的是把陈述句变成问句。</p>
<p>首先，parse tree 的 “S” 需要被重新标记为“SQ”，表示它是问题的一部分。然后移动 auxiliary 或 copula 移动，使它成为 SQ 节点的第一个 child。然后使用 “SQ” 节点为该句子形成一个新树。在 yes-no questions 的情况下，问题树的根节点将具有 “SQ” 节点作为该树唯一的 child。在 WH-questions 的情况下，根节点具有 “SBARQ” 子节点。然后，该 “SBARQ” 节点具有一个问题短语节点(例如“WHNP”) 以及一个 “SQ” 作为子节点。</p>
<p>在转换成问句后，在这个节点和逗号之前的所有修饰语都会被移动到句子的前面，以便产生更自然的问题，如产生 Following Thomas Jefferson, who was elected the 4th president? 而不是 Who, following Thomas Jefferson, was elected the 4th president?</p>
<h3 id="Inserting-Question-Phrases"><a href="#Inserting-Question-Phrases" class="headerlink" title="Inserting Question Phrases"></a>Inserting Question Phrases</h3><p>将每个可能的 question phrase 插入作为 SBARQ 的子节点，如果是一般疑问句，就不用进行这一步了。</p>
<h3 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h3><p>为了保证正确的 formatting and punctuation。句号要改为问号，输出要 detokenized，移除多余的空格。另外，所有包含了代词(pronouns)的问句都太模糊了(e.g., What does it have as a head of state)，所以可以过滤掉所有有人称代词(personal pronouns)，所属代词(possessive pronouns)，以及只包含了限定词(determiners)的名词短语。</p>
<p>当然，改进的版本是进行指代消解。</p>
<h2 id="Stage-3"><a href="#Stage-3" class="headerlink" title="Stage 3"></a>Stage 3</h2><h3 id="Discriminative-reranker"><a href="#Discriminative-reranker" class="headerlink" title="Discriminative reranker"></a>Discriminative reranker</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/7.jpg" class="ful-image" alt="7.jpg">
<p>上图描述了 bad question 出现的一些原因。在产生一大堆可能的问题后，我们要对问句进行排序。论文提到的是用 logistic regression 来定义一个 acceptable probability p(a|q,t) 和 unacceptable probability p(u|q,t) = 1-p(a|q,t)。先训练一个 boolean model ，只要出现上面的任何一种问题，就认为这个问句是 unacceptable 的。接着训练一个 aggregate model，模型如下</p>
<p>$$p(a|q,t)=\prod^K_{i=1}p_i(a_i|q,t)$$</p>
<p>其中 i 是影响 acceptable 的各个因素(grammaticality, making sense, vagueness, etc.), K 在这里是 8。</p>
<p>用到的特征如下图，目标函数是 regularized log-likelihood，regularizaiton constant 由 cross-validation 选择产生。用到的训练集有 WIKI-ENG，WIKI-SIMP 和 theWall Street Journal data in the Penn Treebank</p>
<p>(Marcus et al., 1993)。实验结果表明，aggregate ranking 在 P@10 上提升了 43.3%，在 P@25 上提升了40%。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/8.jpg" class="ful-image" alt="8.jpg">
<h3 id="Practice-3"><a href="#Practice-3" class="headerlink" title="Practice"></a>Practice</h3><p>实际中我们并没有来训练模型，主要是因为找不到合适的有标注的训练集。我们用了非常简单的几个标准来进行排序。</p>
<ul>
<li>Complexity<br>看 question 包含了多少个单词<br>划分4 个 bucket [0, 5], [5, 20], [20, 30], [30,…]，如果在 [5,20] 之间，给最高分，在 [0, 5] 或者 [30,…] 间，就给 penalty</li>
<li>Difficulty<br>基本假设是一个好的问题应该有一个确定的答案与之相配。理想情况下是看这个问句能不能被回答，以及容不容易被回答。<br>然而这里的答案抽取、问题产生大部分依赖规则，所以也用规则来定义问题难度了，认为 what, why, how 比较难回答， question word 在中间的句子比较难回答，这些规则通过观察部分有标签的数据集得到</li>
<li>Diversity<br>给每种类型的问句设置权重，在结果集里各类问句都按比例出现，保证多样性</li>
<li>Fluency<br>用 ginger server 来检查语法</li>
</ul>
<h1 id="Yes-No-Question"><a href="#Yes-No-Question" class="headerlink" title="Yes/No Question"></a>Yes/No Question</h1><p>举个例子说明 binary question<br>例句1 (包含 auxiliary verb 或者 modal)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The cat had/MD slept/VBD on the mat.</div></pre></td></tr></table></figure></p>
<p>我们需要做的是将 MD 前置，拼接句子，将句尾符号变为问号，成为答案是 YES 的问句。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">=&gt; Had the cat slept on the mat?</div><div class="line"></div><div class="line">Yes!</div></pre></td></tr></table></figure></p>
<p>例句2 (不包含 auxiliary verb 或者 modal)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The cat slept/VBD on the mat.</div></pre></td></tr></table></figure></p>
<p>我们需要做的是 decompose verb，把 slept 变成 did sleep，然后将 did 前置，拼接句子，将句尾符号变为问号，成为答案是 YES 的问句。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">=&gt; Did the cat slept on the mat?</div><div class="line"></div><div class="line">Yes!</div></pre></td></tr></table></figure></p>
<p><strong>怎么产生答案是 NO 的句子？</strong></p>
<p>将句子中的 head word 随机替换成同等词性的词。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">=&gt; Had the dog slept on the mat?</div><div class="line"></div><div class="line">No!</div></pre></td></tr></table></figure></p>
<h1 id="WH-Question"><a href="#WH-Question" class="headerlink" title="WH Question"></a>WH Question</h1><p> <strong>Detect Candidate Sentences and Mark Movable Phrases</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- Input:</div><div class="line">In 2008, because Olympics is coming, 5 valuable pandas including Jingjing were born in Beijing, China.</div><div class="line">- Rule:</div><div class="line">S &lt; (NP=np ?$PP=pp1 $.. (VP=vp &lt; (/VB.?/=tensed ?$.. SBAR=reason)))</div></pre></td></tr></table></figure>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/9.jpg" class="ful-image" alt="9.jpg">
<p><strong>Yes-no Questions</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- Yes Question:</div><div class="line">Were 5 valuable pandas including Jingjing born in Beijing , China in 2008?</div><div class="line">- No Question:</div><div class="line">Were the swallows born in Beijing , China in 2008?</div></pre></td></tr></table></figure>
<p><strong>Choose Answer Phrases and Generate Question Phrases</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/QA%20system%20-%20Question%20Generation/10.jpg" class="ful-image" alt="10.jpg">
<p><strong>Decompose Verb</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Eg.1. In 2008, because Olympics is coming, 5 valuable pandas including Jingjing were born in Beijing, China.</div><div class="line">were =&gt; were</div><div class="line">Eg.2. All four allies retained shared responsibility for Berlin.</div><div class="line">retained =&gt; did retain</div></pre></td></tr></table></figure>
<p><strong>Invert Subject and Auxiliary and Insert Question Phrase</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">- Input:</div><div class="line">5 valuable pandas including Jingjing were born in Beijing, China.</div><div class="line">- Output:</div><div class="line">were 5 valuable pandas including jingjing born in Beijing , China in 2008</div><div class="line">were 5 valuable pandas including jingjing born in Beijing , China in 2008</div><div class="line">where were 5 valuable pandas including Jingjing born in 2008</div><div class="line">when were 5 valuable pandas including Jingjing born in Beijing , China</div><div class="line">how many valuable pandas including Jingjing were born in Beijing , China in 2008</div><div class="line">what were born in Beijing , China in 2008</div></pre></td></tr></table></figure>
<p><strong>Post Processing</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">- Input:</div><div class="line">5 valuable pandas including Jingjing were born in Beijing, China.</div><div class="line">- Output:</div><div class="line">Where were 5 valuable pandas including Jingjing born in 2008?</div><div class="line">Were 5 valuable pandas including jingjing born in Beijing , China in 2008?</div><div class="line">When were 5 valuable pandas including Jingjing born in Beijing , China?</div><div class="line">How many valuable pandas including Jingjing were born in Beijing , China in 2008?</div><div class="line">What were born in Beijing , China in 2008?</div><div class="line">Were 5 valuable pandas including Jingjing born in Beijing , China in 2008?</div></pre></td></tr></table></figure>
<h1 id="Tools-Packages"><a href="#Tools-Packages" class="headerlink" title="Tools/Packages"></a>Tools/Packages</h1><ul>
<li>python==2.7</li>
<li>spaCy==1.7.5</li>
<li>pattern==2.6</li>
<li>textblob==0.12.0</li>
<li>nltk==3.2.2</li>
<li>requests==2.13.0</li>
<li>scipy==0.18.1</li>
<li>sklearn==0.18.1</li>
<li>numpy==1.11.3</li>
<li>stanford-corenlp-full-2016-10-31</li>
</ul>
<h1 id="历程"><a href="#历程" class="headerlink" title="历程"></a>历程</h1><h2 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h2><ul>
<li>只寻找 NP VP 结构，用的 pattern 为 S &lt; (NP=np $.. (VP=vp &lt; /VB.?/=tensed))</li>
<li>主语为代词(PROP)跳过，不做指代消解(速度慢)</li>
<li>NER 考虑了 PERSON, LOC, GPE</li>
<li>对每个句子产生 yes/no 问句，包含 NER 的产生对应的 WHAT/WHO/WHERE 问句</li>
</ul>
<p>原文(PART)</p>
<blockquote>
<p>Ant</p>
<p>Ants are social insects of the family Formicidae ( ), and along with the related wasps and bees, they belong to the order Hymenoptera. Ants evolved from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants. Today, more than 12,500 species are classified with upper estimates of about 22,000 species.          They are easily identified by their elbowed antennae and a distinctive node-like structure that forms a slender waist.</p>
<p>Ants form colonies that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies which may occupy large territories and consist of millions of individuals. These larger colonies consist mostly of sterile wingless females forming castes of “workers”, “soldiers”, or other specialised groups. Nearly all ant colonies also have some fertile males called “drones” and one or more fertile females called “queens”. The colonies are sometimes described as superorganisms because the ants appear to operate as a unified entity, collectively working together to support the colony.</p>
</blockquote>
<p>产生问句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">NP VP(aux)	Are ants social insects of the family Formicidae -LRB- -RRB-</div><div class="line">WHO NP VP	Who are ants</div><div class="line">WHAT NP VP	What are ants</div><div class="line">NP VP(aux)	Are more than 12,500 species classified with upper estimates of about 22,000 species</div><div class="line">WHAT NP VP(aux)	What are more than 12,500 species</div><div class="line">NP VP(no-aux)	Do ants form colonies that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies which may occupy large territories and consist of millions of individuals</div><div class="line">WHAT NP VP(aux)	What do ants form</div><div class="line">NP VP(no-aux)	Do these larger colonies consist mostly of sterile wingless females forming castes of `` workers &apos;&apos; , `` soldiers &apos;&apos; , or other specialised groups</div><div class="line">WHAT NP VP(aux)	What do these larger colonies consist</div><div class="line">NP VP(no-aux)	Did males call `` drones &apos;&apos; and one</div><div class="line">WHAT NP VP(aux)	What did males call</div><div class="line">NP VP(aux)	Have nearly all ant colonies some fertile males called `` drones &apos;&apos; and one or more fertile females called `` queens &apos;&apos;</div><div class="line">WHAT NP VP(aux)	What did nearly all ant colonies some</div><div class="line">NP VP(no-aux)	Do the ants appear to operate as a unified entity , collectively working together to support the colony</div><div class="line">WHAT NP VP(aux)	What do the ants appear</div><div class="line">NP VP(aux)	Are the colonies sometimes described as superorganisms because the ants appear to operate as a unified entity , collectively working together to support the colony</div><div class="line">WHAT NP VP(aux)	What are the colonies</div><div class="line">NP VP(aux)	Have ants colonised almost every landmass on Earth</div><div class="line">WHERE NP VP(aux) 	Where did ants colonise colonised</div><div class="line">NP VP(aux)	Are the only places lacking indigenous ants Antarctica and certain remote or inhospitable islands</div><div class="line">WHERE NP VP(aux) 	Where are the only places lacking indigenous ants are</div><div class="line">NP VP(aux)	Has their long co-evolution with other species led to mimetic , commensal , parasitic , and mutualistic relationships</div><div class="line">WHAT NP VP(aux)	What did their long co-evolution with other species lead</div><div class="line">NP VP(aux)	Have hlldobler &amp; Wilson -LRB- 1990 -RRB- , p. 471 Ant societies division of labour , communication between individuals , and an ability to solve complex problems</div><div class="line">WHAT NP VP(aux)	What did hlldobler &amp; Wilson -LRB- 1990 -RRB- , p. 471 Ant societies division</div><div class="line">NP VP(aux)	Have these parallels with human societies long been an inspiration and subject of study</div><div class="line">WHAT NP VP(aux)	What did these parallels with human societies long</div><div class="line">NP VP(no-aux)	Do many human cultures make use of ants in cuisine , medication and rituals</div><div class="line">WHAT NP VP(aux)	What do many human cultures make</div><div class="line">NP VP(aux)	Are some species valued in their role as biological pest control agents</div><div class="line">WHAT NP VP(aux)	What are some species</div><div class="line">NP VP(aux)	Is the word ant derived from ante of Middle English which is derived from mette of Old English and is related to the Old High German meiza , hence the modern German Ameise</div><div class="line">WHERE NP VP(aux) 	Where is the word ant derived is derived is related</div><div class="line">WHO NP VP	Who is the word ant</div><div class="line">WHAT NP VP	What is the word ant</div><div class="line">NP VP(aux)	Was the original meaning of the word `` the biter &apos;&apos; -LRB- from Proto-Germanic * ai - , `` off , away &apos;&apos; + * mait - `` cut &apos;&apos; -RRB-</div><div class="line">WHAT NP VP(aux)	What was the original meaning of the word</div><div class="line">NP VP(no-aux)	Do all of these words come from West Germanic * amaitjo</div><div class="line">WHERE NP VP(aux) 	Where do all of these words come</div><div class="line">NP VP(aux)	Are the words in other Romance languages such as the Portuguese formiga , Italian formica , Spanish hormiga , Romanian furnic and French fourmi derived</div><div class="line">WHAT NP VP(aux)	What are the words in other Romance languages such as the Portuguese formiga , Italian formica , Spanish hormiga , Romanian furnic and French fourmi</div><div class="line">NP VP(aux)	Is the family name Formicidae derived from the Latin formca -LRB- `` ant &apos;&apos; -RRB- from which the words in other Romance languages such as the Portuguese formiga , Italian formica , Spanish hormiga , Romanian furnic and French fourmi are derived</div><div class="line">WHAT NP VP(aux)	What is the family name Formicidae</div><div class="line">NP VP(no-aux)	Does formicidae belong to the order Hymenoptera , which also includes sawflies , bees and wasps</div><div class="line">WHAT NP VP(aux)	What does formicidae belong</div><div class="line">NP VP(no-aux)	Did ants fossilise in Baltic amber The family Formicidae belongs to the order Hymenoptera , which also includes sawflies , bees and wasps</div><div class="line">WHO NP VP	Who did ants fossilise</div><div class="line">WHAT NP VP	What did ants fossilise</div><div class="line">NP VP(no-aux)	Did ants evolve from a lineage within the vespoid wasps</div><div class="line">WHAT NP VP(aux)	What did ants evolve</div></pre></td></tr></table></figure>
<p>第一个问题是指代消解，如下面这个句子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Ants are social insects of the family Formicidae ( ), and along with the related wasps and bees, they belong to the order Hymenoptera.</div><div class="line"></div><div class="line">Is supposed to generate</div><div class="line">=&gt;</div><div class="line">NP VP(no-aux)Do ants belong to the order Hymenoptera</div><div class="line">WH NP VP(no-aux)What do ants belong to</div></pre></td></tr></table></figure>
<p>应该同时能产生这两个问题，然而现在的代码遇到代词直接跳过，并没有进行匹配，所以不能产生。可以用 standford parser 做一次指代消解，然而再请求一次，加上标注、parse，时间上耗费太久。所以现在想着把上一步的 np 存到一个 list 里，如果下一个 np 匹配找到的是 PRP，就用 np 里的替换。这个解决方案也可以为产生 yes-no question 的 no question 做准备。</p>
<p>第二个问题是介词造成的不完整句子，需要对介词进行进一步处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">What does ants consist =&gt; What does ants consist of</div></pre></td></tr></table></figure>
<p>第三个问题是关于 have，have 可以作为完成时的 auxiliary，也可以作为行为动词 ‘have’ 表示拥有，而这里我默认了 have 作为 auxiliary 出现，其实是不对的，需要分情况讨论</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Have nearly all ant colonies some fertile males called `` drones &apos;&apos; and one or more fertile females called `` queens &apos;&apos;</div><div class="line"></div><div class="line">is supposed to be</div><div class="line">=&gt;</div><div class="line">Do nearly all ant colonies have some fertile males called `` drones &apos;&apos; and one or more fertile females called `` queens &apos;&apos;</div></pre></td></tr></table></figure>
<p>类似的问题是 be，比如 ‘A is B’ 和 ‘A is described as B’ 应该区分对待。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">NP VP(aux)	Are the colonies sometimes described as superorganisms because the ants appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">=&gt;</div><div class="line">WHAT NP VP	What are the colonies ?</div><div class="line"></div><div class="line">is supposed to be</div><div class="line">=&gt;</div><div class="line">What are the colonies described as ?</div></pre></td></tr></table></figure>
<p>然而这里有一个难点是 ‘A is described as B’ 和 ‘A is valued ‘</p>
<p>第四个问题是疑问词，因为只 handle 了 WHO, WHAT, WHERE 的问题，而实际上，一些问句应该是 WHEN 的问题，如下。现在想到的是对于包含介词短语的句子，同时产生 WHERE 和 WHEN 两种问句类型，同样的，对于之前产生 WHO 的句子，也产生一个 WHAT 版本，然后由 QUESTION RANKING 来排序筛选。</p>
<p>关于 WHEN，尽量用命名实体 TIME 以及正则规则匹配，准确率会更高一些。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Ants evolved from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago and diversified after the rise of flowering plants.</div><div class="line">=&gt; What did ants diversify</div><div class="line">Supposed to be</div><div class="line">=&gt; When did ants diversify</div></pre></td></tr></table></figure>
<p>最后还有一些特殊符号的匹配，可以通过 post-processing 做处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-LRB- -RRB-   =&gt; ()</div><div class="line">`` 	=&gt; &quot;</div></pre></td></tr></table></figure>
<h2 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h2><p>基本解决了第一阶段的问题，并加了 ranking。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">Do ants form colonies that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies which may occupy large territories and consist of millions of individuals ?</div><div class="line">More than 12,500 species are classified with upper estimates of about how many species ?</div><div class="line">What do ants form that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies which may occupy large territories and consist of millions of individuals ?</div><div class="line">What have some fertile males called drones and one or more fertile females called queens ?</div><div class="line">Do males appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">How many species are more than 12,500 species classified with ?</div><div class="line">What are classified with upper estimates of about 22,000 species ?</div><div class="line">Are ants social insects of the family Formicidae ?</div><div class="line">What do these larger colonies consist forming castes of workers , soldiers , or other specialised groups ?</div><div class="line">Are more than 12,500 species classified with upper estimates of about 22,000 species ?</div><div class="line">Do more than 12,500 species consist mostly of sterile wingless females forming castes of workers , soldiers , or other specialised groups ?</div><div class="line">Do these larger colonies consist mostly of sterile wingless females forming castes of workers , soldiers , or other specialised groups ?</div><div class="line">What did males call and one ?</div><div class="line">What form colonies that range in size from a few dozen predatory individuals living in small natural cavities to highly organised colonies which may occupy large territories and consist of millions of individuals ?</div><div class="line">Are people classified with upper estimates of about 22,000 species ?</div><div class="line">Do people have some fertile males called drones and one or more fertile females called queens ?</div><div class="line">What do the ants appear , collectively working together to support the colony ?</div><div class="line">Do the ants appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">What do nearly all ant colonies have called drones and one or more fertile females called queens ?</div><div class="line">Do nearly all ant colonies have some fertile males called drones and one or more fertile females called queens ?</div><div class="line">Are nearly all ant colonies sometimes described as superorganisms because the ants appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">How many species are classified with upper estimates of about 22,000 species ?</div><div class="line">Did males call drones and one ?</div><div class="line">What are ants of the family Formicidae ?</div><div class="line">Are people social insects of the family Formicidae ?</div><div class="line">What are more than 12,500 species classified with of about 22,000 species ?</div><div class="line">What are the colonies because the ants appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">What appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">What called drones and one ?</div><div class="line">What are social insects of the family Formicidae ?</div><div class="line">What are sometimes described as superorganisms because the ants appear to operate as a unified entity , collectively working together to support the colony ?</div><div class="line">What consist mostly of sterile wingless females forming castes of workers , soldiers , or other specialised groups ?</div><div class="line">Are the colonies sometimes described as superorganisms because the ants appear to operate as a unified entity , collectively working together to support the colony ?</div></pre></td></tr></table></figure>
<!--

# 系统

把文档转化为 sentence parse tree 的集合，首先进行一次过滤:

- 删掉太短/太长的句子
- 删掉没有动词(Verb)的句子
- 删掉有很多介词短语(PRP)的句子


## 人工规则

人工定义一些规则，比如说有两个名词短语(NP)中间由逗号隔开，那么，很可能是同位语，就可以转化成 NP1 is NP2 的形式



## WH Question

主要根据有 NER 标签的句子产生，通常看句子第一个单词的 NER 属性，如果是 person，就转化成 WHO 的问句，如

Donovan/B-noun.PERSON went/O to/O Paris/B-noun.LOCATION.

=> Who went to Paris?



## Score

### Reasonable
看句子是不是太长/太短，是不是有否定词看起来让人摸不着头脑

### Fluent
找 question treebank，看句子符合一个英文问句语法的概率是多少


### Intelligent
一个好的问题往往有一个好的答案



# Process
1. Answer type
2. Tag
3. Look only the words that are not in the question, see if the tag matches the answer type
4. Return the smallest subtree containing these words




YES/NO
all the important words in question are in the same answer source, if not, we say not, if reasonable, we say yes
-->
<blockquote>
<p>参考链接：<br><a href="http://www.cs.cmu.edu/~mheilman/papers/heilman-smith-qg-tech-report.pdf" target="_blank" rel="external">Question Generation via Overgenerating Transformations and Ranking</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[EMR 大文件操作及 Mapreduce 按 Value 排序若干]]></title>
      <url>http://www.shuang0420.com/2017/04/03/EMR%20%E5%A4%A7%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%8F%8A%20Mapreduce%20%E6%8C%89%20Value%20%E6%8E%92%E5%BA%8F%E8%8B%A5%E5%B9%B2/</url>
      <content type="html"><![CDATA[<p>关于 EC2 出现的 No space left on device 问题以及 Mapreduce postprocessing 怎么按 Value 排序<br><a id="more"></a></p>
<h1 id="No-space-问题"><a href="#No-space-问题" class="headerlink" title="No space 问题"></a>No space 问题</h1><p>EMR 开的 m3.xlarge 的机器，但是发现不能下载 8G 的压缩包，更别说是 30G 的文件了，显示的是 No space left on device 的错误。来看一下当前磁盘的情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[hadoop@ip-172-31-24-108 ~]$ df -h</div><div class="line">Filesystem      Size  Used Avail Use% Mounted on</div><div class="line">devtmpfs        7.4G   28K  7.4G   1% /dev</div><div class="line">tmpfs           7.4G     0  7.4G   0% /dev/shm</div><div class="line">/dev/xvda1      9.8G  8.2G  1.5G  85% /</div><div class="line">/dev/xvdb1      5.0G   34M  5.0G   1% /emr</div><div class="line">/dev/xvdb2       33G  334M   33G   2% /mnt</div><div class="line">/dev/xvdc        38G   35M   38G   1% /mnt1</div><div class="line">[hadoop@ip-172-31-24-108 ~]$ sudo file -s /dev/xvdc</div><div class="line">/dev/xvdc: SGI XFS filesystem data (blksz 4096, inosz 256, v2 dirs)</div><div class="line">[hadoop@ip-172-31-24-108 ~]$ lsblk</div><div class="line">NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</div><div class="line">xvdb    202:16   0 37.5G  0 disk</div><div class="line">├─xvdb1 202:17   0    5G  0 part /emr</div><div class="line">└─xvdb2 202:18   0 32.5G  0 part /mnt</div><div class="line">xvdc    202:32   0 37.5G  0 disk /mnt1</div><div class="line">xvda1   202:1    0   10G  0 disk /</div></pre></td></tr></table></figure>
<p>发现硬盘 /dev/xvdc 的大小为 37.5G，足够容纳我们的文件，并且它有 MOUNTPOINT，建立好了文件系统，说明已经可以投入使用啦。</p>
<p>新建文件夹并 MOUNT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[hadoop@ip-172-31-24-108 ~]$ sudo mkdir /localinput</div><div class="line">[hadoop@ip-172-31-24-108 ~]$ sudo mount /dev/xvdc /localinput</div><div class="line">[hadoop@ip-172-31-24-108 ~]$ cd /localinput/</div></pre></td></tr></table></figure></p>
<p>COPY 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[hadoop@ip-172-31-24-108 localinput]$ aws s3 cp s3://95869/com-friendster.ungraph.txt .</div><div class="line">download: s3://95869le/com-friendster.ungraph.txt to ./com-friendster.ungraph.txt</div></pre></td></tr></table></figure></p>
<h1 id="Mapreduce-按-Value-排序"><a href="#Mapreduce-按-Value-排序" class="headerlink" title="Mapreduce 按 Value 排序"></a>Mapreduce 按 Value 排序</h1><p>Input 是 (key, frequency)，主要思路是 Mapper 里将 key,value 反转变成 (frequency, key)，然后定义 sort 的方法为 numerical descending order，reducer 再按正常顺序 (key, frequency) 输出。</p>
<h2 id="Hadoop-batch-Java"><a href="#Hadoop-batch-Java" class="headerlink" title="Hadoop batch + Java"></a>Hadoop batch + Java</h2><p>用 JAVA 做其实是件很简单的事，在 main 函数里设置 Configuration，然后写个 Comparator<br><strong>Comparator</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">public static class IntComparator extends WritableComparator &#123;</div><div class="line">  public IntComparator() &#123;</div><div class="line">    super(IntWritable.class);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  @Override</div><div class="line">  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123;</div><div class="line">    Integer v1 = ByteBuffer.wrap(b1, s1, l1).getInt();</div><div class="line">    Integer v2 = ByteBuffer.wrap(b2, s2, l2).getInt();</div><div class="line">    return v2.compareTo(v1);</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>Configuration</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">public static void main(String[] args) throws Exception &#123;</div><div class="line"></div><div class="line">  Configuration conf = new Configuration();</div><div class="line">  Job job = Job.getInstance(conf, &quot;word count&quot;);</div><div class="line"></div><div class="line">  job.setJarByClass(WordCount.class);</div><div class="line">  job.setMapperClass(TokenizerMapper.class);</div><div class="line">  job.setNumReduceTasks(1);//number of reducers for the job</div><div class="line">  job.setReducerClass(MyReducer.class);</div><div class="line">  job.setOutputKeyClass(IntWritable.class);//change this</div><div class="line">  job.setOutputValueClass(Text.class);//change this</div><div class="line"></div><div class="line">  FileInputFormat.addInputPath(job, new Path(args[0]));</div><div class="line">  FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">  System.exit(job.waitForCompletion(true) ? 0 : 1);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="Hadoop-streaming-python"><a href="#Hadoop-streaming-python" class="headerlink" title="Hadoop streaming + python"></a>Hadoop streaming + python</h2><p>用 EMR Streaming + python 也很简单。在 Add Steps 时加上 argument 配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">-D mapreduce.job.reduces=1</div><div class="line">-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</div><div class="line">-D  mapred.text.key.comparator.options=-nr</div></pre></td></tr></table></figure></p>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p>Spark 就更是超级无敌简单了，假设 calRdd 计算了 (key, frequency)，那么就只要做如下操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Sorts the nodes in decreasing order of their degrees</div><div class="line">sortRdd=calRdd.map(lambda x:(x[1],x[0])).sortByKey(ascending=False).map(lambda x:(x[1],x[0]))</div><div class="line"># Prints the top-100 nodes which have the highest degrees</div><div class="line">sortRdd.take(100)</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> EMR </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AWS EC2 部署 django 项目]]></title>
      <url>http://www.shuang0420.com/2017/03/30/AWS%20EC2%20%E9%83%A8%E7%BD%B2%20django%20%E9%A1%B9%E7%9B%AE/</url>
      <content type="html"><![CDATA[<p>关于 EC2 部署 django 项目以及 pip DistributionNotFound Error。<br><a id="more"></a></p>
<h1 id="EC2-部署-django-项目"><a href="#EC2-部署-django-项目" class="headerlink" title="EC2 部署 django 项目"></a>EC2 部署 django 项目</h1><p>其实不只是  django，大多数的 server 部署到 EC2 上要修改的地方其实都差不多。<br><strong>三个注意点：</strong></p>
<ol>
<li>确保 EC2 的端口开了</li>
<li>修改 django project 的 settings.py，把 EC2 的 public dns 加入 ALLOWED_HOSTS<pre><code>ALLOWED_HOSTS = [&apos;ec2-52-14-223-164.us-east-2.compute.amazonaws.com&apos;]
</code></pre></li>
<li>开启 Server，注意监听地址<pre><code>python manage.py runserver 0.0.0.0:8000
</code></pre></li>
</ol>
<h1 id="EC2-pip-DistributionNotFound-Error"><a href="#EC2-pip-DistributionNotFound-Error" class="headerlink" title="EC2 pip DistributionNotFound Error"></a>EC2 pip DistributionNotFound Error</h1><p>顺便提一下 pip，好好的在安装依赖的过程中，突然出现了错误，pip 完全用不了了…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$ sudo pip --version</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/usr/bin/pip&quot;, line 5, in &lt;module&gt;</div><div class="line">    from pkg_resources import load_entry_point</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 3138, in &lt;module&gt;</div><div class="line">    @_call_aside</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 3124, in _call_aside</div><div class="line">    f(*args, **kwargs)</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 3151, in _initialize_master_working_set</div><div class="line">    working_set = WorkingSet._build_master()</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 663, in _build_master</div><div class="line">    return cls._build_from_requirements(__requires__)</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 676, in _build_from_requirements</div><div class="line">    dists = ws.resolve(reqs, Environment())</div><div class="line">  File &quot;/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py&quot;, line 849, in resolve</div><div class="line">    raise DistributionNotFound(req, requirers)</div><div class="line">pkg_resources.DistributionNotFound: The &apos;pip==6.1.1&apos; distribution was not found and is required by the application</div></pre></td></tr></table></figure></p>
<p>起因是某小哥突然更新了 pip… 太忧伤……小哥 Google 了若干帖子，然而事实证明，bug 这种事，有时就算试过了所有网上的解决方案也不一定能 de 出来，哪怕是卸载了重来。。</p>
<p>当然收获还是有的，Stack overflow 上有个解释不错</p>
<blockquote>
<p>On upgrading pip on the system, as the root user, you actually overwrite your system PIP program, and are subject to severe problem when further installing Python packages for your Linux system (with yum/dnf). The correct way to work with this is to create a virtualenv as a user, and on that virtualenv you upgrade PIP. Isolated from the system Python installation. Anything remotely serious you will want to do with Python on this machine should be running at least Python 2.7 anyway - or 3.6 if it is Python 3 compatible. (Your system Python is 2.6 and you have a Python2. on /usr/local which might conflict, exactly depending on the order of PATH as you found out).</p>
</blockquote>
<p>Anyway，应该是版本冲突 + 配置的问题，于是先看一下 pip 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ which pip</div><div class="line">/usr/local/bin/pip</div><div class="line">$ vim /usr/local/bin/pip</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python2.7</div><div class="line"># EASY-INSTALL-ENTRY-SCRIPT: &apos;pip==6.1.1&apos;,&apos;console_scripts&apos;,&apos;pip&apos;</div><div class="line">__requires__ = &apos;pip==6.1.1&apos;</div><div class="line">import re</div><div class="line">import sys</div><div class="line">from pkg_resources import load_entry_point</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    sys.argv[0] = re.sub(r&apos;(-script\.pyw?|\.exe)?$&apos;, &apos;&apos;, sys.argv[0])</div><div class="line">    sys.exit(</div><div class="line">        load_entry_point(&apos;pip==6.1.1&apos;, &apos;console_scripts&apos;, &apos;pip2.7&apos;)()</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>要知道 pip9.0.1 其实已经有了，所以手动把所有 pip==6.1.1 改成 pip==9.0.1，然而并没有起作用。</p>
<p>那就直接追踪下问题出现的目录吧</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ cd /usr/local/lib/python2.7/site-packages/pkg_resources/</div><div class="line">$ ls</div><div class="line">__init__.py  __init__.pyc</div><div class="line">$ cd ..</div><div class="line">$ ls</div><div class="line">appdirs-1.4.3.dist-info        djangorestframework-3.6.2.dist-info  olefile-0.44.egg-info     pip-9.0.1-py2.7.egg        setuptools</div><div class="line">appdirs.py                     easy-install.pth                     OleFileIO_PL.py           pkg_resources              setuptools-34.3.3.dist-info</div><div class="line">appdirs.pyc                    easy_install.py                      OleFileIO_PL.pyc          pyparsing-2.2.0.dist-info  six-1.10.0.dist-info</div><div class="line">django                         easy_install.pyc                     packaging                 pyparsing.py               six.py</div><div class="line">Django-1.10.6.dist-info        markdown                             packaging-16.8.dist-info  pyparsing.pyc              six.pyc</div><div class="line">django_filter-1.0.2.dist-info  Markdown-2.6.8.egg-info              pip                       README</div><div class="line">django_filters                 olefile                              pip-9.0.1.dist-info       rest_framework</div></pre></td></tr></table></figure>
<p>然后注意了，我们确实有 pip-9.0.1，然而我们并木有 pip2.7 这个东西，有的只是 pip！所以 =&gt;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python2.7</div><div class="line"># EASY-INSTALL-ENTRY-SCRIPT: &apos;pip==9.0.1&apos;,&apos;console_scripts&apos;,&apos;pip&apos;</div><div class="line">__requires__ = &apos;pip==9.0.1&apos;</div><div class="line">import re</div><div class="line">import sys</div><div class="line">from pkg_resources import load_entry_point</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    sys.argv[0] = re.sub(r&apos;(-script\.pyw?|\.exe)?$&apos;, &apos;&apos;, sys.argv[0])</div><div class="line">    sys.exit(</div><div class="line">        load_entry_point(&apos;pip==9.0.1&apos;, &apos;console_scripts&apos;, &apos;pip&apos;)()</div><div class="line">    )</div></pre></td></tr></table></figure></p>
<p>然后，问题解决～～</p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> django </tag>
            
            <tag> EC2 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ParseTree操作若干-Tregex and Stanford CoreNLP]]></title>
      <url>http://www.shuang0420.com/2017/03/24/ParseTree%E6%93%8D%E4%BD%9C%E8%8B%A5%E5%B9%B2-Tregex%20and%20Stanford%20CoreNLP/</url>
      <content type="html"><![CDATA[<p>网上教程太少，只能自己摸索了。关于怎么用 python 来调用 Stanford Parser。–持续更新中–<br><a id="more"></a></p>
<p>Tregex 用来做句子层面的识别及操作，简单理解就是关于 tree 的 regex。一些语法知识见<a href="https://nlp.stanford.edu/software/tregex/The_Wonderful_World_of_Tregex.ppt/" target="_blank" rel="external">The Wonderful World of Tregex</a>。用 java 来调用 API 更简单一点，然而项目需要，所以这一篇讲怎么用 python 来调用。</p>
<h1 id="Stanford-CoreNLP"><a href="#Stanford-CoreNLP" class="headerlink" title="Stanford CoreNLP"></a>Stanford CoreNLP</h1><p>Stanford NLP 的工具还可以有 Server 端！简直是 python 使用者一大福利。<br>下载安装<a href="http://stanfordnlp.github.io/CoreNLP/corenlp-server.html" target="_blank" rel="external">CoreNLP Server</a></p>
<p>先测试一下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -cp &quot;*&quot; -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt</div></pre></td></tr></table></figure></p>
<p>ok，可以运行，然后开启 server</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Run the server using all jars in the current directory (e.g., the CoreNLP home directory)</div><div class="line">java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000</div></pre></td></tr></table></figure>
<p>之后就可以通过 python 发送 http 请求来调用接口啦~ Python 代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"></div><div class="line">url = &quot;http://localhost:9000/tregex&quot;</div><div class="line">request_params = &#123;&quot;pattern&quot;: &quot;(NP[$VP]&gt;S)|(NP[$VP]&gt;S\\n)|(NP\\n[$VP]&gt;S)|(NP\\n[$VP]&gt;S\\n)&quot;&#125;</div><div class="line">text = &quot;Pusheen and Smitha walked along the beach.&quot;</div><div class="line">r = requests.post(url, data=text, params=request_params)</div><div class="line">print r.json()</div></pre></td></tr></table></figure></p>
<p>结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;u&apos;sentences&apos;: [&#123;u&apos;0&apos;: &#123;u&apos;namedNodes&apos;: [], u&apos;match&apos;: u&apos;(NP (NNP Pusheen)\n  (CC and)\n  (NNP Smitha))\n&apos;&#125;&#125;]&#125;</div></pre></td></tr></table></figure></p>
<h1 id="Tregex-的基本语法"><a href="#Tregex-的基本语法" class="headerlink" title="Tregex 的基本语法"></a>Tregex 的基本语法</h1><p>之后再慢慢补充吧。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/ParseTree%E6%93%8D%E4%BD%9C%E8%8B%A5%E5%B9%B2-Tregex%20and%20Stanford%20CoreNLP/1.jpg" class="ful-image" alt="1.jpg"></p>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>假定已经安装好了 nltk, stanford nlp 各类包，并设置好了路径。</p>
<h2 id="Parse-Tree-from-NLTK"><a href="#Parse-Tree-from-NLTK" class="headerlink" title="Parse Tree from NLTK"></a>Parse Tree from NLTK</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">from __future__ import division, unicode_literals</div><div class="line">import nltk</div><div class="line">from nltk.parse.stanford import StanfordParser</div><div class="line"></div><div class="line">parser = StanfordParser(model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;)</div><div class="line"></div><div class="line">def getParserTree(line):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    return parse tree of the string</div><div class="line">    :param line: string</div><div class="line">    :return: list of tree nodes</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    return list(parser.raw_parse(line))</div><div class="line"></div><div class="line"></div><div class="line"># get parse tree</div><div class="line">text = &apos;Harry Potter, a young boy, is very famous in US&apos;</div><div class="line">testTree = getParserTree(text)</div><div class="line"></div><div class="line">print testTree</div></pre></td></tr></table></figure>
<p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[Tree(&apos;ROOT&apos;, [Tree(&apos;S&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NNP&apos;, [&apos;Harry&apos;]), Tree(&apos;NNP&apos;, [&apos;Potter&apos;])]), Tree(&apos;,&apos;, [&apos;,&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;a&apos;]), Tree(&apos;JJ&apos;, [&apos;young&apos;]), Tree(&apos;NN&apos;, [&apos;boy&apos;])]), Tree(&apos;,&apos;, [&apos;,&apos;])]), Tree(&apos;VP&apos;, [Tree(&apos;VBZ&apos;, [&apos;is&apos;]), Tree(&apos;ADJP&apos;, [Tree(&apos;RB&apos;, [&apos;very&apos;]), Tree(&apos;JJ&apos;, [&apos;famous&apos;]), Tree(&apos;PP&apos;, [Tree(&apos;IN&apos;, [&apos;in&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;NNP&apos;, [&apos;US&apos;])])])])])])])]</div></pre></td></tr></table></figure></p>
<h2 id="Parse-Tree-from-CoreNLP-Server"><a href="#Parse-Tree-from-CoreNLP-Server" class="headerlink" title="Parse Tree from CoreNLP Server"></a>Parse Tree from CoreNLP Server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"></div><div class="line">#&quot;annotators&quot;, &quot;tokenize, ssplit, pos, lemma, ner, parse, dcoref&quot;</div><div class="line">url = &apos;http://localhost:9000/?properties=&#123;&quot;annotators&quot;: &quot;parse&quot;, &quot;outputFormat&quot;: &quot;text&quot;&#125;&apos;</div><div class="line">text=&apos;Harry Potter, a young boy, is very famous in US&apos;</div><div class="line">r = requests.post(url, data=text)</div><div class="line">print r.content</div></pre></td></tr></table></figure>
<p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">Sentence #1 (12 tokens):</div><div class="line">Harry Potter, a young boy, is very famous in US</div><div class="line">[Text=Harry CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP]</div><div class="line">[Text=Potter CharacterOffsetBegin=6 CharacterOffsetEnd=12 PartOfSpeech=NNP]</div><div class="line">[Text=, CharacterOffsetBegin=12 CharacterOffsetEnd=13 PartOfSpeech=,]</div><div class="line">[Text=a CharacterOffsetBegin=14 CharacterOffsetEnd=15 PartOfSpeech=DT]</div><div class="line">[Text=young CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=JJ]</div><div class="line">[Text=boy CharacterOffsetBegin=22 CharacterOffsetEnd=25 PartOfSpeech=NN]</div><div class="line">[Text=, CharacterOffsetBegin=25 CharacterOffsetEnd=26 PartOfSpeech=,]</div><div class="line">[Text=is CharacterOffsetBegin=27 CharacterOffsetEnd=29 PartOfSpeech=VBZ]</div><div class="line">[Text=very CharacterOffsetBegin=30 CharacterOffsetEnd=34 PartOfSpeech=RB]</div><div class="line">[Text=famous CharacterOffsetBegin=35 CharacterOffsetEnd=41 PartOfSpeech=JJ]</div><div class="line">[Text=in CharacterOffsetBegin=42 CharacterOffsetEnd=44 PartOfSpeech=IN]</div><div class="line">[Text=US CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=NNP]</div><div class="line">(ROOT</div><div class="line">  (S</div><div class="line">    (NP</div><div class="line">      (NP (NNP Harry) (NNP Potter))</div><div class="line">      (, ,)</div><div class="line">      (NP (DT a) (JJ young) (NN boy))</div><div class="line">      (, ,))</div><div class="line">    (VP (VBZ is)</div><div class="line">      (ADJP (RB very) (JJ famous)</div><div class="line">        (PP (IN in)</div><div class="line">          (NP (NNP US)))))))</div><div class="line"></div><div class="line">root(ROOT-0, famous-10)</div><div class="line">compound(Potter-2, Harry-1)</div><div class="line">nsubj(famous-10, Potter-2)</div><div class="line">punct(Potter-2, ,-3)</div><div class="line">det(boy-6, a-4)</div><div class="line">amod(boy-6, young-5)</div><div class="line">appos(Potter-2, boy-6)</div><div class="line">punct(Potter-2, ,-7)</div><div class="line">cop(famous-10, is-8)</div><div class="line">advmod(famous-10, very-9)</div><div class="line">case(US-12, in-11)</div><div class="line">nmod:in(famous-10, US-12)</div></pre></td></tr></table></figure></p>
<p>annotators 可以加其他 parameter，得到更多的 ner, lemma 等信息，输出也可以设定为 json 或 html 等格式。</p>
<h2 id="同位语"><a href="#同位语" class="headerlink" title="同位语"></a>同位语</h2><p>得到 parse tree 的同位语部分，规则如下，第一个 NP 是 parent，后面两个 NP 是 sisters，中间由逗号隔开，这是同位语的基本形式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">NP=n1 &lt; (NP=n2 $.. (/,/ $.. NP=n3))</div></pre></td></tr></table></figure></p>
<p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">from __future__ import division, unicode_literals</div><div class="line">import nltk</div><div class="line">from nltk.parse.stanford import StanfordParser</div><div class="line">import requests</div><div class="line"></div><div class="line">APPOSITION = &quot;NP=n1 &lt; (NP=n2 $.. (/,/ $.. NP=n3))&quot;</div><div class="line"></div><div class="line">def getAppositions(tree):</div><div class="line">    url = &quot;http://localhost:9000/tregex&quot;</div><div class="line">    request_params = &#123;&quot;pattern&quot;: APPOSITION&#125;</div><div class="line">    r = requests.post(url, data=text, params=request_params)</div><div class="line">    return r.json()</div><div class="line"></div><div class="line"></div><div class="line">text = &apos;Harry Potter, a young boy, is very famous in US&apos;</div><div class="line">print getAppositions(text)</div></pre></td></tr></table></figure></p>
<p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;u&apos;sentences&apos;: [&#123;u&apos;0&apos;: &#123;u&apos;namedNodes&apos;: [&#123;u&apos;n1&apos;: u&apos;(NP\n  (NP (NNP Harry) (NNP Potter))\n  (, ,)\n  (NP (DT a) (JJ young) (NN boy))\n  (, ,))\n&apos;&#125;, &#123;u&apos;n2&apos;: u&apos;(NP (NNP Harry) (NNP Potter))\n&apos;&#125;, &#123;u&apos;n3&apos;: u&apos;(NP (DT a) (JJ young) (NN boy))\n&apos;&#125;], u&apos;match&apos;: u&apos;(NP\n  (NP (NNP Harry) (NNP Potter))\n  (, ,)\n  (NP (DT a) (JJ young) (NN boy))\n  (, ,))\n&apos;&#125;&#125;]&#125;</div></pre></td></tr></table></figure></p>
<p>再进一步处理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def getAppositions(tree):</div><div class="line">    url = &quot;http://localhost:9000/tregex&quot;</div><div class="line">    request_params = &#123;&quot;pattern&quot;: APPOSITION&#125;</div><div class="line">    r = requests.post(url, data=text, params=request_params)</div><div class="line">    js = r.json()</div><div class="line">    if js[&apos;sentences&apos;][0] and &apos;0&apos; in js[&apos;sentences&apos;][0] and &apos;namedNodes&apos; in js[&apos;sentences&apos;][0][&apos;0&apos;]:</div><div class="line">        return js[&apos;sentences&apos;][0][&apos;0&apos;][&apos;namedNodes&apos;]</div><div class="line">    return None</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">text = &apos;Harry Potter, a young boy, is very famous in US&apos;</div><div class="line">testTree = getParserTree(text)</div><div class="line">res = getAppositions(testTree)</div><div class="line">if res:</div><div class="line">    for c in res:</div><div class="line">        print c</div></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;u&apos;n1&apos;: u&apos;(NP\n  (NP (NNP Harry) (NNP Potter))\n  (, ,)\n  (NP (DT a) (JJ young) (NN boy))\n  (, ,))\n&apos;&#125;</div><div class="line">&#123;u&apos;n2&apos;: u&apos;(NP (NNP Harry) (NNP Potter))\n&apos;&#125;</div><div class="line">&#123;u&apos;n3&apos;: u&apos;(NP (DT a) (JJ young) (NN boy))\n&apos;&#125;</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> ParseTree </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - 平滑方法(Smoothing)小结]]></title>
      <url>http://www.shuang0420.com/2017/03/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95(Smoothing)%E5%B0%8F%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>系统总结下之前零零散散提到过的 smoothing 方法。<br><a id="more"></a></p>
<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><p>我们来看一个概率模型，也就是 p(e)在 event space E 下的概率分布，模型很可能会用最大似然估计(MLE)，如下<br>$$P_{MLE}={c(x) \over \sum_ec(e)}$$</p>
<p>然而，由于并没有足够的数据，很多事件 x 并没有在训练数据中出现，也就是 c(x)=0，$P_{MLE}=0$，这是有问题的，没有在训练数据中出现的数据，并不代表不会在测试数据中出现，如果没有考虑到数据稀疏性，你的模型就太简单了！</p>
<p>Data sparsity 是 smoothing 的最大原因。Chen &amp; Goodman 在1998 年提到过，几乎所有数据稀疏的场景下，smoothing 都可以帮助提高 performance，而数据稀疏性几乎是所有统计模型(statistical modeling)都会遇到的问题。而如果你有足够多的训练数据，所有的 parameters 都可以在没有 smoothing 的情况下被准确的估计，那么你总是可以扩展模型，如原来是 bigram，没有数据稀疏，完全可以扩展到 trigram 来提高 performance，如果还没有出现稀疏，就再往高层推，当 parameters 越来越多的时候，数据稀疏再次成为了问题，这时候，用合适的平滑方法可以得到更准确的模型。实际上，无论有多少的数据，平滑几乎总是可以以很小的代价来提高 performance。</p>
<h1 id="平滑方法"><a href="#平滑方法" class="headerlink" title="平滑方法"></a>平滑方法</h1><ul>
<li>Additive smoothing</li>
<li>Good-Turing estimate</li>
<li>Jelinek-Mercer smoothing (interpolation)</li>
<li>Katz smoothing (backoff)</li>
<li>Witten-Bell smoothing</li>
<li>Absolute discounting</li>
<li>Kneser-Ney smoothing</li>
</ul>
<h2 id="差值-Interpolation-vs-回退-backoff"><a href="#差值-Interpolation-vs-回退-backoff" class="headerlink" title="差值(Interpolation) vs. 回退(backoff)"></a>差值(Interpolation) vs. 回退(backoff)</h2><p>先来理解下平滑方法的两种思想，一个是差值，简单来讲，就是把不同阶的模型结合起来，另一种是回退，直观的理解，就是说如果没有 3gram，就用 bigram，如果没有 bigram，就用 unigram，两者的区别：</p>
<ul>
<li>插值（Jelinek-Mercer）和回退（Katz）都涉及到来自较高和较低阶模型的信息</li>
<li>主要区别：在决定非零计数的 n-gram 的概率时，差值模型使用低阶模型的信息，而回退模型则不使用</li>
<li>相同点：在决定没有出现过的(零计数) n-gram 时，两者都用了低阶模型的信息</li>
<li>事实证明，做差值模型的回退版本和做回退模型的差值版本都不难~(Kneser-Ney最开始是回退模型，而 Chen&amp;Goodman 也做了差值版本)</li>
</ul>
<p>另外还有一点，与差值平滑算法相比，回退算法所需参数较少，而且可以直接确定，无需通过某种迭代重估算法反复训练，因此实现更加方便。</p>
<h2 id="Additive-smoothing"><a href="#Additive-smoothing" class="headerlink" title="Additive smoothing"></a>Additive smoothing</h2><h3 id="Add-one-smoothing"><a href="#Add-one-smoothing" class="headerlink" title="Add-one smoothing"></a>Add-one smoothing</h3><p>也叫 Laplace smoothing，假设 we saw each word one more time than we did，下面以 bigram model 为例给出加 1 平滑的模型<br>MLE estimate:<br>$$P_{MLE}(w_i|w_{i-1})={c(w_{i-1}w_i) \over c(w_{i-1})}$$<br>Add-1 estimate:<br>$$P_{Add-1}(w_i|w_{i-1})={c(w_{i-1}w_i)+1 \over c(w_{i-1})+V}$$</p>
<p>通常情况下，V={w:c(w)$\gt$0}$\cup${UNK}</p>
<p>举个例子，假设语料库为下面三个句子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">JOHN READ MOBY DICK</div><div class="line">MARY READ A DIFFERENT BOOK</div><div class="line">SHE READ A BOOK BY CHER</div></pre></td></tr></table></figure></p>
<p>那么 JOHN READ A BOOK 这个句子的概率是：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/1.jpg" class="ful-image" alt="1.jpg"></p>
<p>而 CHEN READ A BOOK 这个句子出现的概率是：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>如果使用加1平滑，那么概率就变成了：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/3.jpg" class="ful-image" alt="3.jpg"></p>
<p>加 1 平滑通常情况下是一种很糟糕的算法，与其他平滑方法相比显得非常差，然而我们可以把加 1 平滑用在其他任务中，如文本分类，或者非零计数没那么多的情况下。</p>
<h3 id="Additive-smoothing-1"><a href="#Additive-smoothing-1" class="headerlink" title="Additive smoothing"></a>Additive smoothing</h3><p>对加 1 平滑的改进就是把 1 改成 $\delta$，且 $0 \lt \delta \le 1$，也就是 pretend we’ve seen each n-gram $\delta$ times more than we have，当然，Gale &amp; Church(1994) 吐槽了这种方法，说表现很差。</p>
<p>$$P_{Add-1}(w_i|w_{i-1})={c(w_{i-1}w_i)+\delta \over c(w_{i-1})+\delta V}$$</p>
<h2 id="Good-Turing-smoothing"><a href="#Good-Turing-smoothing" class="headerlink" title="Good-Turing smoothing"></a>Good-Turing smoothing</h2><p><strong>基本思想:</strong> 用观察计数较高的 N-gram 数量来重新估计概率量大小，并把它指派给那些具有零计数或较低计数的 N-gram</p>
<blockquote>
<p>Idea: reallocate the probability mass of n-grams that occur r+1 times in the training data to the n-grams that occur r times.</p>
</blockquote>
<p>一般情况下，我们选出现过一次的概率，也就是 Things seen once 这一概念：<br><strong>Things seen once</strong>: 使用刚才已经看过一次的事物的数量来帮助估计从来没有见过的事物的数量。举个例子，假设你在钓鱼，然后抓到了 18 条鱼，种类如下：10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel，那么<br><strong>下一条鱼是 trout 的概率是多少？</strong><br>很简单，我们认为是 1/18</p>
<p><strong>那么，下一条鱼是新品种的概率是多少？</strong><br>不考虑其他，那么概率是 0，然而根据 Things seen once 来估计新事物，概率是 3/18</p>
<p><strong>在此基础上，下一条鱼是 trout 的概率是多少？</strong><br>肯定就小于 1/18，那么怎么估计呢？<br>在 Good Turing 下，对每一个计数 r，我们做一个调整，变为 r*，公式如下，其中 $n_r$ 表示出现过 r 次的 n-gram。<br>$$r^* = (r+1){n_{r+1} \over n_r}$$</p>
<p>然后，我们就有<br>$$P_{GT}(x:c(x)=r) = {r^* \over N}$$</p>
<p>所以，c=1时，<br>$C^* (trout)= 2*1/3 = 2/3$<br>$P^* (trout)={2/3 \over 18}={1 \over 27}$</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>然后，问题来了，如果 $n_{r+1}=0$ 怎么办？这在  r 很高的情况下很常见，因为在对计数进行计数时(counts of counts)，会出现 “holes”。即使没有这个 hole，对很高的 r 来说，$n^r$ 也是有噪音的(noisy)。</p>
<p>所以，我们应该这样来看 $r^*$:<br>$$r^* = (r+1){E(n_{r+1}) \over E(n_r)}$$</p>
<p>但是，问题是怎么估计这种期望呢？这需要更多的解释。</p>
<h2 id="Jelinek-Mercer-smoothing-Interpolation-差值"><a href="#Jelinek-Mercer-smoothing-Interpolation-差值" class="headerlink" title="Jelinek-Mercer smoothing(Interpolation, 差值)"></a>Jelinek-Mercer smoothing(Interpolation, 差值)</h2><p>同样，以语言模型为例，我们看这样一种情况，如果 c(BURNISH THE) 和 c(BURNISH THOU) 都是 0，那么在前面的平滑方法 additive smoothing 和 Good-Turing 里，p(THE|BURNISH)=p(THOU|BURNISH)，而这个概率我们直观上来看是错误的，因为 THE 要比 THOU 常见的多，$p(THE|BURNISH)\gt p(THOU|BURNISH)$ 应该是大概率事件。要实现这个，我们就希望把 bigram 和 unigram 结合起来，interpolate 就是这样一种方法。</p>
<p>用线性差值把不同阶的 N-gram 结合起来，这里结合了 trigram，bigram 和 unigram。用 lambda 进行加权<br>$$<br>  \begin{aligned}<br>  p(w_n|w_{n-1}w_{n-2}) &amp; = \lambda_1 p(w_n|w_{n-1}w_{n-2}) \\<br>   &amp; + \lambda_2 p(w_n|w_{n-1})  \\<br>   &amp; + \lambda_3 p(w_n)<br>  \end{aligned}<br>$$</p>
<p>$$\sum_i \lambda_i=1$$</p>
<p><strong>怎样设置 lambdas?</strong><br>把语料库分为 training data, held-out data, test data 三部分，然后</p>
<ul>
<li>Fix the N-gram probabilities(on the training data)</li>
<li>Search for lambdas that give the largest probabilities to held-out set:<br>$logP(w_1…w_n|M(\lambda_1…\lambda_k))=\sum_ilogP_M(\lambda_1…\lambda_k)(w_i|w_{i-1})$</li>
</ul>
<p>这种差值其实是一个递归的形式，我们可以把每个 lambda 看成上下文的函数，如果对于一个特定的 bigram 有特定的计数，假定 trigram 的计数是基于 bigram 的，那么这样的办法将更可靠，因此，可以使 trigram 的 lambda 值更高，给 trigram 更高权重。</p>
<p>$$<br>  \begin{aligned}<br>  p(w_n|w_{n-1}w_{n-2}) &amp; = \lambda_1 (w^{n-1}_{n-2})p(w_n|w_{n-1}w_{n-2}) \\<br>   &amp; + \lambda_2 (w^{n-1}_{n-2})p(w_n|w_{n-1})  \\<br>   &amp; + \lambda_3 (w^{n-1}_{n-2})p(w_n)<br>  \end{aligned}<br>$$</p>
<p>通常 $\lambda w^{n-1}_{n-2}$ 是用 EM 算法，在 held-out data 上训练得到(held-out interpolation) 或者在 cross-validation 下训练得到(deleted interpolation)。$\lambda w^{n-1}_{n-2}$ 的值依赖于上下文，高频的上下文通常会有高的 lambdas。</p>
<h2 id="Katz-smoothing"><a href="#Katz-smoothing" class="headerlink" title="Katz smoothing"></a>Katz smoothing</h2><p>回退式算法。和 Good-Turing 一样，对计数进行调整，以 bigram 为例，非 0 计数的 bigram 都根据打折比率 $d_r$ 进行打折，比率约为 ${r^* \over r}$，这个比率是由 Good-Turing 计算得到的，然后根据下一个低阶分布(如 unigram)，对没有出现过的 bigram 重新分配从非零计数中减去的值。</p>
<p>以 bigram 为例看看它是怎么做的：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/4.jpg" class="ful-image" alt="4.jpg"></p>
<p>对于 $d_r$ 的一些探讨，当一个 n-gram 出现次数足够大时，MLE 其实是一个可靠的概率估计，而当计数不是足够大时，就可以采用 Good-Turing 对其进行平滑，让计数为 0 时，退回低阶模型。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/5.jpg" class="ful-image" alt="5.jpg"></p>
<h2 id="Witten-Bell-smoothing"><a href="#Witten-Bell-smoothing" class="headerlink" title="Witten-Bell smoothing"></a>Witten-Bell smoothing</h2><p>一个 JM smoothing 的实例，当 n-gram $w^i_{i-n+1}$ 在训练数据中出现的时候，我们应该用高阶模型，否则，用低阶模型。所以 $1-w^i_{i-n+1}$ 应该是一个单词在训练数据中没有出现在 $w^i_{i-n+1}$ 后面的概率，可以用跟在 $w^i_{i-n+1}$ 后的 unique words 的数量来估计，公式为：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/9.jpg" class="ful-image" alt="9.jpg"></p>
<p>举个例子来理解一下，考虑 spite 和 constant 的 bigram，在 Europarl corpus 中，两个 bigram 都出现了 993 次，以 spite 开始的 bigram 只有 9 种，大多数情况下 spite 后面跟着 of(979次)，因为 in spite of 是常见的表达，而跟在 constant 后的单词有 415 种，所以，我们更有可能见到一个跟在 constant 后面的 bigram，Witten-Bell 平滑就考虑了这种单词预测的多样性，所以：</p>
<p>$1-\lambda_{spite}={9 \over 9+993}=0.00898$<br>$1-\lambda_{constant}={415 \over 415+993}=0.29474$</p>
<h2 id="Absolute-discounting"><a href="#Absolute-discounting" class="headerlink" title="Absolute discounting"></a>Absolute discounting</h2><p>和 Jelinek-Mercer 相似，Absolute discounting 包括了对高阶和低阶模型的差值，然而它并不是用高阶模型的 $P_{ML}$ 乘以一个 lambda，而是从每个非零计数里减掉一个固定的 discount $\delta \in [0,1]$</p>
<p>先简单的来看下 bigram model，加上 Absolute discounting 就是：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/8.jpg" class="ful-image" alt="8.jpg"></p>
<p>完整的公式<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>一般来说，$\delta$ 就直接取 0.75 好啦~</p>
<h2 id="Kneser-Ney-smoothing"><a href="#Kneser-Ney-smoothing" class="headerlink" title="Kneser-Ney smoothing"></a>Kneser-Ney smoothing</h2><p>是 Absolute discounting 的一个扩展，对回退部分做了一个修正。</p>
<p><strong>Idea: </strong> 只有在高阶模型的计数很小或者为 0 时，低阶模型才显得重要，(换种说法，只有在 bigram 没有出现过时，unigram 才有用)，因此应针对该目的进行优化</p>
<p>举个例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Shannon game: I can&apos;t see without my reading (Francisco/glasses)?</div></pre></td></tr></table></figure></p>
<p>reading 后面跟 Francisco 还是 glasses 呢？因为 “San Francisco” 是个很常见的地名，所以 Francisco 有一个很高的 unigram 概率，比 glasses 高，而 Francisco 偏偏只在 San 后面出现，这就非常不合理。而更合理的方式是，给 Francisco 一个较低的 unigram 概率，这样，bigram 模型表现会更好。</p>
<p>改进的方案是，我们不看 how likely is w，而去看 how likely is w to appear as a novel continuation，也就是，对每个单词，我们看它对应的 bigram type 的数量，然后，每个 bigram type 在第一次出现的时候都是 novel continuation</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/7.jpg" class="ful-image" alt="7.jpg">
<p>所以最后的公式为:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/8.jpg" class="ful-image" alt="8.jpg"></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul>
<li>影响最大的因素是使用 modified backoff distribution，如 Kneser-Ney smoothing</li>
<li>Jelinek-Mercer 平滑在小型训练数据集上表现更好，而 Katz 在大型训练集上表现更好</li>
<li>Katz 在计数大的 n-grams 上效果很好，而 Kneser-Ney 适合小的计数</li>
<li>Absolute discounting 要优于 linear discounting</li>
<li>在非零计数较低的情况下，Interpolated models 优于 backoff</li>
<li>添加 free parameters 到算法中，通过 held-out data 上优化这些参数可以提高性能</li>
</ul>
<blockquote>
<p>参考链接<br><a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" target="_blank" rel="external">Stanford Language Modeling</a><br><a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" target="_blank" rel="external">NLP Lunch Tutorial: Smoothing</a><br><a href="http://www.statmt.org/book/slides/07-language-models.pdf" target="_blank" rel="external">Language models</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 平滑 </tag>
            
            <tag> smoothing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - 再谈词向量]]></title>
      <url>http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。再次探讨词向量。<br><a id="more"></a></p>
<p>之前有过一篇总结，<a href="http://www.shuang0420.com/2016/06/21/词向量总结笔记（简洁版）/">词向量总结笔记（简洁版）</a>，这一次，从 intuition 角度来谈词向量的发展，对上一篇是一个补充。</p>
<p>大纲：<br><strong>Distributional (vector) models of meaning</strong></p>
<ul>
<li><strong>Sparse:</strong><br>  PPMI-weighted word-word co-occurrence matrices</li>
<li><strong>Dense:</strong><br>  Word-word SVD 50-2000 dimensions<br>  Skip-grams and CBOW<br>  Brown clusters 5-20 binary dimensions</li>
</ul>
<h1 id="Thesaurus-based-meaning"><a href="#Thesaurus-based-meaning" class="headerlink" title="Thesaurus-based meaning"></a>Thesaurus-based meaning</h1><p>之前谈到单词的相似度，可能会用到词典(thesaurus)。然而并不是每一种语言都有词典，我们也很难每年去更新词典，再者，基于词典的方法的召回率(recall)很低，很多单词和短语被漏掉了，而且词典在动词、形容词上的表现并不是很好。</p>
<h1 id="Joint-distributions-model-of-meaning"><a href="#Joint-distributions-model-of-meaning" class="headerlink" title="Joint distributions model of meaning"></a>Joint distributions model of meaning</h1><p>用 joint distribution 来表示一个词？几乎不可能。英语大概有 1 百万的词，如果我们只考虑 unigram model，那么我们也得有 1 百万个参数，如果考虑 bigram，那么，至少有 10^12 的参数，想要稳定的估计，还必须要非常大的样本量，不管是获取还是处理这样的大的样本，都不容易。</p>
<h1 id="Distributional-models-of-meaning"><a href="#Distributional-models-of-meaning" class="headerlink" title="Distributional models of meaning"></a>Distributional models of meaning</h1><p>所以有了新的方法，distributional models of meaning，它的 intuition 是词类的分布特征，如果两个单词能够出现在相似的环境中，它们往往是相似的，换句话说，就是 <strong>一个词是由它周围的单词来定义的</strong>，所以我们要得到的是当前词和它的周围词共同出现的次数。当然，这种方法并不能区分近义词反义词，如 fast 和 slow 在这种情况下就可能是同义的。</p>
<p>Distributional models of meaning = vector-space models of meaning = vector semantics</p>
<p>有几种不同类型的向量模型(vecotr models)</p>
<ul>
<li><strong>Sparse vector representations</strong><br>  Mutual-information weighted word co-occurrence matrices</li>
<li><strong>Dense vector representations:</strong><br>  Singular value decomposition (and Latent Semantic Analysis)<br>  Neural-network-inspired models (skip-grams, CBOW)<br>  Brown clusters</li>
</ul>
<h2 id="Sparse-vectors"><a href="#Sparse-vectors" class="headerlink" title="Sparse vectors"></a>Sparse vectors</h2><h3 id="Words-and-co-occurrence-vectors"><a href="#Words-and-co-occurrence-vectors" class="headerlink" title="Words and co-occurrence vectors"></a>Words and co-occurrence vectors</h3><p>先来看两个矩阵，<strong>词-文档矩阵(Term-document matrix)</strong> 和 <strong>词共现矩阵(Term-term matrix)</strong></p>
<p><strong>Term-document matrix</strong><br>表示每个单词在文档中出现的次数(词频)，每一行是一个 term，每一列是一个 document<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/1.jpg" class="ful-image" alt="1.jpg"></p>
<p>两篇文档的向量相似 =&gt; 两篇文档相似，如上图 doc3 和 doc4，我们就认为它们是相似的。<br>两个单词的向量相似 =&gt; 两个单词相似，如上图的 fool 和 clown，就是相似的。</p>
<p><strong>Term-term matrix</strong><br>然后我们可以考虑更小的粒度，更小的上下文，也就是不用整篇文档，而是用段落(paragraph)，或者小的窗口(window of $\pm 4$ words)，所以这个时候，向量就是对上下文单词的计数，大小不再是文档长度 |D|，而是窗口长度 |V| 了，所以现在 word-word matrix 是 |V|*|V|</p>
<p>看一个上下文为 7 个单词的 word-word matrix，同样，每一行是一个 term，每一列是一个上下文(context)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>关于窗口的大小，还有一个基本知识，如果窗口很小，比如说在 $\pm$ 1-3 之间，那么这个向量是更加基于语法(syntactic)的向量，而如果窗口更长一些，比如说在 $\pm$ 4-10 之间，那么向量是更加基于语义(semantic)的。</p>
<p>实际情况下，这个矩阵依然是非常稀疏的，大多数值都是 0，然而这并没有什么问题，我们还是可以用很多高效的算法来处理这种稀疏矩阵。</p>
<h3 id="Positive-Pointwise-Mutual-Information-PPMI"><a href="#Positive-Pointwise-Mutual-Information-PPMI" class="headerlink" title="Positive Pointwise Mutual Information(PPMI)"></a>Positive Pointwise Mutual Information(PPMI)</h3><p>先来看看两种词共现(co-occurrence)的类型。</p>
<ul>
<li>First-order co-occurrence(syntagmatic association)<br>  通常是临近词，比如说，wrote 是 book 或者 poem 的 first-order associate</li>
<li>Second-order co-occurrence(paradigmatic association)<br>  通常有相似的临近词/上下文，比如说，wrote 是 said 或者 remarked 的 second-order associate</li>
</ul>
<p>原始的词频对单词间的联系可能不是一个很好的估计，因为它非常的 skewed，一些停顿词如 the, of 非常的常见，也就没有区分度，而我们想要的是看 context word 能不能对 target word 提供有用的信息，于是就有了 PPMI。</p>
<p>先看下 PMI，看两个事件 x,y 同时出现的概率是不是比单独出现的概率高呢？<br>$$PMI(X,Y)=log_2{P(x,y) \over P(x)P(y))}$$</p>
<p>PMI 的范围是负无穷到正无穷，但是负值会带来各种各样的问题，所以我们通常会把负的 PMI 替换成 0，也就是有了 Positive PMI(PPMI)</p>
<p>$$PPMI(word_1,word_2)=max(log_2{P(word1,word2) \over P(word1)P(word2)},0)$$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/3.jpg" class="ful-image" alt="3.jpg">
<p>P(w=information, c=data)=6/19=.32<br>P(w=information)=11/19=.58<br>P(c=data)=7/19=.37</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/4.jpg" class="ful-image" alt="4.jpg">
<p>所以<br>$PMI(information,data)=log_2(.32 / (.37 * .58))=.57$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/5.jpg" class="ful-image" alt="5.jpg">
<p>PMI 有 bias，rare words 会有很高的 PMI 值，有两个解决办法</p>
<ul>
<li>Give rare words slightly higher probabilites</li>
<li>Add-one smoothing</li>
</ul>
<h3 id="Weighting-PPMI"><a href="#Weighting-PPMI" class="headerlink" title="Weighting PPMI"></a>Weighting PPMI</h3><p>这也就是第一种解决办法，加上一个 $\alpha$，比如 $\alpha=0.75$</p>
<p>$$PPMI\alpha (w,c)=max(log_2{P(w,c) \over P(w)P\alpha (c)},0)$$</p>
<p>$$P\alpha (c)={count(c)^\alpha \over \sum_c count(c)^\alpha}$$</p>
<p>对罕见的 c 来说，$P\alpha(c) &gt;&gt; P(c)$，来个例子，P(a)=.99, P(b)=.01，那么 $P\alpha(a)={.99^.75 \over .99^.75 + .01^.75}=.97$，同理，$P\alpha(b)=.03$。</p>
<h3 id="Add-k-smoothing"><a href="#Add-k-smoothing" class="headerlink" title="Add-k smoothing"></a>Add-k smoothing</h3><p>前面讲过很多次 Add-k smoothing 啦，就不多说了，直接上图<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/6.jpg" class="ful-image" alt="6.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/7.jpg" class="ful-image" alt="7.jpg"></p>
<h3 id="Measuring-similarity-the-cosine"><a href="#Measuring-similarity-the-cosine" class="headerlink" title="Measuring similarity: the cosine"></a>Measuring similarity: the cosine</h3><p>cosine 就没啥好多讲的了，+1 就是相同方向，-1 就是相反方向，0 就是正交，垂直方向，因为 PPMI 不会是负的，也就是 cosine 的范围是 0-1，很简单，就不举例了，看一下其他的相似性度量方法。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/8.jpg" class="ful-image" alt="8.jpg"></p>
<h3 id="Adding-syntax"><a href="#Adding-syntax" class="headerlink" title="Adding syntax"></a>Adding syntax</h3><p>加入句法特征，如果两个单词有相似的句法结构上下文，那么这两个单词就是相似的，比如说 duty 和 responsibility，看一下句法分布</p>
<ul>
<li>Modified by adjectives:<br>additional, administrative, assumed, collective, congressional, constitutional…</li>
<li>Objects of verbs:<br>assert, assign, assume, attend to, avoid, become, breach..。</li>
</ul>
<h3 id="Co-occurrence-vectors-based-on-syntactic-dependencies"><a href="#Co-occurrence-vectors-based-on-syntactic-dependencies" class="headerlink" title="Co-occurrence vectors based on syntactic dependencies"></a>Co-occurrence vectors based on syntactic dependencies</h3><p>每个维度：在 R 个语法关系集合中的一个 context word，比如说 Subject-of-“absorb”</p>
<p>所以一个向量不是 |V| 个特征，而是 R|V| 个特征，举个例子，看一下 cell 这个单词的计数</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/9.jpg" class="ful-image" alt="9.jpg">
<p>注意这里的矩阵其实是 |V|*R|V| 大小的，也可以把它变成 |V|*|V| 大小的，举个例子<br>$$M(“cell”,”absorb”) = count(subj(cell,absorb)) + count(obj(cell,absorb)) + count(pobj(cell,absorb))$$</p>
<p>PMI 在依存关系的应用，看下面的例子，drink it 比 drink wine 更常见，然而 wine 比 it 更加 drinkable<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/10.jpg" class="ful-image" alt="10.jpg"></p>
<h3 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h3><p>tfidf 是另一种替代 PPMI 来衡量关联性(association)的方法，通常来说，并不是从来计算单词与单词之间的相似度，而是来考虑单词与文档之间的联系。</p>
<h2 id="Dense-Vectors"><a href="#Dense-Vectors" class="headerlink" title="Dense Vectors"></a>Dense Vectors</h2><p>PPMI 的向量很长(length |V|= 20,000 to 50,000)，而且很稀疏(大多数元素都是 0)，与之相对应的是稠密向量(dense vectors)，它更加短(length 200-1000)，更加稠密(大多数元素不是 0)，稠密向量的好处在于，它更容易作为机器学习中的特征，因为需要更少的 weights，而另一方面，它更容易泛化，也能更好的捕获同义词。</p>
<p>怎样用更少的维度来估计 N 维的数据集？有很多相关方法</p>
<ul>
<li>PCA – principle components analysis</li>
<li>Factor Analysis</li>
<li>SVD</li>
</ul>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>奇异值分解和特征值分解的目的都是提取一个矩阵最重要的特征，特征值分解要求变换的矩阵必须是方阵，而奇异值分解是一个能适用于任意矩阵的一种分解的方法。$A=U \Sigma V^T$，$\Sigma$ 除了对角线，其余元素都是 0，对角线元素就是奇异值，U 和 V 都是方阵，里面的向量分别是左奇异向量和右奇异向量，如下：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/13.jpg" class="ful-image" alt="13.jpg">
<p>把奇异值和特征值对应起来，首先，我们将一个矩阵 A 的转置再乘以 A，将会得到一个方阵，我们用这个方阵求特征值可以得到 $(A^T A)v_i=\lambda_iv_i$，得到的 v 就是右奇异向量，然后还可以得到</p>
<p>$$\sigma_i = \sqrt {\lambda_i}$$<br>$$u_i={1 \over \sigma_i}Av_i$$</p>
<p>里面的 $\sigma$ 就是奇异值，u 就是左奇异向量。奇异值 σ 跟特征值类似，在矩阵 Σ 中也是从大到小排列，而且σ 减少特别的快，在很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上了。也就是说，我们也可以用前 r 大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：<br>$$A_{m*n} \approx U_{m*r} \Sigma_{r*r}V^T_{r*n}$$</p>
<p>右边的三个矩阵相乘的结果将会是一个接近于 A 的矩阵，在这儿，r 越接近于 n，则相乘的结果越接近于 A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵 A，我们如果想要压缩空间来表示原矩阵 A，我们存下这里的三个矩阵：U、Σ、V就好了。</p>
<p>看一下奇异值分解在词向量中的应用，</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/11.jpg" class="ful-image" alt="11.jpg">
<p>没必要保留所有的维度，我们可以只留 top k 个奇异值，比如说 300，结果是对原始 X 的最小二乘近似。我们也不需要做乘法，直接用 W 表示就好了，W 的每一行，是一个 K 维的向量，来表示单词 W。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/12.jpg" class="ful-image" alt="12.jpg">
<p>对于 LSA，我们一般会用 300 维，通常会给权重，local weight 就是取 log term frequency，global weight 就是取 idf 或者用 entropy 来衡量。</p>
<p>SVD 在词共现矩阵(term-term matrix)的应用<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/14.jpg" class="ful-image" alt="14.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/15.jpg" class="ful-image" alt="15.jpg"></p>
<p>dense SVD embeddings 有时比 sparse PPMI 的效果好，尤其是在 word similarity 这类任务上。主要原因是</p>
<ul>
<li>dense SVD embeddings 进行了去噪，丢掉了低维的，可能不重要的信息</li>
<li>截断有助于提高泛化能力</li>
<li>相对较小的维度有利于分类器来训练权重</li>
<li>更好的捕获 higher order 共现信息</li>
</ul>
<h3 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h3><p>Word2Vec 的算法，Skip-grams 和 CBOW，见 <a href="http://www.shuang0420.com/2016/06/21/词向量总结笔记（简洁版）/">词向量总结笔记（简洁版）</a></p>
<h3 id="GloVe-vs-Word2vec"><a href="#GloVe-vs-Word2vec" class="headerlink" title="GloVe vs. Word2vec"></a>GloVe vs. Word2vec</h3><p>学习词向量的两种方式，一种是用 Global matrix factorization methods，对词共现矩阵进行 factorization，如 LSA，另一种是用 local context window methods，如 Skip-gram 之类。然而，两种方法各有利弊，LSA 利用了统计数据，但是在单词相似性的任务上表现相对很差，是一个次优的向量空间结构，而 Skip-gram 等方法在单词相似性的任务上表现很好，但没有利用整个语料库的统计数据，因为它只对局部的上下文进行训练，而不是基于全局的共现矩阵。</p>
<p>GloVe 和 Word2vec 是现在常用的两种 word vector 方法/工具，两者最为显著的不同，是 word2vec 认为所有 co-occurrences 的权重都应该是相同的，而 GloVe 认为权重不应该相同，所以 GloVe 提出了 weighted least squares regression model，用加权最小二乘法来最小化两个词的向量点积与它们共现次数的对数之间的差异。事实上，GloVe 的作者表明两个词共现概率的比值（而不是它们的共现概率本身），更适合作为向量差来编码信息。</p>
<p>这两者在大部分 NLP 任务中的表现其实差不多，然而 GloVe 比 Word2Vec 多了个优势，它更容易来做并行化，也就是，更加容易训练更多的数据。</p>
<p>关于 <strong>count-based model</strong> 和 <strong>predictive model</strong>，具体的区别见 <a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf" target="_blank" rel="external">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a>。两个模型都会学习单词的共现特征，不同的是，predictive model 训练一个神经网络，预测周围的词语/目标词，优化损失函数(the loss of predicting the target words from the context words given the vector representations)，而 count-based model 首先对整个语料库建立一个共现矩阵，然后对这个矩阵做 Factorization，以此产生低维向量，通常，是通过最小化 “reconstruction loss” 来实现的。</p>
<blockquote>
<p>“reconstruction loss”: tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data. In the specific case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. This turns out to be A Good Thing in terms of the quality of the learned representations.</p>
</blockquote>
<p>2014 年，Baroni 等人表明 predictive model 几乎在所有的任务中都优于 count-based model，然而，当 GloVe 被 Levy 等人认为是一个预测模型时，它显然是正在分解一个词语上下文共现矩阵，更接近于诸如主成分分析（PCA）和潜在语义分析（LSA）等传统的方法，不止如此，Levy 等人还表示 word2vec 隐晦地分解了词语上下文的 PMI 矩阵。</p>
<p>另外提几点，来自 Illinois 的 Jiaqi Mu 做的分享中提到的，关于两者的一些统计数据<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/16.jpg" class="ful-image" alt="16.jpg"></p>
<p>从这些学习到的词向量来看，它们并不是 0 均值的，词向量聚集在均值周围，均值模长是词向量模长的 1/3 左右，我们知道向量之间的相似度是由夹角来描述的，如果有偏移，对相似度会有影响。另一方面，它们不是各向同性的，也就是说，向量并不是均匀分布在整个空间内，对空间的利用并不充分，有可能在某个空间上聚集了更多的能量，Jiaqi Mu 针对这两点做了 postprocessing，移除了非 0 偏置，并用主成分分析算出哪些维度的能量比较集中，进行了降维，然后实验发现，neural network 本身就能学习去掉均值、去掉某些方向这类的操作(automatically learn the postprocessing operation)。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/17.jpg" class="ful-image" alt="17.jpg">
<h3 id="Brown-clustering"><a href="#Brown-clustering" class="headerlink" title="Brown clustering"></a>Brown clustering</h3><p>在<a href="http://www.shuang0420.com/2017/02/24/NLP%20笔记%20-%20Part%20of%20speech%20tags/">NLP 笔记 - Part of speech tags</a>中提到过，就不多说啦。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word2vec </tag>
            
            <tag> Deep learning </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP笔记 - Information Extraction]]></title>
      <url>http://www.shuang0420.com/2017/03/18/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/</url>
      <content type="html"><![CDATA[<p>Stanford NLP 关于信息抽取的笔记，只是一个比较笼统的介绍，两块内容，命名实体识别(Named Entity Recognition)和序列模型(Sequence Model)。提到的具体算法，之后整理后再贴出~<br><a id="more"></a></p>
<p><strong>信息抽取(IE)</strong> 系统抽取的往往是清楚的、事实性的信息，如：Who did what to whom when? 这种比较结构化的信息。有很多方面的应用，最简单的比如说从公司报告中抽取盈利数据、总部信息、董事会成员等等信息，或者 Apple，Google mail 抽取日期时间，给出加入日历的建议这种(如下图)，非常 low-level 的信息抽取。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/1.jpg" class="ful-image" alt="1.jpg">
<h1 id="Named-Entity-Recognition-NER"><a href="#Named-Entity-Recognition-NER" class="headerlink" title="Named Entity Recognition(NER)"></a>Named Entity Recognition(NER)</h1><p>命名实体识别(NER)在信息抽取中扮演着重要角色，它有两个关键词：find &amp; classify，找到命名实体，并进行分类。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/2.jpg" class="ful-image" alt="2.jpg"></p>
<p><strong>应用：</strong></p>
<ul>
<li>命名实体作为索引和超链接</li>
<li>情感分析的准备步骤，在情感分析的文本中需要识别公司和产品，才能进一步为情感词归类</li>
<li>关系抽取(Relation Extraction)</li>
<li>QA 系统，大多数答案都是命名实体</li>
</ul>
<h2 id="Machine-learning-approach"><a href="#Machine-learning-approach" class="headerlink" title="Machine learning approach"></a>Machine learning approach</h2><p>非常标准的机器学习方法<br><strong>Training:</strong></p>
<ol>
<li>收集代表性的训练文档</li>
<li>为每个 token 标记命名实体(不属于任何实体就标 Others O)</li>
<li>设计适合该文本和类别的特征提取方法</li>
<li>训练一个 sequence classifier 来预测数据的 label</li>
</ol>
<p><strong>Testing:</strong></p>
<ol>
<li>收集测试文档</li>
<li>运行 sequence classifier 给每个 token 做标记</li>
<li>输出命名实体</li>
</ol>
<h3 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h3><p>看一下下面两种给 sequence labeling 的编码方式，IO encoding 简单的为每个 token 标注，如果不是 NE 就标为 O(other)，所以一共需要 C+1 个类别(label)。而 IOB encoding 需要 2C+1 个类别(label)，因为它标了 NE boundary，B 代表 begining，NE 开始的位置，I 代表 continue，承接上一个 NE，如果连续出现两个 B，自然就表示上一个 B 已经结束了。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/3.jpg" class="ful-image" alt="3.jpg"></p>
<p>在 Stanford NER 里，用的其实是 IO encoding，有两个原因，一是 IO encoding 运行速度更快，而是在实践中，两种编码方式的效果差不多。IO encoding 确定 boundary 的依据是，如果有连续的 token 类别不为 O，那么类别相同，同属一个 NE；类别不相同，就分割，相同的 sequence 属同一个 NE。而实际上，两个 NE 是相同类别这样的现象出现的很少，如上面的例子，Sue，Mengqiu Huang 两个同是 PER 类别，并不多见，更重要的是，在实践中，虽然 IOB encoding 能规定 boundary，而实际上它也很少能做对，它也会把 Sue Mengqiu Huang 分为同一个 PER，这主要是因为更多的类别会带来数据的稀疏。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>Features for sequence labeling:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">• Words</div><div class="line">    Current word (essentially like a learned dictionary)</div><div class="line">    Previous/next word (context)</div><div class="line">• Other kinds of inferred linguistic classification</div><div class="line">    Part of speech tags</div><div class="line">• Label context</div><div class="line">    Previous (and perhaps next) label</div></pre></td></tr></table></figure></p>
<p>再来看两个 feature<br><strong>Word substrings</strong><br>Word substrings 的作用是很大的，以下面的例子为例，NE 中间有 ‘oxa’ 的十有八九是 drug，NE 中间有 ‘:’ 的则大多都是 movie，而以 field 结尾的 NE 往往是 place。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/4.jpg" class="ful-image" alt="4.jpg"></p>
<p><strong>Word shapes</strong><br>可以做一个 mapping，把单词长度(length)、大写(capitalization)、数字(numerals)、希腊字母(Greek eltters)、单词内部标点(internal punctuation)都作为特征。<br>如下面的表，把所有大写字母映射为 X，小写字母映射为 x，数字映射为 d…<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/5.jpg" class="ful-image" alt="5.jpg"></p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>评估 IR 系统或者文本分类的任务，我们通常会用到 precision，recall，F1 这种 set-based metrics，见<a href="http://www.shuang0420.com/2016/09/20/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/">信息检索评价的 Unranked Boolean Retrieval Model 部分</a>，但是在这里对 NER 这种 sequence 类型任务的评估，如果用这些 metrics，可能会有一些问题，如 boundary error。因为 NER 的评估是按每个 entity 而不是每个 token 来计算的，我们需要看 entity 的 boundary。<br>以下面一句话为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">First Bank of Chicago announced earnings...</div></pre></td></tr></table></figure></p>
<p>正确的 NE 应该是 First Bank of Chicago，类别是 ORG，然而系统识别了 Bank of Chicago，类别 ORG，也就是说，右边界(right boundary)是对的，但是左边界(left boundary)是错误的，这其实是一个常见的错误。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">正确的标注：</div><div class="line">ORG - (1,4)</div><div class="line"></div><div class="line">系统：</div><div class="line">ORG - (2,4)</div></pre></td></tr></table></figure>
<p>而计算 precision，recall 的时候，我们会发现，对 ORG - (1,4) 而言，系统产生了一个 false negative，对 ORG - (2,4) 而言，系统产生了一个 false positive！所以系统有了 2 个错误。<strong>F1 measure</strong> 对 precision，recall 进行加权平均，结果会更好一些，所以经常用来作为 NER 任务的评估手段。另外，专家提出了别的建议，比如说给出 partial credit，如 MUC scorer metric，然而，对哪种 case 给多少的 credit，也需要精心设计。</p>
<h1 id="Sequence-models"><a href="#Sequence-models" class="headerlink" title="Sequence models"></a>Sequence models</h1><p>下一篇博客会具体讲 MEMM 和 CRF，这里大概整理下课程提到的内容，当做是一个预告好了。</p>
<h2 id="MEMM-inference"><a href="#MEMM-inference" class="headerlink" title="MEMM inference"></a>MEMM inference</h2><p>NLP 的很多数据都是序列类型的，像 sequence of characters, words, phrases, lines, sentences，我们可以暂时把任务当做是给每一个 item 打标签，如下图所示。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>对于 Maximum Entropy Markov Model (MEMM) 及 Conditional Markov Model (CMM) 这类模型，分类器在给定 observation 以及之前的决策下，每一次做一个决策，以当前观察和之前的决策为基础。</p>
<blockquote>
<p>For a Conditional Markov Model (CMM) a.k.a. a Maximum Entropy Markov Model (MEMM), the classifier makes a single decision at a time, conditioned on evidence from observations and previous decisions</p>
</blockquote>
<p>在每个 decision point，使用了右边表格里的所有特征。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/9.jpg" class="ful-image" alt="9.jpg"></p>
<p>流程如图所示，非常清楚。Inference in Systems：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/7.jpg" class="ful-image" alt="7.jpg"></p>
<h3 id="Greedy-Inference"><a href="#Greedy-Inference" class="headerlink" title="Greedy Inference"></a>Greedy Inference</h3><p>讨论下各种 inference<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-Information%20Extraction/8.jpg" class="ful-image" alt="8.jpg"></p>
<p>像上面所说的，”decide one sequence at a time and move on”，实际上是一个 greedy inference。举个例子，在词性标注中，可能模型在位置 2 的时候挑了当前最好的 PoS tag，但是到了位置 4 的时候，其实发现位置 2 应该有更好的选择，然而，greedy inference 并不会 care 这些。因为它是贪婪的，只要当前最好就行了。</p>
<p><strong>Greedy Inference:</strong><br><strong>优点:</strong></p>
<ol>
<li>速度快，没有额外的内存要求</li>
<li>非常易于实现</li>
<li>有很丰富的特征，表现不错</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>贪婪</li>
</ol>
<h3 id="Beam-Inference"><a href="#Beam-Inference" class="headerlink" title="Beam Inference"></a>Beam Inference</h3><ul>
<li>在每一个位置，都保留 top k 种可能(当前的完整序列)</li>
<li>在每个状态下，考虑上一步保存的序列来进行推进</li>
</ul>
<p><strong>优点:</strong></p>
<ol>
<li>速度快，没有额外的内存要求</li>
<li>易于实现(不用动态规划)</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>不精确，不能保证找到全局最优</li>
</ol>
<h3 id="Viterbi-Inference"><a href="#Viterbi-Inference" class="headerlink" title="Viterbi Inference"></a>Viterbi Inference</h3><ul>
<li>动态规划</li>
<li>需要维护一个 fix small window</li>
</ul>
<p><strong>优点:</strong></p>
<ol>
<li>非常精确，能保证找到全局最优序列</li>
</ol>
<p><strong>缺点:</strong></p>
<ol>
<li>难以实现远距离的 state-state interaction</li>
</ol>
<h2 id="CRFs"><a href="#CRFs" class="headerlink" title="CRFs"></a>CRFs</h2><p>另一种 sequence model 是条件随机场(Coditional Random Fields, CRFs)，是一个完整的序列模型(whole-sequence conditional model)而不是局部模型的连接。</p>
<p>训练会比较慢，但是可以防止 causal-competition biases。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Information Extraction </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[会议笔记 - Nuts and Bolts of Applying Deep Learning]]></title>
      <url>http://www.shuang0420.com/2017/03/17/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/</url>
      <content type="html"><![CDATA[<p>对第 30 届神经信息处理系统大会（NIPS 2016）中百度首席科学家吴恩达教授的演讲<a href="https://www.youtube.com/watch?v=F1ka6a13S9I" target="_blank" rel="external">Nuts and Bolts of Building Applications using Deep Learning</a>，做的笔记。最重要内容是 <strong>偏差/方差(bias/variance)分析框架</strong>。<br><a id="more"></a></p>
<p><a href="http://www.leiphone.com/news/201612/PTmKczqq5XWr2n8c.html" target="_blank" rel="external">中文 PPT</a></p>
<h1 id="深度学习的崛起"><a href="#深度学习的崛起" class="headerlink" title="深度学习的崛起"></a>深度学习的崛起</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/1.jpg" class="ful-image" alt="1.jpg">
<p>吴恩达在开场提到：深度学习为何这么火？答案很简单：因为数据规模正在推动深度学习的进步。</p>
<p>从图中得到的：</p>
<ol>
<li>不同于传统机器学习模型，对 DL 来说，数据量和 performance 几乎是线性的关系</li>
<li>在小数据集部分(small data regime)，也就是 x轴最左边，不同方法的差别是非常小的，然而随着数据量的增大，变化就非常快了</li>
</ol>
<p>吴恩达还提到了一点，你为了某个 task 去得到/使用一个数据集是一回事，你去维护这个数据集让它 scaleable 又是另一回事。所以对商业应用，一般需要两个团队，系统团队(systems team)和算法团队(algorithms team)。</p>
<h1 id="主要的-DL-模型"><a href="#主要的-DL-模型" class="headerlink" title="主要的 DL 模型"></a>主要的 DL 模型</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/2.jpg" class="ful-image" alt="2.jpg">
<ul>
<li>普通神经网络(General DL)<br>  全连接模型</li>
<li>顺序模型(Sequence Models)   (1D  顺序)<br>  RNN,  GRU,  LSTM,  CTC,  注意力模型</li>
<li>图像模型(Image Models)<br>  CNN</li>
<li>未来的 AI(Unsupervised and Reinforcement learning)<br>  无监督学习（稀疏编码 ICA,  SFA,）增强学习</li>
</ul>
<h1 id="端到端的学习"><a href="#端到端的学习" class="headerlink" title="端到端的学习"></a>端到端的学习</h1><p>AI 工程师并不用花很多时间在特征工程或者说中间层的特征表达上(intermediate representations)，而是可以直接从一端(原始输入)到另一端(输出)。另外，DL 能产生的不是一个简单的数字(i.e. a class prediction)，它还能产生特征向量。比如在图像标注(Image captioning)中，CNN 用来产生输入图像的特征向量，然后这个向量又会被作为 RNN 的主要输入，来为图像产生标注(caption)。另外的例子如翻译、语音识别，图像生成等。</p>
<p>这看起来很简单，貌似 DL 就可以完全取代之前的 ML 模型了？<strong>并！不！是！</strong> 始终要记得一个前提，这种端对端的方法只有在数据量足够大的时候才能有好的表现，所以，如果大的数据集很难获得，那么就要用 DL 模型就要很谨慎了。在实践过程中，并不是所有的场合都有大的标注数据集，所以加入了人工设计的特征信息(hand-engineered information)和领域知识(feild expertise)的模型往往占了上风。</p>
<h1 id="深度学习策略"><a href="#深度学习策略" class="headerlink" title="深度学习策略"></a>深度学习策略</h1><p>一个机器学习工程师在建模的时候会有很多问题：</p>
<ul>
<li>什么时候去找更多的数据？</li>
<li>要不要用更长的时间去训练?</li>
<li>什么时候要重新思考一下架构(architecture)？</li>
<li>什么时候引入/丢掉正则项？<br>…</li>
</ul>
<p>为了系统解决这个问题，Ng 带来了一个 <strong>偏差/方差(bias/variance)</strong> 分析框架。</p>
<h2 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/3.jpg" class="ful-image" alt="3.jpg">
<p>在大多数的 DL 问题中，训练集和测试集通常来自不同的分布，这种情况下，将数据分成 train/dev/test 可能会有点 tricky。有些人可能会从训练集中砍一部分来做开发集，如第一行。而实际上这样的效果是很差的，因为通常意义上来说，我们希望我们的开发集和测试集来自同一个分布，否则，很有可能工程师花费很多很多的时间在开发集上进行调参，然而测试集和开发集上的结果非常的不同，就浪费了很多精力。</p>
<p>因此，聪明的做法是像图中的第二行，将测试集分为 dev/test 两部分。Ng 建议说，在实践中，可以对两个分布的数据集都划分一部分作为开发集，如第三行的图，这样，不同错误之间的 gap 可以帮助我们更好的分析问题(如下图)。</p>
<h2 id="错误分类"><a href="#错误分类" class="headerlink" title="错误分类"></a>错误分类</h2><p>先来了解一下三种错误</p>
<ol>
<li>人类水平误差(human level error)</li>
<li>训练集误差(training set error)</li>
<li>开发集误差(validation set error)</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/4.jpg" class="ful-image" alt="4.jpg">
<h3 id="高偏差-high-bias"><a href="#高偏差-high-bias" class="headerlink" title="高偏差(high bias)"></a>高偏差(high bias)</h3><p>来看一下 <strong>高偏差(high bias)</strong>，对应的问题是 underfitting，训练集的错误率就很高，这种情况下，需要建立更大的模型<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/7.jpg" class="ful-image" alt="7.jpg"></p>
<h3 id="高方差-high-variance"><a href="#高方差-high-variance" class="headerlink" title="高方差(high variance)"></a>高方差(high variance)</h3><p><strong>高方差(high variance)</strong>，对应的问题是 overfitting，在训练集上表现的非常完美，然而开发集和测试集却有很高的错误率。下一步是引入正则或者多加些数据来调优。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/8.jpg" class="ful-image" alt="8.jpg">
<h3 id="高偏差和高方差"><a href="#高偏差和高方差" class="headerlink" title="高偏差和高方差"></a>高偏差和高方差</h3><p><strong>高偏差(high bias) 和 高方差(high variance)</strong> 都很高的话，就要多加入数据，或者重新设计一下架构(或者用新的模型)了<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/9.jpg" class="ful-image" alt="9.jpg"></p>
<h2 id="分析框架"><a href="#分析框架" class="headerlink" title="分析框架"></a>分析框架</h2><p>所以分析框架就来了<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/5.jpg" class="ful-image" alt="5.jpg"></p>
<p>中文版<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/10.jpg" class="ful-image" alt="10.jpg"></p>
<h1 id="人类的表现水平"><a href="#人类的表现水平" class="headerlink" title="人类的表现水平"></a>人类的表现水平</h1><p>当 DL 在处理某项任务上比人类表现还差时，你经常会看到最快的进步，而当它能达到甚至超越人类的精度后，模型就应该趋于稳定了(意思说我们就不要再去动它了)。为什么呢？因为数据集会有一个理论上的”极限”，之后对模型的种种优化很有可能只是无用功，另外，人类往往很擅长做这些任务，要超过人类的表现，从边际收益看太不划算了。</p>
<p>再次重申下，<strong>人类水平的误差和贝叶斯最优误差不是一回事</strong>，如果你不知道这一点，那么很可能在训练模型后的下一步做无用功。</p>
<p>举个例子，一个图像识别的任务，如果下图左下角的表现，也就是 train error: 8%，dev error: 10%，而人类水平的误差是 1%，那么你可以尝试增加模型大小，增加训练时间等方式来提升模型，然而如果人类水平的误差是 7.5%，那这更多的是一个方差的问题，你可能需要花更多时间来合成数据，或者找和测试集更相似的数据来训练模型。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E4%BC%9A%E8%AE%AE%E7%AC%94%E8%AE%B0%20-%20Nuts%20and%20Bolts%20of%20Applying%20Deep%20Learning/6.jpg" class="ful-image" alt="6.jpg"></p>
<p>顺便提一句，总是有优化空间的，即使已经达到了人类水平的精度，也总有一些数据子集效果不怎么好，可以在这上面努力。</p>
<p>最后，说一下怎么来定义人类水平的误差，以医疗领域的图像标签任务为例，有下面的集中误差，选哪一个作为人类水平误差？</p>
<ul>
<li>普通人类(typical human): 5%</li>
<li>普通医生(general doctor): 1%</li>
<li>专家医生(specialized doctor): 0.8%</li>
<li>专家会诊(group of specialized doctors): 0.5%</li>
</ul>
<p>答案是选最后一个，因为和贝叶斯最优误差最接近。</p>
<h1 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h1><p>最后，Ng 提出了两点建议，来帮助大家提高作为 DL 工程师的能力。</p>
<ul>
<li><strong>Practice, Practice, Practice:</strong> 参与 Kaggle 的竞赛，多读相关的博客，参与论坛讨论…</li>
<li><strong>Do the Dirty Work:</strong> 读大量的论文，做实验尝试去得到和论文一样的结果，很快，你就会有自己的想法，建自己的模型啦~</li>
</ul>
<blockquote>
<p>参考链接：<br><a href="https://www.youtube.com/watch?v=F1ka6a13S9I" target="_blank" rel="external">Nuts and Bolts of Applying Deep Learning (Andrew Ng)</a><br><a href="https://medium.com/@aniketvartak/nuts-and-bolts-of-applying-deep-learning-summary-84b8a8e873d5#.97e4f1hq3" target="_blank" rel="external">Nuts and Bolts of Applying Deep Learning — Summary</a><br><a href="https://kevinzakka.github.io/2016/09/26/applying-deep-learning/" target="_blank" rel="external">Nuts and Bolts of Applying Deep Learning</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - LightRNN - Memory and Computation-Efficient Recurrent Neural Networks]]></title>
      <url>http://www.shuang0420.com/2017/03/14/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/</url>
      <content type="html"><![CDATA[<p>关于如何用 2-Component 共享词向量来优化 RNN。<br><a id="more"></a></p>
<p><strong>原文：</strong><a href="https://arxiv.org/pdf/1610.09893v1.pdf" target="_blank" rel="external">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks</a><br><strong>部分译文：</strong> <a href="http://www.tuicool.com/articles/2QjeMzn" target="_blank" rel="external">微软重磅论文提出LightRNN：高效利用内存和计算的循环神经网络</a></p>
<p>这里只是稍微总结一下。</p>
<h1 id="研究问题"><a href="#研究问题" class="headerlink" title="研究问题"></a>研究问题</h1><p>LightRNN 解决的问题是在 perplexity 差不多的情况下 <strong>减少模型大小(model size) + 加快训练速度(computational complexity)</strong>。论文在多个基准数据集进行语言建模任务来评价 LightRNN，实验表明，在困惑度（perplexity）上面，LightRNN 实现了可与最先进的语言模型媲美或更好的准确度，同时还减少了模型大小高达百倍，加快了训练过程两倍。这带来的意义无疑是深远的，它使得先前昂贵的 RNN 算法变得非常经济且规模化了，RNN 模型运用到 GPU 甚至是移动设备成为了可能，另外，如果训练数据很大，需要分布式平行训练时，聚合本地工作器（worker）的模型所需要的交流成本也会大大降低。</p>
<h1 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h1><h2 id="单词表示-Word-Representation"><a href="#单词表示-Word-Representation" class="headerlink" title="单词表示(Word Representation)"></a>单词表示(Word Representation)</h2><p>主要思路是使用 <strong>二分量（2-Component）</strong> 来共享 embeddings。</p>
<p>将词汇表中的每一个词都分配(或者说填入)到一个二维表格中，然后每一行关联一个向量，每一列关联另一个向量。根据一个词在表中的位置，该词可由行向量和列向量两个维度联合表示。表格中每一行的单词共享一个行向量，每一列的单词共享一个列向量，所以我们仅需要 $2 \sqrt |V|$ 个向量来表示带有|V|个词的词汇表，远少于现有的方法所需要的向量数|V|。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/1.jpg" class="ful-image" alt="1.jpg"></p>
<p>$x^r_i$: 第 i 行<br>$x^c_j$: 第 j 列</p>
<h2 id="引入-RNN"><a href="#引入-RNN" class="headerlink" title="引入 RNN"></a>引入 RNN</h2><p>知道了怎么用两个向量来表示一个词语，下一步就是如何将这种表示方法引入到 RNN 中。论文的做法非常简单，将一个词的行向量和列向量按顺序分别送入 RNN 中，以语言模型(Language Model, LM)为例，要计算下一个词是 $w_t$ 的概率，先根据前文计算下一个词的行向量是 $w_t$ 的概率，在根据前文和  $w_t$ 的行向量来计算下一个词的列向量是 $w_t$ 的概率，行向量和列向量的概率乘积就是下一个词是  $w_t$ 的概率。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/4.jpg" class="ful-image" alt="4.jpg">
<h2 id="单词分配"><a href="#单词分配" class="headerlink" title="单词分配"></a>单词分配</h2><p>怎么来分配单词形成这个表格呢？</p>
<ol>
<li>对冷启动(cold start)来说，随机初始化分配单词</li>
<li>对给定的 allocation 训练 embedding vectors 直到收敛(convergence)<br> 停止条件(stopping criterion)可以是训练时间或者是 perplexity(for LM model)</li>
<li>固定上一步中学习到的 embedding vectors，重新分配单词(refine allocation)，标准当然是最小化损失函数了</li>
</ol>
<h2 id="损失函数-优化问题"><a href="#损失函数-优化问题" class="headerlink" title="损失函数/优化问题"></a>损失函数/优化问题</h2><p><strong>损失函数：</strong><br>交叉熵(corss-entropy)<br>给定 T 个单词，损失函数 NNL(negative log-likelihood) 为：<br>$$NNL = \sum^T_{t=1}-logP(w_t)=\sum^T_{t=1}-logP_r{w_t}-logP_c(w_t)$$</p>
<p>扩展一下，$NNL=\sum^{|V|}_{w=1}NLL_w$，而单词 w 的损失函数 $NNL_w$ 为：<br>$$<br>  \begin{aligned}<br>  NNL_w &amp; = \sum_{t \in S_w} -logP(w_t) = l(w,r(w),c(w))\\<br>  &amp; = \sum_{t \in S_w} -logP_r(w_t)+ \sum_{t \in S_w} -logP_c(w_t) = l_r(w,r(w)) + l_c(w,c(w)) \\<br>  \end{aligned}<br>$$</p>
<ul>
<li>$S_w$: 单词 w 的所有可能出现的位置的集合</li>
<li>$(r(w),c(w))$: 单词 w 在 allocation table 的位置</li>
<li>$l_r(w,r(w))$: 单词 w 的行损失(row loss)</li>
<li>$l_c(w,c(w))$: 单词 w 的列损失(column loss)</li>
</ul>
<p>假定 $l(w,i,j)=l(w,i)+l(w,j)$ 的情况下，计算 $l(w,i,j)$ 的复杂度是 $O(|V|^2)$，而事实上，所有的 $l_r(w,i)$ 和  $l_c(w,j)$ 都在 LightRNN 训练的前向传播中计算过了。对所有的 $w,i,j$ 计算 $l(w,i,j)$ 后，我们可以把 reallocation 的问题看做下面的优化问题：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/2.jpg" class="ful-image" alt="2.jpg">
<p>这个优化问题又可以等价为一个标准的 <strong>最小权完美匹配(minimum weight perfect matching problem)</strong>，可以用 <a href="https://dspace.mit.edu/bitstream/handle/1721.1/49424/networkflows00ahuj.pdf?sequence=1" target="_blank" rel="external">minimum cost maximum flow(MCMF)</a> 算法来实现，复杂度为 $O(|V|^3)$，主要思路大概如下图，论文的实验中用的是一个最小权完美匹配的近似算法 <a href="https://link.springer.com/chapter/10.1007/3-540-49116-3_24" target="_blank" rel="external">1/2-approximation algorithm</a>，复杂度为 $O(|V|^2)$，这和整个 LightRNN 的训练复杂度(约为 $O(|V|KT)$，K 是训练过程的 epoch 数，T 是训练集中的 token 总数)比起来不算什么。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/5.jpg" class="ful-image" alt="5.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20LightRNN%20-%20Memory%20and%20Computation-Efficient%20Recurrent%20Neural%20Networks/6.jpg" class="ful-image" alt="6.jpg">
<p>[1] Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network flows. Technical report, DTIC Document, 1988.<br>[2] Jeremy Appleyard, Tomas Kocisky, and Phil Blunsom. Optimizing performance of recurrent neural networks on gpus. arXiv preprint arXiv:1604.01946, 2016.<br>[3] Yoshua Bengio, Jean-Sébastien Senécal, et al. Quick training of probabilistic neural nets by importance sampling. In AISTATS, 2003.<br>[4] Jan A Botha and Phil Blunsom. Compositional morphology for word representations and language modelling. arXiv preprint arXiv:1405.4273, 2014.<br>[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.<br>[6] Welin Chen, David Grangier, and Michael Auli. Strategies for training large vocabulary neural language models. arXiv preprint arXiv:1512.04906, 2015.<br>[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.<br>[8] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural computation, 12(10):2451–2471, 2000.<br>[9] Joshua Goodman. Classes for fast maximum entropy training. In Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on, volume 1, pages 561–564. IEEE, 2001.<br>[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.<br>[11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.<br>[12] Shihao Ji, SVN Vishwanathan, Nadathur Satish, Michael J Anderson, and Pradeep Dubey. Blackout: Speeding up recurrent neural network language models with very large vocabularies. arXiv preprint arXiv:1511.06909, 2015.<br>[13] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. arXiv preprint arXiv:1508.06615, 2015.<br>[14] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock<code>y, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH, volume 2, page 3, 2010.
[15] TomášMikolov, Stefan Kombrink, Lukáš Burget, Jan Honza Cˇ ernocky</code>, and Sanjeev Khudanpur. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528–5531. IEEE, 2011.<br>[16] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088, 2009.<br>[17] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Aistats, volume 5, pages 246–252. Citeseer, 2005.<br>[18] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1982.<br>[19] Jan Pomikálek, Milos Jakubícek, and Pavel Rychl`y. Building a 70 billion word corpus of english from clueweb. In LREC, pages 502–506, 2012.<br>[20] Robert Preis. Linear time 1/2-approximation algorithm for maximum weighted matching in general graphs. In STACS 99, pages 259–269. Springer, 1999.<br>[21] Ha¸sim Sak, Andrew Senior, and Françoise Beaufays. Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. arXiv preprint arXiv:1402.1128, 2014.<br>[22] Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks for language modeling. In INTERSPEECH, pages 194–197, 2012.<br>[23] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112, 2014.<br>[24] Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network for sentiment classification. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432, 2015.<br>[25] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560, 1990.<br>[26] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.<br>[27] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, et al. An introduction to computational networks and the computational network toolkit. Technical report, Technical report, Tech. Rep. MSR, Microsoft Research, 2014, 2014. research. microsoft. com/apps/pubs, 2014.<br>[28] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> RNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Wide and Deep Learning for Recommender Systems]]></title>
      <url>http://www.shuang0420.com/2017/03/13/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/</url>
      <content type="html"><![CDATA[<p>Google Play 用的深度神经网络推荐系统，主要思路是将 Memorization(Wide Model) 和 Generalization(Deep Model) 取长补短相结合。论文见 <a href="https://arxiv.org/pdf/1606.07792.pdf" target="_blank" rel="external">Wide &amp; Deep Learning for Recommender Systems</a><br><a id="more"></a></p>
<h1 id="Overview-of-System"><a href="#Overview-of-System" class="headerlink" title="Overview of System"></a>Overview of System</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/1.jpg" class="ful-image" alt="1.jpg">
<p>先来看一下推荐系统的整体架构，由两个部分组成，<strong>检索系统(或者说候选生成系统）</strong> 和 <strong>排序系统(排序网络)</strong>。首先，用 <strong>检索(retrieval)</strong> 的方法对大数据集进行初步筛选，返回最匹配 query 的一部分物品列表，这里的检索通常会结合采用 <strong>机器学习模型(machine-learned models)</strong> 和 <strong>人工定义规则(human-defined rules)</strong> 两种方法。从大规模样本中召回最佳候选集之后，再使用 <strong>排序系统</strong> 对每个物品进行算分、排序，分数 P(y|x)，y 是用户采取的行动(比如说下载行为)，x 是特征，包括</p>
<ul>
<li><strong>User features</strong><br>  e.g., country, language, demographics</li>
<li><strong>Contextual features</strong><br>  e.g., device, hour of the day, day of the week</li>
<li><strong>Impression features</strong><br>  e.g., app age, historical statistics of an app</li>
</ul>
<p>WDL 就是用在排序系统中。</p>
<h1 id="Wide-and-Deep-Learning"><a href="#Wide-and-Deep-Learning" class="headerlink" title="Wide and Deep Learning"></a>Wide and Deep Learning</h1><p>简单来说，人脑就是一个不断记忆（memorization）并且归纳（generalization）的过程，而这篇论文的思想，就是将宽线性模型（Wide Model，用于记忆，下图左侧）和深度神经网络模型（Deep Model，用于归纳，下图右侧）结合，汲取各自优势形成了 Wide &amp; Deep 模型用于推荐排序（下图中间）。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/3.jpg" class="ful-image" alt="3.jpg"></p>
<h2 id="Wide-Model"><a href="#Wide-Model" class="headerlink" title="Wide Model"></a>Wide Model</h2><blockquote>
<p><strong>Memorization</strong> can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data.</p>
</blockquote>
<p>要理解的概念是 <strong>Memorization</strong>，主要是学习特征的共性或者说相关性，产生的推荐是和已经有用户行为的物品直接相关的物品。</p>
<p>用的模型是 <strong>逻辑回归(logistic regression, LR)</strong>，LR 的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable)。LR 的特征往往是二值且稀疏的(binary and sparse)，这里同样采用 one-hot 编码，如 “user_installed_app=netflix”，如果用户安装了 Netflix，这个特征的值为 1，否则为 0。</p>
<p>为了达到 Memorization，我们对稀疏的特征采取 cross-product transformation，比如说  AND(user_installed_app=netflix, impression_app=pandora”) 这个特征，只有 Netflix 和 Pandora 两个条件都达到了，值才为 1，这类 feature 解释了 co-occurrence 和 target label 之间的关系。一个 cross-product transformation 的局限在于，对于在训练集里没有出现过的 query-item pair，它不能进行泛化(Generalization)</p>
<p>到此，总结一下，宽度模型的输入是用户安装应用(installation)和为用户展示（impression）的应用间的向量积（叉乘），模型通常训练 one-hot 编码后的二值特征，这种操作不会归纳出训练集中未出现的特征对。</p>
<p><strong>Linear model</strong> 大家都很熟悉了<br>$$y = w^Tx+b$$</p>
<p>$x = [x_1, x_2, …, x_d]$ 是包含了 d 个特征的向量，$w = [w_1, w_2, …, w_d]$ 是模型参数，b 是偏置。特征包括了原始的输入特征以及 cross-product transformation 特征，cross-product transformation 的式子如下：<br>$$\varnothing_k(x)=\prod^d_{i=1}x_i^{c_{ki}}$$</p>
<p>$c_{kj}$是一个布尔变量，如果第 i 个特征是第 k 个 transformation φk 的一部分，那么值就为 1，否则为 0，作用：</p>
<blockquote>
<p>This captures the interactions between the binary features, and adds nonlinearity to the generalized linear model.</p>
</blockquote>
<h2 id="Deep-Model"><a href="#Deep-Model" class="headerlink" title="Deep Model"></a>Deep Model</h2><blockquote>
<p><strong>Generalization</strong> is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.</p>
</blockquote>
<p>要理解的概念是 <strong>Generalization</strong>，可以理解为相关性的传递(transitivity)，会学习新的特征组合，来提高推荐物品的多样性，或者说提供泛化能力(Generalization)</p>
<p>泛化往往是通过学习 low-dimensional dense embeddings 来探索过去从未或很少出现的新的特征组合来实现的，通常的 embedding-based model 有 <strong>Factorization Machines(FM)</strong> 和 <strong>Deep Neural Networks(DNN)</strong>。特殊兴趣或者小众爱好的用户，query-item matrix 非常稀疏，很难学习，然而 dense embedding 的方法还是可以得到对所有 query-item pair 非零的预测，这就会导致 over-generalize，推荐不怎么相关的物品。这点和 LR 正好互补，因为 LR 只能记住很少的特征组合。</p>
<p>为了达到 <strong>Generalization</strong>，我们会引入新的小颗粒特征，如类别特征（安装了视频类应用，展示的是音乐类应用，等等）AND(user_installed_category=video, impression_category=music)，这些高维稀疏的类别特征（如人口学特征和设备类别）映射为低纬稠密的向量后，与其他连续特征（用户年龄、应用安装数等）拼接在一起，输入 MLP 中，最后输入逻辑输出单元。</p>
<p>一开始嵌入向量(embedding vectors)被随机初始化，然后训练过程中通过最小化损失函数来优化模型。每一个隐层(hidden-layer)做这样的计算：<br>$$a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)})$$</p>
<p>f 是激活函数(通常用 ReLU)，l 是层数。</p>
<p>总结一下，基于 embedding 的深度模型的输入是 <strong>类别特征(产生embedding)+连续特征</strong>。</p>
<h2 id="Joint-Training"><a href="#Joint-Training" class="headerlink" title="Joint Training"></a>Joint Training</h2><p>对两个模型的输出算 log odds ratio 然后加权求和，作为预测。</p>
<p><strong>Joint Training vs Ensemble</strong></p>
<ul>
<li>Joint Training 同时训练 wide &amp; deep 模型，优化的参数包括两个模型各自的参数以及 weights of sum</li>
<li>Ensemble 中的模型是分别独立训练的，互不干扰，只有在预测时才会联系在一起</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/4.jpg" class="ful-image" alt="4.jpg">
<p>用 mini-batch stochastic optimization 来进行训练，可以看下这篇论文<a href="https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf" target="_blank" rel="external">Efficient Mini-batch Training for Stochastic Optimization</a>。</p>
<p>在论文提到的实验中，训练时 Wide Model 部分用了 <a href="https://pdfs.semanticscholar.org/7bdf/20d18b5a9411d729a0736c6a3a9a4b52bf4f.pdf" target="_blank" rel="external">Follow-the-regularized-learder(FTRL)</a>+ L1 正则，Deep Model 用了 <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="external">AdaGrad</a>，对于逻辑回归，模型预测如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/5.jpg" class="ful-image" alt="5.jpg"></p>
<h1 id="System-Implementation"><a href="#System-Implementation" class="headerlink" title="System Implementation"></a>System Implementation</h1><p>pipeline 如下图<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/6.jpg" class="ful-image" alt="6.jpg"></p>
<h2 id="Data-Generation"><a href="#Data-Generation" class="headerlink" title="Data Generation"></a>Data Generation</h2><p><strong>Label:</strong> 标准是 app acquisition，用户下载为 1，否则为 0<br><strong>Vocabularies:</strong> 将类别特征(categorical features)映射为整型的 id，连续的实值先用累计分布函数CDF归一化到[0,1]，再划档离散化。</p>
<blockquote>
<p>Continuous real-valued features are normalized to [0, 1] by mapping a feature value x to its cumulative distribution function P(X ≤ x), divided into $n_q$ quantiles. The normalized value is $i-1 \over n_q-1$for values in the i-th quantiles.</p>
</blockquote>
<h2 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h2><p>训练数据有 500 billion examples， Input layer 会同时产生稀疏(sparse)的和稠密(dense)的特征，具体的 Model 上面已经讨论过了。需要注意的是，当新的训练数据来临的时候，我们用的是热启动(warm-starting)方式，也就是从之前的模型中读取 embeddings 以及 linear model weights 来初始化一个新模型，而不是全部推倒重新训练。</p>
<h2 id="Model-Serving"><a href="#Model-Serving" class="headerlink" title="Model Serving"></a>Model Serving</h2><p>当模型训练并且优化好之后，我们将它载入服务器，对每一个 request，排序系统从检索系统接收候选列表以及用户特征，来为每一个 app 算分排序，分数就是前向传播的值(forward inference)啦，可以并行训练提高 performance。</p>
<blockquote>
<p>参考链接<br><a href="http://blog.csdn.net/dinosoft/article/details/52581368" target="_blank" rel="external">《Wide &amp; Deep Learning for Recommender Systems 》笔记</a><br><a href="https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx7ba47eadabd9ddde&amp;redirect_uri=http://www.gitbook.cn/weixin/m/callback/login&amp;response_type=code&amp;scope=snsapi_userinfo&amp;state=http://gitbook.cn/m/mazi/article/58a6de96f919c2152af25d1f&amp;connect_redirect=1#wechat_redirect" target="_blank" rel="external">深度学习第二课：个性化推荐</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Recommender Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Recommender Systems </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习知识框架]]></title>
      <url>http://www.shuang0420.com/2017/03/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A1%86%E6%9E%B6/</url>
      <content type="html"><![CDATA[<p>之前也写过 DNN/CNN…，然而都是哪里需要学哪里，对深度学习并没有一个知识框架，现在来补一补~<br><a id="more"></a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%A1%86%E6%9E%B6/01.jpg" class="ful-image" alt="01.jpg">
<p>图片来自小象学院公开课，下面直接解释几条线</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p><strong>线性回归 (+ 非线性激励) → 神经网络</strong></p>
<ul>
<li>有线性映射关系的数据，找到映射关系，非常简单，只能描述简单的映射关系</li>
<li>大部分关系是非线性的，所以改进方法就是加一个非线性激励，某种程度是一个 NORMALIZE，但是是非线性的，对参数有更强的描述能力</li>
<li>+非线性激励，描述稍微复杂的映射关系，形成神经网络</li>
<li>神经网络输入是 1 维信息，普通网络之间进行的是代数运算，然后经过非线性激励，形成新的神经网络</li>
</ul>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><strong>神经网络 (+时域递归) → RNN</strong></p>
<ul>
<li>神经网络处理一维信息，然而一维信息很可能是有前后的时间联系的，如人的语音，前面说的话与后面是有联系的，RNN 学习前后关系</li>
<li>相当于某一刻的输出同时也作为下一刻的输入，所以这一刻的输入不仅是这一刻的输入+上一刻输出的信息</li>
</ul>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p><strong>RNN (+记忆GATE) → LSTM</strong></p>
<ul>
<li>RNN 只考虑前一刻信息，Tn 时刻只考虑 Tn-1 的，那么 Tn-2，就是  Tn-2 → Tn-1 → Tn  逐层衰减，信息也会越来越弱</li>
<li>如果很久之前的记忆很重要，要把它记下来，就相当于有一个记忆方程，那么就可以用 LSTM，实现长记忆短记忆</li>
<li>Gate 来分析哪一部分存储，哪一部分进行传递</li>
<li><strong>应用：</strong> 语句生成 → 自动翻译智能对话</li>
</ul>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p><strong>神经网络 (+卷积核) → CNN</strong></p>
<ul>
<li>基本的代数运算用卷积核来代替。一维到二维甚至是三维的转化，相当于一个空间上的扩展</li>
<li><strong>应用：</strong> 图片分类 → 目标分类(人脸识别/物品识别/场景识别/文字识别)、目标检测(安防/自动驾驶)</li>
</ul>
<p><strong>更先进的CNN</strong></p>
<ul>
<li>深度，宽度，递归的变化<br>  增加深度(网络层数)，E.g.OverFeat-accurate，VGG<br>  增加宽度(filter数)，E.g.zf-big，OverFeat-accurate<br>  递归的变化，可以跳过下一层，传到后面几层</li>
<li>结构与性能</li>
<li>特定问题的具体结构<br>  比如说人脸识别，我们知道人脸有大体结构，用 CNN 来做识别时，可以让不同位置的像素不共享参数，眼睛有处理眼睛部分的卷积核，鼻子有处理鼻子部分的卷积核，它们之间不共享参数，这样的话参数会很多，但这样训练的结果可能会更好一些，专门对眼睛/鼻子进行训练</li>
</ul>
<h1 id="LSTM-CNN"><a href="#LSTM-CNN" class="headerlink" title="LSTM + CNN"></a>LSTM + CNN</h1><p><strong>LSTM 卷积化(LSTM + CNN)</strong></p>
<ul>
<li><strong>应用：</strong> 产生理解图片的语言 → 图片描述/标注 → 看图说话，时域的图片 → 视频分类 → 视频搜索</li>
<li>NLP 方向比较成熟的只有语音识别，语义挖掘方面还是目前的热点</li>
</ul>
<h1 id="增强学习"><a href="#增强学习" class="headerlink" title="增强学习"></a>增强学习</h1><p><strong>外部反馈 → 增强学习</strong></p>
<ul>
<li>模仿人类学习的模型</li>
<li>CNN 能理解，把它放在游戏中，做决策，给出反馈，让它学会决策的能力</li>
<li><strong>应用：</strong> 围棋，德州扑克，自动游戏，路径规划</li>
</ul>
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p><strong>生成网络 + 判别网络 → 对抗网络</strong></p>
<ul>
<li>生成网络学会怎么生成数据，如输入有表情图片，学习怎么输出没有表情的图片，实际生成质量不是很好</li>
<li>判别网络判断生成网络生成的图片是不是真的</li>
<li>两者结合生成网络生成的图片越来越逼真，判别网络鉴别图片的能力也越来越强</li>
<li><strong>作用：</strong> 生成数据，相当于无监督学习</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python os.environ.get() return None]]></title>
      <url>http://www.shuang0420.com/2017/03/10/Python%20os.environ.get()%20return%20None/</url>
      <content type="html"><![CDATA[<p>为这个问题痛苦了很久！所以就决定把本该放在 Wiki 上的东西放这了！起因是明明设置了环境变量，却不能在 python 文件中获取。<br><a id="more"></a></p>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>非常诡异的一件事<br>Jupter 和 python shell 都能运行的代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from nltk.parse.stanford import StanfordParser</div><div class="line">&gt;&gt;&gt; parser=StanfordParser(model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;)</div><div class="line">&gt;&gt;&gt; print list(parser.raw_parse(&quot;the quick brown fox jumps over the lazy dog&quot;))</div><div class="line">[Tree(&apos;ROOT&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;JJ&apos;, [&apos;quick&apos;]), Tree(&apos;JJ&apos;, [&apos;brown&apos;]), Tree(&apos;NN&apos;, [&apos;fox&apos;])]), Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NNS&apos;, [&apos;jumps&apos;])]), Tree(&apos;PP&apos;, [Tree(&apos;IN&apos;, [&apos;over&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;JJ&apos;, [&apos;lazy&apos;]), Tree(&apos;NN&apos;, [&apos;dog&apos;])])])])])])]</div></pre></td></tr></table></figure></p>
<p>运行 py 文件就不行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">from nltk.parse.stanford import StanfordParser</div><div class="line">def getParserTree(line):</div><div class="line">    parser=StanfordParser(model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;)</div><div class="line">    print list(parser.raw_parse(line))</div><div class="line"></div><div class="line">getParserTree(&quot;the quick brown fox jumps over the lazy dog&quot;)</div></pre></td></tr></table></figure></p>
<p>Error:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;Helper.py&quot;, line 70, in &lt;module&gt;</div><div class="line">    getParserTree(doc1)</div><div class="line">  File &quot;Helper.py&quot;, line 55, in getParserTree</div><div class="line">    parser=StanfordParser(model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;)</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/nltk/parse/stanford.py&quot;, line 51, in __init__</div><div class="line">    key=lambda model_name: re.match(self._JAR, model_name)</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/nltk/internals.py&quot;, line 714, in find_jar_iter</div><div class="line">    raise LookupError(&apos;\n\n%s\n%s\n%s&apos; % (div, msg, div))</div><div class="line">LookupError:</div><div class="line"></div><div class="line">===========================================================================</div><div class="line">  NLTK was unable to find stanford-parser\.jar! Set the CLASSPATH</div><div class="line">  environment variable.</div><div class="line"></div><div class="line">  For more information, on stanford-parser\.jar, see:</div><div class="line">    &lt;http://nlp.stanford.edu/software/lex-parser.shtml&gt;</div><div class="line">===========================================================================</div></pre></td></tr></table></figure></p>
<h1 id="问题探索"><a href="#问题探索" class="headerlink" title="问题探索"></a>问题探索</h1><p>来看一下环境变量，同样诡异的事就发生了，用 jupter 或 python shell 运行下面的代码，可以得到环境变量路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import os</div><div class="line">&gt;&gt;&gt; os.environ.get(&apos;CLASSPATH&apos;)</div><div class="line">/Users/sure/stanford-tools//stanford-postagger-full-2015-04-20/stanford-postagger.jar:/Users/sure/stanford-tools//stanford-ner-2015-04-20/stanford-ner.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</div></pre></td></tr></table></figure></p>
<p>然而放到 py 文件里，就只能 output 出一个 None</p>
<p>为啥！明明设置好了环境变量！看 ~/.bash_profile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">export STANFORDTOOLSDIR=/Users/sure/stanford-tools/</div><div class="line">export CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</div><div class="line"></div><div class="line">export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers</div></pre></td></tr></table></figure></p>
<p>索性一个简单粗暴的方法是直接在 py 文件里，StanfordParser 传进绝对地址参数，第一个参数是 path/to/jar，第二个参数是 path/to/module，然后就 可以运行文件了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">parser = StanfordParser(&apos;/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser.jar&apos;, &apos;/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar&apos;,model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;)</div><div class="line"></div><div class="line">print list(parser.raw_parse(&quot;the quick brown fox jumps over the lazy dog&quot;))</div></pre></td></tr></table></figure></p>
<p>然而想一想，我们要用 Stanford PoS tagging，要用 NER，要用 parser，还要用 dependency parser，那得多麻烦！而且作为一个 team，share code，每个人的路径都不一样。。。</p>
<p>有两个解决方案，一是把对应的 jar 包，model 之类的所有东西放进当前目录下，然后在 py 文件里 get 当前目录，再进行拼接，传入参数；二是单独设置一个配置文件，每个人都配好各自的环境，在 py 文件里调用配置文件，这两种方法都可以解决个人路径不一样的问题。</p>
<p>然而，为什么还是这么麻烦。为什么 get 不到环境变量！</p>
<h1 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h1><p>来看一下 os.environ 这个函数，才发现 os.environ 保存的是 python 运行的当前 shell 中的环境变量。于是就想到了：我们根本没有在当前 shell 中 mark &amp; export varible 好不好！所以：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ STANFORDTOOLSDIR=/Users/sure/stanford-tools/</div><div class="line">$ CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</div><div class="line">$ STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers</div><div class="line">$ export STANFORDTOOLSDIR</div><div class="line">$ export CLASSPATH</div><div class="line">$ export STANFORD_MODELS</div><div class="line">$ python2.7 -c &apos;import os;print os.environ.get(&quot;CLASSPATH&quot;)&apos;</div><div class="line">/Users/sure/stanford-tools//stanford-postagger-full-2015-04-20/stanford-postagger.jar:/Users/sure/stanford-tools//stanford-ner-2015-04-20/stanford-ner.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</div></pre></td></tr></table></figure>
<p>Done!<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ python2.7 Helper.py</div><div class="line">/Users/sure/stanford-tools//stanford-postagger-full-2015-04-20/stanford-postagger.jar:/Users/sure/stanford-tools//stanford-ner-2015-04-20/stanford-ner.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser.jar:/Users/sure/stanford-tools//stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</div><div class="line">[Tree(&apos;ROOT&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;JJ&apos;, [&apos;quick&apos;]), Tree(&apos;JJ&apos;, [&apos;brown&apos;]), Tree(&apos;NN&apos;, [&apos;fox&apos;])]), Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;NNS&apos;, [&apos;jumps&apos;])]), Tree(&apos;PP&apos;, [Tree(&apos;IN&apos;, [&apos;over&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;JJ&apos;, [&apos;lazy&apos;]), Tree(&apos;NN&apos;, [&apos;dog&apos;])])])])])])]</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> shell </tag>
            
            <tag> StanfordParser </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Dependency Parsing and Treebank]]></title>
      <url>http://www.shuang0420.com/2017/03/09/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。关于 Treebank，parsing algorithm，advanced grammar，这一章介绍的非常简略，以后会补充。<br><a id="more"></a></p>
<h1 id="Treebank"><a href="#Treebank" class="headerlink" title="Treebank"></a>Treebank</h1><p>来考虑一下 production rules 是怎么产生的，过去很长一段时间用的都是 hand-written grammars，需要专家编写，很难 scale，覆盖率也非常有限，所以人们手工建立了 treebank，也就是对某些语料集做的标注(annotated data)，之后 production rules 就可以通过算法直接从 treebank 抽取。<br>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/4.jpg" class="ful-image" alt="4.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/5.jpg" class="ful-image" alt="5.jpg"></p>
<p>利用 Head rules，我们可以将 Penn Treebank tree 自动转化为一个 dependency tree，一些规则如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">NP</div><div class="line">If there is an NN daughter, the rightmost NN daughter is the head.</div><div class="line">If there is an NP daughter, the leftmost NP daughter is the head.</div><div class="line">If there is an NNP daughter , the rightmost NNP daughter is the head.</div><div class="line">S</div><div class="line">If there is a VP daughter, the leftmost VP daughter is the head.</div><div class="line">ETC.</div></pre></td></tr></table></figure></p>
<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ul>
<li>Reusability of the labor</li>
<li>Many parsers, POS taggers, etc.<br>  一个 treebank 包含了很多种信息，可以用于多种 parser</li>
<li>Valuable resource for linguistics</li>
<li>Broad coverage<br>  Penn Treebank 包含了多个语料库(Brown Corpus/Wall Street Journal/ATIS/Switchboard)，每个语料库有大约一百万的单词，覆盖很广</li>
<li>Frequencies and distributional information<br>  包含了 frequency 信息</li>
<li>Use Machine Learning algorithms to train parsers<br>  把 treebank 数据分为 training set/dev set/test set，来训练 parser 吧~</li>
<li>A way to evaluate systems<br>  可以用来评估 parser 效果</li>
</ul>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>最大的一个问题是 <strong>too big to fail</strong>。因为建立这些 treebank 很费时费力费钱，所以它们不能轻易的被替代；另外，尽管大多数的决定是由专家来做的，然而大多数的 coding 确是由非专家来完成的，而这些人也处于高压以及有限预算下，treebank 并不是尽善尽美的。</p>
<h2 id="Treebank-Dataset"><a href="#Treebank-Dataset" class="headerlink" title="Treebank Dataset"></a>Treebank Dataset</h2><p><a href="https://ufal.mff.cuni.cz/pdt2.0/" target="_blank" rel="external">The Prague Dependency Treebanks</a>: Extremely high quality</p>
<p><a href="http://universaldependencies.org/" target="_blank" rel="external">The Google Universal Dependency Treebanks</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">• There were many different dependency treebanks in different formats, using different tagsets.</div><div class="line">• A consortium of researchers (spurred by Google) created universal tagsets.</div><div class="line">    – around 12 parts of speech</div><div class="line">    – around 12 dependencylabels</div><div class="line">        • e.g., nsubj, dobj, aux, etc.</div><div class="line">• The tagsets were optimized for cross-lingual transfer of models.</div><div class="line">• It is important to note that many good linguists were involved.</div><div class="line">    – The trees are not universal.</div><div class="line">    – Just the tag sets and the guidelines for how to correctly apply the tagsets.</div></pre></td></tr></table></figure></p>
<h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><p><a href="http://nlp.stanford.edu/software/tregex-faq.shtml" target="_blank" rel="external">Tregex</a><br><a href="http://nlp.stanford.edu/manning/courses/ling289/Tregex.html" target="_blank" rel="external">T-Regex operators</a><br><a href="http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/tsurgeon/Tsurgeon.html" target="_blank" rel="external">T-surgeon</a></p>
<h1 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h1><ul>
<li>Dynamic programming (CFG with heads + CKY)<br>  和 lexicalized PCFG parsing 类似<br>  时间复杂度：O(n^5)<br>  Eisner (1996) 提出一种 O(n^3) 的算法, 在结束的时候再产生 items with heads 而不是在中间产生</li>
<li>Graph algorithms<br>  McDonald et al.’s (2005) MSTParser(Maximum Spanning Tree), 单独用 ML 分类器来给 dependencies 算分</li>
<li>Constraint Satisfaction<br>  Karlsson (1990), etc. 建立所有 links, 然后删掉不符合 hard constraints 的 link</li>
<li>“Deterministic parsing”<br>  Nivre et al. (2008): MaltParser, Greedy choice of aKachments guided by ML classifiers  </li>
</ul>
<p>实现 Dependency Parsing 主要要解决 linking 和 shifting 的问题，通常可以用机器学习分类器来解决</p>
<p><strong>Source of information/Features:</strong></p>
<ul>
<li>Bilexical affinities<br>  issues→the is plausible</li>
<li>Dependency distance<br>  mostly short links</li>
<li>Intervening material<br>  Dependencies rarely span intervening verbs or punctuation</li>
<li>Valency of heads<br>  How many dependents on which side are usual for a head?</li>
<li>Some lexical word links are more common</li>
</ul>
<h2 id="MaltParser"><a href="#MaltParser" class="headerlink" title="MaltParser"></a>MaltParser</h2><p>有时间再回来填坑<br><a href="http://spark-public.s3.amazonaws.com/nlp/slides/Parsing-Dependency.pdf" target="_blank" rel="external">Dependency Parsing</a></p>
<h1 id="Advanced-Grammars"><a href="#Advanced-Grammars" class="headerlink" title="Advanced Grammars"></a>Advanced Grammars</h1><ul>
<li>Standard CFG</li>
<li>Lexicalized Grammars</li>
<li>Other formalisms<br>  Tree Adjoining Grammars<br>  Unification Grammars<br>  Categorial Grammars</li>
</ul>
<h2 id="Tree-Adjoining-Grammars-TAG"><a href="#Tree-Adjoining-Grammars-TAG" class="headerlink" title="Tree Adjoining Grammars(TAG)"></a>Tree Adjoining Grammars(TAG)</h2><p>TAG 是一个改写树的系统(formal tree rewriting system)</p>
<h3 id="Basics-of-TAG-Formalism"><a href="#Basics-of-TAG-Formalism" class="headerlink" title="Basics of TAG Formalism"></a>Basics of TAG Formalism</h3><ul>
<li><strong>Primitive elements:</strong> elementary trees<br>  Initial trees<br>  Auxiliary trees</li>
<li><strong>Operations</strong><br>  Substitution<br>  Adjoining</li>
<li><strong>Derivations</strong><br>  Derived trees<br>  Derivation trees</li>
</ul>
<p>TAG 是 CFG 的一个局部域，一级树(One level tree)对应一条规则，不是每条规则都要词汇化(lexicalized)。</p>
<p><strong>Is a grammar capable of</strong></p>
<ul>
<li>Lexicalization of each elementary domain</li>
<li>Encapsulation of the arguments of the lexical anchor</li>
</ul>
<h4 id="Elementary-trees"><a href="#Elementary-trees" class="headerlink" title="Elementary trees"></a>Elementary trees</h4><p>至少包括了一个边界节点(frontier node)是 terminal 符号，或者我们说是 lexical anchor。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/2.jpg" class="ful-image" alt="2.jpg"></p>
<h4 id="Initial-tree-amp-Substitution"><a href="#Initial-tree-amp-Substitution" class="headerlink" title="Initial tree &amp; Substitution"></a>Initial tree &amp; Substitution</h4><ul>
<li>所有内部节点(interior nodes)是 non-terminal 符号</li>
<li>边界节点(frontier node)是 terminal/non-terminal 符号，用来替换，标记为↓</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/3.jpg" class="ful-image" alt="3.jpg">
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/7.jpg" class="ful-image" alt="7.jpg"></p>
<h4 id="Auxiliary-Trees-amp-Adjoining"><a href="#Auxiliary-Trees-amp-Adjoining" class="headerlink" title="Auxiliary Trees &amp; Adjoining"></a>Auxiliary Trees &amp; Adjoining</h4><ul>
<li>有一个边界节点(frontier node)必须被标记为 foot node(*)</li>
<li>这个 foot node 必须是 non-terminal 符号，并且和根节点(root node)相同</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/6.jpg" class="ful-image" alt="6.jpg">
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/8.jpg" class="ful-image" alt="8.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/9.jpg" class="ful-image" alt="9.jpg"></p>
<p><a href="http://www.umiacs.umd.edu/~bonnie/courses/cmsc723-Fall07/yuqing_tag.pdf" target="_blank" rel="external">An Introduction to<br>Tree Adjoining Grammars</a></p>
<h2 id="Unification-Grammars"><a href="#Unification-Grammars" class="headerlink" title="Unification Grammars"></a>Unification Grammars</h2><p>Unification Grammars 主要用于解决 <strong>一致性(agreement)问题</strong>，希望不用重复执行 NP-single 和 NP-plural 的 NP 规则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">• S → NP VP</div><div class="line">[NP NUMBER] = [VP NUMBER]</div><div class="line">• Det → these</div><div class="line">[Det NUMBER] = plural</div><div class="line">• MD → does</div><div class="line">[MD NUMBER] = singular [MD PERSON] = third</div></pre></td></tr></table></figure>
<p>貌似要研究一下 feature structure，参考<a href="http://www.cs.haifa.ac.il/~shuly/malta-slides.pdf" target="_blank" rel="external">Unification Grammars</a></p>
<h2 id="Categorial-Grammars-CCG"><a href="#Categorial-Grammars-CCG" class="headerlink" title="Categorial Grammars(CCG)"></a>Categorial Grammars(CCG)</h2><p>大多数的 CCG 是从成分角度来分析句子结构的，所以它们是 phrase structure grammars<br>基本的 5 条规则：</p>
<ul>
<li>A/B + B = A</li>
<li>B + A\B = A</li>
<li>A/B + B/C = A/C</li>
<li>A CONJ A’ = A</li>
<li>A = X/(X\A)</li>
</ul>
<p><strong>Forward: X/Y Y =&gt; X</strong> X 后面如果接 Y，那么这个 phrase 就变成 X<br><strong>Backward: Y X\Y =&gt; X</strong> X 前面如果是 Y，那么这个 phrase 就变成 X</p>
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/1.jpg" class="ful-image" alt="1.jpg"></p>
<h1 id="Dependency-vs-Constituent"><a href="#Dependency-vs-Constituent" class="headerlink" title="Dependency vs Constituent"></a>Dependency vs Constituent</h1><p>可以通过 head rules 来从 CFG 中得到 dependency parse<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/10.jpg" class="ful-image" alt="10.jpg"></p>
<p>CT → DT 要比 DT → CT 简单多了。</p>
<h2 id="Tree-Example"><a href="#Tree-Example" class="headerlink" title="Tree Example"></a>Tree Example</h2><h3 id="Treebank-Tree"><a href="#Treebank-Tree" class="headerlink" title="Treebank Tree"></a>Treebank Tree</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/11.jpg" class="ful-image" alt="11.jpg">
<h3 id="Parent-Annotated-Tree"><a href="#Parent-Annotated-Tree" class="headerlink" title="Parent-Annotated Tree"></a>Parent-Annotated Tree</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/12.jpg" class="ful-image" alt="12.jpg">
<h3 id="Headed-Tree"><a href="#Headed-Tree" class="headerlink" title="Headed Tree"></a>Headed Tree</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/13.jpg" class="ful-image" alt="13.jpg">
<h3 id="Lexicalized-Tree"><a href="#Lexicalized-Tree" class="headerlink" title="Lexicalized Tree"></a>Lexicalized Tree</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Dependency%20Parsing%20and%20Treebank/14.jpg" class="ful-image" alt="14.jpg">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Question Answering System]]></title>
      <url>http://www.shuang0420.com/2017/03/02/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/</url>
      <content type="html"><![CDATA[<p>关于如何建立一个 QA 系统。整理自 Stanford NLP 公开课及 Jurafsky &amp; Chris Manning 的 NLP book draft。<br><a id="more"></a></p>
<h1 id="Single-Document-QA"><a href="#Single-Document-QA" class="headerlink" title="Single Document QA"></a>Single Document QA</h1><p>基于某个 document 的 QA 系统，一般来说假定问题答案出现在了文档中。这是我们要做的项目，之后再具体介绍。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/1.jpg" class="ful-image" alt="1.jpg"></p>
<h1 id="Question-Types"><a href="#Question-Types" class="headerlink" title="Question Types"></a>Question Types</h1><ul>
<li><strong>Simple (factoid) questions</strong> (most commercial systems)<br>  简单的问题，可以用简单的事实回答，答案简短通常是一个 named entity<br>  Who wrote the Declaration of Independence?<br>  What is the average age of the onset of autism?<br>  Where is Apple Computer based?</li>
<li><strong>Complex (narrative) questions</strong><br>  稍微复杂的叙述问题，答案略长<br>  What do scholars think about Jefferson’s position on dealing with pirates?<br>  What is a Hajj?<br>  In children with an acute febrile illness, what is the efficacy of single medication therapy with acetaminophen or ibuprofen in reducing fever?</li>
<li><strong>Complex (opinion) questions</strong><br>  复杂的问题，通常是关于观点／意见<br>  Was the Gore/Bush election fair?</li>
</ul>
<h1 id="IR-Based-Corpus-based-Approaches"><a href="#IR-Based-Corpus-based-Approaches" class="headerlink" title="IR-Based(Corpus-based) Approaches"></a>IR-Based(Corpus-based) Approaches</h1><p>简言之，就是用信息检索的方法来找最佳 answer，下面是 IR-based factoid AQ 的流程图，包括三个阶段，<strong>问题处理(question processing)</strong>，<strong>篇章检索(passage retrieval)</strong>和<strong>答案处理(answering processing)</strong></p>
<ul>
<li><strong>Question Processing</strong><br>  Answer Type Detection: 分析 question，决定 answer type<br>  Query Formulation: 形成合适的查询语句进行检索</li>
<li><strong>Passagge Retrieval</strong><br>  通过检索得到 top N documents<br>  把 documents 拆分称合适的单位(unit/passage)</li>
<li><strong>Answer Processing</strong><br>  得到候选的 answer<br>  进行排序，选出最佳 answer</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/2.jpg" class="ful-image" alt="2.jpg">
<h2 id="Question-Processing"><a href="#Question-Processing" class="headerlink" title="Question Processing"></a>Question Processing</h2><p>两个任务，确定 answer type，形成 query。有些系统还会从 question 里提取出 focus。</p>
<blockquote>
<p><strong>Answer type: </strong> the kind of entity the answer consists of (person, location, time, etc.)<br><strong>Query: </strong> the keywords that should be used for the IR system to use in searching for documents<br><strong>Focus: </strong> the string of words in the question that are likely to be replaced by the answer in any answer string found</p>
</blockquote>
<p>这一阶段需要做的事：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/17.jpg" class="ful-image" alt="17.jpg"></p>
<p>E.g.<br><strong>Question: </strong> Which US state capital has the largest population?<br><strong>Answer type: </strong> city<br><strong>Query: </strong> US state capital, largest, population<br><strong>Focus: </strong> state capital</p>
<h3 id="Answer-Type-Detection"><a href="#Answer-Type-Detection" class="headerlink" title="Answer Type Detection"></a>Answer Type Detection</h3><p>通常而言，我们把它当作一个机器学习的分类问题</p>
<ul>
<li><strong>定义</strong>类别</li>
<li><strong>注释</strong>训练数据，给数据打上分类标签</li>
<li><strong>训练</strong>分类器，所用特征可以包括 hand‐written rules</li>
</ul>
<h4 id="Define-answer-types"><a href="#Define-answer-types" class="headerlink" title="Define answer types"></a>Define answer types</h4><p>前人已经提供了一些 answer type 的层次型分类结构，如 Answer Type Taxonomy(from Li &amp; Roth)</p>
<ul>
<li>Two‐layered taxonomy</li>
<li>6 coarse classes<br>  ABBEVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC_VALUE</li>
<li>50 fine classes<br>  HUMAN: group, individual, title, description<br>  ENTITY: animal, body, color, currency…<br>  LOCATION: city, country, mountain…</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/3.jpg" class="ful-image" alt="3.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/4.jpg" class="ful-image" alt="4.jpg">
<h4 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h4><p><strong>Features:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">- words in the questions</div><div class="line">- part-of-speech of each word</div><div class="line">- named entities in the questions</div><div class="line">- question headword</div><div class="line">- semantic information about the words in the questions</div><div class="line">    WordNet synset ID of the word</div></pre></td></tr></table></figure></p>
<h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><p>一些分类方法:</p>
<ul>
<li>Hand-written rules</li>
<li>Machine Learning</li>
<li>Hybrid</li>
<li>Regular expression‐based rules can get some cases:<br>  Who {is|was|are|were} PERSON<br>  PERSON (YEAR – YEAR)</li>
<li>Other rules use the question headword<br>  = headword of first noun phrase after wh‐word:<br>  Which city in China has the largest number of foreign financial companies?<br>  What is the state flower of California?</li>
</ul>
<p>当然也可以把上述某些分类方法当作特征一起进行训练。就分类效果而言，PERSON, LOCATION, TIME 这类的问题类型有更高的准确率，REASON，DESCRIPTION 这类的问题更难识别。</p>
<h3 id="Query-Formulation"><a href="#Query-Formulation" class="headerlink" title="Query Formulation"></a>Query Formulation</h3><p>根据 question 产生一个 keyword list，作为 IR 系统的输入 query。可能的流程是去除 stopwords，丢掉 question word(where, when, etc.)，找 noun phrases，根据 tfidf 判断 keywords 的去留等等。<br>如果 keywords 太少，还可以通过 <a href="http://www.shuang0420.com/2016/10/10/Search%20Engines笔记%20-%20Pseudo%20Relevance%20Feedback/">query expansion</a> 来增加 query terms。</p>
<p>Keyword selection algorithm：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/18.jpg" class="ful-image" alt="18.jpg"></p>
<p>Results:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/5.jpg" class="ful-image" alt="5.jpg"></p>
<p><a href="https://www.youtube.com/watch?v=K7VwMBRArgw" target="_blank" rel="external">21 - 2 - Answer Types and Query Formulation-NLP-Dan Jurafsky &amp; Chris Manning</a></p>
<h2 id="Passage-Retrieval"><a href="#Passage-Retrieval" class="headerlink" title="Passage Retrieval"></a>Passage Retrieval</h2><p>有了 query，我们进行检索，会得到 top N 的文档，然而文档并不是得到 answer 的最好的单位，下一步我们需要从文档抽取 potential answer passages，来方便后面的 answer processing。passage 可以是 sections, paragraphs, sentence，具体情况具体分析。</p>
<p>两个步骤：</p>
<ol>
<li>过滤掉那些不可能包含 answer 信息的 passage<br> 可以运行 named entity 或者 answer type 的分类器</li>
<li>对剩下的 passage 进行排序<br> 监督机器学习方法<br> 特征:<pre><code>• The number of named entities of the right type in the passage
• The number of question keywords in the passage
• The longest exact sequence of question keywords that occurs in the passage
• The rank of the document from which the passage was extracted
• The proximity of the keywords from the original query to each other
    For each passage identify the shortest span that covers the keywords contained in that passage. Prefer smaller spans that include more keywords (Pasca 2003, Monz 2004).
• The N-gram overlap between the passage and the question
    Count the N-grams in the question and the N-grams in the answer passages. Prefer the passages with higher N-gram overlap with the question (Brill et al., 2002).
</code></pre></li>
</ol>
<p>对于基于 web 的 QA 系统，我们可以依靠网页搜索来做 passage extraction，简单来说，可以把网页搜索产生的 snippets 作为 passages。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/6.jpg" class="ful-image" alt="6.jpg">
<h2 id="Answer-Processing"><a href="#Answer-Processing" class="headerlink" title="Answer Processing"></a>Answer Processing</h2><p>现在，我们已经有了 question type，也给文本打了 Named Entities 的标签，就可以进一步缩小 candidate answer 的范围。</p>
<h3 id="Answer-Identification"><a href="#Answer-Identification" class="headerlink" title="Answer Identification"></a>Answer Identification</h3><p>Answer Identification 模块复杂找到问题的最佳 answer，对于具有 named entity 的问题，这个模块必须确定有正确 answer 的句子，对于没有 named entity 类型的问题，这个模块基本上得从头开始。</p>
<p>经常使用的方法是 <strong>word overlap</strong></p>
<ul>
<li><strong>Basic Word Overlap:</strong> 对每个候选答案计算分数，依据是答案中/附近的 question words 的数量</li>
<li><strong>Stop Words:</strong> 有时 closed class words（通常称为IR中的停止词）不包括在 word overlap 中。</li>
<li><strong>Stemming:</strong> 有时要用到 morphological analysis，仅仅比较词根(e.g. “walk” and “walked” would match).</li>
<li><strong>Weights:</strong> 一些 word 可能比其他 word 的权重更重（例如，动词可能被赋予比名词更多的权重）</li>
</ul>
<h3 id="Answer-Extraction"><a href="#Answer-Extraction" class="headerlink" title="Answer Extraction"></a>Answer Extraction</h3><h4 id="Pattern‐extraction-methods"><a href="#Pattern‐extraction-methods" class="headerlink" title="Pattern‐extraction methods"></a>Pattern‐extraction methods</h4><p>在 passage 上运行 answer‐type tagger，返回包含了正确的 answer-type 的文本。</p>
<p>E.g.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Who is the prime minister of India” (PERSON)</div><div class="line">    Manmohan Singh, Prime Minister of India, had told left leaders that the deal would not be renegotiated.</div><div class="line"></div><div class="line">“How tall is Mt. Everest? (LENGTH)</div><div class="line">    The official height of Mount Everest is 29035 feet</div></pre></td></tr></table></figure></p>
<p>有时候光用 pattern-extraction 方法是不够的，一方面我们不能创造规则，另一方面 passage 里也可能有多个 potential answer。另外，对于没有特定定命名实体类型的答案，我们可以使用正则表达式(人工编写或自动学习)。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/7.jpg" class="ful-image" alt="7.jpg">
<h4 id="N-gram-tiling-methods"><a href="#N-gram-tiling-methods" class="headerlink" title="N-gram tiling methods"></a>N-gram tiling methods</h4><p><strong>N-gram tiling</strong> 又被称为 <strong>redundancy-based approach</strong>(Brill et al. 2002, Lin 2007)，基于网页搜索产生的 snippet，进行 ngram 的挖掘，具体步骤如下：</p>
<ol>
<li><strong>N-gram mining</strong><br> 提取每个片段中出现的 unigram, bigram, and trigram，并赋予权重</li>
<li><strong>N-gram filtering</strong><br> 根据 ngram 和预测的 answer type 间的匹配程度给 ngram 计算分数<br> 可以通过为每个 answer type 人工编写的过滤规则来计算具体分数</li>
<li><strong>N-gram tiling</strong><br> 将重叠的 ngram 连接称更长的答案<br> <strong>standard greedy method:</strong><pre><code>1. start with the highest-scoring candidate and try to tile each other candidate with this candidate
2. add the best-scoring concatenation to the set of candidates
3. remove the lower-scoring candidate
4. continue until a single answer is built
</code></pre></li>
</ol>
<h3 id="Rank-Candidate-Answers"><a href="#Rank-Candidate-Answers" class="headerlink" title="Rank Candidate Answers"></a>Rank Candidate Answers</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/8.jpg" class="ful-image" alt="8.jpg">
<p>可能用到的 feature<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">• Answer type match: True if the candidate answer contains a phrase with the cor- rect answer type.</div><div class="line">• Pattern match: The identity of a pattern that matches the candidate answer. Number of matched question keywords: How many question keywords are contained in the candidate answer.</div><div class="line">• Keyword distance: The distance between the candidate answer and query key- words (measured in average number of words or as the number of keywords that occur in the same syntactic phrase as the candidate answer).</div><div class="line">• Novelty factor: True if at least one word in the candidate answer is novel, that is, not in the query.</div><div class="line">• Apposition features: True if the candidate answer is an appositive to a phrase con- taining many question terms. Can be approximated by the number of question terms separated from the candidate answer through at most three words and one comma (Pasca, 2003).</div><div class="line">• Punctuation location: True if the candidate answer is immediately followed by a comma, period, quotation marks, semicolon, or exclamation mark.</div><div class="line">• Sequences of question terms: The length of the longest sequence of question terms that occurs in the candidate answer.</div></pre></td></tr></table></figure></p>
<h1 id="Knowledge-based-Approaches-Siri"><a href="#Knowledge-based-Approaches-Siri" class="headerlink" title="Knowledge-based Approaches(Siri)"></a>Knowledge-based Approaches(Siri)</h1><p>对 query 建立语义模型，然后做一个 mapping 来找 answer。</p>
<blockquote>
<p><strong>semantic parsers: </strong> map from a text string to any logical form<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/10.jpg" class="ful-image" alt="10.jpg"></p>
</blockquote>
<ul>
<li><strong>Build a semantic representation of the query</strong><br>  Times, dates, locations, entities, numeric quantities</li>
<li><strong>Map from this semantics to query structured data or resources</strong><br>  Geospatial databases<br>  Ontologies (Wikipedia infoboxes, dbPedia, WordNet, Yago)<br>  Restaurant review sources and reservation services<br>  Scientific databases</li>
</ul>
<p>流行的本体库(ontologies)有 Freebase (Bollacker et al., 2008) 和 DBpedia (Bizer et al., 2009)，它们从 Wikipedia 的 infoboxes 提取了大量的三元组，可以用来回答问题。如下面这个三元组就可以回答 factoid questions，像是 ‘When was Ada Lovelace born?’ 或者 ‘Who was born in 1815?’的问题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">subject         predicate     object</div><div class="line">AdaLovelace     birth-year     1815</div></pre></td></tr></table></figure></p>
<p>Mapping<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">“When was Ada Lovelace born?” → birth-year (Ada Lovelace, ?x)</div><div class="line">“What is the capital of England?” → capital-city(?x, England)</div></pre></td></tr></table></figure></p>
<h2 id="Rule-based-Methods"><a href="#Rule-based-Methods" class="headerlink" title="Rule-based Methods"></a>Rule-based Methods</h2><p>对于一些非常常见的问题，我们可以用人工编写规则来从 question 里抽取关系，比如说 birth-year 的关系，我们可以用 question word <strong><em>When</em></strong>，动词 <strong><em>born</em></strong>，来抽取 named entity。</p>
<h2 id="Supervised-Methods"><a href="#Supervised-Methods" class="headerlink" title="Supervised Methods"></a>Supervised Methods</h2><p>一般来说，系统会有一个初始的 lexicon(每个命名实体对应着一些实例)，也会有一些默认的匹配规则，这些规则能够把 question parse tree 的某些部分映射为特定关系。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/11.jpg" class="ful-image" alt="11.jpg"></p>
<p>下面是一个二元组例子，用于训练分类器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">“When was Ada Lovelace born?” → birth-year (Ada Lovelace, ?x)</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/12.jpg" class="ful-image" alt="12.jpg">
<p>有很多这样的 pair，更多的规则将被引入，然后我们就可以预测 unseen data。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/13.jpg" class="ful-image" alt="13.jpg"></p>
<h2 id="Semi-Supervised-Methods"><a href="#Semi-Supervised-Methods" class="headerlink" title="Semi-Supervised Methods"></a>Semi-Supervised Methods</h2><p>有监督的学习当然很棒，可是我们很难得到庞大的训练数据，所以一个办法就是利用<strong>textual redundancy</strong>。最常见的 redundancy 的例子当然就是 web 了，网络中存在着大量的能够表达关系的 textual variants，所以很多方法都利用了网络文本，如半监督学习算法 distant supervision，或者是无监督学习算法 open information extraction。把网络数据与规范的知识来源(canonical knowledge source)如 wikipedia 对齐，我们能创建/学习新的映射规则。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/14.jpg" class="ful-image" alt="14.jpg"></p>
<p>另一个 redundancy 的来源是 paraphrase databases，如 wikianswers.com<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/15.jpg" class="ful-image" alt="15.jpg"></p>
<p>这些 question paraphrases 的 pair 可以通过 MT alignment 方法建立 MT-style phrase table 来将原始问题翻译成同义的问题，这种方法在现代的 QA 系统中经常用到。</p>
<h1 id="Hybrid-DeepQA-System-IBM-Watson"><a href="#Hybrid-DeepQA-System-IBM-Watson" class="headerlink" title="Hybrid/DeepQA System(IBM Watson)"></a>Hybrid/DeepQA System(IBM Watson)</h1><ul>
<li>构建问题的浅层语义表达(shallow semantic representation)</li>
<li>用信息检索方法来产生候选答案<br>  利用本体和半结构化数据</li>
<li>用更多的 knowledge source 来为候选答案计算分数<br>  Geospatial databases<br>  Temporal reasoning<br>  Taxonomical classification</li>
</ul>
<p>一些基本事实：</p>
<blockquote>
<p>“The operative goal for primary search eventually stabilized at about 85 percent binary recall for the top 250 candidates; that is, the system generates the correct answer as a candidate answer for 85 percent of the questions somewhere within the top 250 ranked candidates.”</p>
<p>“If the correct answer(s) are not generated at this stage as a candidate, the system has no hope of answering the question. This step therefore significantly favors recall over precision, with the expectation that the rest of the processing pipeline will tease out the correct answer, even if the set of candidates is quite large.”</p>
</blockquote>
<p>IBM 的 Watson 系统架构：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Question%20Answering%20System/16.jpg" class="ful-image" alt="16.jpg"></p>
<p>后面补充。</p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>如果只返回一个 answer，就用 Accuracy，如果有多个，就用 <a href="http://www.shuang0420.com/2016/09/20/Search%20Engines笔记%20-%20Evaluating%20Search%20Effectiveness/">MRR</a>，MRR 对所有的 RR 值求平均，RR(Reciprocal rank) 指的是 1/rank of first right answer。</p>
<blockquote>
<p>参考链接：<br><a href="https://web.stanford.edu/~jurafsky/slp3/28.pdf" target="_blank" rel="external">Chapter 28: Question Answering</a><br><a href="http://people.cs.pitt.edu/~litman/courses/cs2731/lec/slp23-handout.pdf" target="_blank" rel="external">Chapter23.2: Question Answering</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Constituency Parsing]]></title>
      <url>http://www.shuang0420.com/2017/02/28/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。讲句法的相关概念及各种 parsing 算法包括 CFG / PCFG / CKY / Earley 等。<br><a id="more"></a></p>
<h1 id="Context-Free-Grammars"><a href="#Context-Free-Grammars" class="headerlink" title="Context-Free Grammars"></a>Context-Free Grammars</h1><p>我们怎么知道哪些单词可以组合在一起(形成一个成分)呢？一个明显的依据是它们都可以出现在相同的句法环境中。Context-Free Grammar，简称 CFG，又称<strong>短语结构语法(Phrase-Structure Grammar)</strong>，形式化方法等价于<strong>Backus-Naur范式(Backus-Naur Form，简称BNF)</strong>。</p>
<p>CFG 对形式语言(formal language)进行了限制，production rule 箭头(→)左边只能是一个 non-terminal symbol，表示某种聚类或概括性，右边的项是一个或多个 terminal 和 non-terminal 符号构成的有序表，可以把 CFGs 想象成句子的 generator，那么可以把“→”读为“用右边的符号串来重写左边的符号”。定义如下：</p>
<ul>
<li><strong>N:</strong> a set of non-terminal symbols</li>
<li><strong>$\Sigma$:</strong> a set of terminal symbols</li>
<li><strong>R:</strong> a set of production rules of the form $X \ → \ Y_1Y_2Y_n$ for n&gt;=0, $X \in N$, $Y_i \in (N \cup \Sigma)$</li>
<li><strong>$S \in N$:</strong> a distinguished/special start symbol</li>
</ul>
<p>来个例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/eg1.jpg" class="ful-image" alt="eg1.jpg"></p>
<p>我们可以把 CFGs 想象成 declarative programs，像是 Prolog, SQL, XQuery，Re，只声明最后想要做什么(ultimate goal)，而不是怎么做(intermediary steps)，也就是说相同程序可以被用在各种不同的 context 下面，而命令式编程(imperative programs)则是基于上下文的。</p>
<blockquote>
<p>CFGs specify what is to be computed in terms of rules and let generalized computation mechanisms solve for the particular cases</p>
</blockquote>
<p>判断一些语言是不是 Context-Free Grammar<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Example 1: L1 = &#123; anbn | n is a positive integer &#125; is a context-free language. For the following context-free grammar G1 = &lt; V1 , , S , P1 &gt; generates L1 :</div><div class="line">V1 = &#123; S &#125; ,  = &#123; a , b &#125; and P1 = &#123; S -&gt; aSb , S -&gt; ab &#125;.</div><div class="line"></div><div class="line">Example 2: L2 = &#123; wwr| w  &#123;a, b &#125;+ &#125; is a context-free language , where w is a non-empty string and wr denotes the reversal of string w, that is, w is spelled backward to obtain wr . For the following context-free grammar G2 = &lt; V2 , , S , P2 &gt; generates L2 :</div><div class="line">V2 = &#123; S &#125; ,  = &#123; a , b &#125; and P2 = &#123; S -&gt; aSa , S -&gt; bSb , S -&gt; aa , S -&gt; bb &#125;.</div><div class="line"></div><div class="line">Example 3: Let L3 be the set of algebraic expressions involving identifiers x and y, operations + and * and left and right parentheses. Then L3 is a context-free language. For the following context-free grammar G3 = &lt; V3 , 3, S , P3 &gt; generates L3 :</div><div class="line">V3 = &#123; S &#125; , 3 = &#123; x , y , ( , ) , + , * &#125; and P3 = &#123; S -&gt; ( S + S ) , S -&gt; S*S , S -&gt; x , S -&gt; y &#125;.</div><div class="line"></div><div class="line">Example 4: Portions of the syntaxes of programming languages can be described by context-free grammars. For example</div><div class="line">&#123; &lt; statement &gt; -&gt; &lt; if-statement &gt; , &lt; statement &gt; -&gt; &lt; for-statement &gt; , &lt; statement &gt; -&gt; &lt; assignment &gt; , . . . , &lt; if-statement &gt; -&gt; if ( &lt; expression &gt; ) &lt; statement &gt; , &lt; for-statement &gt; -&gt; for ( &lt; expression &gt; ; &lt; expression &gt; ; &lt; expression &gt; ) &lt; statement &gt; , . . . , &lt; expression &gt; -&gt; &lt; algebraic-expression &gt; , &lt; expression &gt; -&gt; &lt; logical-expression &gt; , . . . &#125; .</div></pre></td></tr></table></figure></p>
<p>CFG 有两个相关任务，一个是 Recognition(识别)，一个是 Parsing(剖析)，输入与输出如下：</p>
<ul>
<li><strong>Input:</strong>  sentence w=(w1,…,wn) and CFG G</li>
<li><strong>Output(recognition):</strong>  true iff $w \in Language(G)$</li>
<li><strong>Output(parsing):</strong>  one or more derivations for w, under G</li>
</ul>
<p>可以用 Earley 算法在 O(n^3) 的时间复杂度内识别 CFG，下面会具体介绍。</p>
<p><a href="http://www.cs.odu.edu/~toida/nerzic/390teched/cfl/cfg.html" target="_blank" rel="external">Context-Free Grammar</a></p>
<h2 id="Chomsky-Normal-Form-CNF"><a href="#Chomsky-Normal-Form-CNF" class="headerlink" title="Chomsky Normal Form(CNF)"></a>Chomsky Normal Form(CNF)</h2><h3 id="FSA-and-CFG"><a href="#FSA-and-CFG" class="headerlink" title="FSA and CFG"></a>FSA and CFG</h3><p>当一个 non-terminal 符号的展开式中也包含了这个 non-terminal 符号时，就会产生语法的递归(recursion)问题，如 Norminal → Norminal PP中，就有递归问题。</p>
<p>Chomsky(1959)证明了一个上下文无关语言(L)能够被有限自动机(FSA)生成，当且仅当存在一个生成语言 L 的、没有任何中心-自嵌入(center-embedded)递归的上下文语法($A→\alpha A \beta$)</p>
<p>之后会讨论 FSA 的一个扩充版本，递归转移网络(recursive transition network，简称 RTN)，它给 FSA 增加了很强的地柜能力。由 RTN 形成的自动机恰好与上下文无关语法同构(isomorphic)，在一定场合下，可以作为研究 CFG 的一个有用的比喻。</p>
<p>L(G) 是一种 push-down automata，可以被 normalized(Chomsky normal form)</p>
<ul>
<li>Chomsky normal form</li>
<li>Only one or two symbols on RHS</li>
</ul>
<h3 id="CNF"><a href="#CNF" class="headerlink" title="CNF"></a>CNF</h3><p>让各个语法都拥有一个标准的形式非常有用(语法的规则部分都采用一种特殊的形式)。CNF 就是这样一种标准形式。如果一个 CFG 是 ε-free （ ε 表示空串），而且它的 rules 只有如下两种形式之一，且 $X, Y, Z \in N, \ w \in T$，那个这个 CFG 就是采用 CNF 形式的。CNF 语法都是二分叉的。</p>
<ul>
<li>X → Y Z</li>
<li>X → w</li>
</ul>
<h4 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h4><p>任何语法都可以转化成一个弱等价的 CNF 形式，只改变树的结构，且能识别相同的语言。基本思路如下：</p>
<ul>
<li>Empties and unaries are removed recursively</li>
<li>n-ary rules are divided by introducing new nonterminals(n&gt;2)</li>
</ul>
<p>CFGs 到 CNF 的转化过程：</p>
<ul>
<li><strong>For each rule</strong><br>  X → A B C</li>
<li><strong>Rewrite as</strong><br>  X → A X2<br>  X2 → B C<br>相当于引入了一个新的 non-terminal</li>
</ul>
<p>一个上下文无关语法 $G=(N, \Sigma, R, S)$ 在 Chomsky Normal Form 下的表达如下：</p>
<ul>
<li><strong>N:</strong>  a set of non-terminal symbols</li>
<li><strong>$\Sigma $:</strong>  a set of terminal symbols</li>
<li><strong>R:</strong>  a set of rules which take one of two forms:<br>  X → $Y_1Y_2$ for X $\in$ N, and $Y_1Y_2 \in N$<br>  X → Y for X $\in$ N, and Y $\in \Sigma$</li>
<li><strong>$S \in N$:</strong>  distinguished start symbol</li>
</ul>
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ccnf1.jpg" class="ful-image" alt="ccnf1.jpg"></p>
<h4 id="Binarization"><a href="#Binarization" class="headerlink" title="Binarization"></a>Binarization</h4><blockquote>
<p>Binarization is crucial for cubic time CFG parsing.</p>
</blockquote>
<p>这是我们必须知道的一条法则，在高效的 CFG parsing 中，Binarization 几乎总是必不可少的，可能是在 parsing 算法之前(如 CKY)，也可能是隐藏在了 parsing 算法中，但它总会被用到。</p>
<p>看一下 binarization 之前的 parse tree，VP → V NP PP 这一条是不符合 CNF 规则的<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ccnf2.jpg" class="ful-image" alt="ccnf2.jpg"></p>
<p>添加新的 non-terminal @VP_V，进行 binarization 之后的 parse tree<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ccnf3.jpg" class="ful-image" alt="ccnf3.jpg"></p>
<h4 id="Unaries-Empties"><a href="#Unaries-Empties" class="headerlink" title="Unaries/Empties"></a>Unaries/Empties</h4><p>进一步讨论下 unaries/empties，处理过程如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ccnf4.jpg" class="ful-image" alt="ccnf4.jpg"></p>
<p>对 CNF 来说，重建 n-aries 是非常容易的，然而重建 unaries/empties 就非常的 tricky 了，一个 neat and clean 的 CNF 往往需要删除 empties and unaries，但另一方面，我们也可以简单的保留它们(通常只删除 emptie，保留 unaries，如上图的 non-empties 的树)，以便能够重建原来的 tree。</p>
<h2 id="Recognition"><a href="#Recognition" class="headerlink" title="Recognition"></a>Recognition</h2><p>把识别当作一个搜索来做，算法如下非常简单：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Agenda = &#123; state0 &#125;</div><div class="line">while(Agenda not empty)</div><div class="line">    s = pop a state from Agenda</div><div class="line">    if s is a success-state return s // valid parse tree</div><div class="line">    else if s is not a failure-state:</div><div class="line">        generate new states from s</div><div class="line">        push new states onto Agenda</div><div class="line">return nil // no parse!</div></pre></td></tr></table></figure></p>
<h2 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h2><p>上下文无关语法最后能产生剖析树(parse tree)，这里看一下输入输出以及产生的信息，下面具体介绍 parse 过程。<br><strong>INPUT: </strong> The burglar robbed the apartment.<br><strong>OUTPUT: </strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/pt1.jpg" class="ful-image" alt="pt1.jpg"></p>
<p>Parse Trees 可以传达的信息</p>
<ol>
<li><strong>Part of speech for each word</strong><br> N = noun, V = verb, DT = determiner</li>
<li><strong>Phrases</strong><br> Noun Phrases (NP): “the burglar”, “the apartment” VerbPhrases(VP): “robbedtheapartment”<br> Sentences (S): “the burglar robbed the apartment”</li>
<li><strong>Useful Relationships</strong><br> “the burglar” is the subject of “robbed”, see picture below<br> Application: Machine Translation<br> E.g. English word order: subject-verb-object, Japanese word order: subject-object-verb</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">      S</div><div class="line">  /       \</div><div class="line"> NP       VP</div><div class="line"> |         |</div><div class="line">Subject   Verb</div></pre></td></tr></table></figure>
<p>Parsing 的算法有两类，一个是 top-down，一个是 bottom-up。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ps.jpg" class="ful-image" alt="ps.jpg"></p>
<h3 id="Top-down-Parser"><a href="#Top-down-Parser" class="headerlink" title="Top-down Parser"></a>Top-down Parser</h3><ul>
<li><strong>Start state:</strong>  (S, 0)</li>
<li><strong>Scan:</strong>  From (wj+1 β, j), you can get to (β, j + 1).</li>
<li><strong>Predict:</strong>  If Z → γ, then from (Z β, j), you can get to (γβ, j).</li>
<li><strong>Final state:</strong>  (ε, n)</li>
</ul>
<p>假定剖析要并行地构造出所有可能的树。算法开始时假定，给初始符号指派 S，输入就可以从 S 开始被推导出来。下一步搜索所有能够以 S 为顶点的树，寻找在语法的所有规则中左手边为 S 的规则。如下图，原始句子为 Book that flight，有三条规则可以展开 S，所以在图中的搜索空间第二层中，创造了三个局部树。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/top-down.jpg" class="ful-image" alt="top-down.jpg">
<p>在第三层中，只有第五个 parse tree(由规则 VP → Verb NP 展开的树)最后与输入句子 Book that flight 相匹配。这里用的是 Left-Most Derivations 方法，每次都从最左边的 non-terminal X 开始，把 X 替换成 $\beta$($X→\beta$ 是 R 里的一条规则)，再举一个例子：<br>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/pt3.jpg" class="ful-image" alt="pt3.jpg"><br>[S], [NP VP], [D N VP], [the N VP], [the man VP], [the man Vi], [the man sleeps]</p>
<h3 id="Bottom-up-Parser"><a href="#Bottom-up-Parser" class="headerlink" title="Bottom-up Parser"></a>Bottom-up Parser</h3><ul>
<li><strong>Start state:</strong>  (ε, 0)</li>
<li><strong>Shift:</strong>  From (α, j), you can get to (α wj+1, j + 1).</li>
<li><strong>Reduce:</strong>  If Z → γ, then from (αγ, j) you can get to (α Z, j).</li>
<li><strong>Final state:</strong>  (S, n)</li>
</ul>
<p>从输入的单词开始，每次都是用语法中的规则，试图从底部的单词向上构造剖析树。如果剖析器成功地构造了以初始符号 S 为根的树，而且这个树覆盖了整个输入，那么剖析就获得了成功。同样，以 Book that flight 为例，book 有歧义，可能是动词也可能是名词，所以就有了刚开始的分叉。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/bottom-up.jpg" class="ful-image" alt="bottom-up.jpg"></p>
<p>对于 bottom-up 的剖析，从一层到下一层时，要寻找被剖析的成分是否与某个规则的右手边相匹配，这与 top-down 的剖析正好相反。最后第二层，把 book 解释为名词的树枝在搜索空间中被剪除，因为语法中没有以 Nominal NP 为右边的规则，因而无法继续剖析。</p>
<h3 id="Top-down-vs-Bottom-up"><a href="#Top-down-vs-Bottom-up" class="headerlink" title="Top-down vs Bottom-up"></a>Top-down vs Bottom-up</h3><p>两种方法各有优缺点。Top-down 策略绝不会浪费时间搜索一个不可能以 S 为根的树，或者说，它不可能去搜索那些在以 S 为根的树中找不到位置的子树；与此相反，bottom-up 策略中，那些不可能导致 S 的树大量存在着，如上面的例子，刚开始就把 book 错误的解释为名词，而在给定语法中，这样的树根本不可能推导出 S。<br>另一方面，top-down 会花费大量努力去产生与输入不一致的根为 S 的树，在 top-down 的图中，第三层六个树里，前四个树的左分支都不能与 book 匹配，因而这些树都不能形成最后的 parse tree。top-down 的这个弱点是由于这种方法在没有检查输入符号之前就开始生成树了，反之，bottom-up 绝不会去搜索那些不是以实际的输入为基础的树。</p>
<p>由此可见，这两种方法都不能有效利用语法和输入单词中的约束条件。</p>
<p>总而言之，Parsing 要解决的两个问题，一个是怎么 avoid repeated work，另一个是怎么解决 ambiguity。</p>
<h2 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h2><p>由 CFG 产生的字符串可能有多个 derivation，这就会产生 ambiguity。<br>E.g.<br>VP → Verb NP prefer a morning flight<br>VP → Verb NP PP leave Boston in the morning<br>VP → Verb PP leaving on Tuesday<br>如上，一个 VP 可能有多个规则，因此一个句子也可能有多个 parse tree。</p>
<p>Ambiguity 的来源：</p>
<ul>
<li><strong>Part-of-Speech ambiguity</strong><br>  NNS → walks<br>  Vi → walks</li>
<li><strong>Prepositional Phrase Attachment</strong><br>  the fast car mechanic under the pigeon in the box</li>
</ul>
<p>关于 attachment 带来的 ambiguity，再多讲几句，放在不同的地方修饰不同的词差别是很大的，一个 Prepositional Phrase(PP) 可以跟在前面的名词/动词前面，这就会产生各种歧义。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/10.jpg" class="ful-image" alt="10.jpg">
<p>怎么选择正确的 parse 呢？有一种方法是基于统计数据，在语料库中，看某个 PP 跟在特定名词/动词后面的概率，取概率大的那种 parse。</p>
<h2 id="Grammaticality"><a href="#Grammaticality" class="headerlink" title="Grammaticality"></a>Grammaticality</h2><p>从语法而言，也有很多种表达，虽然下面的句子有些在我们看来语法上是错误的，然而表达上却是没有问题的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">• I&apos;ll write the company</div><div class="line">• I&apos;ll write to the company</div><div class="line">• It needs to be washed</div><div class="line">• It needs washed</div><div class="line">• They met Friday to discuss it</div><div class="line">• They met on Friday to discuss it</div></pre></td></tr></table></figure></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>CFG 提供了一个用于创建语法的工具集<br>  Grammars that work well (for a given application)<br>  Grammars that work poorly (for a given application)</li>
<li>关于CFG理论，没有一个先验 prior 来告诉你给定应用的“正确”语法看起来应该是怎样的</li>
<li>一个好的语法通常是：<br>  Doesn’t over-generate very much (high precision)<br>  Doesn’t under-generate very much (high recall)</li>
<li>在实践中具体情况具体分析</li>
<li>CFG可能不足以完全捕获自然语言的语法<br>  but almost adequate<br>  computationally well-behaved<br>  not very convenient as a means for hand- crafting a grammar<br>  not probabalistic</li>
<li>有些信息抽取问题可以不使用完全剖析，而使用层叠式 FSA(cascade)来解决</li>
</ul>
<h1 id="Improved-Algorithms"><a href="#Improved-Algorithms" class="headerlink" title="Improved Algorithms"></a>Improved Algorithms</h1><h2 id="Probabilistic-Context-Free-Grammar-PCFGs"><a href="#Probabilistic-Context-Free-Grammar-PCFGs" class="headerlink" title="Probabilistic Context-Free Grammar(PCFGs)"></a>Probabilistic Context-Free Grammar(PCFGs)</h2><p>对 CFG 的最简单的提升就是概率上下文无关语法(PCFG)，又称随机上下文无关语法(Stochastic Context-Free Grammar，简称 SCFG)，其最大贡献是它进行了歧义消解(disambiguation)，来自 Stanford 讲义</p>
<p>上下文无关语法 G 是由四个参数 (N,$\Sigma$, R, S)来定义的</p>
<ul>
<li><strong>N:</strong> a set of non-terminal symbols</li>
<li><strong>$\Sigma$:</strong> a set of terminal symbols</li>
<li><strong>R:</strong> a set of production rules of the form $A \ → \ \beta$, $A \in N$, $\beta \in (N \cup \Sigma)$</li>
<li><strong>$S \in N$:</strong> a distinguished/special start symbol</li>
</ul>
<p>PCFG 的产生式 R 中的每个规则都加上来一个条件概率，从而增强了这些规则：<br>$$A → \beta [p]$$</p>
<p>所以 PCFG 是一个五元组 (N,$\Sigma$, P, S, D)，D 的功能是给 R 中的每个规则指派一个概率，或者说是把给定的 non-terminal 符号 p 展开为符号序列 $\beta$ 时的概率，这个概率通常表示为 $P(A → \beta)$，或者 $P(A → \beta|A)$，一个 non-terminal 符号的所有展开，概率之和为1.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/pcfg1.jpg" class="ful-image" alt="pcfg1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/pcfg2.jpg" class="ful-image" alt="pcfg2.jpg"></p>
<p>PCFG 可以用来估计关于一个句子及其 parse tree 的有用概率的数量。PCFG 可以对于一个句子 S 的每个 parse tree T(也就是每个推导结果)都指派一个概率，这在<strong>歧义消解(disambiguation)</strong> 中是非常有用的。<br>一个特定 parse tree T 的概率定义为在该 parse tree 中用来展开每个节点 n 的所有规则 r 的概率的乘积：<br>$$P(T,S)=\prod_{n \in T} p(r(n))$$<br>所以作为结果的概率 P(T,S)既是剖析和句子的联合概率，又是剖析 P(T)的概率。这是 make sense 的，因为剖析包含了句子中的所有单词，所以 P(S|T)=1,所以有<br>$$P(T,S)=P(T)*P(S|T)=P(T)$$</p>
<p>歧义消解(disambiguation)例子：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/da.jpg" class="ful-image" alt="da.jpg"></p>
<p>算出 $P(T_l)=1.5*10^{-6}$，$P(T_r)=1.7*10^{-6}$，右侧的 parse tree 具有比较高的概率，所以如果歧义消解算法选择具有最大 PCFG 概率的剖析，那么这个剖析便可以通过这样的歧义消解算法选择正确的结果。</p>
<p>形式化一下，得到给定句子 S 的最佳 parse tree T<br>$$<br>  \begin{aligned}<br>  \hat T(S) &amp;= argmax_{T \in \tau(S)} P(T|S) \\<br>   &amp; = argmax_{T \in \tau(S)} {P(T|S) \over P(S)} \\<br>   &amp; = argmax_{T \in \tau(S)} P(T, S) \\<br>   &amp; = argmax_{T \in \tau(S)} P(T) \\<br>  \end{aligned}<br>$$</p>
<p>$$P(S) = \sum_{T \in \tau(S)} P(T,S) = \sum_{T \in \tau(S)} P(T)$$</p>
<p>在 PCFG 中，如果一种语言的所有句子的概率之和为 1，就可以说这个 PCFG 是坚固的(consistent)，有些递归规则会使语法变得不坚固，如概率为 1 的规则 S→S 就会导致概率量的丧失，因为推导永远不会终止。</p>
<p>PCFG 是 robust 的，它考虑了所有的可能，虽然通常带来了很低的概率；它部分解决了 grammar ambiguity 的问题，但并没有那么完美，因为它的独立性假设太强了；同时，PCFG 给出了一个基于概率的语言模型(probabilistic language model)，然而它往往比 trigram model 的表现差的多，因为它缺少 lexicalization</p>
<p>另外，PCFG 假设任何一个 non-terminal 符号的展开与任何其他 non-terminal 符号的展开是独立的，然而这个假设太强了，可以通过 state-splitting 的方式来放宽假设，如下面的 Parent Annotation Tree 和 Marking possessive NPs Tree。同时要注意的是，如果分割太多了，可能会导致稀疏性问题。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/refine.jpg" class="ful-image" alt="refine.jpg"></p>
<h2 id="Cocke-Kasami-Younger"><a href="#Cocke-Kasami-Younger" class="headerlink" title="Cocke-Kasami-Younger"></a>Cocke-Kasami-Younger</h2><p>主要是一种 bottom-up 的剖析算法，使用动态规划表来存储结果。输入必须是具有 Chomsky 范式(CNF)的。以 People fish tanks 这个句子，形象化的理解 PCFG 下的 CKY 算法。首先为一个句子建立一个三角形的表格，然后 bottom-up 一层一层向上填充 chart<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckyeg1.jpg" class="ful-image" alt="ckyeg1.jpg"></p>
<p>以前两个单词为例，假定单词的 PoS 已经填充完毕，现在我们要填充最上面的 cell。根据右边的规则找出所有可能的组合，计算概率。注意，如果规则的等式左边是 S，我们只保留最大概率的那个组合。在这个例子里，我们看到有两个可能性能够组成 sentence，S → NP VP 和 S → VP，这种情况下，我们只S → NP VP。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckyeg2.jpg" class="ful-image" alt="ckyeg2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckyeg3.jpg" class="ful-image" alt="ckyeg3.jpg"></p>
<p>核心算法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/cky.jpg" class="ful-image" alt="cky.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/cky2.jpg" class="ful-image" alt="cky2.jpg"></p>
<p>完整算法(stanford slides)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckya1.jpg" class="ful-image" alt="ckya1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckya2.jpg" class="ful-image" alt="ckya2.jpg"></p>
<p>score, back 两个数组的作用是用空间换时间</p>
<p>worked example<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckyworkedeg1.jpg" class="ful-image" alt="ckyworkedeg1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/ckyworkedeg2.jpg" class="ful-image" alt="ckyworkedeg2.jpg"></p>
<p>时间复杂度：</p>
<ul>
<li>Worst case: O(n^3*g)</li>
<li>Best in worst case</li>
<li>Others better in average case</li>
</ul>
<p><strong>Summary:</strong></p>
<ul>
<li>Fills in table bottom-up, using dynamic programming<br>  • Only builds constituents that have evidence in the input<br>  • Never builds a constituent instance more than once<br>  • But it builds things that cannot be used</li>
<li>Chomsky Normal Form is annoying</li>
</ul>
<p><a href="https://www.youtube.com/watch?v=hq80J8kBg-Y" target="_blank" rel="external">15 - 3 - CKY Parsing -Stanford NLP-Professor Dan Jurafsky &amp; Chris Manning</a><br><a href="https://www.youtube.com/watch?v=MiEKnFyErbQ" target="_blank" rel="external">15 - 4 - CKY Example-Stanford NLP-Professor Dan Jurafsky &amp; Chris Manning</a></p>
<h2 id="Earley"><a href="#Earley" class="headerlink" title="Earley"></a>Earley</h2><p>主要是一种 top-down 的剖析算法，使用动态规划表来有效存储中间结果。与 CKY 算法相比，Earley 的优势在于：</p>
<ul>
<li>Never build things that are useless (goes top-down)，大多数情况下时间复杂度小于 $O(n^3)$</li>
<li>不用把 grammar 转化成 CNF 范式</li>
</ul>
<p>如果输入有 N 个单词，Earley 算法会创建一个 N+1 大小的 chart，对于句子中每一个单词的位置，chart 包含一个状态表来表示已经生成的部分剖析树，在句子结尾，chart 把对于给定输入的所有可能的剖析结果进行编码，每个可能的子树只表示一次，并且这个子树表示可以被需要它的所有的剖析共享。</p>
<p>我们用点规则(dotted rules)来表示状态，来分隔走过的进程以及未完成的进程，每个 chart entry 可能含有三种信息：</p>
<ul>
<li><strong>Completed constituents and their locations: </strong><br>  S → · VP [0,0]<br>  点在成分左侧，表示这个特定的开始节点 S，第一个 0 表示预测的成分开始于 input string 的开头，第二个 0 表示点也在开头的位置</li>
<li><strong>In-progress constituents: </strong><br>  NP → Det · Nominal [1,2]<br>  NP 开始于位置 1，Det 已经被成功剖析，期待下一步处理 Nominal</li>
<li><strong>Predicted constituents: </strong><br>  VP → V NP · [0,3]<br>  点处于两个成分右侧，表示已经成功找到了与 VP 相对应的树，而且这个 VP 横跨在整个 input string 上</li>
</ul>
<p>从左到右走过 chart 的由 N+1 个状态组成的集合，按顺序处理每个集合中的各个状态，每一步根据具体情况将下面描述的三个操作中的一个应用于每个状态，向前移动后，不会再回溯，直到最后一个状态 S → $\alpha$ · [0,N]，表示剖析成功。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/earley1.jpg" class="ful-image" alt="earley1.jpg"></p>
<p>简单版的步骤</p>
<ol>
<li>Predict all the states you can upfront</li>
<li>Read a word<br> Extend states based on matches<br> Generate new predictions<br> Go to step 2</li>
<li>When you’re out of words, look at the chart to see if you have a winner</li>
</ol>
<p>假设设有CFG如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">S → NP VP</div><div class="line">NP → DT NN</div><div class="line">VP → VBD NP</div><div class="line">DT → the</div><div class="line">NN → rat</div><div class="line">NN → cheese</div><div class="line">VBD → ate</div></pre></td></tr></table></figure></p>
<p>下面以 the rat ate the cheese 为输入来演示一下 Earley 算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">S → ● NP VP [0,0]         （Predictor）:初始状态                      -(0)</div><div class="line">NP → ● DT NN [0,0]        （Predictor）:由初始状态 (0) 得到            -(1)</div><div class="line"></div><div class="line">DT → the ● [0,1]           (Scanner)   :由 (1) 得到                  -(2)</div><div class="line">NP → DT ● NN [0,1]         (Completer):由 (1) 和 (2) 归并得到         -(3)</div><div class="line"></div><div class="line">NN → rat ● [1,2]           (Scanner)   :由 (3) 得到                  -(4)</div><div class="line">NP → DT NN ● [0,2]         (Completer):由 (3) 和 (4) 归并得到         -(5)</div><div class="line">S → NP ● VP [0,2]          (Completer):由 (0) 和 (5) 归并得到         -(6)</div><div class="line">VP → ● VBD NP [2,2]        (Predictor）:由 (6) 得到                  -(7)</div><div class="line"></div><div class="line">VBD → ate ● [2,3]          (Scanner)   :由 (7) 得到                  -(8)</div><div class="line">VP → VBD ● NP [2,3]        (Completer):由 (7) 和 (8) 归并得到         -(9)</div><div class="line">NP → ● DT NN [3,3]        （Predictor）:由 (9) 得到                   -(10)</div><div class="line"></div><div class="line">DT → the ● [3,4]           (Scanner)   :由 (10) 得到                 -(11)</div><div class="line">NP → DT ● NN [3,4]         (Completer):由 (10) 和 (1) 归并得到        -(12)</div><div class="line"></div><div class="line">NN → cheese ● [4,5]        (Scanner)   :由 (12) 得到                 -(13)</div><div class="line">NP → DT NN ● [3,5]         (Completer):由 (12) 和 (13) 归并得到       -(14)</div><div class="line">VP → VBD NP ● [2,5]        (Completer):由 (9) 和 (14) 归并得到        -(15)    </div><div class="line">S → NP VP ● [0,5]          (Completer):由 (6) 和 (15) 归并得到        -(16)</div></pre></td></tr></table></figure></p>
<p>具体算法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/earley2.jpg" class="ful-image" alt="earley2.jpg"></p>
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/earley3.jpg" class="ful-image" alt="earley3.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/earley4.jpg" class="ful-image" alt="earley4.jpg"></p>
<p>具体来看三个操作<strong>预测(Predictor)、完成(Completer)、扫描(Scanner)</strong>。</p>
<ul>
<li><strong>Predictor: </strong> for non-terminals<br>  用于 dotted-rule 中 dot 右侧为 non-terminal 符号但又不是词类范畴(part-of-speech category)的任何状态<br>  对语法提供的 non-terminal 符号的不同展开，都创造一个新的状态，放到同样一个 chart 中，开始和结束位置与之前相同<br>  S → · VP [0,0]，用 Predictor，就是在第一个 chart entry 中增加状态 VP → · Verb, [0,0] 和 VP → · Verb NP, [0,0]</li>
<li><strong>Scanner: </strong> for words<br>  用于 dotted-rule 中 dot 右侧为词类范畴(part-of-speech category)的状态<br>  检查 input string，把对应于所预测的词类范畴(part-of-speech category)的状态加入到 chart 中，scanner 操作后，从输入状态中造出一个新的状态<br>  VP → · Verb NP [0,0]，dot 后面是词类，所以 scanner 要在输入中寻找当前的单词，而 book 是一个 verb，与当前状态中的预测匹配，所以创造出一个新的状态 VP → Verb · NP [0,1]，然后把这个新的状态加到 chart 中，跟随在当前处理过的状态之后</li>
<li><strong>Completer: </strong> otherwise<br>  状态中的 dot 到达规则右端时，剖析算法成功找到了在输入的某个跨度上的一个特定的语法范畴，completer 的目标就是寻找输入中在这个位置的语法范畴，发现并且推进前面造出的所有状态<br>  NP → Det Nominal · [1,3]，completer 要寻找以 1 为结尾并预测 NP 的状态，找到由 Scanner 造出的状态 VP → Verb · NP [0,1]，结果是加入一个新的完成状态 VP → Verb NP · [0,3]</li>
</ul>
<h2 id="CKY-and-Earley"><a href="#CKY-and-Earley" class="headerlink" title="CKY and Earley"></a>CKY and Earley</h2><p>CKY 和 Earley 都是 chart parsing 的方法，Earley 是 top-down 的，CKY 是 bottom-up 的。也有两个算法都不能解决的问题，如一致性(agreement)问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">• Number</div><div class="line">    Chen is/people are</div><div class="line">• Person</div><div class="line">    I am/Chen is</div><div class="line">• Tense</div><div class="line">    Chen was reading/Chen is reading/Chen will be reading</div><div class="line">• Case</div><div class="line">    not in English but in many other languages such as German, Russian, Greek</div><div class="line">• Gender</div><div class="line">    not in English but in many other languages such as German, French, Spanish</div></pre></td></tr></table></figure></p>
<p>可以用 Combinatorial Explosion 方法来解决，如下，分开表示第一人称NP、第二人称NP、第三人称NP的规则，然而这样的组合太多了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">– S → NP VP</div><div class="line">– S → 1sgNP 1sgVP</div><div class="line">– S → 2sgNP 2sgVP</div><div class="line">– S → 3sgNP 3sgVP</div><div class="line">...</div><div class="line">– 1sgNP → 1sgN</div></pre></td></tr></table></figure></p>
<p>另外一个问题是次范畴化的问题(Subcategorization Frames)，这需要依存算法来解决。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">• Direct object</div><div class="line">    The dog ate a sausage</div><div class="line">• Prepositional phrase</div><div class="line">    Mary left the car in the garage</div><div class="line">• Predicative adjective</div><div class="line">    The receptionist looked worried</div><div class="line">• Bare infinitive</div><div class="line">    She helped me buy this place</div><div class="line">• To-infinitive</div><div class="line">    The girl wanted to be alone</div><div class="line">• Participial phrase</div><div class="line">    He stayed crying after the movie ended</div><div class="line">• That-clause</div><div class="line">    Ravi doesn’t believe that it will rain tomorrow</div><div class="line">• Question-form clauses</div><div class="line">    She wondered where to go</div></pre></td></tr></table></figure></p>
<p>最后，来回顾一下 CFG 的假设，CFG 假设任何一个 non-terminal 符号的展开与任何其他 non-terminal 符号的展开是独立的，然而这个假设是不成立的，可以看一下下面的例子，这个问题的解决需要引入 Lexicalized grammars。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Non-independence</div><div class="line">– All NPs</div><div class="line">    11% NP PP, 9% DT NN, 6% PRP</div><div class="line">– NPs under S</div><div class="line">    9% NP PP, 9% DT NN, 21% PRP</div><div class="line">– NPs under VP</div><div class="line">    23% NP PP, 7% DT NN, 4% PRP</div><div class="line">– (example from Dan Klein)</div></pre></td></tr></table></figure></p>
<h1 id="Constituency-Parser-Evaluation"><a href="#Constituency-Parser-Evaluation" class="headerlink" title="Constituency Parser Evaluation"></a>Constituency Parser Evaluation</h1><p>需要有 label 数据，评价方法如下，上面的 parse tree 是标准的，下面的 parse tree 是算法产生的，我们标注每一个成分的起始和终止位置，然后比较两个 tree，看两个 tree 对应成分的起始和终止位置是否相同，计算 precision，recall，F1，和 tagging accuracy。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20and%20Parsing/eva1.jpg" class="ful-image" alt="eva1.jpg"></p>
<p><strong>Labeled-Precision:</strong> 3/7=42.9%<br><strong>Labeled-Recall:</strong> 3/8=37.5%<br><strong>LP/LR-F1:</strong> 40.0%<br><strong>Tagging-Accuracy:</strong> 11/11=100.0%</p>
<p>这种评估方式有其弱点，上层的错误会被下层持续继承，如上面的例子，只有最后一个单词 yesterday 的识别出现了错误，然而一层层传承下来，准确率就变得很低。</p>
<blockquote>
<p>参考链接：<br><a href="http://blog.csdn.net/baimafujinji/article/details/6495823" target="_blank" rel="external">自然语言处理中的Earley算法</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Syntax Introduction]]></title>
      <url>http://www.shuang0420.com/2017/02/27/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。讲句法的相关概念，sentence parsing 的两大结构(Constituency structure and Dependency structure)，以及Chomsky Hierarchy。<br><a id="more"></a></p>
<h1 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax"></a>Syntax</h1><p><strong>句法(syntax)</strong> 具体来说就是把单词安排在一起的方法，前面我们其实已经涉猎了一些句法知识，如 POS, ngram 等，这一章将介绍相对更复杂的概念。</p>
<p><strong>Syntax is NOT Morphology:</strong></p>
<ul>
<li>形态学以单词为单位，处理的是单词内部的结构；而句法处理多个单词的各种组合，单位是词组和句子</li>
<li>形态学通常是不规则的；而句法通常是规则的，不规则占小比例，大多数情况下句法由全局的一般规则组成。</li>
</ul>
<p><strong>Syntax is NOT Semantics:</strong></p>
<ul>
<li>语义是关于句子的含义；句法是关于句子的结构本身</li>
<li>一个句子在句法结构上可以是 well-formed，然而语义上非常糟糕<br>  E.g. Colorless green ideas sleep furiously</li>
<li>一些比较有名的语言学理论尝试从句法表达中“读取”语义表达</li>
</ul>
<h2 id="Constituency-组成性"><a href="#Constituency-组成性" class="headerlink" title="Constituency(组成性)"></a>Constituency(组成性)</h2><p><strong>基本思想：</strong> 单词的组合可以具有像一个单独的单位或短语那样的功能，这样的单词组合称为<strong>成分(constituent)</strong>，我们可以通过成分的集合来观察句子结构。</p>
<blockquote>
<p><strong>constituent:</strong> a group of words that “go together” (or relate more closely to one another than to other words in the sentence). We can view the structure of a sentence as a collection of nested constituents</p>
</blockquote>
<p>可以从几个角度来识别<strong>成分</strong>：</p>
<ul>
<li><strong>Distribution:</strong><br>  一个成分可以是一个能放在句子不同位置的单位(unit)<pre><code>E.g. John talked [to the children] [about drugs].
     John talked [about drugs] [to the children].
</code></pre></li>
<li><strong>Substituion/expansion/pro-forms:</strong><br>  可以被替换/扩充的一些 unit，比如说一些状语<pre><code>E.g. I sat [on the box/right on top of the box/there]
</code></pre></li>
<li><strong>Coordination/regular internal sturcture/no intrusion/fragments/semantics…</strong></li>
</ul>
<p>最简单粗暴的方法就是理解为 <strong>词组(phrase)</strong>，多个单词组成的成分就是词组(phrase)，一个词组可以内嵌多个词组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">E.g.</div><div class="line">Noun Phrases</div><div class="line">• **The elephant** arrived.</div><div class="line">• **The big ugly elephant** arrived.</div><div class="line"></div><div class="line">Prepositional Phrases(also contains a noun phrase)</div><div class="line">• I arrived **on ***Tuesday*** **.</div><div class="line">• I arrived **under the ***leaking roof*** **.</div></pre></td></tr></table></figure>
<h2 id="Grammatical-relation-语法关系"><a href="#Grammatical-relation-语法关系" class="headerlink" title="Grammatical relation(语法关系)"></a>Grammatical relation(语法关系)</h2><p><strong>语法关系(grammatical relation)</strong> 是传统语法关于主语(SUBJECTS)和宾语(OBJECTS)的思想的形式化，在之后讨论一致关系(agreement)中再具体介绍语法关系，这里先有个印象。</p>
<p>关于主语，我们有下面三种认定事实：</p>
<ul>
<li>主语是句子中的第一个名词短语</li>
<li>主语是句子中的动作发出者(actor)</li>
<li>主语是 what the sentence is about</li>
</ul>
<p>所有这些都是对的，但没有一个 always right。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">E.g.</div><div class="line">– Oswald shot Kennedy</div><div class="line">– Kennedy was shot by Oswald – Oswald was shot by Ruby</div><div class="line">– Who shot Oswald?</div><div class="line"></div><div class="line"></div><div class="line">• Active/Passive</div><div class="line">– Oswald shot Kennedy</div><div class="line">– Kennedy was shot by Oswald</div><div class="line"></div><div class="line">• Relative clauses</div><div class="line">– Oswald who shot Kennedy was shot by Ruby</div><div class="line">– Kennedy who Oswald shot didn&apos;t shoot anybody</div><div class="line"></div><div class="line">• Syntactic (not semantic)</div><div class="line">– The batter hit the ball. [subject is semantic agent]</div><div class="line">– The ball was hit by the batter. [subject is semantic patient]</div><div class="line">– The ball was given a whack by the batter. [subject is semantic recipient]</div><div class="line">– &#123;George, the key, the wind&#125; opened the door.</div><div class="line"></div><div class="line">• Subject ≠ topic</div><div class="line">– I just married the most beautiful woman in the world.</div><div class="line">– Now beans, I like.</div><div class="line">– As for democracy, I think it’s the best form of government.</div><div class="line"></div><div class="line">• English subjects</div><div class="line">– agree with the verb</div><div class="line">– when pronouns, in nominative case (I/she/he vs. me/her/him)</div><div class="line">– omitted from infinitive clauses (I tried __ to read the book, I hoped __ to be chosen)</div><div class="line"></div><div class="line">• English objects</div><div class="line">– when pronouns, in accusative case</div><div class="line">– become subjects in passive sentences</div></pre></td></tr></table></figure>
<h2 id="Subcategorization-and-dependency-次范畴化和依存关系"><a href="#Subcategorization-and-dependency-次范畴化和依存关系" class="headerlink" title="Subcategorization and dependency(次范畴化和依存关系)"></a>Subcategorization and dependency(次范畴化和依存关系)</h2><p><strong>次范畴化和依存关系(subcategorization and dependency)</strong> 涉及到单词和短语之间的某些类别的关系，如动词 want 后可以接不定式(I want to fly to Detroit) 或名词短语(I want a flight to Detroit)，这种事实称为动词的次范畴(subcategorization)，也是之后再讨论。</p>
<h1 id="Two-views-of-linguistic-structure"><a href="#Two-views-of-linguistic-structure" class="headerlink" title="Two views of linguistic structure"></a>Two views of linguistic structure</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/1.jpg" class="ful-image" alt="1.jpg">
<h2 id="Constituency-phrase-structure"><a href="#Constituency-phrase-structure" class="headerlink" title="Constituency (phrase structure)"></a>Constituency (phrase structure)</h2><blockquote>
<p><strong>Phrase structure</strong> organizes words into nested constituents. It’s represented by trees generated by a context-free grammar. An important construct is the constituent (complete sub-tree).</p>
</blockquote>
<p>通常由<strong>上下文无关语法</strong> 产生，重要的组成是 <strong>成分</strong>。一种 bottom-up 的理解是，对一个句子的所有单词进行词性标注，然后根据一定的规则，一步步将其合并为“更大”的成分(产生子树)，直到组成一个句子。</p>
<h2 id="Dependency-structure"><a href="#Dependency-structure" class="headerlink" title="Dependency structure"></a>Dependency structure</h2><blockquote>
<p><strong>Dependency structure</strong> shows which words depend on (modify or are arguments of) which other words. The basic unit is a binary relation between words called a dependency</p>
</blockquote>
<p><strong>依存结构(Dependency structure)</strong> 两个主要构成部分是 <strong>head</strong> 和 <strong>dependents</strong></p>
<p><strong>head:</strong></p>
<ul>
<li>提供了主要含义(main meaning)的单词<br>  “this smart student of linguistics with long hair”<br>  这个例子中的 head 是 student，而不是 hair 之类的东西</li>
<li>提供了最重要的屈折特征(inflectional features)的单词<br>  Inflection includes things like tense, number, and gender</li>
</ul>
<p>来看一些 Dependency structure 的作用，一个从句子产生的角度来看，可以通过 head 确定名词词组(noun phrase)的单复数，以此来决定后面动词的单复数。看下面的例子，只有 “teacher/teachers” 这个 head 决定了 noun phrase 是单数还是复数，“class/classes”，“child/children” 并没有什么用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Singular Plural</div><div class="line">• The teacher blinks</div><div class="line">• The short teacher blinks</div><div class="line">• The teacher of the class blinks</div><div class="line">• The teacher of the classes blinks</div><div class="line">• The children’s teacher blinks</div><div class="line">• The child’s teacher blinks</div><div class="line"></div><div class="line"></div><div class="line">• The teachers blink</div><div class="line">• The short teachers blink</div><div class="line">• The teachers of the class blink</div><div class="line">• The teachers of the classes blink</div><div class="line">• The children’s teachers blink</div><div class="line">• The child’s teachers blink</div></pre></td></tr></table></figure></p>
<p>另一个应用是 QA 系统，如下，要回答 Who won an award? 这个问题，可能会有两个答案，student 或者 Alan，然而看 dependency parse tree 可以发现，student 和 won 之间有直接的联系(direct link)，所以结果当然是 student won an award<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/2.jpg" class="ful-image" alt="2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/3.jpg" class="ful-image" alt="3.jpg"></p>
<h1 id="Chomsky-Hierarchy"><a href="#Chomsky-Hierarchy" class="headerlink" title="Chomsky Hierarchy"></a>Chomsky Hierarchy</h1><h2 id="Formal-Grammar"><a href="#Formal-Grammar" class="headerlink" title="Formal Grammar"></a>Formal Grammar</h2><p>一个形式语法(formal grammar) G 定义了一个 formal language，用 L(G)来表示，包括以下部分：</p>
<ul>
<li><strong>N:</strong> a set of non-terminal symbols</li>
<li><strong>$\Sigma$:</strong> a set of terminal symbols</li>
<li><strong>R:</strong> a set of production rules of the form $(\Sigma \cup N)^* N(\Sigma \cup N)^* \rightarrow (\Sigma \cup N)^* $</li>
<li><strong>$S \in N$:</strong> a distinguished/special start symbol</li>
</ul>
<p>一些术语</p>
<ul>
<li><strong>Grammatical:</strong>  a sentence in the language</li>
<li><strong>Ungrammatical:</strong>  a sentence not in the language</li>
<li><strong>Derivation:</strong>  sequence of top-down production steps</li>
<li><strong>Parse tree:</strong>  graphical representation of the derivation<br>  A string is grammatical iff there exists a derivation for it.</li>
</ul>
<h2 id="Chomsky-Hierarchy-1"><a href="#Chomsky-Hierarchy-1" class="headerlink" title="Chomsky Hierarchy"></a>Chomsky Hierarchy</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/ch.jpg" class="ful-image" alt="ch.jpg">
<p>Chomsky 根据 production rule 的形式，把形式语法分为4类：</p>
<ol>
<li>0型语法(type 0 grammar)：重写规则为 φ → ψ，并且要求 φ 不是 empty string。</li>
<li>上下文有关语法(context-sensitive grammar)：重写规则为 φ1Aφ2  → φ1ωφ2，在上下文 φ1-φ2 中，单个的 non-terminal 符号 A 被重写为符号串 ω，所以，这种语法对上下文敏感，是上下文有关的。上下文有关语法又叫做 1型语法。</li>
<li>上下文无关语法(context-free grammar)：重写规则为A → ω，左边是一个 non-terminal 符号，右边是一个或多个 terminal 和 non-terminal 符号。当 A 重写为 ω 时，没有上下文的限制，所以，这种语法对上下文自由，是上下文无关的。上下文无关语法又叫做 2型语法。把上下文无关语法应用于自然语言的形式分析中，就形成了“短语结构语法”(phrase structure grammar)</li>
<li>有限状态语法/正则语法(finite state grammar/regular grammar)：重写规则为 A→aQ 或 A→a。左边必须是一个 non-terminal 符号，右边可以是一个 empty string，或者是一个 terminal 符号，又或者是一个 terminal 符号跟一个 non-terminal 符号，只有这三种情况。如果把 A 和 Q 看成不同的状态，那么，由重写规则可知，由状态 A 转入状态 Q 时，可生成一个终极符号 a，因此，这种语法叫做有限状态语法。有限状态语法又叫做 3型语法。</li>
</ol>
<p>每一个有限状态语法的都是上下文无关的，每一个上下文无关语法都是上下文有关的，而每一个上下文有关语法都是0型的，Chomsky把由0型语法生成的语言叫0型语言，把由上下文有关语法、上下文无关语法、有限状态语法生成的语言分别叫做上下文有关语言、上下文无关语言、有限状态语言。有限状态语言包含于上下文无关语言之中，上下文无关语言包含于上下文有关语言之中，上下文有关语言包含于0型语言之中。这样就形成了语法的“Chomsky层级”（Chomsky hierarchy）。在自然语言处理中,我们最感兴趣的是上下文无关语法和上下文无关语言,它们是短语结构语法理论的主要研究对象。</p>
<h2 id="Pumping-Lemma-for-Regular-Languages"><a href="#Pumping-Lemma-for-Regular-Languages" class="headerlink" title="Pumping Lemma for Regular Languages"></a>Pumping Lemma for Regular Languages</h2><blockquote>
<p>An intuition (from Jurafsky &amp; MarEn, p. 533): “…if a regular language has any long strings (longer than the number of states in the automaton), there must be some sort of loop in the automaton for the language. We can use this fact by showing that if a language doesn’t have such a loop, then it can’t be regular.”</p>
</blockquote>
<p>如果一个 RL 有任何长字符串（比自动机中的状态数更长），在该语言的自动机中必定有某种循环。我们可以使用这个事实表明，如果一种语言没有这样的循环，那么它不能是 regular 的。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Syntax%20Introduction/rl.jpg" class="ful-image" alt="rl.jpg">
<p>由此可见英语是 regular language<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">L1 = (the cat|dog|mouse|...)* (chased|bit|ate|...)* likes tuna fish</div><div class="line">L2 = English</div><div class="line">L1 ∩ L2 = (the cat|dog|mouse|...)^n (chased|bit|ate|...)^(n-1) likes tuna fish</div></pre></td></tr></table></figure></p>
<p><a href="http://blog.sina.com.cn/s/blog_72d083c70100pkir.html" target="_blank" rel="external">语法的Chomsky层级</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Part of speech tags]]></title>
      <url>http://www.shuang0420.com/2017/02/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。介绍 POS 的一些概念和基本算法。<br><a id="more"></a></p>
<p>句法(syntax)是 NLP 的骨架，研究单词之间的形式关系</p>
<ul>
<li>单词怎样聚类称为词类(part-of-speech)</li>
<li>怎样与相邻的单词组合成短语</li>
<li>一个句子的单词与单词之间彼此依赖的方式</li>
</ul>
<p>这一篇介绍 POS，单词怎样聚类称为词类。</p>
<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p><strong>词类</strong> 又称为 POS(Part Of Speech)、词的分类、形态类或词汇标记，根据形态和句法功能来定义，有以下特点：</p>
<ul>
<li>分布特征(Distributional)<br>  单词能够出现在相似的环境中<br>  单词有相似的功能</li>
<li>形态特征(Morphological)<br>  单词有相同的前缀后缀(词缀具有相似的功能)<br>  在句法结构中有相似的上下文环境</li>
<li>无关于含义(meaning)，也无关于语法(可以是主语/宾语，等等)</li>
</ul>
<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>分为两大类：<strong>封闭类(closed class) 和 开放类(open class)</strong>。</p>
<ul>
<li>封闭类<br>  包含的单词成员相对固定的词类<br>  介词(prepositions)/限定词(determiners)/代词(pronouns)/连接词(conjunctions)/助动词(auxiliary verbs)/小品词(particles)/数词(numerals)</li>
<li>开放类<br>  不断有新的词被创造出来<ul>
<li>名词<ul>
<li>专有名词(proper noun)</li>
<li>普通名词(common noun)<br>  可数名词(count noun)，物质名词(mass noun)</li>
</ul>
</li>
<li>动词</li>
<li>形容词</li>
<li>副词</li>
</ul>
</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/openclass.jpg" class="ful-image" alt="openclass.jpg">
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>做自然语言处理的相关任务时，往往我们有太多的单词，需要很多的数据来训练规则，而训练的这些规则会非常的 specific。POS 有助于模型的泛化，并帮助 reduce model size，另外，POS 还有以下的应用：</p>
<ul>
<li>提供关于单词及其邻近成分的大量有用信息<br>  如区分名词、动词等<br>  知道一个词是主有代词还是人称代词，就能知道什么词会出现在它的近邻(主有代词＋名词，人称代词＋动词)</li>
<li>提供单词发音的信息<br>  有些词既可以做名词也可以做形容词，发音是不同的 -&gt; 更自然的发音 -&gt; 语音合成系统</li>
<li>分割词干(stemming)<br>  单词词类 -&gt; 形态词缀</li>
<li>用于信息检索/信息抽取</li>
<li>自动词义排歧算法/局部剖析(parital parsing)</li>
</ul>
<h1 id="词类标注及算法"><a href="#词类标注及算法" class="headerlink" title="词类标注及算法"></a>词类标注及算法</h1><p>POS tagging，给语料库中的每个单词指派一个词类或者词汇类别标记的过程。</p>
<ul>
<li>输入<br>  单词的符号串和标记集</li>
<li>输出<br>  让每个单词都标上一个单独的而且是最佳的标记</li>
</ul>
<p>主要有三种<strong>标注算法：</strong></p>
<ul>
<li>基于规则的标注(rule-based tagging)<br>  一个手工制定的歧义消解规则的数据库，规则说明歧义消解的条件<br>  E.g. 当一个歧义单词的前面是限定词时，就可以判断它是名词，而不是动词</li>
<li>随机标注(stochastic tagging)<br>  使用一个训练语料库来计算在给定的上下文中某一给定单词具有某一给定标记的概率<br>  E.g. HMM</li>
<li>基于转换的标注(transformation-based tagging)<br>  规则+机器学习，规则可以由前面已标注好的训练语料库自动推导出来</li>
</ul>
<h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p>HMM 又称为隐马尔可夫模型，用概率方法进行 POS 标注。假定标注问题可以通过观察周围的单词和标记来解决。对于一个给定的句子或单词序列，HMM 标注算法选择使得下面的公式为最大值的标记序列：<br>$$P(word|tag)*P(tag|previous \ n \ tags)$$</p>
<p>HMM 通常是针对一个句子来选择最好的标记序列，而不是基于一个单词，二元语法 HMM 标注算法对于单词 $w_i$ 选择标记 $t_i$，使得对于给定的前面的标记 $t_{i-1}$ 和当前单词 $w_i$，其概率最大：<br>$$t_i=argmax_jP(t_j|t_{i-1},w_i)$$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/hmm.jpg" class="ful-image" alt="hmm.jpg">
<ul>
<li>Prior p(Y)<br>  language model, likelihood of a tag sequence</li>
<li>Posterior p(x|y)<br>  likelihood of word given ta</li>
<li>find the max for both<br>  Bayes Rule: P(Y|X)=P(Y)P(X|Y)/P(X)</li>
</ul>
<p>这个模型也可以进行平滑(用<a href="http://www.shuang0420.com/2017/02/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20Language%20models%20and%20smoothing/">NLP 笔记 - Language models and smoothing</a>提到的回退或者线性差值，来避免零概率</p>
<p>用 Viterbi 算法找出概率最大的标记序列，见<a href="http://www.shuang0420.com/2016/11/26/Hidden-Markov-Models/">Hidden-Markov-Models</a></p>
<p>举个例子：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/eg1.jpg" class="ful-image" alt="eg1.jpg"><br>…</p>
<p><strong>Prior: Likelihood of tag sequence</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/eg3.jpg" class="ful-image" alt="eg3.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/eg4.jpg" class="ful-image" alt="eg4.jpg"><br>…</p>
<p><strong>Posterior: MLE of word</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/eg5.jpg" class="ful-image" alt="eg5.jpg"><br>…</p>
<p><strong>注：</strong> P(PropN|Bill) = P(Bill|PropN) * P(PropN|Start) / P(Bill)，这里没有给出 P(Bill)。</p>
<h2 id="Unsupervised-PoS-Tagging"><a href="#Unsupervised-PoS-Tagging" class="headerlink" title="Unsupervised PoS Tagging"></a>Unsupervised PoS Tagging</h2><p>如果单词有相同的上下文(前驱词和后继词分布)，那么它们也会有相同的标记，所以</p>
<ul>
<li>找到所有的上下文：w1 X w2</li>
<li>找到频率最高的 Xs，然后给 Xs 标记</li>
<li>重复以上过程，直到终止条件(自定义)<br>  对英语来说，可以重复 20 次<br>  E.g. BE/HAVE  MR/MRS  AND/BUT/AT/AS  TO/FOR/OF/IN  VERY/SO  SHE/HE/IT/I/YOU<br>  然而没有 Nonuns/Verb/Adj 这些识别标记</li>
</ul>
<h2 id="Brown-Clustering"><a href="#Brown-Clustering" class="headerlink" title="Brown Clustering"></a>Brown Clustering</h2><p>是一种自底向上的层次聚类算法(hierarchical agglomeretive)，基于 n-gram 模型和马尔科夫链模型。是一种硬聚类(hard clustering)，每一个词都在且只在唯一的一个类中。</p>
<ul>
<li>输入：语料库</li>
<li>输出：二叉树<br>  树的叶子节点是一个个词，每个词用类似霍夫曼编码的方式进行编码<br>  树的中间节点是我们想要的类（中间结点作为根节点的子树上的所有叶子为类中的词）<br>  容易 scale</li>
</ul>
<p><strong>基本思想：</strong> 初始的时候，将每一个词独立的分成一类，然后，将两个类合并，使得合并之后 quality 函数最大，然后不断重复上述过程，达到想要的类的数量的时候停止合并。quality 函数，是对于 n 个连续的词（w）序列能否组成一句话的概率的对数的归一化结果。跟踪 cluster 合并的过程，可以从下往上建一个二叉树，每一个单词都被表示为一个 binary string(从根到叶子节点的路径)，每一个中间的节点都是一个类。</p>
<blockquote>
<p>Quality = merges two words that have similar probabilities of preceding and following words<br>(More technically quality = smallest decrease in the likelihood of the corpus according to a class-based language model)</p>
</blockquote>
<p>关于 class-based language model，可以看<a href="http://www.cs.cmu.edu/~roni/11661/PreviousYearsHandouts/classlm.pdf" target="_blank" rel="external">Class-Based n-gram Models of Natural Language</a></p>
<p>形成的二叉树：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/5.jpg" class="ful-image" alt="5.jpg"></p>
<p>算法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- Begin with every word in its own cluster</div><div class="line">- Until we have one cluster:</div><div class="line">    - Merge two clusters based on some measure of quality</div><div class="line">- Assign each word a bit-string based on the clusters it was in and when they were merged. Ex: Each isolated cluster will have no bit-string. Every time we merge two clusters, set the bit-strings in one cluster to &quot;0&quot; + c1_old_bit_string and the other to &quot;1&quot; + c2_old_bit_string</div></pre></td></tr></table></figure></p>
<h2 id="补充算法-Feature-based-tagger"><a href="#补充算法-Feature-based-tagger" class="headerlink" title="补充算法 Feature-based tagger"></a>补充算法 Feature-based tagger</h2><p>——————————————–我是分界线——————————————–<br>之所以叫补充算法，是因为上面所述的算法来自书/PPT，而下面的算法来自 <a href="http://web.stanford.edu/class/cs124/lec/postagging.pdf" target="_blank" rel="external">stanford 讲义</a></p>
<h3 id="Sequence-Labeling-as-Classification"><a href="#Sequence-Labeling-as-Classification" class="headerlink" title="Sequence Labeling as Classification"></a>Sequence Labeling as Classification</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">• Word</div><div class="line">    the:	the → DT</div><div class="line">• Lowercased word</div><div class="line">    Importantly:	importantly → RB</div><div class="line">• Prefixes</div><div class="line">    unfathomable:	un- → JJ</div><div class="line">• Suffixes</div><div class="line">    Importantly:	-ly → RB</div><div class="line">• Capitalization</div><div class="line">    Meridian:	CAP → NNP</div><div class="line">• Word shapes</div><div class="line">    35-year:	d-x → JJ</div></pre></td></tr></table></figure>
<p>然后训练一个分类器来预测 tag，当然特征也可以引入上下文(sliding window)，这种方法称为 <strong>Sequence Labeling as Classification</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/c1.jpg" class="ful-image" alt="c1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/c2.jpg" class="ful-image" alt="c2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/c3.jpg" class="ful-image" alt="c3.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cn.jpg" class="ful-image" alt="cn.jpg"></p>
<p>更好的特征往往是当前 token 前后 token 的类别，但是我们不知道这些类别，所以要往前/往后看，使用之前的 output<br><strong>Forward Classification</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cf1.jpg" class="ful-image" alt="cf1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cf2.jpg" class="ful-image" alt="cf2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cf3.jpg" class="ful-image" alt="cf3.jpg"></p>
<p><strong>Backward Classification</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cb1.jpg" class="ful-image" alt="cb1.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cb2.jpg" class="ful-image" alt="cb2.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/cb3.jpg" class="ful-image" alt="cb3.jpg"></p>
<h3 id="Maximum-Entropy-Markov-Model-MEMM"><a href="#Maximum-Entropy-Markov-Model-MEMM" class="headerlink" title="Maximum Entropy Markov Model(MEMM)"></a>Maximum Entropy Markov Model(MEMM)</h3><p>A sequence version of the logistic regression(also called maximum entropy) classifier<br>找到最佳的标记序列，基础算法:<br>$$<br>  \begin{aligned}<br>  \hat T &amp; = argmax_T P(T|W) \\<br>  &amp; =  argmax_T \prod_i P(t_i|w_i,t_{i-1}) \\<br>  \end{aligned}<br>$$</p>
<p>每个 tag 的特征<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/feature.jpg" class="ful-image" alt="feature.jpg"></p>
<p>更多 feature<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/moref.jpg" class="ful-image" alt="moref.jpg"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/formula.jpg" class="ful-image" alt="formula.jpg">
<p>用 Viterbi 算法来实现<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/viterbi.jpg" class="ful-image" alt="viterbi.jpg"></p>
<h2 id="未知词"><a href="#未知词" class="headerlink" title="未知词"></a>未知词</h2><p>如何来估计未知词的标记？大概有下面几种办法</p>
<ul>
<li>假定每个未知词在所有可能的标记中都是有歧义的，也就是说都具有相同的概率<br>  根据上下文相关的 POS trigram 给未知词恰当的标签</li>
<li>在未知词上的标记的概率分布与在训练集中只出现一次的单词的标记的概率分布非常相似<br>  与<a href="http://www.shuang0420.com/2017/02/12/NLP%20%E7%AC%94%E8%AE%B0%20-%20Language%20models%20and%20smoothing/">NLP 笔记 - Language models and smoothing</a>提到的 Good-Turing smoothing 思想类似</li>
<li>使用关于单词拼写的信息<br>  4 类特殊的正词法特征，包括 3 个屈折词尾(-ed,-s,-ing)，32个派生后缀(-ion,-al,-ive,-ly)，4个大写(captial)值(首字母大写，非首字母大写等)以及连字符(hyph)规则<br>  $P(w_i)=P(unknown-word|t_i)P(captial|t_i)P(endings|hyph/t_i)$</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>各种方法的准确率<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/acc.jpg" class="ful-image" alt="acc.jpg"></p>
<p>词类标注是<strong>歧义消解(disambiguation)</strong> 的一个重要方面。英语中大多数单词是没有歧义的，然而最常用的单词很多都是有歧义的，如 can 可以是助动词，也可以是名词，还可以是动词，POS 标注的问题就是消解这样的歧义，在一定的上下文中选择恰如其分的标记。</p>
<p>E.g.<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Part%20of%20speech%20tags/tag.jpg" class="ful-image" alt="tag.jpg"></p>
<blockquote>
<p>参考链接<br><a href="http://blog.csdn.net/u014516670/article/details/50574147" target="_blank" rel="external">Brown Clustering算法和代码学习</a><br><a href="http://blog.csdn.net/dark_scope/article/details/8879656" target="_blank" rel="external">Brown Clustering &amp;&amp; Global Linear Models</a></p>
</blockquote>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> part-of-speech </tag>
            
            <tag> 词性标注 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Language models and smoothing]]></title>
      <url>http://www.shuang0420.com/2017/02/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20Language%20models%20and%20smoothing/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。Language model 在别的课中不管是单机还是分布式都实现过好几次，然而却没有深入系统的研究过。<br><a id="more"></a></p>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>Language model 通常有两类目标，一个是 <strong>计算句子或一系列单词出现的概率</strong>，另一个是 <strong>单词预测(word prediction)</strong>，通常的应用场景有：</p>
<ul>
<li>Machine Translation<br>  P(high winds tonite)&gt;P(large winds tonite)</li>
<li>Spell Correction<br>  The office is about fifteen minuets from my house<br>  P(about fifteen minutes from)&gt;P(about fifteen minuets from)</li>
<li>Speech Recognition<br>  P(I saw a van) &gt;&gt; P(eyes awe of an)</li>
<li>+Summarization,question,answering,etc.</li>
</ul>
<h1 id="文本预处理问题"><a href="#文本预处理问题" class="headerlink" title="文本预处理问题"></a>文本预处理问题</h1><h2 id="标点"><a href="#标点" class="headerlink" title="标点"></a>标点</h2><p>是否把标点算为单词，取决于不同的任务。对于诸如 <strong>语法检查、拼写错误检查、作者辨认</strong> 这样的任务，标点符号的位置是很重要的。因此这些应用中经常把标点符号看做单词。</p>
<h2 id="大小写"><a href="#大小写" class="headerlink" title="大小写"></a>大小写</h2><p>大多数统计应用来说，是忽略大小写的，尽管有时候也把大写作为个别的单独特征来处理(在拼写错误更正或词类标注中)</p>
<h2 id="屈折形式"><a href="#屈折形式" class="headerlink" title="屈折形式"></a>屈折形式</h2><ul>
<li>词形(wordform): 在语料库中以屈折形式出现的单词形式</li>
<li>词目(lemma): 具有同一词干、同一主要词类、同一词义的词汇形式的集合</li>
<li>型(type): 语料库中不同单词的数目</li>
<li>例(token): 使用中的单词数目</li>
</ul>
<p>当前大多数基于 N-gram 的系统都是以词形(wordform)为基础的。所以，cat 和 cats 要分别处理为两个单词。然而在很多领域中，我们想把 cat 和 cats 看成同一个抽象单词的实例，可以用词典，词典中不包含有屈折变化的形式。用词典来计算词目比计算词形更方便。</p>
<h2 id="口语语料库"><a href="#口语语料库" class="headerlink" title="口语语料库"></a>口语语料库</h2><ul>
<li>话段(fragment): 相当于句子</li>
<li>片段(fragment): 一个单词在中间被拦腰切开而形成的，如 main-mainly</li>
<li>有声停顿(filled pause): 如 um, uh</li>
</ul>
<p>取决于应用的具体情况，如果在自动语音识别的基础上建立一个自动听写系统，我们可能需要把片段剔除。而有声停顿，更倾向于被当成单词来处理，um 和 uh 的意思稍有不同，一般当说话人胸有成竹要说某个话段时，他就 um，而当说话人想说但还没找到恰当的单词来表达时，就用 uh，另外，uh 经常可以用来作为预测下一个单词的线索，所以很多系统都把它当做一个单词来处理。</p>
<h1 id="简单的-非平滑的-N-元语法"><a href="#简单的-非平滑的-N-元语法" class="headerlink" title="简单的(非平滑的) N 元语法"></a>简单的(非平滑的) N 元语法</h1><h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>$$P(x_1,x_2,x_3,…,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)…P(x_n|x_1,…,x_n,1)$$</p>
<h3 id="Markov-Assumption"><a href="#Markov-Assumption" class="headerlink" title="Markov Assumption"></a>Markov Assumption</h3><p>N-gram 的基本假设是 <strong>一个单词的概率只依赖于它前面单词的概率</strong>，这也就是 <strong>马尔可夫假设</strong>。举个例子：<br>P(the | its water is so transparent that) $\approx$ P(the | that)<br>P(the | its water is so transparent that) $\approx$ P(the | transparent that)</p>
<p>事实上，马尔可夫链就是一种加权有限状态自动机，加权 FSA 的下一个状态总是依赖于他前面有限的历史(有限自动机的状态数目总是有限的)，bigram 可以看成是每个单词只有一个状态的马尔可夫链，也称为一阶马尔可夫模型，N-gram 称为 N-1 阶马尔可夫模型。</p>
<p>$$P(w_1w_2…w_n) \approx \sum_iP(w_i|w_{i-k}…w_{i-1})$$</p>
<p>再转换下<br>$$P(w_1|w_1w_2…w_n) \approx \sum_iP(w_i|w_{i-k}…w_{i-1})$$</p>
<h3 id="Maximum-Likelihood-Estimate"><a href="#Maximum-Likelihood-Estimate" class="headerlink" title="Maximum Likelihood Estimate"></a>Maximum Likelihood Estimate</h3><p>很简单啦，以 Bigram 为例，就是 $P(w_i|w_{i-1})={count(w_{i-1},w_i) \over count(w_{i-1})}$</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p>注意，在下面的模型中，我们用到了 padding，所有 N-gram 模型都用了一个特殊符号来标记句子的结束(STOP_SYMBOL)，N&gt;1 时，另外用一个特殊符号来标记句子的开始(START_SYMBOL)，来方便计算。</p>
<h3 id="Uniform-Model"><a href="#Uniform-Model" class="headerlink" title="Uniform Model"></a>Uniform Model</h3><p>假定语言中的任何一个单词后面可以跟随该语言中的任何一个单词，且概率是相等的。如果英语中有 100,000个单词，那么任何一个单词后面跟随其他任何单词的概率将是 1/100,000，即 0.00001</p>
<h3 id="Unigram-Model"><a href="#Unigram-Model" class="headerlink" title="Unigram Model"></a>Unigram Model</h3><p>任何一个单词后面可以跟随着其他任何单词，但后面一个单词按照它正常的频度来出现，所以可以根据相对频度对下面将要出现的单词指派一个概率分布的估值。</p>
<p>$$<br>  \begin{aligned}<br>  p(W=w) &amp; = p(W_1=w_1, W_2=w_2,…,W_{L+1}=stop) \\<br>   &amp; = (\prod^L_{l=1}p(W_l=w_l))p(W_{L+1}=stop) \\<br>   &amp; = (\prod^L_{l=1}p(w_l))p(stop)<br>  \end{aligned}<br>$$</p>
<h3 id="Full-History-Model"><a href="#Full-History-Model" class="headerlink" title="Full History Model"></a>Full History Model</h3><p>如果不是简单地看单词相对频度，而是要看单词对于给定历史的条件概率，那么就有了 full history model。</p>
<blockquote>
<p>Every word is assigned some probability, conditioned on every history.</p>
</blockquote>
<p>$$<br>\begin{aligned}<br>  p(W=w) &amp; = p(W_1=w_1, W_2=w_2,…,W_{L+1}=stop) \\<br>   &amp; = (\prod^L_{l=1}p(W_l=w_l|W_{1:l-1}=w_{1:l-1}))p(W_{L+1}=stop|W_{1:L}=w_{1:L}) \\<br>   &amp; = (\prod^L_{l=1}p(w_l|history_l))p(stop|history_L)<br>  \end{aligned}<br>$$</p>
<h3 id="N-Gram-Model"><a href="#N-Gram-Model" class="headerlink" title="N-Gram Model"></a>N-Gram Model</h3><blockquote>
<p>Every word is assigned some probability, conditioned on a fixed-length history(n-1)</p>
</blockquote>
<p>$$<br>  \begin{aligned}<br>  p(W=w) &amp; = p(W1=w1, W2=w2,…,W_{L+1}=stop) \\<br>   &amp; = (\prod^L_{l=1}p(W_l=w_l|W_{l-n+1:l-1}=w_{l-n+1:l-1}))p(W_{L+1}=stop|W_{L-n+1:L}=w_{L-n+1:L}) \\<br>   &amp; = (\prod^L_{l=1}p(w_l|history_l))p(stop|history_{L+1})<br>  \end{aligned}<br>$$</p>
<p>以 Bigram 为例，假设 &lt;s&gt; 为 START_SYMBOL，&lt;/s&gt; 为 STOP_SYMBOL<br>$$p(w^1_L)=\prod^L_{l=1}P(w_l|w_{l-1})p(stop|w_{L+1})$$</p>
<p>文本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;s&gt; I am Sam &lt;/s&gt;</div><div class="line">&lt;s&gt; Sam I am &lt;/s&gt;</div><div class="line">&lt;s&gt; I do not like green eggs and ham &lt;/s&gt;</div></pre></td></tr></table></figure></p>
<p>那么有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">P(I|&lt;s&gt;)=2/3</div><div class="line">P(Sam|&lt;s&gt;=1/3</div><div class="line">P(am|I)=2/3</div><div class="line">P(&lt;/s&gt;|Sam)=1/2</div><div class="line">P(Sam|am)=1/2</div><div class="line">P(do|I)=1/3</div></pre></td></tr></table></figure></p>
<p>第一句话的概率就是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">P(I|&lt;s&gt;)*P(am|I)*P(Sam|am)*P(&lt;/s&gt;|Sam)</div></pre></td></tr></table></figure></p>
<h3 id="对数-Logprob"><a href="#对数-Logprob" class="headerlink" title="对数(Logprob)"></a>对数(Logprob)</h3><p>由于概率都小于 1，相乘的概率越多，所有概率的乘积就越小，这样就会有数值下溢(underflow)的危险，所以习惯上会采用对数空间来进行计算(加法比乘法快)，给每个概率取对数再相加，最后再取结果的反对数。</p>
<h3 id="归一化-Normalizing"><a href="#归一化-Normalizing" class="headerlink" title="归一化(Normalizing)"></a>归一化(Normalizing)</h3><p>归一化，就是用某个总数来除，使最后得到的概率的值处于 0 和 1 之间，以保持概率的合法性，也就是：<br>$$p(w_n|w_{n-1})={C(w_{n-1}w_n) \over \sum_wC(w_{n-1}w)}$$</p>
<p>要注意的一个优化是，给定单词 $w_{n-1}$ 开头的所有二元语法的计数必定等于该单词 $w_{n-1}$ 的一元语法的计数，也就是说，对一般的 N-gram，参数估计为：</p>
<p>$$p(w_n|w^{n-1}_{n-N+1})={C(w^{n-1}_{n-N+1}w_n) \over C(w^{n-1}_{n-N+1})}$$</p>
<h3 id="Unknown-word"><a href="#Unknown-word" class="headerlink" title="Unknown word"></a>Unknown word</h3><p>对 unknown word 的处理，一般我们建一个固定大小的 lexicon(比如说语料库里 frequency&gt;5 的单词)，再新建一个 token &lt;UNK&gt;，不在 lexicon 里的 token (也就是 frequency&lt;5 的单词)都编译成 &lt;UNK&gt;，然后把 &lt;UNK&gt; 当做普通单词处理。</p>
<h3 id="N-gram-对语料库的敏感性"><a href="#N-gram-对语料库的敏感性" class="headerlink" title="N-gram 对语料库的敏感性"></a>N-gram 对语料库的敏感性</h3><p>直观上的两个重要事实：</p>
<ol>
<li>N 越大 N-gram 的精度也应相应增加<br> 训练模型的上下文越长，句子连贯性也就越好</li>
<li>N-gram 性能强烈依赖于它们的语料库(特别是语料库的种类和单词的容量)<br> 为了很好的在统计上逼近英语，需要一个规模很大、包含不同种类、覆盖不同领域的语料库</li>
</ol>
<h1 id="平滑-Smoothing"><a href="#平滑-Smoothing" class="headerlink" title="平滑(Smoothing)"></a>平滑(Smoothing)</h1><p>每个特定的语料库都是有限的，从任何训练语料库得到的 bigram 矩阵都是稀疏的(sparse)，存在着大量的零概率 bigram 的场合，当非零计数很小时，MLE 会产生很糟糕的估计值。平滑就是给某些零概率和低概率的 N-gram 重新赋值并给它们指派非零值。</p>
<h2 id="加-1-平滑-add-one-smoothing"><a href="#加-1-平滑-add-one-smoothing" class="headerlink" title="加 1 平滑(add-one smoothing)"></a>加 1 平滑(add-one smoothing)</h2><p>也叫 Laplace smoothing，假设 we saw each word one more time than we did<br>MLE estimate:<br>$$P_{MLE}(w_i|w_{i-1})={c(w_{i-1}w_i) \over c(w_{i-1})}$$<br>Add-1 estimate:<br>$$P_{Add-1}(w_i|w_{i-1})={c(w_{i-1}w_i)+1 \over c(w_{i-1})+V}$$</p>
<p>加 1 平滑是一种很糟糕的算法，与其他平滑方法相比显得非常差，然而我们可以把加 1 平滑用在其他任务中，如文本分类，或者非零计数没那么多的情况下。</p>
<h2 id="Good-Turing-smoothing"><a href="#Good-Turing-smoothing" class="headerlink" title="Good-Turing smoothing"></a>Good-Turing smoothing</h2><p><strong>基本思想:</strong> 用观察计数较高的 N-gram 数量来重新估计概率量大小，并把它指派给那些具有零计数或较低计数的 N-gram</p>
<p>首先理解一个概念<br><strong>Things seen once</strong>: 使用刚才已经看过一次的事物的数量来帮助估计从来没有见过的事物的数量。举个例子，假设你在钓鱼，然后抓到了 18 条鱼，种类如下：10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel，那么<br><strong>下一条鱼是 trout 的概率是多少？</strong><br>很简单，我们认为是 1/18</p>
<p><strong>那么，下一条鱼是新品种的概率是多少？</strong><br>不考虑其他，那么概率是 0，然而根据 Things seen once 来估计新事物，概率是 3/18</p>
<p><strong>在此基础上，下一条鱼是 trout 的概率是多少？</strong><br>肯定就小于 1/18，那么怎么估计呢？<br>在 Good Turing 下，<br>$$P^*_{GT} (things \ with \ zero \ frequency)={N_1 \over N_2}$$<br>$$c^* = {(c+1)N_{c+1} \over N_c}$$<br>Nc = the count of things we’ve seen c times</p>
<p>所以，c=1时，<br>$C^* (trout)=2*{N2 \over N1} = 2*1/3 = 2/3$<br>$P^* (trout)={2/3 \over 18}={1 \over 27}$</p>
<h2 id="Backoff-回退"><a href="#Backoff-回退" class="headerlink" title="Backoff(回退)"></a>Backoff(回退)</h2><p>直观的理解，如果没有 3gram，就用 bigram，如果没有 bigram，就用 unigram。</p>
<h2 id="Linear-Interpolation-线性差值"><a href="#Linear-Interpolation-线性差值" class="headerlink" title="Linear Interpolation(线性差值)"></a>Linear Interpolation(线性差值)</h2><p>我们看这样一种情况，如果 c(BURNISH THE) 和 c(BURNISH THOU) 都是 0，那么在前面的平滑方法 additive smoothing 和 Good-Turing 里，p(THE|BURNISH)=p(THOU|BURNISH)，而这个概率我们直观上来看是错误的，因为 THE 要比 THOU 常见的多，$p(THE|BURNISH) \ ge p(THOU|BURNISH)$ 应该是大概率事件。要实现这个，我们就希望把 bigram 和 unigram 结合起来，interpolate 就是这样一种方法。</p>
<p>用线性差值把不同阶的 N-gram 结合起来，这里结合了 trigram，bigram 和 unigram。用 lambda 进行加权<br>$$<br>  \begin{aligned}<br>  p(w_n|w_{n-1}w_{n-2}) &amp; = \lambda_1 p(w_n|w_{n-1}w_{n-2}) \\<br>   &amp; + \lambda_2 p(w_n|w_{n-1})  \\<br>   &amp; + \lambda_3 p(w_n)<br>  \end{aligned}<br>$$</p>
<p>$$\sum_i \lambda_i=1$$</p>
<p><strong>怎样设置 lambdas?</strong><br>把语料库分为 training data, held-out data, test data 三部分，然后</p>
<ul>
<li>Fix the N-gram probabilities(on the training data)</li>
<li>Search for lambdas that give the largest probabilities to held-out set:<br>$logP(w_1…w_n|M(\lambda_1…\lambda_k))=\sum_ilogP_M(\lambda_1…\lambda_k)(w_i|w_{i-1})$</li>
</ul>
<p>这其实是一个递归的形式，我们可以把每个 lambda 看成上下文的函数，如果对于一个特定的 bigram 有特定的计数，假定 trigram 的计数是基于 bigram 的，那么这样的办法将更可靠，因此，可以使 trigram 的 lambda 值更高，给 trigram 更高权重。</p>
<p>$$<br>  \begin{aligned}<br>  p(w_n|w_{n-1}w_{n-2}) &amp; = \lambda_1 (w^{n-1}_{n-2})p(w_n|w_{n-1}w_{n-2}) \\<br>   &amp; + \lambda_2 (w^{n-1}_{n-2})p(w_n|w_{n-1})  \\<br>   &amp; + \lambda_3 (w^{n-1}_{n-2})p(w_n)<br>  \end{aligned}<br>$$</p>
<p>通常 $\lambda w^{n-1}_{n-2}$ 是用 EM 算法，在 held-out data 上训练得到(held-out interpolation) 或者在 cross-validation 下训练得到(deleted interpolation)。$\lambda w^{n-1}_{n-2}$ 的值依赖于上下文，高频的上下文通常会有高的 lambdas。</p>
<p>更多平滑见 <a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" target="_blank" rel="external">Stanford Language Modeling</a></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>在训练集上训练参数，在测试集上测试 performance，那么之后怎么来评估模型呢？最好的办法当然是 <strong>外部评价方法</strong> 是把两个模型放到具体的任务环境中(spelling corrector/speech recognizer/MT system)，然后测试模型，比较两个模型的准确率。然而这种方法比较麻烦，所以一般还是用 <strong>内部评价方法</strong>，perplexity 来评估模型，尽管它不是一个很好的估计(除非真实情况就是 test data 和 training data 是非常相似的)，但好歹也是有用的，一般用于 pilot experiments 中。</p>
<h2 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h2><blockquote>
<p>Perplexity is the inverse probability of test set normalized by the number of words</p>
</blockquote>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Language%20models%20and%20smoothing/perplexity.jpg" class="ful-image" alt="perplexity.jpg">
<p>第二个式子用了 chain rule，第三个式子是 bigram 的情况。举个例子来理解 perplexity，如果一个句子由 0-9 位数字随机生成，那么这个句子的 perplexity 是多少？假定每个 digit 出现的概率都是 1/10。=&gt; PP(W)=(1/10)^-1=10</p>
<p>我们的目标是最小化 perplexity，lower perplexity=better model。</p>
<p>理论基础是 <strong>熵(entropy)</strong>，熵在 NLP 中是非常有价值的，可以用来度量一个特定语法中的信息量是多少，度量给定语法和给定语言的匹配程度有多高，预测一个给定的 N-gram 中的下一个单词是什么，如果给定两个语法和一个语料库，我们还可以使用熵来估计哪个语法和语料库匹配的更好…</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul>
<li>Relative frequencies (count &amp; normalize)</li>
<li>Transform the counts:<br>  Laplace/“add one”/“add λ”<br>  Good-Turing discounting</li>
<li>Interpolate or “backoff”:<br>  With Good-Turing discounting: Katz backoff – “Stupid” backoff<br>  Absolute discounting: Kneser-Ney</li>
</ul>
<p>更复杂的一些 N-gram 方法</p>
<ul>
<li>cache LM(Kuhn and de Mori 1990)</li>
<li>用 long-distance trigger 来替代局部 N-gram(Rosenfeld, 1996; Niesler and Woodland, 1999; Zhou and Lua, 1998)</li>
<li>可变长 N-gram(variable-length N-gram)(Ney et al., 1994; Kneser, 1996; Niesler and Woodland, 1996)</li>
<li>用语义信息来丰富 N-gram<br>  基于潜在语义索引(latent semantic indexing)的语义词联想方法(Coccaro and Jurafsk, 1988; Bellegarda, 1999)<br>  从联机词典和类属词典中提取语义信息的方法(Demetriou et al., 1997)</li>
<li>基于类的 N-gram</li>
<li>基于更结构化的语言知识的语言模型(如概率剖析)</li>
<li>使用当前话题的知识来提升 N-gram(Chen et al., 1998; Seymore and Rosenfeld 1997; Seymore et al., 1998; Florian and Yarowsky 1999; Khudanpur and Wu, 1999）</li>
<li>使用言语行为和对话知识来提升 N-gram</li>
</ul>
<blockquote>
<p>参考链接：<br><a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf" target="_blank" rel="external">Stanford Language Modeling</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[django + bootstrap 使用网页模板]]></title>
      <url>http://www.shuang0420.com/2017/02/23/django%20+%20bootstrap%20%E4%BD%BF%E7%94%A8%E7%BD%91%E9%A1%B5%E6%A8%A1%E6%9D%BF/</url>
      <content type="html"><![CDATA[<p>项目需要，快速上手 Django 的笔记。<br><a id="more"></a></p>
<h1 id="初识-Django"><a href="#初识-Django" class="headerlink" title="初识 Django"></a>初识 Django</h1><p>Django 使用了类似 MVC 的架构，只是在定义和解释上略为不同，称为 MTV ( Model–Template–View )。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/django%20%2B%20bootstrap%20%E4%BD%BF%E7%94%A8%E7%BD%91%E9%A1%B5%E6%A8%A1%E6%9D%BF/process.jpg" class="ful-image" alt="process.jpg"></p>
<p>Django 处理 request 的流程:</p>
<ul>
<li>浏览器送出 HTTP request</li>
<li>Django 依据 URL configuration 分配至对应的 View</li>
<li>View 进行资料库的操作或其他运算，并回传 HttpResponse</li>
<li>浏览器依据 HTTP response 显示网页画面</li>
</ul>
<p>另外 Django 无需数据库就可以使用，它提供了对象关系映射器（ORM），可以使用 Python 代码来描述数据库结构。</p>
<h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install django</div></pre></td></tr></table></figure>
<p>查看版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ python -m django --version</div><div class="line">1.10.5</div></pre></td></tr></table></figure></p>
<h1 id="Simple-Example"><a href="#Simple-Example" class="headerlink" title="Simple Example"></a>Simple Example</h1><h2 id="Start-project"><a href="#Start-project" class="headerlink" title="Start project"></a>Start project</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ django-admin startproject mysite</div></pre></td></tr></table></figure>
<p>目录结构:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mysite/</div><div class="line">├── manage.py</div><div class="line">└── mysite</div><div class="line">    ├── __init__.py</div><div class="line">    ├── settings.py</div><div class="line">    ├── urls.py</div><div class="line">    └── wsgi.py</div></pre></td></tr></table></figure></p>
<p>各个文件的作用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">The outer mysite/ root directory is just a container for your project. Its name doesn’t matter to Django; you can rename it to anything you like.</div><div class="line"></div><div class="line">manage.py: A command-line utility that lets you interact with this Django project in various ways. You can read all the details about manage.py in django-admin and manage.py.</div><div class="line">The inner mysite/ directory is the actual Python package for your project. Its name is the Python package name you’ll need to use to import anything inside it (e.g. mysite.urls).</div><div class="line"></div><div class="line">mysite/__init__.py: An empty file that tells Python that this directory should be considered a Python package. If you’re a Python beginner, read more about packages in the official Python docs.</div><div class="line"></div><div class="line">mysite/settings.py: Settings/configuration for this Django project. Django settings will tell you all about how settings work.</div><div class="line"></div><div class="line">mysite/urls.py: The URL declarations for this Django project; a “table of contents” of your Django-powered site. You can read more about URLs in URL dispatcher.</div><div class="line"></div><div class="line">mysite/wsgi.py: An entry-point for WSGI-compatible web servers to serve your project. See How to deploy with WSGI for more details.</div></pre></td></tr></table></figure></p>
<p>现在就可以在 mysite 目录下运行了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python manage.py runserver</div></pre></td></tr></table></figure></p>
<p>浏览器打开 ‘<a href="http://127.0.0.1:8000/" target="_blank" rel="external">http://127.0.0.1:8000/</a>‘  能看到 “Welcome to Django” 的页面。</p>
<p>如果有别的程序占用端口，可以修改 django 运行的端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python manage.py runserver 8080</div></pre></td></tr></table></figure>
<p>修改 server 的 IP<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python manage.py runserver 0.0.0.0:8000</div></pre></td></tr></table></figure></p>
<p>修改 python 代码并保存时会自动加载 server，不用重新启动。然而，如果有添加文件等操作，就要重新启动了。</p>
<h2 id="Create-app"><a href="#Create-app" class="headerlink" title="Create app"></a>Create app</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python manage.py startapp mysample</div></pre></td></tr></table></figure>
<p>目录结构:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">mysite/</div><div class="line">├── manage.py</div><div class="line">├── mysample</div><div class="line">│   ├── __init__.py</div><div class="line">│   ├── admin.py</div><div class="line">│   ├── apps.py</div><div class="line">│   ├── migrations</div><div class="line">│   │   └── __init__.py</div><div class="line">│   ├── models.py</div><div class="line">│   ├── tests.py</div><div class="line">│   └── views.py</div><div class="line">└── mysite</div><div class="line">    ├── __init__.py</div><div class="line">    ├── __pycache__</div><div class="line">    │   ├── __init__.cpython-36.pyc</div><div class="line">    │   └── settings.cpython-36.pyc</div><div class="line">    ├── settings.py</div><div class="line">    ├── urls.py</div><div class="line">    └── wsgi.py</div></pre></td></tr></table></figure></p>
<p><strong>Projects vs apps</strong><br>所谓 app 是指完成一些功能的 web 应用，比如博客系统（weblog system），公共记录的数据库（a database of public records）或者是一个简单的投票系统（a simple poll app）。project 是指一个特定网站的一系列配置文件和应用的集合。一个项目（project）可以包含多个应用（app）,一个应用（app）可以被多个项目（project）使用。</p>
<h2 id="Views"><a href="#Views" class="headerlink" title="Views"></a>Views</h2><p>Django view 其实是一个 function，处理 HttpRequest，并回传 HttpResponse</p>
<ul>
<li>收到 HttpRequest 参数<br>Django 从网页接收到 request 后，会将 request 中的信息封装产生一个 HttpRequest，并当成第一个参数，传入对应的 view function</li>
<li>需要回传 HttpResponse<br>HttpResponse.content<br>HttpResponse.status_code …等</li>
</ul>
<p>编辑 mysample/views.py:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from django.shortcuts import render</div><div class="line">from django.http import HttpResponse</div><div class="line">from django.template import loader</div><div class="line"></div><div class="line"># Create your views here.</div><div class="line">def index(request):</div><div class="line">    return HttpResponse(&quot;Hello, world.&quot;)</div></pre></td></tr></table></figure></p>
<p>这段代码做的是：</p>
<ul>
<li>从 django.http 中引用 HttpResponse 类别</li>
<li>宣告 hello_world 这个 view</li>
<li>当 hello_world 被呼叫时，回传包含字串 Hello World! 的 HttpResponse</li>
</ul>
<h2 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h2><p>Django 需要知道 URL 与 view 的对应关系，这个对应关系就是 URL conf (URL configuration)，通常定义在 urls.py，包含了一连串的规则 (URL patterns)，Django 收到 request 时，会一一比对 URL conf 中的规则，决定要执行哪个 view function</p>
<p>在 mysample 里新建一个 urls.py 文件来 map url，现在的目录结构:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">mysite/</div><div class="line">├── manage.py</div><div class="line">├── mysample</div><div class="line">│   ├── __init__.py</div><div class="line">│   ├── admin.py</div><div class="line">│   ├── apps.py</div><div class="line">│   ├── migrations</div><div class="line">│   │   └── __init__.py</div><div class="line">│   ├── models.py</div><div class="line">│   ├── tests.py</div><div class="line">│   ├── urls.py</div><div class="line">│   └── views.py</div><div class="line">└── mysite</div><div class="line">    ├── __init__.py</div><div class="line">    ├── __pycache__</div><div class="line">    │   ├── __init__.cpython-36.pyc</div><div class="line">    │   └── settings.cpython-36.pyc</div><div class="line">    ├── settings.py</div><div class="line">    ├── urls.py</div><div class="line">    └── wsgi.py</div></pre></td></tr></table></figure></p>
<p>编辑 mysample/urls.py:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">from django.conf.urls import url</div><div class="line"></div><div class="line">from . import views</div><div class="line"></div><div class="line">app_name = &apos;mysample&apos;</div><div class="line">urlpatterns = [</div><div class="line">    # ex: /mysample/</div><div class="line">    url(r&apos;^$&apos;, views.index, name=&apos;index&apos;),</div><div class="line">]</div></pre></td></tr></table></figure></p>
<p>现在要把这个 urls.py 连到 mysite 上，编辑 mysite/urls.py:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from django.conf.urls import include, url</div><div class="line">from django.contrib import admin</div><div class="line"></div><div class="line">urlpatterns = [</div><div class="line">    # point the root URLconf at the mysample.urls module</div><div class="line">    url(r&apos;^mysample/&apos;, include(&apos;mysample.urls&apos;)),</div><div class="line">    url(r&apos;^admin/&apos;, admin.site.urls),</div><div class="line">]</div></pre></td></tr></table></figure></p>
<p>url() 函数接收四个参数，两个是必需的： regex 和 view，还有两个是可选的：kwargs 和 name。</p>
<ul>
<li>regex – 定义的 URL 规则<pre><code>－ 规则以 regular expression（正规表示式）来表达
－ r&apos;^mysample/&apos; 代表的是 mysample/ 这种 URL
－ Django 从第一个开始，按顺序依次使用列表里的正则表达式来尝试匹配请求的 URL，直到遇到一个可以匹配的表达式
－ 不尝试匹配 GET 或 POST 的参数，也不匹配域名部分
</code></pre></li>
<li>view – 对应的 view function<pre><code>－ 当 Django 找到匹配的正则表达式时，就会调用这个视图函数
－ 调用时传递的第一个参数是一个 HttpRequest 对象，后续的参数是所有被正则表达数捕捉的部分
－ 如果正则式使用简单捕获，捕获结果会作为位置参数传递；如果使用名字捕获，捕获结果会作为关键字参数传递
</code></pre></li>
<li>url() 参数之 kwargs<pre><code>－ 任意个关键字参数可以作为一个字典传递给目标视图函数
</code></pre></li>
<li>url() 参数之 name<pre><code>－ 给 URL 起个名字，以便在 Django 的模板里无二义性的引用到它
－ 这个非常有用的特性允许你只用更改一个文件就能全局性的改变某个 URL
</code></pre></li>
</ul>
<p>我们使用的 include() 函数只是简单的引用其他的 URLconf 文件，让 URLconf 也能轻松的「即插即用」。请注意 include() 函数的正则表达式不包含 $ 符号（匹配字符串结尾）但是结尾有斜线。当 Django 遇到一个 include()，它砍掉被正则表达式匹配上的部分，并把剩余的字符串发送个作为参数的 URLconf 做进一步处理。</p>
<p>然后运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python manage.py runserver</div></pre></td></tr></table></figure></p>
<p>打开 ‘<a href="http://localhost:8000/mysample/&#39;，就可以看到" target="_blank" rel="external">http://localhost:8000/mysample/&#39;，就可以看到</a> “Hello, world.” 主页。</p>
<h1 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h1><p>这里使用的模板是<a href="https://startbootstrap.com/template-overviews/sb-admin/" target="_blank" rel="external">SB Admin</a></p>
<h2 id="static-和-templates-文件"><a href="#static-和-templates-文件" class="headerlink" title="static 和 templates 文件"></a>static 和 templates 文件</h2><p>在 mysite 下新建文件夹 static，把 bootstrap 下载的模板复制到里面，然后在 mysite/mysite 下新建一个 templates 文件夹，把 html 文件复制到里面</p>
<p>目录结构:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line">└── mysite</div><div class="line">    ├── manage.py</div><div class="line">    ├── mysample</div><div class="line">    │   ├── __init__.py</div><div class="line">    │   ├── __pycache__</div><div class="line">    │   │   ├── __init__.cpython-36.pyc</div><div class="line">    │   │   ├── urls.cpython-36.pyc</div><div class="line">    │   │   └── views.cpython-36.pyc</div><div class="line">    │   ├── admin.py</div><div class="line">    │   ├── apps.py</div><div class="line">    │   ├── migrations</div><div class="line">    │   │   └── __init__.py</div><div class="line">    │   ├── models.py</div><div class="line">    │   ├── tests.py</div><div class="line">    │   ├── urls.py</div><div class="line">    │   └── views.py</div><div class="line">    ├── mysite</div><div class="line">    │   ├── __init__.py</div><div class="line">    │   ├── __pycache__</div><div class="line">    │   │   ├── __init__.cpython-36.pyc</div><div class="line">    │   │   ├── settings.cpython-36.pyc</div><div class="line">    │   │   ├── urls.cpython-36.pyc</div><div class="line">    │   │   └── wsgi.cpython-36.pyc</div><div class="line">    │   ├── settings.py</div><div class="line">    │   ├── templates</div><div class="line">    │   │   └── mysample</div><div class="line">    │   │       └── dashboard.html</div><div class="line">    │   ├── urls.py</div><div class="line">    │   └── wsgi.py</div><div class="line">    └── static</div><div class="line">        ├── css</div><div class="line">        │   ├── bootstrap-rtl.css</div><div class="line">        │   ├── bootstrap-rtl.min.css</div><div class="line">        │   ├── bootstrap.css</div><div class="line">        │   ├── bootstrap.min.css</div><div class="line">        │   ├── plugins</div><div class="line">        │   │   └── morris.css</div><div class="line">        │   ├── sb-admin-rtl.css</div><div class="line">        │   └── sb-admin.css</div><div class="line">        ├── font-awesome</div><div class="line">        │   ├── css</div><div class="line">        │   │   ├── font-awesome.css</div><div class="line">        │   │   └── font-awesome.min.css</div><div class="line">        │   ├── fonts</div><div class="line">        │   │   ├── FontAwesome.otf</div><div class="line">        │   │   ├── fontawesome-webfont.eot</div><div class="line">        │   │   ├── fontawesome-webfont.svg</div><div class="line">        │   │   ├── fontawesome-webfont.ttf</div><div class="line">        │   │   └── fontawesome-webfont.woff</div><div class="line">        │   ├── less</div><div class="line">        │   │   ├── bordered-pulled.less</div><div class="line">        │   │   ├── core.less</div><div class="line">        │   │   ├── fixed-width.less</div><div class="line">        │   │   ├── font-awesome.less</div><div class="line">        │   │   ├── icons.less</div><div class="line">        │   │   ├── larger.less</div><div class="line">        │   │   ├── list.less</div><div class="line">        │   │   ├── mixins.less</div><div class="line">        │   │   ├── path.less</div><div class="line">        │   │   ├── rotated-flipped.less</div><div class="line">        │   │   ├── spinning.less</div><div class="line">        │   │   ├── stacked.less</div><div class="line">        │   │   └── variables.less</div><div class="line">        │   └── scss</div><div class="line">        │       ├── _bordered-pulled.scss</div><div class="line">        │       ├── _core.scss</div><div class="line">        │       ├── _fixed-width.scss</div><div class="line">        │       ├── _icons.scss</div><div class="line">        │       ├── _larger.scss</div><div class="line">        │       ├── _list.scss</div><div class="line">        │       ├── _mixins.scss</div><div class="line">        │       ├── _path.scss</div><div class="line">        │       ├── _rotated-flipped.scss</div><div class="line">        │       ├── _spinning.scss</div><div class="line">        │       ├── _stacked.scss</div><div class="line">        │       ├── _variables.scss</div><div class="line">        │       └── font-awesome.scss</div><div class="line">        ├── fonts</div><div class="line">        │   ├── glyphicons-halflings-regular.eot</div><div class="line">        │   ├── glyphicons-halflings-regular.svg</div><div class="line">        │   ├── glyphicons-halflings-regular.ttf</div><div class="line">        │   ├── glyphicons-halflings-regular.woff</div><div class="line">        │   └── glyphicons-halflings-regular.woff2</div><div class="line">        └── js</div><div class="line">            ├── bootstrap.js</div><div class="line">            ├── bootstrap.min.js</div><div class="line">            ├── jquery.js</div><div class="line">            └── plugins</div><div class="line">                ├── flot</div><div class="line">                │   ├── excanvas.min.js</div><div class="line">                │   ├── flot-data.js</div><div class="line">                │   ├── jquery.flot.js</div><div class="line">                │   ├── jqu</div><div class="line"></div><div class="line">                ery.flot.pie.js</div><div class="line">                │   ├── jquery.flot.resize.js</div><div class="line">                │   └── jquery.flot.tooltip.min.js</div><div class="line">                └── morris</div><div class="line">                    ├── morris-data.js</div><div class="line">                    ├── morris.js</div><div class="line">                    ├── morris.min.js</div><div class="line">                    └── raphael.min.js</div></pre></td></tr></table></figure></p>
<p>注意要修改 html 里引用 css 和 js 的路径，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;link href=&quot;/static/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt;</div></pre></td></tr></table></figure></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>修改 mysite 下的 settings.py，添加 templates 的目录路径以及 static files 的目录路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">TEMPLATES = [</div><div class="line">    &#123;</div><div class="line">        &apos;BACKEND&apos;: &apos;django.template.backends.django.DjangoTemplates&apos;,</div><div class="line">        &apos;DIRS&apos;: [&apos;your directory/mysite/mysite/templates&apos;],</div><div class="line">        &apos;APP_DIRS&apos;: True,</div><div class="line">        &apos;OPTIONS&apos;: &#123;</div><div class="line">            &apos;context_processors&apos;: [</div><div class="line">                &apos;django.template.context_processors.debug&apos;,</div><div class="line">                &apos;django.template.context_processors.request&apos;,</div><div class="line">                &apos;django.contrib.auth.context_processors.auth&apos;,</div><div class="line">                &apos;django.contrib.messages.context_processors.messages&apos;,</div><div class="line">            ],</div><div class="line">        &#125;,</div><div class="line">    &#125;,</div><div class="line">]</div><div class="line"></div><div class="line"></div><div class="line"># Static files (CSS, JavaScript, Images)</div><div class="line"># https://docs.djangoproject.com/en/1.10/howto/static-files/</div><div class="line"></div><div class="line">STATIC_URL = &apos;/static/&apos;</div><div class="line">STATICFILES_DIRS = (</div><div class="line">        &apos;your directory/mysite/static/&apos;,</div><div class="line">)</div></pre></td></tr></table></figure></p>
<h2 id="渲染"><a href="#渲染" class="headerlink" title="渲染"></a>渲染</h2><p>修改 mysample/views.py，渲染 html 文件而不是仅仅输出一段文字<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from django.shortcuts import render</div><div class="line">from django.http import HttpResponse</div><div class="line">from django.template import loader</div><div class="line"></div><div class="line"># Create your views here.</div><div class="line">def index(request):</div><div class="line">    return render_to_response(&apos;index.html&apos;)</div></pre></td></tr></table></figure></p>
<h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/django%20%2B%20bootstrap%20%E4%BD%BF%E7%94%A8%E7%BD%91%E9%A1%B5%E6%A8%A1%E6%9D%BF/pic.jpg" class="ful-image" alt="pic.jpg">
<blockquote>
<p>参考链接<br><a href="http://www.jianshu.com/p/9caff5c3ab49" target="_blank" rel="external">Django简易教程之一（models）</a><br><a href="https://docs.djangoproject.com/en/1.8/intro/" target="_blank" rel="external">Django document</a><br><a href="http://django-intro-zh.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">Django-intro 中文版</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Web Application </category>
            
        </categories>
        
        
        <tags>
            
            <tag> django </tag>
            
            <tag> 前端 </tag>
            
            <tag> bootstrap </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统--隐语义模型LFM]]></title>
      <url>http://www.shuang0420.com/2017/02/17/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/</url>
      <content type="html"><![CDATA[<p>主要介绍 <strong>隐语义模型 LFM(latent factor model)</strong>。<br><a id="more"></a></p>
<p>隐语义模型最早在文本挖掘领域被提出，用于找到文本的隐含语义，相关名词有 LSI、pLSA、LDA 等。在推荐领域，隐语义模型也有着举足轻重的地位。下述的实验设计见 <a href="http://www.shuang0420.com/2017/02/08/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%92%8C%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/">推荐系统–用户行为和实验设计</a></p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p><strong>核心思想:</strong> 通过隐含特征(latent factor)联系用户兴趣和物品。具体来说，就是对于某个用户，首先得到他的兴趣分类，然后从分类中挑选他可能喜欢的物品。<br>基于兴趣分类的方法需要解决3个问题：</p>
<ul>
<li>如何对物品进行分类？</li>
<li>如何确定物品对哪些类的物品感兴趣，以及感兴趣的程度？</li>
<li>对于一个给定的类，选择哪些属于这个类的物品推荐给用户，以及如何确定这些物品在一个类中的权重？</li>
</ul>
<p><strong>如何对物品进行分类？</strong><br>物品分类往往是通过人工编辑进行，然而人工编辑存在很多缺陷</p>
<ul>
<li>编辑的分类大部分是从书的内容出发，而不是从书的读者群出发。<br>  比如说《具体数学》这本书，人工编辑可能认为属于数学，而这本书的读者可能更多是计算机出身的，会认为它属于计算机</li>
<li>编辑很难控制分类的粒度<br>  有些推荐我们做粗粒度就可以了(比如说初学者)，而有些推荐我们需要深入到细分领域(比如资深研究人员)</li>
<li>编辑很难给一个物品多个分类</li>
<li>编辑很难给出多个维度的分类</li>
<li>编辑很难决定一个物品在某一个分类中的权重</li>
</ul>
<p>隐含语义分析技术(latent variable analysis)采取基于用户行为统计的自动聚类，可以较好解决上面提出的问题。</p>
<ul>
<li>代表用户意见<br>  分类来自对用户行为的统计，和 ItemCF 在物品分类方面的思想类似，如果两个物品同时被多个用户喜好，那么这两个物品可能属于同一个类</li>
<li>控制分类粒度<br>  自定义分类个数</li>
<li>一个物品多分类<br>  计算出物品属于某个类的权重，因此每个物品都不是硬性地被分到某一个类中</li>
<li>多维度分类<br>  基于用户的共同兴趣计算出来的，如果用户的共同兴趣是某一个维度，那么 LFM 给出的类也是相同维度</li>
<li>物品在分类下的权重<br>  统计用户行为决定物品在某一个分类中的权重，如果某个类的用户都会喜欢某个物品，那么这个物品在这个类中的权重可能比较高</li>
</ul>
<h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><p>隐含语义分析技术有很多著名的模型和方法，相关的名词有 <strong>pLSA、LDA、隐含类别模型(latent class model)、隐含主题模型(latent topic model)、矩阵分解(matrix factorization)</strong>，这些技术和方法本质上是相通的，很多方法都可以用于个性化推荐系统。本篇只介绍 LFM。</p>
<h3 id="用户对物品的兴趣"><a href="#用户对物品的兴趣" class="headerlink" title="用户对物品的兴趣"></a>用户对物品的兴趣</h3><p>计算用户 u 对物品 i 的兴趣<br>$$Preference(u,i)=r_{ui}=P^T_uq_i=\sum^F_{f=1}p_{u,k}q_{i,k}$$</p>
<ul>
<li>$p_{u,k}$: 模型参数，用户 u 的兴趣和第 k 个隐类的关系</li>
<li>$q_{i,k}$: 模型参数，第 k 个隐类和物品 i 之间的关系</li>
</ul>
<h3 id="产生负样本"><a href="#产生负样本" class="headerlink" title="产生负样本"></a>产生负样本</h3><p>我们这里用的是隐反馈数据集，只有正样本(用户喜欢什么物品)，而没有负样本(用户对什么物品不感兴趣)，因此第一个问题是如何对每个用户产生负样本。</p>
<p>Rong Ran 提出了以下方法。</p>
<ol>
<li>对于一个用户，用他所有没有过行为的物品作为负样本</li>
<li>对于一个用户，从他没有过行为的物品中均匀采样出一些物品作为负样本</li>
<li>对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，保证没给用户的正负样本数目相当</li>
<li>对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，偏重采样不热门的物品</li>
</ol>
<p>Rong Ran 表示第一种负样本太多，计算复杂度高，精度也差，而第三种优于第二种，第二种优于第四种。</p>
<p>另外需要遵循的原则是：</p>
<ul>
<li>对每个用户，要保证正负样本的平衡(数目相似)</li>
<li>对每个用户采样负样本时，要选取哪些很热门，但用户却没有行为的物品<br>  对于冷门物品，可能用户压根没发现，所以谈不上是否感兴趣</li>
</ul>
<p>负样本采样过程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">items: dictionary of items where user takes action</div><div class="line">items_pool: list of candidate items; the more popular item i is, the more often item i appear</div><div class="line">&apos;&apos;&apos;</div><div class="line">def RandomSelectNegativeSample(self, items):</div><div class="line">    ret = dict()</div><div class="line">    for i in items.keys():</div><div class="line">        ret[i] = 1</div><div class="line">    n=0</div><div class="line">    for i in range(0, len(items) * 3): # make the number of negative samples close to that of positvie</div><div class="line">        item = items_pool[random.randint(0, len(items_pool) - 1)]</div><div class="line">        if item in ret:</div><div class="line">            continue</div><div class="line">        ret[item] = 0</div><div class="line">        n+=1</div><div class="line">        if n &gt; len(items):</div><div class="line">            break</div><div class="line">    return ret</div></pre></td></tr></table></figure></p>
<h3 id="损失函数及学习过程"><a href="#损失函数及学习过程" class="headerlink" title="损失函数及学习过程"></a>损失函数及学习过程</h3><p>得到一个用户-物品集 K={(u,i)}，如果(u,i)是正样本，则有 $r_{ui}=1$，否则$r_{ui}=0$，然后通过随机梯度下降来优化损失函数找到最合适的参数 p 和 q：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/loss_func.jpg" class="ful-image" alt="loss_func.jpg"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/cal.jpg" class="ful-image" alt="cal.jpg">
<p>$\lambda ||p_u||^2 + \lambda ||q_i||^2$ 是防止过拟合的正则化项，$\lambda$ 通过实验获得。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def LatentFactorModel(user_items, F, N, alpha, lambda):</div><div class="line">    [P, Q] = InitModel(user_items, F)</div><div class="line">    for step in range(0,N):</div><div class="line">        for user, items in user_items.items():</div><div class="line">            samples = RandSelectNegativeSamples(items)</div><div class="line">            for item, rui in samples.items():</div><div class="line">                eui = rui - Predict(user, item)</div><div class="line">                for f in range(0, F):</div><div class="line">                    P[user][f] += alpha * (eui * Q[item][f] - lambda * P[user][f])</div><div class="line">                    Q[item][f] += alpha * (eui * P[user][f] - lambda * Q[item][f])</div><div class="line">        alpha *= 0.9</div><div class="line"></div><div class="line">def Recommend(user, P, Q):</div><div class="line">    rank = dict()</div><div class="line">    for f, puf in P[user].items():</div><div class="line">        for i, qfi in Q[f].items():</div><div class="line">            if i not in rank:</div><div class="line">             rank[i] += puf * qfi</div><div class="line">    return rank</div></pre></td></tr></table></figure>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>4 个隐类中排名最高的一些电影<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/exp1.jpg" class="ful-image" alt="exp1.jpg"></p>
<p>参数：</p>
<ul>
<li>隐特征个数 F</li>
<li>学习速率 alpha</li>
<li>正则化参数 lambda</li>
<li>负样本/正样本比例 ratio</li>
</ul>
<p>实验发现，ratio 对 LFM 性能影响最大，随着负样本数目的增加，LFM 的准确率和召回率有明显提高，当 ratio &gt; 10后趋于稳定，同时，随着负样本数目增加，覆盖率不断降低，流行度不断增加，说明 ratio 参数控制了推荐算法发掘长尾的能力。另外，与之前实验比较，在所有指标上都优于 UserCF 和 ItemCF。然而当数据集非常稀疏时，LFM 的性能会明显下降。</p>
<p>固定 F=100, alpha=0.02, lambda=0.01,研究 ratio 对推荐性能的影响。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/exp2.jpg" class="ful-image" alt="exp2.jpg">
<h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>LFM 模型在实际使用中有一个困难，就是很难实现实时推荐。经典的 LFM 模型每次训练都需要扫描所有的用户行为记录，并且需要在用户行为记录上反复迭代来优化参数，所以每次训练都很耗时，实际应用中只能每天训练一次。在新闻推荐中，冷启动问题非常明显，每天都会有大量的新闻，这些新闻往往如昙花一现，在很短的时间获得很多人的关注，然后在很短时间内失去关注，实时性就非常重要。雅虎对此提出了一个解决方案。</p>
<p>首先，利用新闻链接的内容属性(关键词、类别等)得到链接 i 的内容特征向量 yi，其次，实时收集用户对链接的行为，并且用这些数据得到链接 i 的隐特征向量 qi，然后，利用下面的公式预测用户 u 是否会单击链接 i:</p>
<p>$$r_{ui}=x^T_uy_i+p^T_uq_i$$</p>
<p>$y_i$: 根据物品的内容属性直接生成<br>$x_{uk}$: 用户 u 对内容特征 k 的兴趣程度，用户向量 $x_u$ 可以根据历史行为记录获得，每天计算一次<br>$p_u$,$q_i$: 实时拿到的用户最近几小时的行为训练 LFM 模型获得</p>
<p>对于一个新加入的物品 i，可以通过 $x^T_uy_i$估计用户 u 对物品 i 的兴趣，然后经过几个小时后，通过 $p^T_uq_i$得到更准确的预测值。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>与<a href="http://www.shuang0420.com/2017/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/">基于邻域的方法</a>相比的优缺点：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8BLFM/summary.jpg" class="ful-image" alt="summary.jpg"></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Recommender Systems </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统--基于邻域(neighborhood-based)的协同过滤算法]]></title>
      <url>http://www.shuang0420.com/2017/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>主要介绍 <strong>基于用户的协同过滤算法(UserCF)</strong> 和 <strong>基于物品的协同过滤算法(ItemCF)</strong>。<br><a id="more"></a></p>
<p>基于邻域的算法是推荐系统中最基本的算法，主要分两大类，一类是基于用户的协同过滤算法，另一类是基于物品的协同过滤算法。协同过滤的本质其实是 KNN，我们要定义的是 <strong>什么是“最匹配”</strong>。下述的实验设计见 <a href="http://www.shuang0420.com/2017/02/08/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%92%8C%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/">推荐系统–用户行为和实验设计</a></p>
<h1 id="基于用户的协同过滤算法-User-CF"><a href="#基于用户的协同过滤算法-User-CF" class="headerlink" title="基于用户的协同过滤算法(User CF)"></a>基于用户的协同过滤算法(User CF)</h1><p>当一个用户 A 需要个性化推荐时，先找到和他有相似兴趣的其它用户，然后把那些用户喜欢的、而用户 A 没有听说过的物品推荐给 A。强调 <strong>把和你有相似爱好的其他的用户的物品推荐给你</strong></p>
<p><strong>过程：</strong></p>
<ol>
<li>将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到和目标用户兴趣相似的用户集合；</li>
<li>找到这个集合中的用户喜欢的，且目标用户没有访问过的物品，计算得到一个排序的物品列表作为推荐。</li>
</ol>
<h2 id="算法与实验"><a href="#算法与实验" class="headerlink" title="算法与实验"></a>算法与实验</h2><h3 id="用户-用户"><a href="#用户-用户" class="headerlink" title="用户-用户"></a>用户-用户</h3><p>计算用户之间的相似度。</p>
<h4 id="Jaccard-公式"><a href="#Jaccard-公式" class="headerlink" title="Jaccard 公式"></a>Jaccard 公式</h4><p>$$W_{uv}={|N(u) \cap N(v)| \over |N(u) \cup N(v)|}$$</p>
<ul>
<li>N(u): 用户 u 喜欢的物品集合</li>
<li>N(v): 用户 v 喜欢的物品集合</li>
</ul>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h4><p>$$W_{uv}={|N(u) \cap N(v)| \over \sqrt{|N(u) || N(v)|}}$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def UserSimilarity(train):</div><div class="line">    W = dict()</div><div class="line">    for u in train.keys():</div><div class="line">        for v in train.keys():</div><div class="line">            if u == v:</div><div class="line">                continue</div><div class="line">            W[u][v] = len(train[u] &amp; train[v])</div><div class="line">            W[u][v] /= math.sqrt(len(train[u]) * len(train[v]) * 1.0)</div><div class="line">    return W</div></pre></td></tr></table></figure>
<p><strong>算法优化:</strong><br>对两两用户都计算余弦相似度，时间复杂度是 O(|U|*|U|)，在用户数很大的时候非常耗时。而很多用户相互之间并没有对同样的物品产生过行为，很多时候 $|N(u) \cap N(v)|=0$，有计算的浪费。所以可以换个思路，首先计算 $|N(u) \cap N(v)| \neq 0$ 的用户对 (u,v)，然后再对这种情况除以分母 $\sqrt{|N(u) || N(v)|}$</p>
<p>具体做法：</p>
<ul>
<li>建立物品到用户的倒排表<br>对于每个物品都保存对该物品产生过行为的用户列表<br>令稀疏矩阵 $C[u][v]=|N(u) \cap N(v)|$<br>E.g. 用户 u 和 v 同时属于倒排表中 K 个物品对应的用户列表，就有 $C[u][v]=K$</li>
<li>建立用户相似度矩阵<br>扫描倒排表中每个物品对应的用户列表<br>将用户列表中的两两用户对应的 $C[u][v]$ 加 1，得到所有用户之间不为 0 的 $C[u][v]$</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/userInt.jpg" class="ful-image" alt="userInt.jpg">
<p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">def UserSimilarity(train):</div><div class="line">    # build inverse table for item_users</div><div class="line">    item_users = dict()</div><div class="line">    for u, items in train.items():</div><div class="line">        for i in items.keys():</div><div class="line">            if i not in item_users:</div><div class="line">                item_users[i] = set()</div><div class="line">                item_users[i].add(u)</div><div class="line">    #calculate co-rated items between users</div><div class="line">    C = dict()</div><div class="line">    N = dict()</div><div class="line">    for i, users in item_users.items():</div><div class="line">        for u in users:</div><div class="line">            N[u] += 1</div><div class="line">            for v in users:</div><div class="line">                if u == v:</div><div class="line">                    continue</div><div class="line">                C[u][v] += 1</div><div class="line">    #calculate finial similarity matrix W</div><div class="line">    W = dict()</div><div class="line">    for u, related_users in C.items():</div><div class="line">        for v, cuv in related_users.items():</div><div class="line">            W[u][v] = cuv / math.sqrt(N[u] * N[v])</div><div class="line">    return W</div></pre></td></tr></table></figure></p>
<h3 id="用户-物品"><a href="#用户-物品" class="headerlink" title="用户-物品"></a>用户-物品</h3><p>用户 u 对物品 i 的感兴趣程度<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/userCFsim.jpg" class="ful-image" alt="userCFsim.jpg"></p>
<ul>
<li>S(u,K): 和用户 u 兴趣最接近的 K 个用户</li>
<li>N(i): 对物品 i 有过行为的用户集合</li>
<li>$w_{uv}$: 用户 u 和用户 v 的兴趣相似度</li>
<li>$r_{uv}$: 用户 v 对物品 i 的兴趣，单一行为的隐反馈数据下，所有的 $r_{uv}=1$</li>
</ul>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def Recommend(user, train, W):</div><div class="line">    rank = dict()</div><div class="line">    interacted_items = train[user]</div><div class="line">    for v, wuv in sorted(W[u].items, key=itemgetter(1), reverse=True)[0:K]:</div><div class="line">        for i, rvi in train[v].items:</div><div class="line">            if i in interacted_items:</div><div class="line">                #we should filter items user interacted before</div><div class="line">                continue</div><div class="line">            rank[i] += wuv * rvi</div><div class="line">    return rank</div></pre></td></tr></table></figure></p>
<h3 id="UserCF-实验"><a href="#UserCF-实验" class="headerlink" title="UserCF 实验"></a>UserCF 实验</h3><p>UserCF 只有一个重要的参数 K，即每个用户选出 K 个和他兴趣最相似的用户。<br>不同 K 值下 UserCF 算法的性能指标<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t24.jpg" class="ful-image" alt="t24.jpg"></p>
<p><strong>对比实验：</strong></p>
<ul>
<li>Random 算法<br>每次随机挑选 10 个用户没有产生过行为的物品推荐给用户<br>准确率和召回率远高于 Random 算法，但覆盖率非常低，结果都非常热门</li>
<li>MostPopular 算法<br>按照物品的流行度给用户推荐他没产生过行为的物品中最热门的 10 个物品<br>准确率和召回率很低，但覆盖率很高，结果平均流行度很低。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t25.jpg" class="ful-image" alt="t25.jpg">
<p>参数 K 的影响：</p>
<ul>
<li>准确率和召回率<br>没有线性关系，在一定区域内都可以获得不错的精度</li>
<li>流行度<br>K 越大 UserCF 推荐结果越热门<br>K 越大参考的人越多，推荐结果就越趋近于全局热门的物品</li>
<li>覆盖率<br>K 越大，覆盖率越低<br>越趋近于推荐全局热门的物品，长尾物品的推荐越来越少</li>
</ul>
<h3 id="用户相似度优化"><a href="#用户相似度优化" class="headerlink" title="用户相似度优化"></a>用户相似度优化</h3><p>考虑 idf 类似影响，如果绝大多数人都买过《新华字典》，并不能说明他们兴趣相似，所以需要惩罚用户 u 和用户 v 共同兴趣列表中热门物品对他们相似度的影响，通过 ${1 \over log1+|N(i)|}$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/useriif.jpg" class="ful-image" alt="useriif.jpg">
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">def UserSimilarity(train):</div><div class="line">    # build inverse table for item_users</div><div class="line">    item_users = dict()</div><div class="line">    for u, items in train.items():</div><div class="line">        for i in items.keys():</div><div class="line">            if i not in item_users:</div><div class="line">                item_users[i] = set()</div><div class="line">            item_users[i].add(u)</div><div class="line"></div><div class="line">    #calculate co-rated items between users</div><div class="line">    C = dict()</div><div class="line">    N = dict()</div><div class="line">    for i, users in item_users.items():</div><div class="line">        for u in users:</div><div class="line">            N[u] += 1</div><div class="line">            for v in users:</div><div class="line">                if u == v:</div><div class="line">                    continue</div><div class="line">                C[u][v] += 1 / math.log(1 + len(users))</div><div class="line">    #calculate finial similarity matrix W</div><div class="line">    W = dict()</div><div class="line">    for u, related_users in C.items():</div><div class="line">        for v, cuv in related_users.items():</div><div class="line">            W[u][v] = cuv / math.sqrt(N[u] * N[v])</div><div class="line">    return W</div></pre></td></tr></table></figure></p>
<h3 id="UserCF-IIF-实验"><a href="#UserCF-IIF-实验" class="headerlink" title="UserCF-IIF 实验"></a>UserCF-IIF 实验</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t26.jpg" class="ful-image" alt="t26.jpg">
<p>发现 UserCF-IIF 在各项性能上略优于 UserCF。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>相比于 ItemCF，UserCF 在目前的实际应用中使用并不多。</p>
<p><strong>优点和适用场景：</strong></p>
<ul>
<li>可以发现用户感兴趣的热门物品</li>
<li>用户有新行为，不一定造成推荐结果的立即变化</li>
<li>适用于用户较少的场合，否则用户相似度矩阵计算代价很大</li>
<li>适合时效性较强，用户个性化兴趣不太明显的领域</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>数据稀疏性。一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到相似用户。</li>
<li>算法扩展性。随着用户数量越来越大，计算用户相似度矩阵越来越困难，时空复杂度的增长和用户数的增长近似于平方关系。</li>
<li>对新用户不友好，对新物品友好，因为用户相似度矩阵不能实时计算</li>
<li>很难提供令用户信服的推荐解释</li>
</ul>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-01%20%E4%B8%8B%E5%8D%887.31.02.png" alt=""></p>
<h1 id="基于物品的协同过滤算法-Item-CF"><a href="#基于物品的协同过滤算法-Item-CF" class="headerlink" title="基于物品的协同过滤算法(Item CF)"></a>基于物品的协同过滤算法(Item CF)</h1><p><strong>假设：物品 A 和物品 B 具有很大的相似度是因为喜欢物品 A 的用户大多也喜欢物品 B</strong><br>通过分析用户的行为记录计算物品之间的相似度。强调 <strong>把和你喜欢的物品相似的物品推荐给你</strong>。在京东、天猫上看到「购买了该商品的用户也经常购买的其他商品」，就是主要基于 ItemCF。</p>
<p><strong>过程：</strong></p>
<ol>
<li>基于用户对物品的偏好计算相似度，找到相似的物品；</li>
<li>根据物品的相似度和用户的历史行为预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。</li>
</ol>
<p><strong>过程：</strong></p>
<ol>
<li>计算物品之间的相似度</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表</li>
</ol>
<h2 id="算法与实验-1"><a href="#算法与实验-1" class="headerlink" title="算法与实验"></a>算法与实验</h2><h3 id="物品-物品"><a href="#物品-物品" class="headerlink" title="物品-物品"></a>物品-物品</h3><p><strong>Jaccard 公式:</strong><br>$$W_{ij}={|N(i) \cap N(j)| \over |N(i)|}$$</p>
<ul>
<li>N(i): 喜欢物品 i 的用户数</li>
<li>$|N(i) \cap N(j)|$: 喜欢物品 i 和物品 j 的用户数</li>
</ul>
<p>同样的，考虑 idf 影响：如果物品 j 很热门，很多人都喜欢，那么 $W_{ij}$ 就会很大，接近 1，也就是说，任何物品会和热门的物品有很大相似度，所以惩罚物品 j 的权重，减轻热门物品和许多物品相似的可能性。</p>
<p>$$W_{ij}={|N(i) \cap N(j)| \over \sqrt{|N(i) || N(j)|}}$$</p>
<p>和 UserCF 类似，ItemCF 也可以首先建立用户-物品倒排表(每个用户建立一个包含他喜欢的物品的列表)，然后对于每个用户，将他物品列表中的物品两两在共现矩阵 C 中加 1。</p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">def ItemSimilarity(train):</div><div class="line">    # calculate co-rated users between items</div><div class="line">    C = dict()</div><div class="line">    N = dict()</div><div class="line">    for u, items in train.items():</div><div class="line">        for i in users:</div><div class="line">            N[i] += 1</div><div class="line">            for j in users:</div><div class="line">                if i == j:</div><div class="line">                    continue</div><div class="line">                C[i][j] += 1</div><div class="line"></div><div class="line">    # calculate final similarity matrix W</div><div class="line">    W = dict()</div><div class="line">    for i,related_items in C.items():</div><div class="line">        for j, cij in related_items.items():</div><div class="line">            W[u][v] = cij / math.sqrt(N[i] * N[j])</div><div class="line">    return W</div></pre></td></tr></table></figure></p>
<p>左边是输入的用户记录，每一行代表一个用户感兴趣的物品集合，然后对每个物品集合，将里面对物品两两加一，得到一个矩阵。最终将这个矩阵相加得到 C 矩阵，其中 $C[i][j]$ 记录了同时喜欢物品 i 和物品 j 的用户数，最后，将 C 矩阵归一化可以得到物品之间的余弦相似度矩阵 W。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t211.jpg" class="ful-image" alt="t211.jpg">
<h3 id="用户-物品-1"><a href="#用户-物品-1" class="headerlink" title="用户-物品"></a>用户-物品</h3><p>得到物品的相似度后，计算用户 u 对一个物品的兴趣：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/itemcf.jpg" class="ful-image" alt="itemcf.jpg"></p>
<p>和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</p>
<ul>
<li>N(u): 用户喜欢的物品集合</li>
<li>S(j,K): 和物品 j 最相似的 K 个物品的集合</li>
<li>$w_{ji}$: 物品 j 和 i 的相似度</li>
<li>$r_{ui}$: 用户 u 对物品 i 的兴趣<br>隐反馈数据集，如果用户 u 对物品 i 有过行为，即可令 $r_{ui}=1$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def Recommendation(train, user_id, W, K):</div><div class="line">    rank = dict()</div><div class="line">    ru = train[user_id]</div><div class="line">    for i,pi in ru.items():</div><div class="line">        for j, wj in sorted(W[i].items(), key=itemgetter(1), reverse=True)[0:K]:</div><div class="line">            if j in ru:</div><div class="line">                continue</div><div class="line">            rank[j] += pi * wj</div><div class="line">    return rank</div></pre></td></tr></table></figure>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/itemcfeg.jpg" class="ful-image" alt="itemcfeg.jpg">
<h3 id="ItemCF-实验"><a href="#ItemCF-实验" class="headerlink" title="ItemCF 实验"></a>ItemCF 实验</h3><p>不同 K 值下的性能<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t28.jpg" class="ful-image" alt="t28.jpg"></p>
<p>参数 K 的影响：</p>
<ul>
<li>准确率和召回率<br>没有线性关系，在一定区域内都可以获得不错的精度</li>
<li>流行度<br>随着 K 的增加，流行度逐渐提高<br>当 K 增加到一定程度，流行度不再有明显变化</li>
<li>覆盖率<br>K 越大，覆盖率越低</li>
</ul>
<p><strong>用户活跃度对物品相似度的影响</strong><br>举个例子吧，一个用户，是开书店的，买了亚马逊上 80% 的书准备用来自己卖，所以购物车里包含了亚马逊上 80% 的书，假设亚马逊一共有 100 万本书，那么这意味着 80 万本书两两之间产生了相似度，内存里将诞生一个 80w * 80w 的矩阵。<br>然而，虽然用户很活跃，但这些书并非基于兴趣，而且这些书覆盖了亚马逊图书的很多领域，所以这个用户对他所购买书对两两相似度的贡献应该远远小于一个只买了十几本自己喜欢的书的文学青年。<br>所以我们需要一个 IUF(Inverse User Frequency)，来修正物品相似度的计算公式<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/itemcfiuf.jpg" class="ful-image" alt="itemcfiuf.jpg"></p>
<p>为了避免相似度矩阵过于稠密，我们在实际计算中一般直接忽略他的兴趣列表，而不将其纳入到相似度计算的数据集中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def ItemSimilarity(train):</div><div class="line">    #calculate co-rated users between items</div><div class="line">    C = dict()</div><div class="line">    N = dict()</div><div class="line">    for u, items in train.items():</div><div class="line">        for i in users:</div><div class="line">            N[i] += 1</div><div class="line">            for j in users:</div><div class="line">                if i == j:</div><div class="line">                    continue</div><div class="line">                    C[i][j] += 1 / math.log(1 + len(items) * 1.0)</div><div class="line">    #calculate finial similarity matrix W</div><div class="line">    W = dict()</div><div class="line">    for i,related_items in C.items():</div><div class="line">        for j, cij in related_items.items():</div><div class="line">            W[u][v] = cij / math.sqrt(N[i] * N[j])</div><div class="line">    return W</div></pre></td></tr></table></figure></p>
<p>可以看到，ItemCF-IUF 明显提高了覆盖率<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t29.jpg" class="ful-image" alt="t29.jpg"></p>
<h3 id="物品相似度的归一化"><a href="#物品相似度的归一化" class="headerlink" title="物品相似度的归一化"></a>物品相似度的归一化</h3><p>将相似度矩阵按最大值归一化，可以提高推荐的准确率、覆盖率和多样性。对相似度矩阵 w 进行</p>
<p>$$w’_{ij}={w_{ij} \over max_j w_{ij}}$$</p>
<p>举个例子，假设有 A、B 两类物品，A 类物品之间的相似度是 0.5，B 类物品之间的相似度是 0.6，A 类物品和 B 类物品之间的相似度是 0.2，如果一个用户喜欢了 5 个 A 类物品和 5 个 B 类物品，ItemCF 推荐的是 B 类物品，而如果归一化之后，A 类物品的相似度和 B 类物品的相似度都是 1，那么推荐列表中 A 类物品和 B 类物品的数目也应该是大致相等的，这就保障了多样性。</p>
<p>归一化后的性能<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/t210.jpg" class="ful-image" alt="t210.jpg"></p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>因为物品直接的相似性相对比较固定，所以可以预先在线下计算好不同物品之间的相似度，把结果存在表中，当推荐时进行查表，计算用户可能的打分值。</p>
<p><strong>优点和适用场景：</strong></p>
<ul>
<li>可以发现用户潜在的但自己尚未发现的兴趣爱好</li>
<li>有效的进行长尾挖掘</li>
<li>利用用户的历史行为给用户做推荐解释，使用户比较信服</li>
<li>适用于物品数明显小于用户数的场合，否则物品相似度矩阵计算代价很大</li>
<li>适合长尾物品丰富，用户个性化需求强的领域</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对新用户友好，对新物品不友好，因为物品相似度矩阵不需要很强的实时性</li>
</ul>
<h1 id="UserCF-vs-ItemCF"><a href="#UserCF-vs-ItemCF" class="headerlink" title="UserCF vs ItemCF"></a>UserCF vs ItemCF</h1><p>总的而言，UserCF 的推荐更社会化，着重于反应和用户兴趣相似的小群体的热点，ItemCF 的推荐更个性化，着重于维系用户的历史兴趣。大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，但这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的。一般来说，这两种算法经过优化后，最终得到的离线性能是近似的。具体选择还是要看具体情境。</p>
<h2 id="社交网络-新闻领域"><a href="#社交网络-新闻领域" class="headerlink" title="社交网络/新闻领域"></a>社交网络/新闻领域</h2><p><strong>适用 UserCF：</strong></p>
<ul>
<li>用户兴趣不是特别细化<br>绝大多数都喜欢看热门的新闻，即使是个性化，也是比较粗粒度的，如体育/新闻这种大类。</li>
<li>个性化新闻推荐更加强调抓住新闻热点<br>热门程度和时效性是个性化推荐的重点，UserCF 可以给用户推荐和他有相似爱好的一群其他用户今天都在看的新闻，这样在抓住热点和时效性的同时，保证了一定程度的个性化。</li>
<li>物品的更新速度远远快于新用户的加入速度<br>在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且对于新用户，完全可以给他推荐最热门的新闻。<br>ItemCF 需要维护一张物品相关度的表，如果物品更新很快，那么这种表也需要很快更新，这在技术上很难实现。</li>
</ul>
<h2 id="非社交网络"><a href="#非社交网络" class="headerlink" title="非社交网络"></a>非社交网络</h2><p><strong>如图书/电商/电影网站，适用 ItemCF</strong></p>
<ul>
<li>用户的兴趣比较持久</li>
<li>用户不太需要流行度来辅助他们判断一个物品的好坏，而是可以通过自己熟悉领域的只是自己判断物品的质量</li>
<li>个性化推荐的任务是帮助用户发现和他研究领域相关的物品。</li>
<li>物品数目较少，物品相似度相对于用户的兴趣一般比较稳定。</li>
<li>提供推荐解释</li>
</ul>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%9F%BA%E4%BA%8E%E9%82%BB%E5%9F%9F%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/cfsummary.jpg" class="ful-image" alt="cfsummary.jpg">
<h1 id="哈利波特问题"><a href="#哈利波特问题" class="headerlink" title="哈利波特问题"></a>哈利波特问题</h1><p>主要指某个物品太热门导致很多物品都与之相关。</p>
<p><strong>解决方案：</strong></p>
<ul>
<li>分母上加大对热门物品的惩罚<br>$$W_{ij}={|N(i) \cap N(j)| \over |N(i)|^{1- \alpha} |N(j)|^{\alpha}}$$</li>
</ul>
<p>$\alpha \in [0.5,1]$，通过提高 $\alpha$，就可以惩罚热门的 j。$\alpha$ 越大，覆盖率越高，结果的平均热门程度就越低，因此通过这种方法可以在适当牺牲准确率和召回率的情况下显著提升结果的覆盖率和新颖性(降低流行度即提高了新颖性)。</p>
<p>然而，这并不能彻底解决哈利波特问题。每个用户一般都会在不同的领域喜欢一种物品。两个不同领域的最热门物品之间往往具有比较高的相似度，这个时候，仅仅靠用户行为数据是不能解决这个问题的，因为用户的行为表示这种物品之间应该相似度很高。此时，我们只能依靠引入物品的内容数据解决这个问题，比如对不同领域的物品降低权重等，这些就不是协同过滤讨论的范畴了。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Recommender Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Recommender Systems </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统--用户行为和实验设计]]></title>
      <url>http://www.shuang0420.com/2017/02/08/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%92%8C%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/</url>
      <content type="html"><![CDATA[<p>主要介绍推荐系统用户行为数据、实验设计，是接下来算法实验的基础。<br><a id="more"></a></p>
<h1 id="用户行为"><a href="#用户行为" class="headerlink" title="用户行为"></a>用户行为</h1><h2 id="用户行为数据"><a href="#用户行为数据" class="headerlink" title="用户行为数据"></a>用户行为数据</h2><p>用户行为分为 <strong>显性</strong> 和 <strong>隐性</strong> 两种。</p>
<ul>
<li>显性反馈行为(explicit feedback)<br>用户评分、喜欢/不喜欢</li>
<li>隐性反馈行为(implicit feedback)<br>页面浏览行为、消费行为</li>
</ul>
<p>两者比较<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%92%8C%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/beCom.jpg" class="ful-image" alt="beCom.jpg"></p>
<p><strong>如何表示用户：</strong></p>
<ul>
<li>user id<br>产生行为的用户</li>
<li>item id<br>产生行为的对象</li>
<li>behavior type<br>行为的种类(如购买还是浏览)</li>
<li>context<br>产生行为的上下文，包括时间和地点</li>
<li>behavior weight<br>行为的权重(用户评分、观看时长等)</li>
<li>behavior content<br>行为的内容(评论文本、标签等)</li>
</ul>
<p><strong>数据集的一般分类：</strong></p>
<ul>
<li>无上下文信息的隐性反馈数据集<br>包含 user id, item id<br>如 Book-Crossing</li>
<li>无上下文信息的显性反馈数据集<br>包含 user id, item id, 物品评分</li>
<li>有上下文信息的隐性反馈数据集<br>包含 user id, item id, 用户对物品产生行为的 timestamp<br>如 Lastfm</li>
<li>有上下文信息的显性反馈数据集<br>包含 user id, item id, 物品评分, 用户对物品产生行为的 timestamp<br>如 Netflix Prize</li>
</ul>
<h2 id="用户行为分析"><a href="#用户行为分析" class="headerlink" title="用户行为分析"></a>用户行为分析</h2><p><strong>物品流行度</strong> 和 <strong>用户活跃度</strong> 都近似于 <strong>长尾分布</strong>。下图表示用户活跃度和物品流行度的关系(MovieLens 数据集)。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%92%8C%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/uir.jpg" class="ful-image" alt="uir.jpg"></p>
<p>一般认为，新用户倾向于浏览热门物品，因为对网站不熟悉，只能点击首页的热门物品，而老用户会逐渐开始浏览冷门物品。</p>
<p>仅仅基于用户行为数据设计的推荐算法一般称为协同过滤算法，有很多种方法，这里介绍 <strong>基于邻域的方法(neighborhood-based)</strong>, <strong>隐语义模型(latent factor model)</strong>, <strong>基于图的随机游走算法(random walk on graph)</strong>，这其中，最有名、在业界得到最广泛应用的算法是基于邻域的方法。接下来的博客会一一讨论这些算法。</p>
<h1 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>采用 GroupLens 提供的 MovieLens 数据集，选中等大小的数据集，包含 6000 多用户对 4000 多部电影的 100 万条评分。</p>
<h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>研究隐反馈数据集中的 Top N 推荐问题，因此忽略数据集中的评分记录。预测的是用户会不会对某部电影评分，而不是预测用户在准备对某部电影评分对前提下会给电影评多少分。</p>
<p>注意这里隐反馈数据集只有正样本(用户喜欢什么物品)，而没有负样本(用户对什么物品不感兴趣)。</p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>离线实验。</p>
<ul>
<li>将用户行为数据集均匀的随机分成 M 份(这里取 8)<br>挑一份作为测试集，剩下 M-1 份作为训练集</li>
<li>在训练集上建立用户兴趣模型，在测试集上评测，统计评测指标</li>
<li>M 次实验，每次使用不同的测试集<br>M 次试验的评测指标取平均值<br>防止过拟合<br>如果数据集够大，模型够简单，为了快速通过离线实验初步选择算法，也可以只进行一次实验</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def SplitData(data, M, k, seed):</div><div class="line">    test = []</div><div class="line">    train = []</div><div class="line">    random.seed(seed)</div><div class="line">    for user, item in data:</div><div class="line">        if random.randint(0,M) == k:</div><div class="line">            test.append([user,item])</div><div class="line">        else:</div><div class="line">            train.append([user,item])</div><div class="line">    return train, test</div></pre></td></tr></table></figure>
<h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><ul>
<li>准确率</li>
<li>召回率</li>
<li>覆盖率</li>
<li>新颖度</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">def Precision(train, test, N):</div><div class="line">    hit = 0</div><div class="line">    all = 0</div><div class="line">    for user in train.keys():</div><div class="line">        tu = test[user]</div><div class="line">        rank = GetRecommendation(user, N)</div><div class="line">        for item, pui in rank:</div><div class="line">            if item in tu:</div><div class="line">                hit += 1</div><div class="line">        all += N</div><div class="line">    return hit / (all * 1.0)</div><div class="line"></div><div class="line"></div><div class="line">def Recall(train, test, N):</div><div class="line">    hit = 0</div><div class="line">    all = 0</div><div class="line">    for user in train.keys():</div><div class="line">        tu = test[user]</div><div class="line">        rank = GetRecommendation(user, N)</div><div class="line">        for item, pui in rank:</div><div class="line">            if item in tu:</div><div class="line">                hit += 1</div><div class="line">        all += len(tu)</div><div class="line">    return hit / (all * 1.0)</div><div class="line"></div><div class="line"></div><div class="line">def Coverage(train, test, N):</div><div class="line">    recommend_items = set()</div><div class="line">    all_items = set()</div><div class="line">    for user in train.keys():</div><div class="line">        for item in train[user].keys():</div><div class="line">            all_items.add(item)</div><div class="line">        rank = GetRecommendation(user, N)</div><div class="line">        for item, pui in rank:</div><div class="line">            recommend_items.add(item)</div><div class="line">    return len(recommend_items) / (len(all_items) * 1.0)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def Popularity(train, test, N):</div><div class="line">    item_popularity = dict()</div><div class="line">    for user, items in train.items():</div><div class="line">        for item in items.keys():</div><div class="line">            if item not in item_popularity:</div><div class="line">                item_popularity[item] = 0</div><div class="line">                item_popularity[item] += 1</div><div class="line">    ret = 0</div><div class="line">    n=0</div><div class="line">    for user in train.keys():</div><div class="line">        rank = GetRecommendation(user, N)</div><div class="line">        for item, pui in rank:</div><div class="line">            ret += math.log(1 + item_popularity[item])</div><div class="line">            n += 1</div><div class="line">    ret /= n * 1.0</div><div class="line">    return ret</div></pre></td></tr></table></figure>
<p>该实验设计用于接下来 <strong>基于邻域的方法(neighborhood-based)</strong>, <strong>隐语义模型(latent factor model)</strong>, <strong>基于图的随机游走算法(random walk on graph)</strong> 的博客中。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Recommender Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Recommender Systems </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CMU 11642 Search Engines - 大纲梳理]]></title>
      <url>http://www.shuang0420.com/2017/02/06/CMU%2011642%20Search%20Engines%20-%20%E5%A4%A7%E7%BA%B2%E6%A2%B3%E7%90%86/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记大纲。涉及了很多算法，详细见具体的链接，代码就不贴了。欢迎讨论，欢迎指正～<br><a id="more"></a></p>
<p>Jamie 搜索引擎这门课，还是很有收获的，课上除了一些基本概念和算法外还有很多最新研究，涵盖内容非常广，绝对不止一本书。据 Jamie 讲，在 yahoo 等公司搜索部门的学生回来说现在做的工作感觉就是当年做的作业，是否有夸张不知道，然而大家可以感受下。已经过了选课阶段，就当给下一届想选的小盆友一点 workload 信息吧：</p>
<ol>
<li>reading notes。每周有大量的 reading，可能是教材也可能是论文。注意 reading notes 的成绩是 binary 的，1 or 0，不要以为在 blackboard 上看到自己是 80 分就以为有了0.8，80分＝0分！</li>
<li>homework。五次作业完成一个完善的 search engine，语言是 java，大概三四十个类，每次都是在上一次的基础上进一步改进，所以除了最后一次作业外，你做的每一次作业的 performance 都将深深的影响下一次。如果你发现你的运行时间比 Callen 给的时间要长很多，请务必进行优化。作业不难，通常是让你实现各种算法，常用的以及某些论文中的，然而评分很严，很多 corner case 要注意。<br>每次作业完成都有一篇 report，需要做很多实验(四五十个至少吧，不写脚本的话感觉可以从天黑做到天亮)，并做“深刻”总结，之所以说“深刻”是因为有时候我绞尽脑汁写的东西得到的评语是 shallow。一把心酸泪。一般来说一天写算法再一天过全部的 test case，最后做实验写 report。</li>
<li>exam。期中期末两次考试，上过 text analytics 的人都知道，Callen 的一贯风格，考试广度优先，题量大，靠本能，你腾出时间来思考你就输了，给分低。</li>
</ol>
<p>但是说了这么多不要怕！！就算考试成绩再低你的最后分数也会很好看！！</p>
<p>关于能不能 hold 住，这么说吧我上学期还选了 Machine learning(11601A)，Distributed Systems(95702)，以及 Data Structures for Application Programmers(08722)，感觉 4 门课老实说大课只能 focus 一到两门，如果各位还要刷题找工作，还是建议 P/F 或者是 audit 一门。</p>
<p>然后回到正题，高度总结下，这门课就讲了两个问题，一个是如何准确匹配查询与文档，一个是如何快速返回检索结果，就是 <strong>效果 vs 效率</strong> 的一个权衡。下面的总结梳理了这门课的重点，其中会涉及很多具体算法，然而这只是简单的提纲，不能把公式/算法都列出来，具体的可以看下面的链接或者看书/讲义。透露一点：多数的算法项目里你都需要去实现，而不需要实现的算法，Jamie 也不会轻易放过你，所以你们觉得会在哪里出现呢？😁</p>
<h1 id="文档表示"><a href="#文档表示" class="headerlink" title="文档表示"></a>文档表示</h1><p><strong>源文档到索引的过程：</strong><br><strong>Raw data</strong> –(Format Parser)–&gt; <strong>Canonical format</strong> –(Lexical Analyzer)–&gt; <strong>Index terms</strong> –(Index data structures)–&gt; <strong>Indexer</strong></p>
<ul>
<li>元描述(Meta-descriptions)<pre><code>- Fields (author, title, inlink, url)
</code></pre></li>
<li>关键词、受控词汇(controlled vocabulary)<pre><code>scheme 由识别文档主题的规则、指定词库、索引术语、分配索引的规则四部分组成
- 优点
    § 召回率高
    § 支持浏览和搜索
    § 在医学、法律、专利方面比较流行
- 不足
    § 人工标注创建和维护的成本很高
    § 人工标注可能不一致
    § 检索受限制
</code></pre></li>
<li><p>自动文档表示(Free-text or full-text index terms)</p>
<pre><code>一般用词袋(Bag of Words)，文档由该文档中出现的词的集合所表示
○ 优点
    § 简单、有效
○ 缺点
    § 无法从词袋表示恢复原文档
    § 忽略了词之间和句法关系以及篇章结构的信息

过程：
    1. 符号化(Tokenization):识别词的边界
    2. 停用词(Stop Words)
        过滤不具有内容信息的词，停用词表依赖于具体文档集及具体应用
        ○ 优点
            §  停用词并不能提高检索效果
            §  大幅减少索引大小
            §  缩短检索时间
        ○ 不足
            § 难以满足某些特殊的 query(to be or not to be)
    3. 词语形态规范化(Normalization)
        规范化词语的形态信息: 时态，数量，如 company &amp; companies, sell &amp; sold
    4. 词根(Stemming)
        处理词缀信息
        ○ 优点
            § 更准确地表示文档
            § 匹配更广泛的查询
        ○ 不足
            § Stemming 的结果可能不是词语
             § 不相关的词可能具有相同的 stem
    5. 词形还原(Lemmatization)
        将词语变为其语法原型(syntactic stem)，使用一般规则与例外处理，处理过程要考虑词性的不同
        ○ 优点
            § 更准确地表示文档
            § 匹配更广泛的查询

    大部分互联网搜索引擎并不使用 stemming/lemmatization
        § 文档集很大
        § 不太考虑召回率
        § Stemming 结果并不完美
    大部分互联网搜索引擎使用停用词表，Jammie Callen 课上好像说现在不怎么用？
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/09/25/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Representation/">Search Engines笔记 - Document Representations</a><br><a href="http://www.shuang0420.com/2017/02/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words,%20morphology,%20and%20lexicons/">NLP 笔记 - Words, morphology, and lexicons</a></p>
<h1 id="文档索引"><a href="#文档索引" class="headerlink" title="文档索引"></a>文档索引</h1><ul>
<li>目的在于提高检索效率</li>
<li>索引词项的统计特性<pre><code>- Heaps定律：对词项数目的估计
- Zipf定律：对词项的分布建模
</code></pre></li>
<li>索引分类<pre><code>- Term dictionary
- Inverted lists
    § 重点。常用的有 frequency inverted lists 和 positional inverted lists。
    § 可看做连标数组，每个链表的表头包含关键词，其后序单元则包括所有包括这个关键词的文档标号，以及一些其他信息，如该词的频率和位置等
- Attributes
- Fields masks
- Forward Index
- ...
</code></pre></li>
<li>索引对象<pre><code>通常采用 word 构建索引
词组怎么办？
    - 事前处理 precoordinate(只有一个 inverted list)，合并可能的词组，interest rate -&gt; interest_rate
    - 事后处理 postcoordinate(有多个 inverted lists)，查询的时候加上 #NEAR/1。interest rate -&gt; #NEAR/1(interest rate)
</code></pre></li>
<li>索引算法<pre><code>- 单机版
    - BSBI(Block sort-based indexing)
    - SPIMI(Single-pass in-memory indexing)
- 分布式索引
    - 技术：sharding, replication, MapReduce
    - 优化：分层索引
- 索引压缩
    - Delta Gap
    - Variable Byte Encoding
- 索引优化
    - Skip lists，考虑 operators(如 #NEAR,#WINDOW,#SYN,Boolean AND，调整指针) 以及 Top-Docs(截取部分 top docs)
    - 一个 term 多个 inverted list，比如一份不存位置信息，一份存，对于不需要位置信息的 operator，就用前一个索引
- 动态索引
    - 周期性索引重构
    - 辅助索引
</code></pre></li>
<li>数据结构<pre><code>- 一般采用 B-Tree(B+ tree, B* tree)，易于扩展，用于完全匹配(exact-match lookup)，范围寻找(range lookup)，前缀寻找（prefix lookup）
- 哈希表，不易于扩展，用于完全匹配(exact-match lookup)
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/10/14/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Index/">Search Engines笔记 - Index Construction</a></p>
<h1 id="查询处理"><a href="#查询处理" class="headerlink" title="查询处理"></a>查询处理</h1><ul>
<li>TAAT(Term-at-a-time)<pre><code>每处理完一个 inverted list，部分更新文档分数，再处理下一个。
- 优点：高效，易于理解
- 缺点：可能爆内存，因此很少用在 large-scale systems
</code></pre></li>
<li>DAAT(Document-at-a-time)<pre><code>每处理一篇文档，就算出 complete score，再处理下一篇文档。
- 优点：易于进行内存管理，可以进行 query evaluation 的优化，常用在大规模的系统
- 缺点：效率没有 TAAT 高
</code></pre></li>
<li>TAAT/DAAT hybrids<pre><code>平衡 Efficiency 和 memory control
Eg. block-based TAAT(compute TAAT over blocks of document ids)
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/09/11/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Query%20Processing/">Search Engines笔记 - Query Processing</a></p>
<h1 id="信息检索模型"><a href="#信息检索模型" class="headerlink" title="信息检索模型"></a>信息检索模型</h1><ul>
<li>基本思想<pre><code>如果一篇文档与一个查询相似，那么该文档与查询相关
</code></pre></li>
<li>相似性<pre><code>- 字符串匹配
- 相似的词汇
- 相同的语义
</code></pre></li>
<li><p>检索模型</p>
<pre><code>- 完全匹配模型／布尔模型(Boolean Model)
    - Unranked boolean model，只有匹配不匹配，没有分数。
    - Ranked boolean model，为文档计算特定分数。
    ○ 优点
        § 简单，效率高
        § 可预测，可解释，对查询严格掌控
    ○ 缺点
        § 一般用户难以构造布尔查询
        § 严格匹配，导致过少或过多的检索结果
        § 很难在 Precision 和 Recall 间得到平衡
    尽管布尔模型不再用作主流文档检索模型，但其思想常用于实现高级(综合)检索功能

- 最佳匹配模型(Best-Match Model)
    衡量一篇文档与 information need 的匹配程度，与完全匹配模型（匹配／不匹配）相比更注重用户体验，不管有没有匹配都会返回文档结果。有很多种方法，但说到底公式其实都和 tf-idf 的公式相似。
    1. 向量空间 Vector space retrieval model(VSM)
        - 思想: 文档与查询都是高维空间中的一个向量
        - 向量空间表示
            - 文档是词语组成的向量
            - 词语是文档组成的向量
            - 查询是词语组成的向量
        - 相似性
            - 用内积衡量
                ○ 缺点
                    § 长文档由于更可能包含匹配词语，因而更可能相关
                    § 然而，如果两篇文档具有同样的相似值，用户更倾向于短文档，短文档更聚焦在用户信息需求上
                    § 相似性计算中应该考虑文档长度(进行规范化)
            - 用夹角来衡量
                - 考虑 tf, idf, doclen
                - tf: 词语出现的频率
                - idf: 区别不同词语的重要性
                - doclen: 对文档长度进行补偿
            - 其他相似性度量
                - Minkowski metric(dissimilarity)
                - Euclidian distance(dissimilarity)
                - Jacquard measure
                - Dice&apos;s coefficient
        ○ 优点
            § 简单有效
        ○ 缺点
            § 过于灵活，自己设置 term weight，确定相似度度量方法，自己设置怎么支持 query-independent weights
    2. 概率理论 Probabilistic retrieval model(BM25)
        - RSJ weight * tf weight * user weight
        ○ 优点
            § 有很强的概率理论支持
            § 在新的环境中参数可以被调整
            § 在大量的 evaluation 中都非常有效
        ○ 缺点
            § 经验调整参数
    3. 统计语言模型 Statistical language model(query likelihood)
        生成两个模型，一个文档的语言模型，一个查询的语言模型，有两种方案来对文档进行排序，
        - query likelihood
            加上 Jelinek-Mercer Smoothing 和 Bayesian Smoothing With Dirichlet Priors
        - KL divergence
    4. 推理网络 Inference networks(Indri)
        - document + smoothing parameter(α,β) -&gt; language model(θ) -&gt; language model vocabulary(r)
</code></pre></li>
<li>Document Priors<pre><code>与 query 无关的用来评估文档价值的 estimates(query independent)，主要有 spam score, PageRank, length of url 等，与上面的算法结合可以综合评估网页的分数。
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/09/06/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Exact-match%20retrieval/">Search Engines笔记 - Exact-match retrieval</a><br><a href="http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/">Search Engines笔记 - Best-Match</a></p>
<h1 id="个性化"><a href="#个性化" class="headerlink" title="个性化"></a>个性化</h1><ul>
<li>基本逻辑：<pre><code>- Representation: 描述用户的兴趣/偏好
- Learning: 从数据中学习兴趣/偏好
- Ranking: 在检索算法中使用兴趣/偏好
</code></pre></li>
<li>主要方法：<pre><code>- 基于话题
    - 根据用户的长期检索历史对用户建模，得到的用户模型是在类别标签上的一个概率分布
    - 对排序列表进行 rerank，top n 的文档是两个分数的组合:
        - 原始文档分数
        - 文档类别和用户兴趣类别的匹配分数
- 长期 vs 短期
    将用户信息分为长期信息、短期信息、长期＋短期信息三组信息，每个特征都分别从这三组信息中分别提取出来，然后训练分类器
- 典型 vs 非典型信息需求
    - 根据用户长期检索历史创建 user profile
    - 判断非典型信息需求
        - 衡量 profile 和 session 的 divergence
          - 从用户历史记录里得到的每个 session feature 的 divergence
          - session 和 historical vocabularies 的 cosine 距离
          - session 和 historical topic categories 的 cosine 距离
    - 提取特征，训练分类器
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/11/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/">Search Engines笔记 - Personalization</a></p>
<h1 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h1><p>一个 query 可能表达了不同的信息需求，相关性模型可能带来的结果是大多文档只能满足同一个信息需求，我们希望检索到的文档是多样性的，能够覆盖到大多数的需求，满足更多用户。</p>
<ul>
<li>算法<pre><code>- Implicit Methods
    query intent 隐含在文档排名中，假设相似的文档涵盖了相似的 intent
    - Maximum Marginal Relevance (MMR)
        - 基于相似度，对文档重排序。
        - 重排序的标准是选择与 query 相似度高的，但和之前已经选出来的文档的相似度低的文档
    - Learning to Rank
        - 相当于 MMR 的有监督版本
        - 和 ListMLE 相同，写下 likelihood function，计算 MLE..
- Explicit Methods
    明确定义了 query intent，对文档重新排序，以便覆盖所有的 query intent
    - xQuAD
        选择涵盖多个 intents 的文档，给需要覆盖的 intents 赋予较高的权重
    - PM-2
        然后选择一个覆盖这个 intent 的文档，如果这个文档能覆盖其它 intents，那么给它加分
</code></pre></li>
<li>评价指标<pre><code>- Precision-IA@k
- α-NDCG
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/">Search Engines笔记 - Diversity</a></p>
<h1 id="联合搜索"><a href="#联合搜索" class="headerlink" title="联合搜索"></a>联合搜索</h1><p>对一个 query，我们可以放到合适的多个垂直数据库里检索，然后合并结果呈现给用户。</p>
<ul>
<li>Resource representation(资源表达)<pre><code>- 获取每个数据库的信息
- 通常用 query-based sampling 方法
</code></pre></li>
<li>Resource selection(资源选择)<pre><code>对资源进行排序，对每个查询选择少量资源进行检索
- 无监督算法
    看作资源排序的问题，估计 P(ri|q)
    - content-based methods
        - Bag-of-words method
            - 对各个资源下 query 和 resource 的相似度 S(q,c) 来对资源进行排序
            - 将一个资源看作一个大的词袋
            - E.g. CORI
            - query 和 resource 的相似度，这并不是我们的初衷
        - Sample documents method
            - 对每个资源采样然后合并得到一个 centralized index，记录每个文档来自哪个 resource
            - E.g. ReDDE
            - 高的召回率和准确率，通常来说效果都大于等于 CORI，尤其是数据库大小分布是 skewed 时
    - query-based methods
        对 query，搜索各个资源下的 query log，找出 match 的 query，然后求当前 query 和历史 query 的相似度 S(q,qpast)，对资源进行排序
- 监督算法
    给定一个查询，选择一个或更多的 verticals
    将问题定义为一个 “one-vs-all” 的分类任务
        - 对每个 vertical 都训练一个分类器
        - 对 “no vertical” (web only) 也训练一个分类器
        - 选择一个/多个具有最高置信度(confidence score)的分类器
</code></pre></li>
<li>Result-merging(结果合并)<pre><code>即对来自不同搜索引擎／数据库对结果进行 merge，产生最终展现给用户的 ranking list。
用所有的 sampled documents 创建一个 index，从选定的资源里检索文档，对每个文档计算 sim (q, d)，同时根据数据库情况计算每个文档的 authority score，最后分数就是 score(d) = f ( sim (q, d), authority (d) )
- Semi-Supervised method
    - 对每个文档我们有两个分数，Sample index (resource neutral) score，Resource i (resource-specific) score
    - 学习 f (resource-specific) = resource-neutral
</code></pre>思路可以用于大规模分布式搜索引擎</li>
</ul>
<p><a href="http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/">Search Engines笔记 - Federated Search</a><br><a href="http://www.shuang0420.com/2016/11/29/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/">Search Engines笔记 - ReDDE Algorithm for Resource Selection</a></p>
<h1 id="Web-检索"><a href="#Web-检索" class="headerlink" title="Web 检索"></a>Web 检索</h1><ul>
<li>Web 检索 = 文档检索 + 针对 Web 检索的新技术</li>
<li>Web 页面采集<br>  爬虫：快速有效地收集尽可能多的有用 Web 页面，包括页面之间的链接结构<pre><code>- Web 爬虫需要具备的特性
    - 健壮 robustness, 避免 spider traps
    - 友好 politeness, 遵守 web server 的采集协议
    - 分布式 distributed, 多台机器分布式采集
    - 可扩展 scalable, 爬虫架构方便扩展
    - 性能与效率，有效利用系统资源
    - 质量 quality, 倾向于采集有用的页面
    - 新颖 freshness, 获取网页的最新版本
    - 可扩充 Extensible, 能够处理新数据类型、新的采集协议等
- Web 页面爬取策略
    - 深度优先
    - 广度优先
    - 实际应用中以广度优先为主，深度优先为辅
- 难点
    - 暗网的采集，只有向数据库提交查询才形成的 Web 页面
    - Web 2.0 内容，脚本语言等生成的动态内容
    - 多媒体内容
</code></pre></li>
<li>Web 页面排序<br>  页面排序值 = 页面内容相关度 +/* 页面重要性</li>
<li>Web 链接分析<pre><code>- 理论基础：Web 页面之间的超链关系非常重要，一条从页面 A 指向页面 B 的链接表明
       - A 与 B 相关
       - A 推荐/引用/投票/赞成 B
- 经典算法
    - PageRank
   - Topic-Sensitive PageRank(TSPR)
   - T-Fresh
   - HITS - Hypertext Induced Topic Selection
</code></pre></li>
<li>基于学习的网页排序<pre><code>Learning to Rank 主要包括三个类别
- Pointwise
    - 训练数据是一个文档类别或分数
    - Accurate score ≠ accurate ranking
    - 忽略文档的位置信息
- Pairwise
    - 训练数据是文档对的一个偏好(一对文档选哪个)
    - Accurate preference ≠ accurate ranking
     - 忽略文档的位置信息
- Listwise
    - 训练数据是文档的排名
    - 难以直接优化 ranking metrics
主流算法有
    - RankSVM
    - RankNet
    - ListNet
Learning to Rank 工具包
    - RankLib
    - people.cs.umass.edu/~vdang/ranklib.html
评价指标
    - WTA(Winners take all)
    - MRR(Mean Reciprocal Rank)
    - MAP(Mean Average Precision)
    - NDCG(Normalized Discounted Cumulative Gain)
    - RC(Rank Correlation)
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/11/04/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/">Search Engines笔记 - Authority Metrics</a><br><a href="http://wdxtub.com/vault/data-analysis-guide.html" target="_blank" rel="external">聚类-小土刀</a><br><a href="http://www.shuang0420.com/2016/06/11/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/">爬虫总结(一)– 爬虫基础 &amp; python实现</a><br><a href="http://www.shuang0420.com/2016/06/12/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%BA%8C-scrapy/">爬虫总结(二)– scrapy</a><br><a href="http://www.shuang0420.com/2016/06/15/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%B8%89-scrapinghub/">爬虫总结(三)– cloud scrapy</a><br><a href="http://www.shuang0420.com/2016/06/17/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E5%9B%9B-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/">爬虫总结(四)– 分布式爬虫</a><br><a href="http://www.shuang0420.com/2016/06/20/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%BA%94-%E5%85%B6%E4%BB%96%E6%8A%80%E5%B7%A7/">爬虫总结(五)– 其他技巧</a><br><a href="http://www.shuang0420.com/2016/06/27/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93--%E6%B1%87%E6%80%BB%E8%B4%B4/">爬虫总结–汇总贴</a></p>
<h1 id="伪相关反馈"><a href="#伪相关反馈" class="headerlink" title="伪相关反馈"></a>伪相关反馈</h1><p>Pseudo Relevance Feedback 自动产生更好的 query。<strong>基本逻辑</strong> 是把原始查询当做分类器，用它来给部分数据打标签，得到 top-ranked documents，然后用 labeled data 来产生更优的 classifier。基本过程：</p>
<ol>
<li>用原始 query 检索文档</li>
<li>取结果的前 N 篇文档作为训练集，这些文档相关度可能不高，然而我们的目的是学习 vocabulary pattern。</li>
<li>应用 relevance feedback algorithm 选取 term 和 term weight</li>
<li>组成新的 query 来检索文档<ul>
<li>Query expansion 平均能使 MAP 提高 20%</li>
<li>但同时也有可能让 1/3 的用户感到 annoy</li>
</ul>
</li>
</ol>
<p>所以通常来说，query expansion 会用在召回率很重要的场景，或者 average performance 很重要的场景，比如 legal retrieval, TREC, research paper 等。</p>
<p><a href="http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/">Search Engines笔记 - Pseudo Relevance Feedback</a></p>
<h1 id="信息检索评价"><a href="#信息检索评价" class="headerlink" title="信息检索评价"></a>信息检索评价</h1><ul>
<li>评价检索模型或搜索引擎的性能<br>  主要考虑搜索质量 vs 搜索效率，这里主要谈搜索质量</li>
<li>评测数据集<pre><code>一般人工构建
构成
    - 文档集合(documents)
    - 信息需求(information needs)及查询集(queries)
    - 相关性判断(relevance judgements)
已有评测集
    - TREC
        § trec.nist.gov
        § 最具影响力
        § 多种信息检索任务，侧重于英文
    - NTCIR
        § research.nii.ac.jp/ntcir/
    - CLEF
        § www.clef-campaign.org
</code></pre></li>
<li>评价指标<pre><code>衡量检索结果与标准答案的一致性
- 对 Unranked Retrieval(非排序检索)的评价
    - Precision and Recall
    - P@n
    - F-Measure
    - Average Results(Micro, Macro)
- 对 Ranked Retrieval(排序结果)的评价
    考虑相关文档在检索结果中的排序位置，考虑在不同 recall levels 的 precision 值
    - AP and MAP
    - MRR (Mean Reciprocal Rank)
    - NDCG (Normalized Discounted Cumulative Gain)
    - RBP (Rank-Biased Precision)
</code></pre></li>
<li><p>评价方法</p>
<pre><code>- Cranfield Methodology
    1. 获得文档(documents)集合
    2. 获得信息需求(information needs)集合
    3. 获得相关性判断(relevance judgments)
    4. 计算(Measure)各种方法找到相关文档的效果
    5. 比较(Compare)各个方法的有效性

- 动态环境下的评价(Interleaved)
 主要用 Interleaved testing 方法。通常输入是两个排序列表，分别由不同方法产生，输出是一个排序列表，由输入的所有文档重新排序产生。
    - Balanced interleaving
        第一次决定从哪个方法开始，然后交替把文档加入 interleaved ranking，如果遇到了已经评估过的文档，就直接 counter++，但是不把文档加进 interleaved ranking 里。
    - Team-draft interleaving
        每一轮都随机产生先取哪种方法，如果有重复，跳过，取下一个。

一般来说，我们更多的会使用 Cranfield，因为
    § Cranfield 更成熟，已经使用了很多年而且易于理解
    § Cranfield 支持大量的 metrics，能提供更多关于 ranking behavior 的信息
    § Cranfield 几乎在所有场景下都使用，而 Interleaving 需要有 query traffic
    § 尽管如此，interleaving 仍然是一个很有用的工具，在 Inexpensive, adaptive, sensitive to small differences 的情景下效果较好
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/09/20/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/">Search Engines笔记 - Evaluating Search Effectiveness</a></p>
<h1 id="搜索日志"><a href="#搜索日志" class="headerlink" title="搜索日志"></a>搜索日志</h1><p>最重要的是两个问题</p>
<ul>
<li>How to represent users<pre><code>1. 从日志中得到 (user,query) pairs
2. 为用户创建 pseudo documents
        - 标题: user id
        - 内容: 每条 query 的前 10 篇文档对应的 Yahoo! 分类目录
3. 计算相似度
        e.g. JS divergence, cosine
</code></pre></li>
<li>Segmenting search logs into sessions<pre><code>- 把 search log 划分为一个个基于信息需求的 dialogue，实际就是用户发出初始查询，搜索引擎给出结果，用户不满意，重新修改查询语句再次搜索，然后得到新的结果，不断循环知道解决问题/放弃检索，这期间的行为就构成了 dialogue
- 划分可以考虑的因素：时间、相同的 term、编辑距离、相同点击、文档类别、分类器等等
</code></pre></li>
</ul>
<p><a href="http://www.shuang0420.com/2016/11/04/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/">Search Engines笔记 - Search logs</a>        </p>
<h1 id="搜索引擎优化-SEO"><a href="#搜索引擎优化-SEO" class="headerlink" title="搜索引擎优化(SEO)"></a>搜索引擎优化(SEO)</h1><p>提高网站或网页在搜索引擎中可见度(排名)的过程</p>
<ul>
<li>基本思想<pre><code>- 基于各类搜索引擎的工作原理(采集、索引、排序等)，对网站进行相关的优化的调整，提高网站在搜索引擎中的排名
- 白帽：合理优化，不牺牲用户体验
- 黑帽：不正当优化，牺牲用户体验
    § 重复、隐藏文字、链接工厂、桥页、跳页
</code></pre></li>
<li>影响排名的因素<pre><code>- 内部因素
    § URL 中出现关键词
    § 网页 Title 中出现关键词
    § 常规内容中出现关键词
    § 在页面的最后一段中出现关键词
    § &lt;Head&gt; 标签 比如 h1, h2 中出现关键词
    § 站内的链接中出现关键词
    § 导向相关内容的导出链接
    § 导出链接中出现关键词
    § 图片文件名中出现关键词
    § Alt 标签中出现关键词
    § comment 中出现关键词
    § 合理的频率更新内容
    § 内容对搜索引擎的展示位置
    § 网站结构循环 PR, 而非散发 PR
    § 关键词进行适当的修饰(加粗、斜体等)
- 外部因素
    § 大量的导入链接
    § 从高 PR 值的网页获得导入链接
    § 从相关内容网站获得导入链接
    § 导入链接指向的网页有具体内容
    § 锚文字中有关键词
    § 锚文字周围有相关词
    § 锚文字存在于文章或句子中
    § 导入链接的时间长度，一般导入链接的存在时间有3-6个月
    § 单向链接的价值高于交换链接
    § 导入链接的页面的导出链接小于 100 个，流出链接越少越好
    § 链接来自不同 IP
    § 合理的导入链接增长频率
</code></pre></li>
<li>SEO 操作<pre><code>- 站外 SEO
    § 网站外部链接优化、网站的链接见识、网站的外部数据分析等
    § 交换、购买链接，提交网址到分类目录等
- 站内 SEO
    § 网站结构的设计、网站代码优化和内部链接优化、网站内容的优化、网站用户体验优化等
</code></pre></li>
<li>建议：丰富网站关键词、主题集中、友好的网页结构、避免重复、有规律的更新等</li>
</ul>
<p><a href="http://wdxtub.com/vault/data-analysis-guide.html" target="_blank" rel="external">聚类-小土刀</a></p>
<p><strong>所有课程笔记:</strong><br><a href="http://www.shuang0420.com/2016/09/06/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Exact-match%20retrieval/">Search Engines笔记 - Exact-match retrieval</a><br><a href="http://www.shuang0420.com/2016/09/11/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Query%20Processing/">Search Engines笔记 - Query Processing</a><br><a href="http://www.shuang0420.com/2016/09/20/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/">Search Engines笔记 - Evaluating Search Effectiveness</a><br><a href="http://www.shuang0420.com/2016/09/25/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Representation/">Search Engines笔记 - Document Representations</a><br><a href="http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/">Search Engines笔记 - Best-Match</a><br><a href="http://www.shuang0420.com/2016/10/02/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Information%20Needs/">Search Engines笔记 - Information Needs</a><br><a href="http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/">Search Engines笔记 - Pseudo Relevance Feedback</a><br><a href="http://www.shuang0420.com/2016/10/14/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Index/">Search Engines笔记 - Index Construction</a><br><a href="http://www.shuang0420.com/2016/10/15/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Cache/">Search Engines笔记 - Cache</a><br><a href="http://www.shuang0420.com/2016/10/25/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/">Search Engines笔记 - Learning to Rank</a><br><a href="http://www.shuang0420.com/2016/11/04/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/">Search Engines笔记 - Search logs</a><br><a href="http://www.shuang0420.com/2016/11/04/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/">Search Engines笔记 - Document Structure</a><br><a href="http://www.shuang0420.com/2016/11/04/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/">Search Engines笔记 - Authority Metrics</a><br><a href="http://www.shuang0420.com/2016/11/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/">Search Engines笔记 - Personalization</a><br><a href="http://www.shuang0420.com/2016/11/29/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/">Search Engines笔记 - ReDDE Algorithm for Resource Selection</a><br><a href="http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/">Search Engines笔记 - Federated Search</a><br><a href="http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/">Search Engines笔记 - Diversity</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[OSX OpenCV for Python2.7/Python3.5 环境配置]]></title>
      <url>http://www.shuang0420.com/2017/02/03/OSX%20OpenCV%20for%20Python2.7:Python3.5%20%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>OpenCV 的环境配置。讲怎样用 conda 为 python 2.7 和 python 3.5 安装 opencv。<br><a id="more"></a></p>
<h1 id="Setup-python3"><a href="#Setup-python3" class="headerlink" title="Setup python3"></a>Setup python3</h1><p>安装<a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda Python 3</a>，因为它自动配置了很多 dependencies，方便我们之后使用。按要求安装好后记得重新开一个 terminal 然后输入 python 验证版本。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">MacBook-Pro-2:~ Shuang$ python</div><div class="line">Python 3.6.0 |Anaconda 4.3.0 (x86_64)| (default, Dec 23 2016, 13:19:00)</div><div class="line">[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure></p>
<h1 id="Install-OpenCV"><a href="#Install-OpenCV" class="headerlink" title="Install OpenCV"></a>Install OpenCV</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ pip install pillow</div><div class="line">...</div><div class="line">Successfully installed olefile-0.44</div><div class="line"></div><div class="line">$ conda install -c menpo opencv3=3.1.0</div><div class="line">Fetching package metadata ...........</div><div class="line">Solving package specifications: .</div><div class="line"></div><div class="line"></div><div class="line">UnsatisfiableError: The following specifications were found to be in conflict:</div><div class="line">  - opencv3 3.1.0* -&gt; python 2.7* -&gt; openssl 1.0.1*</div><div class="line">  - python 3.6*</div><div class="line">Use &quot;conda info &lt;package&gt;&quot; to see the dependencies for each package.</div></pre></td></tr></table></figure>
<p>遇到版本不兼容的问题，尝试了各种各样的方法都不行，最后猜着可能是 python 3.6 版本太高了？于是决定尝试一下 python 3.5，结果居然成功了。。。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ conda create -n python3.5 python=3.5</div><div class="line">$ source activate python3.5</div><div class="line">$ conda install -c menpo opencv3=3.1.0</div></pre></td></tr></table></figure></p>
<p>记得每次都要 source 一下。</p>
<p>测试一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ python3.5</div><div class="line">Python 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12)</div><div class="line">[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">&gt;&gt;&gt; import cv2</div><div class="line">&gt;&gt;&gt; exit()</div></pre></td></tr></table></figure></p>
<p>如法炮制，python 2.7 也成功安装了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ conda create -n python2.7 python=2.7</div><div class="line">$ source activate python2.7</div><div class="line">$ python2.7</div><div class="line">Python 2.7.13 |Anaconda 4.3.0 (x86_64)| (default, Dec 20 2016, 23:05:08)</div><div class="line">[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin</div><div class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</div><div class="line">Anaconda is brought to you by Continuum Analytics.</div><div class="line">Please check out: http://continuum.io/thanks and https://anaconda.org</div><div class="line">&gt;&gt;&gt; import cv2</div><div class="line">&gt;&gt;&gt; exit()</div></pre></td></tr></table></figure></p>
<h1 id="Install-moviepy"><a href="#Install-moviepy" class="headerlink" title="Install moviepy"></a>Install moviepy</h1><p>安装 moviepy 来处理 video，以后要用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip install moviepy</div></pre></td></tr></table></figure></p>
<h1 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h1><p>在 python3 的环境下可以通过 jupyter notebook 来进行交互，运行部分代码并查看结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ jupyter notebook</div></pre></td></tr></table></figure></p>
<p>如何让 jupyter notebook 也能运行 python 2 呢？<br>尝试了网上的方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python2 -m pip install --upgrade ipykernel # install the kernel package for Python 2</div><div class="line">python2 -m ipykernel install # register the Python 2 kernelspec</div></pre></td></tr></table></figure></p>
<p>然而总是报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">Exception:</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/basecommand.py&quot;, line 215, in main</div><div class="line">    status = self.run(options, args)</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/commands/install.py&quot;, line 317, in run</div><div class="line">    prefix=options.prefix_path,</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/req/req_set.py&quot;, line 742, in install</div><div class="line">    **kwargs</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/req/req_install.py&quot;, line 831, in install</div><div class="line">    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/req/req_install.py&quot;, line 1032, in move_wheel_files</div><div class="line">    isolated=self.isolated,</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/wheel.py&quot;, line 247, in move_wheel_files</div><div class="line">    prefix=prefix,</div><div class="line">  File &quot;/Library/Python/2.7/site-packages/pip/locations.py&quot;, line 153, in distutils_scheme</div><div class="line">    i.finalize_options()</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/command/install.py&quot;, line 346, in finalize_options</div><div class="line">    self.create_home_path()</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/command/install.py&quot;, line 565, in create_home_path</div><div class="line">    os.makedirs(path, 0700)</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py&quot;, line 150, in makedirs</div><div class="line">    makedirs(head, mode)</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py&quot;, line 150, in makedirs</div><div class="line">    makedirs(head, mode)</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py&quot;, line 150, in makedirs</div><div class="line">    makedirs(head, mode)</div><div class="line">  File &quot;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py&quot;, line 157, in makedirs</div><div class="line">    mkdir(name, mode)</div><div class="line">OSError: [Errno 13] Permission denied: &apos;/Users/sure/Library/Python/2.7&apos;</div><div class="line">You are using pip version 7.1.2, however version 9.0.1 is available.</div><div class="line">You should consider upgrading via the &apos;pip install --upgrade pip&apos; command.</div></pre></td></tr></table></figure></p>
<p>发现下面这种方法可以成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">conda create -n py2 python=2 anaconda</div><div class="line">source activate py2</div><div class="line">ipython kernel install</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Driverless car </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Spelling, Edit Distance, and Noisy Channels]]></title>
      <url>http://www.shuang0420.com/2017/02/02/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling,%20Edit%20Distance,%20and%20Noisy%20Channels/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。这一篇介绍拼写的检查和更正，主要研究打字者键入的文本，同时这样的算法也可以应用于 OCR 和手写体识别。<br><a id="more"></a></p>
<p>这篇博客要解决的三个问题：</p>
<ul>
<li>Detecting isolated non-words(非词错误检查)<br>如 giraffe 拼写成 graffe</li>
<li>Fixing isolated non-words(孤立词错误改正)<br>把 graffe 更正为 giraffe，但只在孤立的环境中寻找这个词</li>
<li>Fixing errors in context(依赖于上下文的错误检查和更正)<br>真词错误，用上下文来检查和更正拼写错误。如把 I ate desert 改为 I ate dessert</li>
</ul>
<h1 id="拼写错误模式"><a href="#拼写错误模式" class="headerlink" title="拼写错误模式"></a>拼写错误模式</h1><p>Kukich(1992) 把人的打字错误分为两大类：<strong>打字操作错误(typographic error)</strong> 和 <strong>认知错误(cognitive error)</strong>。</p>
<ol>
<li><strong>打字操作错误(typographic error)</strong><br>一般与键盘有关，如 spell 拼成了 speel<br>包括 <strong>插入(insertion)</strong>，<strong>脱落(deletion)</strong>，<strong>替代(substitution)</strong>，<strong>换位(transposition)</strong><br>占所有错误类型的大多数</li>
<li><strong>认知错误(cognitive error)</strong><br>由于写文章的人不知道如何拼写某个单词造成的<ul>
<li><strong>语音错误:</strong> 用语音上等价的字母序列来替代，如 separate 拼成了 seperate</li>
<li><strong>同音词错误:</strong> 如用 piece 来替代 peace</li>
</ul>
</li>
</ol>
<p>所以单词的拼写错误其实有两类，Non-word Errors 和 Real-word Errors。前者指那些拼写错误后的词本身就不合法，如错误的将“giraffe”写成“graffe”；后者指那些拼写错误后的词仍然是合法的情况，如将“there”错误拼写为“three”（形近），将“peace”错误拼写为“piece”（同音），这一篇主要讲 Non-word Errors。</p>
<p><strong>补充：</strong> OCR 错误分为五类：替代、多重替代、空白脱落、空白插入和识别失败</p>
<h1 id="非词错误的检查与更正"><a href="#非词错误的检查与更正" class="headerlink" title="非词错误的检查与更正"></a>非词错误的检查与更正</h1><h2 id="非词错误的检查"><a href="#非词错误的检查" class="headerlink" title="非词错误的检查"></a>非词错误的检查</h2><p>一般有两种方法，一是 <strong>使用词典</strong>，二是 <strong>检查序列</strong></p>
<h3 id="使用词典"><a href="#使用词典" class="headerlink" title="使用词典"></a>使用词典</h3><p>看键入词是否出现在了词典中。用来做拼写错误检查的词典一般还要包括形态分析模式，来表示能产性的屈折变换和派生词。词典通常是哈希表，用 integer 代替 string，来提高 performance。</p>
<h3 id="检查序列"><a href="#检查序列" class="headerlink" title="检查序列"></a>检查序列</h3><p>这个方法是自己概括的。类似于 “letter combination” 的思想。截取单词的部分字母，来检查这个字母序列在词典中出现的频率如何，是不是根本不会出现这种排列组合，如 “xy” 这个序列就基本不会出现在单词中，所以判断这个词是错误的。然而截取的长度很难定义，而且也需要使用词典。</p>
<h2 id="非词错误改正"><a href="#非词错误改正" class="headerlink" title="非词错误改正"></a>非词错误改正</h2><p>查找词典中与 error 最近似的词，常见的方法有 Shortest weighted edit distance 和 Highest noisy channel probability。</p>
<h3 id="编辑距离-edit-distance"><a href="#编辑距离-edit-distance" class="headerlink" title="编辑距离(edit distance)"></a>编辑距离(edit distance)</h3><p>编辑距离，顾名思义，把一个符号串转换为另一个符号串所需的最小编辑操作的次数(how many letter changes to map A to B)。Leetcode 上有相应的题目。</p>
<p>编辑距离的 4 种转换方式</p>
<ul>
<li>插入(insertion)</li>
<li>脱落(deletion)</li>
<li>替代(substitution)</li>
<li>换位(transposition)</li>
</ul>
<p>示例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">• Substitutions</div><div class="line">– E X A M P E L</div><div class="line">– E X A M P L E 2 substitutions</div><div class="line">• Insertions</div><div class="line">– E X A P L E</div><div class="line">– E X A M P L E 1 insertion</div><div class="line">• Deletions</div><div class="line">– E X A M M P L E</div><div class="line">– E X A _ M P L E 1 deletion</div></pre></td></tr></table></figure></p>
<p>下图表示如何从 intention 变换到 execution。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/ed.jpg" class="ful-image" alt="ed.jpg"></p>
<p>由上图可知 intention 变换到 execution 的 Levenshtein 距离是 5。</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>最小编辑距离的算法。用动态规划(dynamic programming)来解决。<br>只有插入(insertion)/脱落(deletion)/替代(substitution)三种操作的 Levenshtein 距离：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/ld.jpg" class="ful-image" alt="ld.jpg"></p>
<p>加上 transposition 的 Levenshtein 距离：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/ld2.jpg" class="ful-image" alt="ld2.jpg"></p>
<h5 id="Levenshtein-Distance"><a href="#Levenshtein-Distance" class="headerlink" title="Levenshtein Distance"></a>Levenshtein Distance</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">function LevenshteinDistance(char s[1..m], char t[1..n]):</div><div class="line">  // for all i and j, d[i,j] will hold the Levenshtein distance between</div><div class="line">  // the first i characters of s and the first j characters of t</div><div class="line">  // note that d has (m+1)*(n+1) values</div><div class="line">  declare int d[0..m, 0..n]</div><div class="line"></div><div class="line">  set each element in d to zero</div><div class="line"></div><div class="line">  // source prefixes can be transformed into empty string by</div><div class="line">  // dropping all characters</div><div class="line">  for i from 1 to m:</div><div class="line">      d[i, 0] := i</div><div class="line"></div><div class="line">  // target prefixes can be reached from empty source prefix</div><div class="line">  // by inserting every character</div><div class="line">  for j from 1 to n:</div><div class="line">      d[0, j] := j</div><div class="line"></div><div class="line">  for j from 1 to n:</div><div class="line">      for i from 1 to m:</div><div class="line">          if s[i] = t[j]:</div><div class="line">            substitutionCost := 0</div><div class="line">          else:</div><div class="line">            substitutionCost := 1</div><div class="line">          d[i, j] := minimum(d[i-1, j] + 1,                   // deletion</div><div class="line">                             d[i, j-1] + 1,                   // insertion</div><div class="line">                             d[i-1, j-1] + substitutionCost)  // substitution</div><div class="line"></div><div class="line">  return d[m, n]</div></pre></td></tr></table></figure>
<p>intention 变换到 execution 的最小编辑距离。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/alg.eg.jpg" class="ful-image" alt="alg.eg.jpg"></p>
<h5 id="Damerau-Levenshtein-DL-distance"><a href="#Damerau-Levenshtein-DL-distance" class="headerlink" title="Damerau-Levenshtein(DL) distance"></a>Damerau-Levenshtein(DL) distance</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">algorithm DL-distance is</div><div class="line">    input: strings a[1..length(a)], b[1..length(b)]</div><div class="line">    output: distance, integer</div><div class="line"></div><div class="line">    da := new array of |Σ| integers</div><div class="line">    for i := 1 to |Σ| inclusive do</div><div class="line">        da[i] := 0</div><div class="line"></div><div class="line">    let d[−1..length(a), −1..length(b)] be a 2-d array of integers, dimensions length(a)+2, length(b)+2</div><div class="line">    // note that d has indices starting at −1, while a, b and da are one-indexed.</div><div class="line"></div><div class="line">    maxdist := length(a) + length(b)</div><div class="line">    d[−1, −1] := maxdist</div><div class="line">    for i := 0 to length(a) inclusive do</div><div class="line">        d[i, −1] := maxdist</div><div class="line">        d[i, 0] := i</div><div class="line">    for j := 0 to length(b) inclusive do</div><div class="line">        d[−1, j] := maxdist</div><div class="line">        d[0, j] := j</div><div class="line"></div><div class="line">    for i := 1 to length(a) inclusive do</div><div class="line">        db := 0</div><div class="line">        for j := 1 to length(b) inclusive do</div><div class="line">            k := da[b[j]]</div><div class="line">            ℓ := db</div><div class="line">            if a[i] = b[j] then</div><div class="line">                cost := 0</div><div class="line">                db := j</div><div class="line">            else</div><div class="line">                cost := 1</div><div class="line">            d[i, j] := minimum(d[i−1, j−1] + cost,  //substitution</div><div class="line">                               d[i,   j−1] + 1,     //insertion</div><div class="line">                               d[i−1, j  ] + 1,     //deletion</div><div class="line">                               d[k−1, ℓ−1] + (i−k−1) + 1 + (j-ℓ−1)) //transposition</div><div class="line">        da[a[i]] := i</div><div class="line">    return d[length(a), length(b)]</div></pre></td></tr></table></figure>
<h5 id="Optimal-String-Alignment-OSA-distance"><a href="#Optimal-String-Alignment-OSA-distance" class="headerlink" title="Optimal String Alignment(OSA) distance"></a>Optimal String Alignment(OSA) distance</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">algorithm OSA-distance is</div><div class="line">    input: strings a[1..length(a)], b[1..length(b)]</div><div class="line">    output: distance, integer</div><div class="line"></div><div class="line">    let d[0..length(a), 0..length(b)] be a 2-d array of integers, dimensions length(a)+1, length(b)+1</div><div class="line">    // note that d is zero-indexed, while a and b are one-indexed.</div><div class="line"></div><div class="line">    for i := 0 to length(a) inclusive do</div><div class="line">        d[i, 0] := i</div><div class="line">    for j := 0 to length(b) inclusive do</div><div class="line">        d[0, j] := j</div><div class="line"></div><div class="line">    for i := 1 to length(a) inclusive do</div><div class="line">        for j := 1 to length(b) inclusive do</div><div class="line">            if a[i] = b[j] then</div><div class="line">                cost := 0</div><div class="line">            else</div><div class="line">                cost := 1</div><div class="line">            d[i, j] := minimum(d[i-1, j  ] + 1,     // deletion</div><div class="line">                               d[i, j-1] + 1,     // insertion</div><div class="line">                               d[i-1, j-1] + cost)  // substitution</div><div class="line">            if i &gt; 1 and j &gt; 1 and a[i] = b[j-1] and a[i-1] = b[j] then</div><div class="line">                d[i, j] := minimum(d[i, j],</div><div class="line">                                   d[i-2, j-2] + cost)  // transposition</div><div class="line">    return d[length(a), length(b)]</div></pre></td></tr></table></figure>
<p>与 唯一的差别就是多了下面几行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if i &gt; 1 and j &gt; 1 and a[i] = b[j-1] and a[i-1] = b[j] then</div><div class="line">    d[i, j] := minimum(d[i, j],</div><div class="line">                       d[i-2, j-2] + cost)  // transposition</div></pre></td></tr></table></figure></p>
<h3 id="噪声信道模型-Noisy-Channel-Model"><a href="#噪声信道模型-Noisy-Channel-Model" class="headerlink" title="噪声信道模型(Noisy Channel Model)"></a>噪声信道模型(Noisy Channel Model)</h3><p>Noisy Channel Model 即噪声信道模型，或称信源信道模型，这是一个普适性的模型，被用于 <strong>语音识别、拼写纠错、机器翻译、中文分词、词性标注、音字转换</strong> 等众多应用领域。噪声信道模型本身是一个贝叶斯推理的特殊情况。</p>
<p>其形式很简单，如下图所示：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/noisyChannel.jpg" class="ful-image" alt="noisyChannel.jpg"></p>
<p>应用于拼写纠错任务的流程如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/noisePro.jpg" class="ful-image" alt="noisePro.jpg"></p>
<p>noisy word（即 spelling error）被看作 original word 通过 noisy channel 转换得到。由于在信道中有噪声，我们很难辨认词汇形式的真实单词的面目。我们的目的就是建立一个信道模型，使得能够计算出这个真实单词是如何被噪声改变面目的，从而恢复它的本来面目。噪声就是给正确的拼写戴上假面具的拼写错误，它有很多来源：发音变异、音子实现时的变异以及来自信道的声学方面的变异（扩音器、电话网络等）。</p>
<p>无论单词 separate 是怎样错误拼写了，我们只想把它识别为 separate。也就是，给定 observation，我们的任务是确定这个 observation 属于哪个类别的集合。所以，我们考虑一切可能的类，也就是一切可能的单词，在这些单词中，我们只想选择那些最有可能给出已有的 observation 的单词。也就是，在词汇 V 的所有单词中，我们只想使得 P(Word|Observation)最大的那个单词，也就是我们对单词 W 的正确估计就是 argmaxP(W|O)</p>
<p>现在已知 noisy word（用 O 表示）如何求得最大可能的 original word（用 W 表示），公式如下：</p>
<p>$$<br>  \begin{aligned}<br>  argmax_{w \in V} P(W|O) &amp; = argmax {P(W)P(O|W) \over P(O)}  \ \ \ (Bayes \ Rule) \\<br>  &amp; = argmax P(W) * P(O|W) \ \ \ (denom \ is \ constant) \\<br>  \end{aligned}<br>$$</p>
<p>看一下留下的两个 factor：</p>
<ul>
<li>P(W): prior, language model<br>how likely the word is going to be a word</li>
<li>P(O|W): likelihood, channel model/error model<br>if it was that word, how likely is that generates this exact error, models the correct word into a misspelled word</li>
</ul>
<p>Bayes 方法应用于拼写的算法分两个步骤：</p>
<ol>
<li>提出候选更正表(proposing candidate correction)<br>怎么产生候选更正表？可以采用编辑距离产生下面两种 words set<ul>
<li>相似拼写(words with similar spelling)</li>
<li>相似发音(words with similar pronunciation)<br>事实上，80%的错误单词与正确单词的编辑距离是 1，而几乎所有的情况下编辑距离都小于 2</li>
</ul>
</li>
<li>对候选进行打分(scoring the candidate)<br>用上面的 bayes 公式</li>
</ol>
<h4 id="Generate-candidate-words"><a href="#Generate-candidate-words" class="headerlink" title="Generate candidate words"></a>Generate candidate words</h4><p>举个例子，给定拼写错误“acress”，首先通过词典匹配容易确定为 “Non-word spelling error”；然后通过计算最小编辑距离获取最相似的 candidate correction。下面是通过 insertion, deletion, substitution, transposition 四种操作转化产生且编辑距离为 1 的 candidate words。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/acress.jpg" class="ful-image" alt="acress.jpg"></p>
<p>此时，我们希望选择概率最大的 W 作为最终的拼写建议，基于噪声信道模型思想，需要进一步计算 P(W) 和 P(O|W)。</p>
<h4 id="Language-model-probability"><a href="#Language-model-probability" class="headerlink" title="Language model probability"></a>Language model probability</h4><p>通过对语料库计数、平滑等处理可以很容易建立语言模型，即可得到 P(w)，如下表所示，计算 Unigram Prior Probability（word 总数：404,253,213）<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/lm.jpg" class="ful-image" alt="lm.jpg"></p>
<h4 id="Channel-model-probability"><a href="#Channel-model-probability" class="headerlink" title="Channel model probability"></a>Channel model probability</h4><p>$P(O|W)=probability \ of \ the \ edit$</p>
<p>P(O|W)的精确计算至今还是一个没有解决的课题，我们可以进行简单的估算，用 confusion matrix，confusion matrix 是一个 26*26 的矩阵，表示一个字母被另一个字母错误替代的次数，有 4 种 confusion matrix(因为有四种错误)。</p>
<p>基于大量<misspelled word="" x="x1" x2="" x3="" ...="" xm,="" correct="" w="w1" w2="" w3="" wn="">pair 计算 del、ins、sub 和 trans 四种转移矩阵，然后求得转移概率 P(O|W)，这里用 P(x|w) 表示:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/ts.jpg" class="ful-image" alt="ts.jpg"></misspelled></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/cm.jpg" class="ful-image" alt="cm.jpg">
<h4 id="Calculation"><a href="#Calculation" class="headerlink" title="Calculation"></a>Calculation</h4><p>计算P(“acress”|w)如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/pacress.jpg" class="ful-image" alt="pacress.jpg"></p>
<p>计算P(w)P(“acress”|w)如下：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/acressF.jpg" class="ful-image" alt="acressF.jpg"></p>
<p>“across”相比其他 candidate 可能性更大。</p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>一些测试集：<br>•  Wikipedia’s list of common English misspelling<br>•  Aspell filtered version of that list<br>•  Birkbeck spelling error corpus<br>•  Peter Norvig’s list of errors (includes Wikipedia and Birkbeck, for training<br>or tes/ng)</p>
<h4 id="Other-application"><a href="#Other-application" class="headerlink" title="Other application"></a>Other application</h4><p>噪声信道模型：Y $\rightarrow$ Channel $\rightarrow$ X<br>看一下其他应用：<br>在 POS tag 里，Y 就是 POS tag 序列，X 就是单词序列。<br>在机器翻译里，如 L1 翻译成 L2，那么 Y 就是 L2， X 就是 L1，P(Y)就是 language model，P(X|Y) 就是 channel model。</p>
<h1 id="真词错误的检查和更正"><a href="#真词错误的检查和更正" class="headerlink" title="真词错误的检查和更正"></a>真词错误的检查和更正</h1><p>25%-40% 的错误是真词(Real-word)，比如说 Can they <strong>lave</strong> him my messages? / The study was conducted mainly <strong>be</strong> John Black. 这类错误。真词错误的检查和更正往往依赖于上下文。</p>
<p><strong>真词(Real-word)的检查和纠正:</strong><br><strong>Detection：</strong> 每个 word 都作为 spelling error candidate。<br><strong>Correction：</strong> 从发音和拼写等角度，查找与每个 word 最近似的 words 集合作为拼写建议，常见的方法有 Highest noisy channel probability 和 classifier</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/realWord.jpg" class="ful-image" alt="realWord.jpg">
<p>对一个句子中的每个单词，都选出与之编辑距离为 1 的所有单词作为候选单词(包括原单词本身)，也就是说一个句子 N 个单词，就有 N 个 candidate set，然后从每个单词 set 里各取出一个单词组成一个句子，求 P(W) 最大的单词序列<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/rw.jpg" class="ful-image" alt="rw.jpg"></p>
<p>简化版，就是在所有 candidate words 里，每次只选出一个单词，与其它原词组成句子，然后同样求 P(W) 最大的单词序列。</p>
<p>真词纠正和非词纠正的逻辑相同，都有 language model 概率和 channel model 概率，不同的是对真词纠正，channel model 的概率包含了 p(w|w) 也就是完全没有错误的概率。</p>
<h1 id="拼写错误更正系统"><a href="#拼写错误更正系统" class="headerlink" title="拼写错误更正系统"></a>拼写错误更正系统</h1><p>为了使人机交互(HCI)的体验更加友好，我们可以根据拼写检查的 confidence 来决定对其进行哪种操作</p>
<ul>
<li>Very confident<br>直接改正(autocorrect)</li>
<li>Less confident<br>给出最佳的更正单词(best correction)</li>
<li>Less confident<br>给出更正列表(correction list)</li>
<li>Unconfident<br>给出错误提示(flag)，就像 MS word 里错误单词下面的红线提示</li>
</ul>
<h2 id="Phone’c-error-model"><a href="#Phone’c-error-model" class="headerlink" title="Phone’c error model"></a>Phone’c error model</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Spelling%2C%20Edit%20Distance%2C%20and%20Noisy%20Channels/pem.jpg" class="ful-image" alt="pem.jpg">
<h2 id="噪声信道模型的改进"><a href="#噪声信道模型的改进" class="headerlink" title="噪声信道模型的改进"></a>噪声信道模型的改进</h2><p>可以改进/思考的方向：</p>
<ul>
<li><strong>language model:</strong> 上文用的是 unigram，实际应用中当然是 bigram/trigram 效果会更好</li>
<li><strong>channel model:</strong> 是否要考虑编辑距离大于 1 的情况</li>
<li><strong>unseen words:</strong> 关于未登录词，尤其是层出不穷的新动词/名词，我们怎么处理。一般还是检查单词序列/组合的概率分布</li>
<li><strong>classifier:</strong> 考虑分类器，综合各种特征</li>
</ul>
<p>在实际应用中，我们并不会直接把 prior 和 error model probability 相乘，因为我们不能作出独立性假设，所以，我们会用权重来计算：<br>$$\hat w = argmax_{w \in V} P(o|w)P(w)^{\lambda}$$</p>
<p>通常从训练集里学习 $\lambda$ 参数。</p>
<p>有其他的方法对噪声信道模型进行改进，如允许更多的编辑(ph → f, le → al, etc.)，把发音特征加入信道等。另外，也可以把 channel model 和 language model 当做特征，并加入其他特征，来训练分类器来进行拼写错误的改正。</p>
<p>根据可能影响 p(misspelling|word) 的因素来提取特征：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">• The source letter</div><div class="line">• The target letter</div><div class="line">• Surrounding letters</div><div class="line">• The position in the word</div><div class="line">• Nearby keys on the keyboard</div><div class="line">• Homology on the keyboard</div><div class="line">• Pronunciations</div><div class="line">• Likely morpheme transformations</div></pre></td></tr></table></figure></p>
<blockquote>
<p>参考链接<br><a href="http://52opencourse.com/138/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%94%E8%AF%BE-%E6%8B%BC%E5%86%99%E7%BA%A0%E9%94%99%EF%BC%88spelling-correction%EF%BC%89" target="_blank" rel="external">斯坦福大学自然语言处理第五课“拼写纠错（Spelling Correction）”</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP 笔记 - Words, morphology, and lexicons]]></title>
      <url>http://www.shuang0420.com/2017/02/01/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words,%20morphology,%20and%20lexicons/</url>
      <content type="html"><![CDATA[<p>CMU 11611 的课程笔记。<br><a id="more"></a></p>
<h1 id="Morphology-形态学"><a href="#Morphology-形态学" class="headerlink" title="Morphology(形态学)"></a>Morphology(形态学)</h1><p><strong>关键概念：</strong> words are not atoms<br>单词不是原子，它是由 morphemes(语素) 构成的。如 misunderstandings，我们可以将其分解为 mis-understand-ing-s。</p>
<h2 id="morphemes-语素"><a href="#morphemes-语素" class="headerlink" title="morphemes(语素)"></a>morphemes(语素)</h2><p><strong>语素的种类:</strong></p>
<ul>
<li>Roots(词根)<br>一个单词最核心的语素，代表着这个单词最主要的含义。或者把它称为 stem(词干)？</li>
<li>Affixes(词缀)<ul>
<li>Prefixes(前缀)<br>•  pre-nuptual, ir-regular</li>
<li>Suffixes(后缀)<br>•  determin-ize, iterat-or</li>
<li>Infixes(中缀)<br>•  Pennsyl-f**kin-vanian</li>
<li>Circumfixes(位缀)<br>•  ge-sammel-t</li>
</ul>
</li>
</ul>
<p><strong>concatenative morphology(毗邻性语素)</strong> 主要指前缀和后缀这一类语素，词是由一定数目的语素毗邻在一起而组成的。<br><strong>nonconcatenative morphology(非毗邻性语素)</strong></p>
<ul>
<li>Umlaut<br>  foot : feet :: tooth : teeth</li>
<li>Ablaut<br>  sing, sang, sung</li>
<li>Root-and-pattern(词根与模式语素) or templatic morphology(模板语素)<br>通常在阿拉伯语和其它闪美特语系中(Eg. Arabic, Hebrew, Afroasiatic languages)。如在希伯来语中，动词通常由词根和模板组成，词根又通常由3个辅音组成。</li>
<li>Infixation</li>
</ul>
<h2 id="words-词"><a href="#words-词" class="headerlink" title="words(词)"></a>words(词)</h2><p>从语素构成单词的方法主要有两大类(可能部分交叉)：inflection(屈折)和 derivation(派生)。</p>
<p><strong>Inflectional morphology(屈折形态学)</strong></p>
<ul>
<li>屈折把词干(stem)和一个语法语素(grammatical morpheme)结合起来，形成的单词一般和原来的词干术语同一个词类(word class)，还会产生诸如“一致关系”之类的句法功能。</li>
<li>Examples<br>•  Number (singular versus plural) automaton → automata<br>•  Case (nomina:ve versus accusa:ve versus…) he, him, his, …</li>
</ul>
<p><strong>Derivational morphology(派生形态学)</strong></p>
<ul>
<li>派生把词干(stem)和一个词缀(suffixes/affixes/infixes)结合起来，但是形成的单词一般属于不同的词类(word class)，具有不同的含义(meaning)。</li>
<li>Examples<br>•  parse → parser<br>•  repulse → repulsive</li>
</ul>
<p><strong>补充：</strong><br>英语的名词通常只有两种屈折变化：一个词缀表示复数(plural)，一个词缀表示领属(possessive)。动词的屈折变化稍为复杂。英文动词有三种：主要动词(如 eat, sleep)，情态动词(如 can, will, should) 和基础动词(如 be, have, do)。主要动词还分规则动词和不规则动词，不同种类的变换各有不同。详见《自然语言处理综论》by Daniel, James P39。<br>英语的屈折比其它语言相对简单，但英语的派生却相当复杂。名词可以由动词或形容词变换得到，形容词也可以从名词和动词派生。</p>
<h2 id="Final-State-Automaton-有限状态自动机"><a href="#Final-State-Automaton-有限状态自动机" class="headerlink" title="Final-State Automaton(有限状态自动机)"></a>Final-State Automaton(有限状态自动机)</h2><p><strong>关键概念: 形式语言(formal language)</strong> 是一个模型，这个模型能够而且只能够生成或识别满足形式语言定义所要求的某一形式语言的符号串。而 <strong>自然语言(Natural language)</strong> 是现实中人们所说的语言，两者可能完全不同，然而我们通常使用形式语言来模拟自然语言的某些部分。</p>
<p><strong>有限状态自动机(FSA)</strong> 是解决形态学(Morphology)问题的主要方法。可以用 FSA 识别的语言我们称为 regular language。</p>
<p><strong>正则表达式 vs 有限状态自动机:</strong></p>
<ul>
<li>正则表达式是描述有限状态自动机的一种方法</li>
<li>任何正则表达式都可以用有限状态自动机来实现</li>
<li>任何有限状态自动机都可以用正则表达式来描述</li>
<li>两者彼此对称</li>
</ul>
<p><strong>FSA 模型：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSAmodel.jpg" class="ful-image" alt="FSAmodel.jpg"></p>
<p>一个经典例子是用 FSA 来识别羊的语言。我们把羊的语言定义为由下面的(无限)集合构成的任何字符串:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">baa!</div><div class="line">baaa!</div><div class="line">baaaa!</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>描述这种羊的语言的正则表达式是/baa+!/，下图就是模拟这种正则表达式的一个自动机(automaton)。这是一个有向图，包括点(或结点)的有限集合和两个点之间的有向连接的弧的集合。圆圈表示点，箭头表示弧，这样一个自动机有 5 个状态，状态 0 是初始状态(start state)，用进入的箭头表示；状态 4 是最后状态(final state) 或接收状态(accepting state)，用双圈来表示，另外还有 4 个转移(transition)，用弧来表示。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSAeg.jpg" class="ful-image" alt="FSAeg.jpg"></p>
<p>自动机从 q0 开始，反复进行如下过程：</p>
<ul>
<li>寻找输入的下一个字母，如果与自动机中离开当前状态的弧相匹配，那么就穿过这个弧，移动到下一个状态</li>
<li>如果输入的字母已经读完，那么进入接收状态(q4)，自动机就成功识别了输入。如果自动机总不能进入最后状态，或者输入已经读完，又或者某些输入与自动机的弧不匹配，或是自动机在某个非最后状态停住了，我们就说，自动机拒绝(reject)输入。</li>
</ul>
<p><strong>算法：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSAalg.jpg" class="ful-image" alt="FSAalg.jpg"></p>
<p><strong>其它例子：</strong><br><strong>FSA for English Nouns:</strong></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/noun.jpg" class="ful-image" alt="noun.jpg">
<p><strong>FSA for English Adjective:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/adj.jpg" class="ful-image" alt="adj.jpg"></p>
<p><strong>FSA for English Deriva:onal Morphology:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/der.jpg" class="ful-image" alt="der.jpg"></p>
<h2 id="Morphological-Parsing-形态剖析"><a href="#Morphological-Parsing-形态剖析" class="headerlink" title="Morphological Parsing(形态剖析)"></a>Morphological Parsing(形态剖析)</h2><p>主要有三种方法</p>
<ul>
<li>Table</li>
<li>Trie</li>
<li>Final-state transducer</li>
</ul>
<p>这里主要介绍第三种:有限状态转录机</p>
<h3 id="Final-State-Transducers-FST"><a href="#Final-State-Transducers-FST" class="headerlink" title="Final State Transducers(FST)"></a>Final State Transducers(FST)</h3><p><strong>FST 模型:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSTmodel.jpg" class="ful-image" alt="FSTmodel.jpg"></p>
<p><strong>FSA vs FST:</strong><br><strong>FSA</strong> 主要是来表达正则语言，主要作用是 <strong>识别(recognize)</strong> 语言；而 <strong>FST</strong> 既能够 <strong>识别(recognize)</strong> 语言，也能够 <strong>产生(generates)</strong> 语言，它可以剖析(parse)输入，或者把输入转化(transform)成另一种表达方式。</p>
<p>举个例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: a word</div><div class="line">Output: the word’s stem(s) and features expressed by other morphemes.</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSTeg.jpg" class="ful-image" alt="FSTeg.jpg">
<p>第二列输出包含了词干和有关的形态特征(feature)，这些特征说明了附加在词干上的有关信息。如 +N 这个特征表示这个词是名词，+SG 表示单数，+PL 表示复数。</p>
<p>输入 cats，经过形态剖析后可以得到输出 cat+N+PL，这样我们就知道 cat 是一个复数名词。我们使用的是 <strong>双层形态学(two-level morphology)</strong> 的方法来进行的形态剖析。把一个词表示为 <strong>词汇层(lexical level)</strong> 和 <strong>表层(surface level)</strong> 之间的对应，<strong>词汇层</strong> 表示组成该词的语素之间的简单毗邻关系，<strong>表层</strong> 表示该层实际拼写的最终情况。形态剖析要建立 <strong>映射规则</strong>，把在表层上的字幕序列(如 cats) 映射为词汇层上的语素和特征的序列(cat+N+PL)，两个层之间的映射的自动机就是 <strong>有限状态转录机(Final State Transducers)</strong>。有限状态转录机通过有限自动机来实现这种转录，因此我们通常把 FST 看成具有两层的 FSA，FST 具有比 FSA 更多的功能；FSA 通过确定符号集合来定义/识别形式语言，而 FST 则定义符号串之间的关系，这样就可以从另一个角度把 FST 看成是读一个符号串并生成另一个符号串的机器。</p>
<p>可以通过 4 个角度来看 FST:</p>
<ul>
<li>作为识别器(recognizer)<br>符号串的偶对作为输入和输出，如果该符号串偶对也在语言的符号串偶对(pair)中就接收，否则拒绝</li>
<li>作为生成器(generator)<br>输出 yes 或 no 以及输出符号串的偶对</li>
<li>作为翻译器(translator)<br>读一个符号串，输出另一个符号串</li>
<li>作为关联器(relater)<br>计算两个集合之间的关系</li>
</ul>
<p><strong>其它例子：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSTeg2.jpg" class="ful-image" alt="FSTeg2.jpg"></p>
<p><strong>补充:</strong><br>为了建立一个形态剖析器，至少需要：</p>
<ul>
<li>词表(lexicon)：词干和词缀表及其基本信息(如一个词干是名词词干还是动词词干等)</li>
<li>形态顺序规则(morphotactics)：关于形态顺序的模型，解释在一个词内什么样的语素跟在什么样的语素后面。如英语表示复数的语素要跟在名词后面而不是前面</li>
<li>正词法规则(orthographic rule)：当两个语素结合时在拼写上发生什么变换。如 y-&gt;ie。</li>
</ul>
<p>有很多 FST 的工具包，可以 compile &amp; rewrite FST 的规则，也可以将不同规则进行合并。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/NLP%20%E7%AC%94%E8%AE%B0%20-%20Words%2C%20morphology%2C%20and%20lexicons/FSTcom.jpg" class="ful-image" alt="FSTcom.jpg"></p>
<h2 id="Stemming"><a href="#Stemming" class="headerlink" title="Stemming"></a>Stemming</h2><p>讲了无数遍的概念，就不展开了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: a word</div><div class="line">Output: the word’s stem (approximately)</div></pre></td></tr></table></figure></p>
<h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><p>讲了无数遍的概念，就不展开了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: raw text</div><div class="line">Output: sequence of tokens normalized for easier processing.</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> CMU 11611 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推荐系统--开坑]]></title>
      <url>http://www.shuang0420.com/2017/01/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/</url>
      <content type="html"><![CDATA[<p>主要介绍推荐系统分析框架、应用场景以及评测方法等。<br><a id="more"></a><br>之前做了个项目 App Recommender System，以为对推荐系统也了解了不少，结果在面试的时候第一次被问到时才发现之前做的并没能没有考虑到商业场景，委实太过于小打小闹。所谓知耻而后勇，就开了这个坑，打算系统学习下 Recommender System 这门课。</p>
<p>以一本通俗入门的书《推荐系统实践》by 项亮开始，以 University of Minnesota 的专项系列课程 Master Recommender Systems 为辅，来学习这个 topic。这一系列笔记仅供学习使用，文字/理念/图片均有可能来自以上两个来源。</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>需要区分的是 <strong>信息检索(Information Retrieval)</strong> 和 <strong>信息过滤(Information Filtering)</strong> 两个概念。</p>
<p><strong>信息检索</strong> 针对的是 static content base + dynamic information need，通常使用的方法是 tfidf。<strong>信息过滤</strong> 则相反，针对的是 static information need + dynamic content base，主要的方法是对用户需求建模。推荐系统其实就是信息过滤的应用。搜索引擎需要用户主动提供准确的关键词来寻找信息，而推荐系统不需要这种明确需求，直接通过用户的历史行为给用户的兴趣建模，从而主动给用户推荐能满足用户兴趣和需求的信息，提高网站的点击率和转化率。</p>
<p>另外，推荐系统可以帮助发现物品的“长尾”。事实上，主流商品代表大多数用户的需求，而长尾商品则代表着小部分用户的个性化需求，后者才是更为重要的。以电商为例，如果只推荐主流产品，那么会产生大量长尾商品的库存积压，用户也不会感到惊喜或者满意。</p>
<p><strong>推荐系统的三个主要组成部分：</strong></p>
<ul>
<li>前台展示页面</li>
<li>后台日志系统</li>
<li>推荐算法</li>
</ul>
<p><strong>推荐系统的三个参与方：</strong></p>
<ul>
<li>用户</li>
<li>物品提供者</li>
<li>提供推荐系统的网站</li>
</ul>
<h1 id="推荐系统分析框架-Analytical-Framework-of-Recommend-System"><a href="#推荐系统分析框架-Analytical-Framework-of-Recommend-System" class="headerlink" title="推荐系统分析框架 Analytical Framework of Recommend System"></a>推荐系统分析框架 Analytical Framework of Recommend System</h1><p>看一下推荐系统分析框架的 8 要素。</p>
<ul>
<li>domain(推荐领域）<br>如已经购买过的东西</li>
<li>purpose（推荐目的）<br>如让用户再次购买；add-on sales</li>
<li>context（推荐背景）<br>推荐活动发生的一些情况和限制。如随意浏览或者是为了购买某件商品而浏览</li>
<li>whose opinions（推荐者）<br>如用户的购买记录；其它购物者</li>
<li>personalization level（个性化或定制化层次）<ol>
<li>Non-personalized recommend，像微博里列出的最热门的新闻、事件。它并不关注你是否对此感兴趣。</li>
<li>基于统计的有目标群体的推荐 Demographic，就好像买尿布的外国奶爸们顺手买的酒。</li>
<li>只针对你当前活动而作出的推荐 Ephemeral，标准格式“喜欢这个X的人们也喜欢……”。</li>
<li>分析长期记录得到的推荐 Consistent，如根据你的以往的消费记录，给你推荐一些物品。</li>
</ol>
</li>
<li>privacy and trustworthiness（隐私性和可信度）<ol>
<li>隐私性。这是不是我们希望网站拥有的数据</li>
<li>可信度。考虑会不会有内在的偏见，会不会有恶意的、非真实的操作等。</li>
</ol>
</li>
<li>interface（接口）<br>考虑输入输出。<ol>
<li>输入分为 Explicit（Rating， Review， Vote， etc.）和 Implicit (Click， Purchase， Follow， etc.)</li>
<li>输出分为预测和推荐两种，预测是得到一个特定的评分结果，推荐是得到一堆推荐的事物。</li>
</ol>
</li>
<li>algorithms（推荐算法）<br>如 profitable products；product association</li>
</ul>
<h1 id="推荐系统的应用"><a href="#推荐系统的应用" class="headerlink" title="推荐系统的应用"></a>推荐系统的应用</h1><h2 id="电子商务"><a href="#电子商务" class="headerlink" title="电子商务"></a>电子商务</h2><p><strong>代表：</strong> 亚马逊。Amazon 被 RWW 称为“推荐系统之王”，主要的应用有个性化商品推荐列表和相关商品的推荐列表。</p>
<p><strong>个性化推荐列表：</strong> 主要采用基于物品的推荐算法(item-based method)，给用户推荐和他们之前喜欢的物品相似的物品。另外还有一种是基于好友的个性化推荐，按照用户在 Facebook 的好友关系，给用户推荐他们的好友在亚马逊上喜欢的物品。感觉后者可能会带来更为严重的隐私争议，不过当然你可以选择禁用。</p>
<p><strong>相关推荐列表：</strong> 亚马逊有两种相关商品列表，一种是包含购买了这个商品的用户也经常购买的其它商品，另一种是包含浏览过这个商品的用户经常购买的其它商品。相关推荐列表最重要的应用是打包销售，可能商品之间是互补的，当你下订单的时候，亚马逊会问你是否要同时购买这些商品，同时，会给出打包的折扣。</p>
<p>亚马逊有 20%-30% 的销售来自于推荐系统。</p>
<h2 id="电影和视频网站"><a href="#电影和视频网站" class="headerlink" title="电影和视频网站"></a>电影和视频网站</h2><p><strong>代表：</strong> Netflix，YouTube，也都是基于物品的推荐算法。YouTube曾经做个一个实验，比较了个性化推荐的点击率和热门视频列表的点击率，实验结果表明个性化推荐的点击率是热门视频点击率的两倍。</p>
<h2 id="个性化音乐网络电台"><a href="#个性化音乐网络电台" class="headerlink" title="个性化音乐网络电台"></a>个性化音乐网络电台</h2><p><strong>代表：</strong> Pandora，Last.fm，豆瓣。Pandora 的算法主要基于内容，其音乐家和研究人员亲自听了上万首来自不同歌手的歌，然后对歌曲的不同特性(比如旋律、节 奏、编曲和歌词等)进行标注，这些标注被称为音乐的基因。然后，Pandora会根据专家标注的基因计算歌曲的相似度，并给用户推荐和他之前喜欢的音乐在基因上相似的其他音乐。Last.fm 并没有使用专家标注，而是利用用户行为计算歌曲的相似度。给用户推荐和他有相似听歌爱好的其他用户喜欢的歌曲。</p>
<p><strong>音乐作为推荐物品的特点：</strong></p>
<ul>
<li>物品空间大</li>
<li>消费每首歌的代价很小</li>
<li>物品种类丰富<br>音乐种类丰富，有很多的流派。</li>
<li>听一首歌耗时很少</li>
<li>物品重用率很高</li>
<li>用户充满激情<br>用户很有激情，一个用户会听很多首歌。</li>
<li>上下文相关<br>用户的口味很受当时上下文的影响，这里的上下文主要包括用户当时的心情(比如沮丧的时候喜欢听励志的歌曲)和所处情境(比如睡觉前喜欢听轻音乐)。</li>
<li>次序很重要<br>用户听音乐一般是按照一定的次序一首一首地听。</li>
<li>很多播放列表资源<br>很多用户都会创建很多个人播放列表。</li>
<li>不需要用户全神贯注</li>
<li>高度社会化<br>用户听音乐的行为具有很强的社会化特性，比如我们会和好友分享自己喜欢的音乐。</li>
</ul>
<p>上面这些特点决定了音乐是一种非常适合用来推荐的物品。因此，尽管现在很多推荐系统都是作为一个应用存在于网站中，比如亚马逊的商品推荐和Netflix的电影推荐，唯有音乐推荐可以支持独立的个性化推荐网站，比如Pandora、Last.fm和豆瓣网络电台。</p>
<h2 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a>社交网络</h2><p><strong>代表：</strong> Facebook，Twitter<br>主要应用在：</p>
<ul>
<li>利用用户的社交网络信息对用户进行个性化的物品推荐</li>
<li>信息流的会话推荐</li>
<li>给用户推荐好友</li>
</ul>
<p>Facebook 有个推荐 API，Instant Personalization，根据用户好友喜欢的信息，给用户推荐他们的好友最喜欢的物品。很多网站都用了这个 API 来实现网站的个性化，如 Yelp。</p>
<h2 id="个性化阅读"><a href="#个性化阅读" class="headerlink" title="个性化阅读"></a>个性化阅读</h2><p><strong>代表：</strong> Google Reader，鲜果网，Zite，Flipboard。</p>
<h2 id="基于位置的服务"><a href="#基于位置的服务" class="headerlink" title="基于位置的服务"></a>基于位置的服务</h2><p><strong>代表：</strong> Foursquare<br>往往和社交网络结合在一起。基于位置给用户推荐他近的且感兴趣的服务，用户就更有可能去消费。</p>
<h2 id="个性化邮件"><a href="#个性化邮件" class="headerlink" title="个性化邮件"></a>个性化邮件</h2><p><strong>代表：</strong> Tapestry， Google</p>
<p>谷歌于2010年推出了优先级收件箱功能。通过分析用户对邮件的历史行为，找到用户感兴趣的邮件，展示在一个专门的收件箱里。用户每天可以先浏览这个邮箱里的邮件，再浏览其他邮件。Google 的研究表明，该产品可以帮助用户节约 6% 的时间。</p>
<h2 id="个性化广告"><a href="#个性化广告" class="headerlink" title="个性化广告"></a>个性化广告</h2><p><strong>代表：</strong> Facebook</p>
<p>广告定向投放目前已经成为了一门独立学科 – 计算广告学。个性化广告投放技术主要分为 3 种：</p>
<ul>
<li>上下文广告 通过分析用户正在浏览的网页内容，投放和网页内容相关的广告。代表系统是谷歌的Adsense。</li>
<li>搜索广告 通过分析用户在当前会话中的搜索记录，判断用户的搜索目的，投放和用户目的相关的广告。</li>
<li>个性化展示广告 我们经常在很多网站看到大量展示广告(就是那些大的横幅图片)，它们是根据用户的兴趣，对不同用户投放不同的展示广告。雅虎是这方面研究的代表。</li>
</ul>
<h1 id="推荐系统评测"><a href="#推荐系统评测" class="headerlink" title="推荐系统评测"></a>推荐系统评测</h1><p>一个完整的推荐系统一般存在 3 个参与方：用户、物品提供者和提供推荐系统的网站。</p>
<p>因此在评测一个推荐算法时，需要同时考虑三方的利益，一个好的推荐系统是能够令三方共赢的系统。以图书推荐为例，好的推荐系统需要：</p>
<ul>
<li>需要满足用户的需求，给用户推荐那些令他们感兴趣的图书。</li>
<li>要让各出版社的书都能够被推荐给对其感兴趣的用户，而不是只推荐几个大型出版社的书。</li>
<li>能够让推荐系统本身收集到高质量的用户反馈，不断完善推荐的质量，增加用户和网站的交互，提高网站的收入。</li>
</ul>
<h2 id="推荐系统实验方法"><a href="#推荐系统实验方法" class="headerlink" title="推荐系统实验方法"></a>推荐系统实验方法</h2><h3 id="离线实验（offline-experiment）"><a href="#离线实验（offline-experiment）" class="headerlink" title="离线实验（offline experiment）"></a>离线实验（offline experiment）</h3><p><strong>步骤：</strong><br>(1) 通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集;<br>(2) 将数据集按照一定的规则分成训练集和测试集;<br>(3) 在训练集上训练用户兴趣模型，在测试集上进行预测;<br>(4) 通过事先定义的离线指标评测算法在测试集上的预测结果。</p>
<p><strong>优点：</strong></p>
<ul>
<li>不需要对实际系统的控制权</li>
<li>不需要真是用户参与</li>
<li>速度快，能快速测试大量不同的算法</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>无法获得更多商业上关注的指标，如点击率、转化率等。</li>
<li>离线实验的指标和商业指标存在差距。高预测率不等于高用户满意度。</li>
</ul>
<h3 id="用户调查（user-study）"><a href="#用户调查（user-study）" class="headerlink" title="用户调查（user study）"></a>用户调查（user study）</h3><p>像是一个过渡，离线实验的指标和实际的商业指标存在差距，算法直接上线测试又具有较高的风险，所以在上线测试前一般需要做一次用户调查。</p>
<p>用户调查需要有一些真实用户，让他们在需要测试的推荐系统上完成一些任务。在他们完成任务时，我们需要观察和记录他们的行为，并让他们回答一些问题。最后，我们需要通过分析他们的行为和答案了解测试系统的性能。</p>
<p><strong>优点：</strong></p>
<ul>
<li>获得很多体现用户主观感受的指标</li>
<li>相对在线实验风险低，出错后容易弥补</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>招募测试用户代价较大</li>
<li>很难组织大规模的测试用户，因此会使测试结果的统计意义不足</li>
<li>设计双盲实验非常困难，而且用户在测试环境下的行为和真实环境下的行为可能有所不同</li>
</ul>
<h3 id="在线实验（online-experiment）"><a href="#在线实验（online-experiment）" class="headerlink" title="在线实验（online experiment）"></a>在线实验（online experiment）</h3><p>也就是传说中的 AB 测试。AB 测试是一种常用的在线评测算法的实验方法，通过一定的规则将用户随机分成几组，并对不同组的用户采取不同的算法，然后通过统计不同组用户的各种不同评测指标比较不同算法，比如统计不同组用户的点击率，通过点击率比较不同算法的性能。详见 <a href="http://www.abtests.com/" target="_blank" rel="external">AB test</a></p>
<p><strong>优点：</strong></p>
<ul>
<li>可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>周期长，必须进行长期的实验才能得到可靠的结果</li>
</ul>
<p>简单的AB测试系统。</p>
<ul>
<li>用户进入网站后，流量分配系统决定用户是否需要被进行AB测试，如果需要的话，流量分配系统会给用户打上在测试中属于什么分组的标签。</li>
<li>用户浏览网页，而用户在浏览网页时的行为都会被通过日志系统发回后台的日志数据库。此时，如果用户有测试分组的标签，那么该标签也会被发回后台数据库。</li>
<li>在后台，实验人员的工作首先是配置流量分配系统，决定满足什么条件的用户参加什么样的测试。其次，实验人员需要统计日志数据库中的数据，通过评测系统生成不同分组用户的实验报告，并比较和评测实验结果。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/AB%E6%B5%8B%E8%AF%95%E7%B3%BB%E7%BB%9F.jpg" class="ful-image" alt="AB%E6%B5%8B%E8%AF%95%E7%B3%BB%E7%BB%9F.jpg">
<p> 一般来说，一个新的推荐算法最终上线，需要完成上面所说的3个实验。</p>
<ol>
<li>首先，需要通过离线实验证明它在很多离线指标上优于现有的算法。</li>
<li>然后，需要通过用户调查确定它的用户满意度不低于现有的算法。</li>
<li>最后，通过在线的 AB 测试确定它在我们关心的指标上优于现有的算法。  </li>
</ol>
<h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><h3 id="用户满意度"><a href="#用户满意度" class="headerlink" title="用户满意度"></a>用户满意度</h3><p>只能通过用户调查或者在线实验获得。</p>
<h4 id="用户调查"><a href="#用户调查" class="headerlink" title="用户调查"></a>用户调查</h4><p>GroupLens 曾经做过一个论文推荐系统的调查问卷，该问卷的调查问题是请问下面哪句话最能描述你看到推荐结果后的感受</p>
<ul>
<li>推荐的论文都是我非常想看的。</li>
<li>推荐的论文很多我都看过了，确实是符合我兴趣的不错论文。</li>
<li>推荐的论文和我的研究兴趣是相关的，但我并不喜欢。</li>
<li>不知道为什么会推荐这些论文，它们和我的兴趣丝毫没有关系。</li>
</ul>
<h4 id="在线实验"><a href="#在线实验" class="headerlink" title="在线实验"></a>在线实验</h4><p>主要通过一些对用户行为的统计得到。如利用购买率、点击率、用户停留时间和转化率等指标度量用户的满意度。</p>
<h3 id="预测准确度"><a href="#预测准确度" class="headerlink" title="预测准确度"></a>预测准确度</h3><p>最重要的离线评测指标。在计算该指标时需要有一个离线的数据集，该数据集包含用户的历史行为记录。然后，将该数据集通过时间分成训练集和测试集。最后，通过在训练集上建立用户的行为和兴趣模型预测用户在测试集上的行为，并计算预测行为和测试集上实际行为的重合度作为预测准确度。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/precisionRecall.jpg" class="ful-image" alt="precisionRecall.jpg"></p>
<h4 id="评分预测"><a href="#评分预测" class="headerlink" title="评分预测"></a>评分预测</h4><p>很多提供推荐服务的网站都有一个让用户给物品打分的功能(如知道了用户对物品的历史评分，就可以从中习得用户的兴趣模型，并预测该用户在将来看到一个所示)。那么，如果他没有评过分的物品时，会给这个物品评多少分。预测用户对物品评分的行为称为评分预测。</p>
<p>评分预测的预测准确度一般通过均方根误差(RMSE)和平均绝对误差(MAE)计算。对于测试集中的一个用户 u 和物品 i，令 $r_{ui}$ 是用户 u 对 u 物品 i 的实际评分，而 $\hat r_{ui}$ 是推荐算法给出的预测评分，那么 RMSE 为<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/RMSE.jpg" class="ful-image" alt="RMSE.jpg"></p>
<p>MAE 采用绝对值计算预测误差：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/MAE.jpg" class="ful-image" alt="MAE.jpg"></p>
<p>代码也非常简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def RMSE(records):</div><div class="line">    return math.sqrt(sum[(rui=pui)**2 for u, i, rui, pui in records]) / float(len(records))</div><div class="line"></div><div class="line"></div><div class="line">def MAE(records):</div><div class="line">    return sum([abs(rui - pui) for u, i, rui, pui in records]) / float(len(records))</div></pre></td></tr></table></figure></p>
<p>Netflix认为RMSE加大了对预测不准的用户物品评分的惩罚(平方项的惩罚)，因而对系统的评测更加苛刻。研究表明，如果评分系统是基于整数建立的(即用户给的评分都是整数)，那么对预测结果取整会降低MAE的误差。</p>
<h4 id="TopN-推荐"><a href="#TopN-推荐" class="headerlink" title="TopN 推荐"></a>TopN 推荐</h4><p>TopN 推荐其实更符合实际需求。以电影为例，评分预测预测的其实是用户看了电影后会给什么样的评分，而电影推荐的目的是找到用户最可能感兴趣的电影，这两者当然不是一个概念。也许有一部历史片／文艺片非常好，用户看了会给非常高的分数，但是用户看的可能性非常小，可能用户就喜欢爱情片／脑残片呢。</p>
<h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><p>覆盖率(coverage)描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。假设系统的用户集合为 U， 推荐系统给每个用户推荐一个长度为 N 的物品列表 R(u)，那么推荐系统的覆盖率就是<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/coverage.jpg" class="ful-image" alt="coverage.jpg"></p>
<p>覆盖率是一个内容提供商会关心的指标。以图书推荐为例，出版社可能会很关心他们的书有没有被推荐给用户。覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。此外，从上面的定义也可以看到，热门排行榜的推荐覆盖率是很低的，它只会推荐那些热门的物品，这些物品在总物品中占的比例很小。一个好的推荐系统不仅需要有比较高的用户满意度，也要有较高的覆盖率。</p>
<p>考虑研究物品在推荐列表中出现次数的分布描述推荐系统挖掘长尾的能力，可以用信息熵或者基尼系数。<br><strong>信息熵:</strong></p>
<p>$H=-\sum_{i=1}^np(i) \ logp(i)$</p>
<p>(p(i) 是物品 i 的流行度除以所有物品流行度之和。)</p>
<p><strong>是基尼系数（Gini Index）:</strong></p>
<p>$G={1 \over n-1}\sum_{j=1}^n(2j-n-1)p(i_j)$</p>
<p>($i_j$ 是按照物品流行度p()从小到大排序的物品列表中第j个物品。)</p>
<p>下面的代码可以用来计算给定物品流行度分布后的基尼系数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def GiniIndex(p):</div><div class="line">    j = 1</div><div class="line">    n = len(p)</div><div class="line">    G = 0</div><div class="line">    for item, weight in sorted(p.items(), key=itemgetter(1)):</div><div class="line">        G += (2 * j - n - 1) * weight</div><div class="line"></div><div class="line">    return G / float(n - 1)</div></pre></td></tr></table></figure></p>
<p>如果这个分布比较平，那么说明推荐系统的覆盖率较高，推荐系统发掘长尾的能力就很好。而如果这个分布较陡峭，说明推荐系统的覆盖率较低。</p>
<p><strong>推荐系统是否有马太效应呢?</strong><br>推荐系统的初衷是希望消除马太效应，使得各种物品都能被展示给对它们感兴趣的某一类人群。但是，很多研究表明现在主流的推荐算法(比如协同过 滤算法)是具有马太效应的。评测推荐系统是否具有马太效应的简单办法就是使用基尼系数。如果 G1 是从初始用户行为中计算出的物品流行度的基尼系数，G2 是从推荐列表中计算出的物品流行度的基尼系数，那么如果G2 &gt; G1，就说明推荐算法具有马太效应。</p>
<h3 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h3><p>用户的兴趣是广泛的，为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果需要具有多样性。多样性描述的是推荐列表中物品两两之间的不相似性。因此，多样性和相似性是对应的。假设 s(i， j)定义了物品 i 和 j 之间的相似度，那么用户 u 的推荐列表 R(u) 的多样性定义如下:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/diversity.jpg" class="ful-image" alt="diversity.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/diversity2.jpg" class="ful-image" alt="diversity2.jpg"></p>
<h3 id="新颖性"><a href="#新颖性" class="headerlink" title="新颖性"></a>新颖性</h3><p>新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。比如在一个视频网站中，新颖的推荐不应该给用户推荐那些他们已经看过、打过分或者浏览过的视频。</p>
<p>评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。但是，用推荐结果的平均流行度度量新颖性比较粗略，因为不同用户不知道的东西是不同的。因此，要准确地统计新颖性需要做用户调查。</p>
<h3 id="惊喜度"><a href="#惊喜度" class="headerlink" title="惊喜度"></a>惊喜度</h3><p>惊喜度(serendipity)是最近这几年推荐系统领域最热门的话题。如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个推荐结果。</p>
<p>目前并没有什么公认的惊喜度指标定义方式，这里只给出一种定性的度量方式。上面提到，令用户惊喜的推荐结果是和用户历史上喜欢的物品不相似，但用户却觉得满意的推荐。那么，定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度。</p>
<h3 id="信任度"><a href="#信任度" class="headerlink" title="信任度"></a>信任度</h3><p>如果你有两个朋友，一个人你很信任，一个人经常满嘴跑火车，那么如果你信任的朋友推荐 你去某个地方旅游，你很有可能听从他的推荐，但如果是那位满嘴跑火车的朋友推荐你去同样的 地方旅游，你很有可能不去。这两个人可以看做两个推荐系统，尽管他们的推荐结果相同，但用户却可能产生不同的反应，这就是因为用户对他们有不同的信任度。</p>
<p>度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。</p>
<p>提高推荐系统的信任度主要有两种方法。</p>
<ol>
<li>增加推荐系统的透明度(transparency)<br>增加推荐系统透明度的主要办法是提供推荐解释。只有让用户了解推荐系统的运行机制，让用户认同推荐系统的运行机制，才会提高用户对推荐系统的信任度。</li>
<li>考虑用户的社交网络信息<br>利用用户的好友信息给用户做推荐，并且用好友进行推荐解释。因为用户对他们的好友一般都比较信任，因此如果推荐的商品是好友购买过的，那么他们对推荐结果就会相对比较信任。</li>
</ol>
<h3 id="实时性"><a href="#实时性" class="headerlink" title="实时性"></a>实时性</h3><p>在很多网站中，因为物品(新闻、微博等)具有很强的时效性，所以需要在物品还具有时效性时就将它们推荐给用户。</p>
<p>推荐系统的实时性包括两个方面。</p>
<ol>
<li>推荐系统需要实时地更新推荐列表来满足用户新的行为变化。比如，当一个用户购买了iPhone，如果推荐系统能够立即给他推荐相关配件，那么肯定比第二天再给用户推荐相关配件更有价值。很多推荐系统都会在离线状态每天计算一次用户推荐列表，然后于在线期间将推荐列表展示给用户。这种设计显然是无法满足实时性的。与用户行 为相应的实时性，可以通过推荐列表的变化速率来评测。如果推荐列表在用户有行为后变化不大，或者没有变化，说明推荐系统的实时性不高。</li>
<li>推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。对于新物品推荐能力，我们可以利用用户推荐列表中有多大比例的物品是当天新加 的来评测。</li>
</ol>
<h3 id="健壮性"><a href="#健壮性" class="headerlink" title="健壮性"></a>健壮性</h3><p>健壮性(即robust，鲁棒性)指标衡量了一个推荐系统抗击作弊的能力。</p>
<p>算法健壮性的评测主要利用模拟攻击。</p>
<ul>
<li>给定一个数据集和一个算法，用这个算法给这个数据集中的用户生成推荐列表。</li>
<li>用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。</li>
<li>通过比较攻击前后推荐列表的相似度评测算法的健壮性。<br>如果攻击后的推荐列表相对于攻击前没有发生大的变化，就说明算法比较健壮。</li>
</ul>
<p>在实际系统中，提高系统的健壮性，除了选择健壮性高的算法，还有以下方法。</p>
<ul>
<li>设计推荐系统时尽量使用代价比较高的用户行为。比如，如果有用户购买行为和用户浏览行为，那么主要应该使用用户购买行为，因为购买需要付费，所以攻击购买行为的代价远远大于攻击浏览行为。</li>
<li>在使用数据前，进行攻击检测，从而对数据进行清理。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/ways.jpg" class="ful-image" alt="ways.jpg">
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>离线实验的优化 <strong>目标</strong> 是:<br>最大化预测准确度 使得 覆盖率 &gt; A，多样性 &gt; B， 新颖性 &gt; C，其中，A、B、C的取值应该视不同的应用而定。</p>
<p>这些指标本身就是相互矛盾的，还有一种统一的方法可能是 AUC(area under curve):</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F--%E5%BC%80%E5%9D%91/comb.jpg" class="ful-image" alt="comb.jpg">
<h2 id="评测维度"><a href="#评测维度" class="headerlink" title="评测维度"></a>评测维度</h2><p>一般来说，评测维度分为如下3种。</p>
<ul>
<li>用户维度<br>主要包括用户的人口统计学信息、活跃度以及是不是新用户等。</li>
<li>物品维度<br>包括物品的属性信息、流行度、平均分以及是不是新加入的物品等。</li>
<li>时间维度<br>包括季节，是工作日还是周末，是白天还是晚上等。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Recommender Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Recommender Systems </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习-从线性到非线性]]></title>
      <url>http://www.shuang0420.com/2017/01/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/</url>
      <content type="html"><![CDATA[<p>这一篇讨论常用的非线性激励函数。<br><a id="more"></a></p>
<h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><p>一个浅层的神经网络，如下图其实就可以看作一个 logistic regression 模型加上非线性激励函数。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/shallow.jpg" class="ful-image" alt="shallow.jpg"></p>
<p>一个神经元的组成：</p>
<ul>
<li>输入：n 维向量</li>
<li>线性加权：$z=\sum^n_{i=1}w_ix_i+b$</li>
<li>激活函数：a=h(z)，要求非线性，容易求导</li>
<li>输出值：a(标量)</li>
</ul>
<p>当然我们可以加 z2, z3, a2, a3… 输入是 x1,x2…xn，输出是 a1,a2…am，如果给一个神经元，就是或 0 或 1 的输出，如果给多个，就从 logistic 回归变成了 softmax 回归。</p>
<p>一个输入，若干个中间层(可能是全连接／非全连接网络)，最后输出层，如果要做分类，就可以给一个或多个全连接网络（可以看作是 softmax）。</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>如果不用激活函数，或者说激活函数是f(x) = x，那么在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，来个例子，假设<br>X1=W0*X0<br>X2=W1*X1<br>Y=W2*X2<br>那么，线性矩阵相乘，可以直接简化为一层：Y=W2*W1*W0*X0=W3*X0，为什么还要用网络？</p>
<p>所以，<strong>有线性回归网络吗？没有！</strong></p>
<p>正因为上面的原因，我们才要引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层输入。</p>
<h1 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h1><p>激活函数通常有如下一些性质：</p>
<blockquote>
<p><strong>非线性：</strong> 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。<br><strong>可微性：</strong> 当优化方法是基于梯度的时候，这个性质是必须的。<br><strong>单调性：</strong> 当激活函数是单调的时候，单层网络能够保证是凸函数。<br><strong>f(x)≈x：</strong> 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。<br><strong>输出值的范围：</strong> 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.</p>
</blockquote>
<p>这些性质，也正是我们使用激活函数的原因。</p>
<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/sigmoid.jpg" class="ful-image" alt="sigmoid.jpg">
<p>公式：<br>$$f(x)=sigmoid(x)={1 \over 1+e^{-x}}$$</p>
<p>对其求导<br>$$<br>  \begin{aligned}<br>  {df \over dx} &amp; = -{1 \over (1+e^{-x})^2}(-e^{-x}) \\<br>  &amp; =  {1 \over 1+e^{-x}} {e^{-x} \over 1+e^{-x}} \\<br>   &amp; = {1 \over 1+e^{-x}} {1+e^{-x}-1 \over 1+e^{-x}} \\<br>   &amp; = f(x)(1-f(x))<br>  \end{aligned}<br>$$</p>
<p>Sigmoid 将数据映射到 [0,1]</p>
<p><strong>缺点：</strong></p>
<blockquote>
<p>Sigmoids saturate and kill gradients.</p>
</blockquote>
<ol>
<li>梯度下降非常明显，且两头过于平坦，容易出现梯度消失的情况<br>当输入非常大或者非常小的时候(saturation)，神经元的梯度是接近于0的，从图中可以看出梯度的趋势。所以，你需要尤其注意参数的初始值来避免 saturation 的情况。如果初始值很大的话，大部分神经元可能都会处在 saturation 的状态而把 gradient kill 掉，这会导致网络变的很难学习。</li>
<li>输出值域不对称（非0均值）<br>后一层的神经元将得到上一层输出的非 0 均值的信号作为输入，产生的一个结果就是：如果数据进入神经元的时候是正的(e.g. x&gt;0 elementwise in f=wTx+b)，那么 w 计算出的梯度也会始终都是正的。</li>
</ol>
<h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/tanh.jpg" class="ful-image" alt="tanh.jpg">
<p>公式：<br>$$f(x)=tanh(x)={2 \over 1+e^{-2x}}-1$$</p>
<p>对其求导<br>我们知道 $tanh(x)={sinh(x) \over cosh(x)}$，所以对 f(x) 求导也就是对 ${sinh(x) \over cosh(x)}$ 求导。<br>$$<br>  \begin{aligned}<br>  {df \over dx} &amp; = {cosh^2(x)-sinh^2(x) \over cosh^2(x)} \\<br>  &amp; =  1-tanh^2(x) \\<br>   &amp; = 1-f(x)^2<br>  \end{aligned}<br>$$</p>
<p>tanh 将数据映射到 [-1,1]，解决了 sigmoid 输出值域不对称问题，然而两头依旧过于平坦，梯度损失仍然明显。</p>
<h2 id="ReLU-Rectified-linear-unit"><a href="#ReLU-Rectified-linear-unit" class="headerlink" title="ReLU(Rectified linear unit)"></a>ReLU(Rectified linear unit)</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/relu.jpg" class="ful-image" alt="relu.jpg">
<p>$$f(x)=max(0,x)$$</p>
<p>也就是 x&lt;0 取0，否则取本身。</p>
<p><strong>优点：</strong></p>
<ol>
<li>收敛速度比 sigmoid/tanh 更快<br> 可能是因为它是linear，而且 non-saturating</li>
<li>计算高效简单<br> 相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，ReLU具有所希望的特性，不需要输入归一化来防止它们达到饱和，也不用去算一大堆复杂的运算。</li>
<li>反向梯度没有损失</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>正向截断负值，损失大量特征</li>
<li>训练时很“脆弱”，有 dead area<br> 如，太高的 learning rate 配合上非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了，那么这个神经元的梯度就永远都会是 0.<br> 实际操作中，如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。<br> 当然，如果你设置了一个合适的较小的 learning rate，这个问题发生的情况其实也不会太频繁。另外可以配合 Xavier 权重初始化方法，使用 adagrad 等方法自动调节 learning rate 来防止这种问题。</li>
</ol>
<blockquote>
<p>(-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.</p>
</blockquote>
<p>有实验说，大概 80%-90%的特征都会被截断，然而 ReLU 仍然是非常常用的激励函数，因为特征足够多。</p>
<h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/leakyrelu.jpg" class="ful-image" alt="leakyrelu.jpg">
<p>对 ReLU 的改进，解决 dying ReLU 的问题。x&lt;0 时乘上一个 a 取较小的值，一般 a=0.01，可以保留更多的参数，反向梯度有部分损失。</p>
<p>为什么不变成 y=x？那不就回到了线性回归了嘛。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>一般现在都直接取 ReLU，然而如果使用 ReLU，一定要小心设置 learning rate，要注意不要让你的网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU、random ReLU 或者 Maxout。另外，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布，见<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%B0%E9%9D%9E%E7%BA%BF%E6%80%A7/activationF.jpg" class="ful-image" alt="activationF.jpg">
<p><strong>sigmoid 缺点：</strong></p>
<ol>
<li>两头过于平坦</li>
<li>输出值域不对称（非0均值）</li>
</ol>
<p><strong>tanh 缺点:</strong></p>
<ol>
<li>两头依旧过于平坦</li>
</ol>
<p><strong>ReLU:</strong></p>
<ol>
<li>收敛速度比 sigmoid/tanh 更快</li>
<li>计算高效简单</li>
<li>Dead Area 中权重不更新(leaky ReLU 不存在 dead area)</li>
</ol>
<blockquote>
<p>参考链接<br><a href="http://blog.csdn.net/u013146742/article/details/51986575" target="_blank" rel="external">RELU 激活函数及其他相关的函数</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> 激活函数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络 CNN 笔记]]></title>
      <url>http://www.shuang0420.com/2017/01/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>CNN 对模式分类非常适合，其最初是为识别二维形状而特殊设计的，这种二维形状对平移、比例缩放、倾斜或其他形式对变形有高度不变性。<br><a id="more"></a></p>
<h1 id="图像识别-分类"><a href="#图像识别-分类" class="headerlink" title="图像识别/分类"></a>图像识别/分类</h1><p>图片识别／分类的一般过程：detect -&gt; align -&gt; represent -&gt; classify。具体到 CNN 就是检测到图片的位置，剪出来对齐，表达特征，对若干层进行不同的卷积、pooling，最后全连接网络做分类。</p>
<p><strong>传统模型:</strong></p>
<pre>Fixed features + unsupervised mid-level features + simple classifier</pre>

<p><strong>神经网络：</strong></p>
<pre>Low-level features －> Mid-level features -> High-level features -> trainable classifier</pre>

<p>李飞飞的 ImageNet 比赛，在 2012 年之前，经典做法是人工选一些原始特征出来(SIFT, Hog, Harr, etc.)，再稍加变换，可能用到一些聚类的算法，做一些中等级别的特征，然后给某个分类器做识别，一般就是 SVM。这种方法每一步都会损失数据，到最后可能就达不到很好的分类效果。</p>
<p><strong>注：</strong> 把图像像素看成 words of bags，不同的原始图像可能分别是 M1*N, M2*N, M3*N 等等的 vectors，通过 K-means 的聚类聚比如说 1000 个类，就能把原来的 vectors 转化成长度为 1000 的直方图，也就形成中等级别的特征，维度就一样了，然后再选一个分类器。</p>
<p>2012 年第一次用了 CNN，正确率提高了 10%，人们意识到深度学习可能是图像识别非常有效的方式。与传统模型不同的是特征是自动选择的。</p>
<p>深度学习现在看来还可能是过冗余的，可以很多改进空间，如果能把 100M -&gt; 100K 的参数，就可以不用离线训练，也可以放到 App 中了。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而 CNN 则通过局部连接、权值共享等方法避免了这一困难。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>通过局部连接和权值共享减少了神经网络需要训练的参数的个数。</p>
<ul>
<li>局部连接</li>
<li>权值共享(每个 feature map 共享参数)</li>
<li>池化</li>
</ul>
<h2 id="一般架构"><a href="#一般架构" class="headerlink" title="一般架构"></a>一般架构</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/struc.jpg" class="ful-image" alt="struc.jpg">
<p>可能有多个卷积层或多个输出层，某些卷积层不跟着 pooling 也是可以的。</p>
<h2 id="卷积层-Convolutional-Layer"><a href="#卷积层-Convolutional-Layer" class="headerlink" title="卷积层(Convolutional Layer)"></a>卷积层(Convolutional Layer)</h2><p>卷积层是卷积神经网络基本结构，它由多个卷积核组合形成，每个卷积核同输入数据做卷积运算，形成新的特征图(feature map)，也就是，有几个卷积核，就有几个特征图。</p>
<p>比如说一张 32*32 的 RGB 图片，做卷积，一张图片就能理解。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/%E5%8D%B7%E7%A7%AF.jpg" class="ful-image" alt="%E5%8D%B7%E7%A7%AF.jpg"></p>
<pre>
Input volume: 32*32*3
Receptive fields: 5*5, stride 3
Number of neurons: 5

Output volume: (32-5)/3+1=10, -> 10*10*5
Weights for each of 10*10*5 neurons: 5*5*3=75
</pre>

<p>这其中，卷积核的 <strong>大小(size)</strong> 是由用户定义的，而 <strong>深度(或者说厚度)</strong>，是由输入数据定义的，一维数据就用一维卷积核，RGB 图片就是三维卷积核。</p>
<p>卷积核的 <strong>数目(kernel number)</strong>，常见参数有 64，128，256，为了使 GPU 并行更加高效。</p>
<p>每一个神经元从上一层的局部接受域得到输入，提取局部特征，每个局部特征相对于其他特征的位置被近似保留下来，原本的精确位置就没那么重要了。每一个计算层都由多个 feature map 组成，每个 feature map 都是平面形式的，平面中单独的神经元在约束下共享相同的权值集。这种结构约束具有平移不变性（强迫 feature map 的执行使用具有小尺度核的卷积，再接着使用一个 sigmoid 函数），另外，权值共享也可以实现自由参数数量的缩减。</p>
<p>卷积核的“矩阵”值，就是卷积神经网络的参数，卷积核的初值，通常随机生成，然后通过反向传播更新。随机生成可以通过高斯分布生成。一个问题：</p>
<p><strong>卷积核初值完全一样好不好？</strong><br>不好！如果初值完全一样，那么反向梯度改变的量也差不多，整体就没有变化性，没有多样性。</p>
<p><strong>padding &amp; stride</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/padding.jpg" class="ful-image" alt="padding.jpg"></p>
<p>padding 也就是边界扩充，在卷积计算过程中，为了允许边界上的数据也能作为中心参与卷积计算，将边界假装延伸，目的是为了确保卷积后特征图尺度一致。卷积核的宽度为 2i+1，添加 pad 的宽度就为 i。如上图，5*5 的图，卷积核 3*3，取 padding=1，对原始数据上下左右各补 1，可能会有偏移量，就相乘相加再加上偏移值。</p>
<p>步长(stride)是对输入特征图的扫描间隔，因为相邻的卷积窗口传达的信息可能会差不多，所以跳着取，提高效率。</p>
<p><strong>权值设置：</strong><br>可以对所有权值做先验处理，按高斯分布做随机处理，然后梯度下降调整权值。</p>
<h2 id="功能层"><a href="#功能层" class="headerlink" title="功能层"></a>功能层</h2><p>卷积神经网络还需要一些额外功能：</p>
<ul>
<li><strong>非线性激励：</strong> 卷积是线性运算，需要增加非线性描述能力</li>
<li><strong>降维：</strong> 特征图稀疏，需要减少数据运算量，保持精度，如做一个 pooling</li>
<li><strong>归一化：</strong> 特征的 scale 保持一致，比如说映射到 [0,1] 之间</li>
<li><strong>区域分割：</strong> 不同区域进行独立学习</li>
<li><strong>区域融合：</strong> 对分开的区域合并，方便信息融合</li>
<li><strong>增维：</strong> 增加图片生成或探测任务中的空间信息</li>
</ul>
<h3 id="非线性激励层-None-linear-activation-layer"><a href="#非线性激励层-None-linear-activation-layer" class="headerlink" title="非线性激励层(None-linear activation layer)"></a>非线性激励层(None-linear activation layer)</h3><p>如 ReLU 函数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/1.jpg" class="ful-image" alt="1.jpg"></p>
<p>更多见 <a href="http://www.shuang0420.com/2017/01/21/神经网络-从线性到非线性/">神经网络-从线性到非线性</a></p>
<h3 id="池化层-Pooling-layer"><a href="#池化层-Pooling-layer" class="headerlink" title="池化层(Pooling layer)"></a>池化层(Pooling layer)</h3><p>每个卷积层跟着一个实现局部平均和子抽样的计算层，能达到降维的目的，由此 feature map 的分辨率降低。这种操作可以使 feature map 的输出对平移和其他形式的变形的敏感度下降。一张图解释下 2*2 的 max-pooling。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/pooling.jpg" class="ful-image" alt="pooling.jpg"></p>
<p>这样 M*M 的图像就成了 M/2 * M/2 的图像。当然还有 min-pooling 和 avg-pooling。<br><strong>作用:</strong></p>
<ul>
<li>降低输出规模</li>
<li>增加可解释性</li>
<li>避免丢失过多信息</li>
</ul>
<h3 id="归一化层-Normalization-layer"><a href="#归一化层-Normalization-layer" class="headerlink" title="归一化层(Normalization layer)"></a>归一化层(Normalization layer)</h3><p>如 <strong>批量归一化(Batch Normalization, BN)</strong>，原因是特征数的 scale 不一致，好处是可以加速训练，提高精度。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>还有 <strong>近邻归一化(Local Response Normalization)</strong>，与 BN 不同的是，BN 依据 mini batch 的数据，而 LRN 仅需要自身，BN 训练中有学习参数，而 LRN 并没有。</p>
<p>$$x_i={x_i \over (k+(\alpha \sum_jx^2_j))^\beta}$$</p>
<h3 id="切分层-Slice-layer"><a href="#切分层-Slice-layer" class="headerlink" title="切分层(Slice layer)"></a>切分层(Slice layer)</h3><p>在某些应用中，希望独立对某些区域单独学习，比如说人脸识别，可以眼睛一套参数，耳朵一套参数。。好处是可以学习多套参数，得到更强的特征描述能力。</p>
<h3 id="融合层-Merge-layer"><a href="#融合层-Merge-layer" class="headerlink" title="融合层(Merge layer)"></a>融合层(Merge layer)</h3><p>对独立进行特征学习的分支进行融合，来构建高效而精简的特征组合。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/3.jpg" class="ful-image" alt="3.jpg">
<p>可以用 <strong>级连(concatenation)</strong> 的方法，其实也就是不同输入网络特征的简单叠加，比如说首尾相接。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/4.jpg" class="ful-image" alt="4.jpg"></p>
<p>也可以是合并，或者说运算的融合，对形状一致的特征曾，通过 +, -, x, max, conv 等原酸，形成形状相同的输出，如微软的残差网络。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/5.jpg" class="ful-image" alt="5.jpg"></p>
<h2 id="全连接层及全卷积网络"><a href="#全连接层及全卷积网络" class="headerlink" title="全连接层及全卷积网络"></a>全连接层及全卷积网络</h2><p>最后的输出一般是连一层全连接层(fully connected layer)，相当于 softmax 回归。当然其实也可以不连，像 <a href="https://arxiv.org/pdf/1411.4038v2.pdf" target="_blank" rel="external">FCN(全卷积网络)</a>。</p>
<p><strong>卷积层</strong> 的操作可以把 kernel 作用于输入的不同区域然后产生对应的特征图，也因此给定一个卷积层，并不要求输入是固定大小的。而 <strong>全连接层</strong> 的操作实际上是把输入拉成一个一维的向量，然后对这个一维向量进行点乘，这要求输入是固定大小的。这有的时候是很不合理的，如下图，如果要把红框的塔输入网络，就会产生图片变形。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/wrap.jpg" class="ful-image" alt="wrap.jpg"></p>
<p><strong>如何网络接受任意的输入？</strong><br>把全连接层变成卷积层，这就是所谓的卷积化。这里需要证明卷积化的等价性。直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。举个例子，比如 AlexNet，fc6 的输入是 256x6x6，那么这时候只需要把 fc6 变成是卷积核为6x6的卷积层就好了。</p>
<p><strong>与传统神经网络相比，CNN 参数和计算量更多还是更少了？</strong><br>参数变少了，因为都使用一套参数，而计算量却是变大了，因为卷积窗口要滑到不同的地方，进行计算、合并等操作。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p><strong>提高泛化能力（减少 overfit）</strong></p>
<ol>
<li>增加神经网络层数。使用卷积层极大地减小了全连接层中的参数的数目，使学习的问题更容易</li>
<li>使用更多强有力的规范化技术（尤其是 dropout 和 regularization）来减小过度拟合</li>
<li>使用修正线性单元而不是 S 型神经元，来加速训练-依据经验，通常是3-5倍</li>
<li>使用 GPU 来计算</li>
<li>利用充分大的数据集，避免过拟合</li>
<li>使用正确的代价函数，避免学习减速</li>
<li>使用好的权重初始化，避免因为神经元饱和引起的学习减速</li>
</ol>
<h1 id="TensorFlow-实战"><a href="#TensorFlow-实战" class="headerlink" title="TensorFlow 实战"></a>TensorFlow 实战</h1><p><a href="http://www.shuang0420.com/2016/06/20/TensorFlow%E5%AE%9E%E6%88%98-MNIST/">TensorFlow 实战 MINST</a></p>
<h1 id="CNN-用于-NLP"><a href="#CNN-用于-NLP" class="headerlink" title="CNN 用于 NLP"></a>CNN 用于 NLP</h1><p><a href="http://www.shuang0420.com/2016/08/05/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%20sentence%20embedding/">实习总结之 sentence embedding</a></p>
<blockquote>
<p>参考链接<br><a href="http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/" target="_blank" rel="external">Concepts and Tricks In CNN(长期更新)</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 堆]]></title>
      <url>http://www.shuang0420.com/2016/12/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E5%A0%86/</url>
      <content type="html"><![CDATA[<p>最快找到一堆数里的最小值–最小堆。<br><a id="more"></a></p>
<h1 id="Python-heapq"><a href="#Python-heapq" class="headerlink" title="Python heapq"></a>Python heapq</h1><p>python heapq 是 binary heap 的变种，见 <a href="https://en.wikipedia.org/wiki/Binary_heap" target="_blank" rel="external">binary heap</a><br><strong>How to customize the heap order?</strong><br>Have each element on the heap to be a tuple, with the first tuple element being one that accepts normal Python comparisons.</p>
<p>Eg.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; def heapsort(iterable):</div><div class="line">...     h = []</div><div class="line">...     for value in iterable:</div><div class="line">...         heappush(h, value)</div><div class="line">...     return [heappop(h) for i in range(len(h))]</div><div class="line">...</div><div class="line">&gt;&gt;&gt; heapsort([1, 3, 5, 7, 9, 2, 4, 6, 8, 0])</div><div class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</div></pre></td></tr></table></figure></p>
<p>基本用法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">This module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm.</div><div class="line"></div><div class="line">Heaps are binary trees for which every parent node has a value less than or equal to any of its children. This implementation uses arrays for which heap[k] &lt;= heap[2*k+1] and heap[k] &lt;= heap[2*k+2] for all k, counting elements from zero. For the sake of comparison, non-existing elements are considered to be infinite. The interesting property of a heap is that its smallest element is always the root, heap[0].</div><div class="line"></div><div class="line">The API below differs from textbook heap algorithms in two aspects: (a) We use zero-based indexing. This makes the relationship between the index for a node and the indexes for its children slightly less obvious, but is more suitable since Python uses zero-based indexing. (b) Our pop method returns the smallest item, not the largest (called a “min heap” in textbooks; a “max heap” is more common in texts because of its suitability for in-place sorting).</div><div class="line"></div><div class="line">These two make it possible to view the heap as a regular Python list without surprises: heap[0] is the smallest item, and heap.sort() maintains the heap invariant!</div><div class="line"></div><div class="line">To create a heap, use a list initialized to [], or you can transform a populated list into a heap via function heapify().</div><div class="line"></div><div class="line">The following functions are provided:</div><div class="line"></div><div class="line">heapq.heappush(heap, item)</div><div class="line">Push the value item onto the heap, maintaining the heap invariant.</div><div class="line"></div><div class="line">heapq.heappop(heap)</div><div class="line">Pop and return the smallest item from the heap, maintaining the heap invariant. If the heap is empty, IndexError is raised. To access the smallest item without popping it, use heap[0].</div><div class="line"></div><div class="line">heapq.heappushpop(heap, item)</div><div class="line">Push item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush() followed by a separate call to heappop().</div><div class="line"></div><div class="line">New in version 2.6.</div><div class="line"></div><div class="line">heapq.heapify(x)</div><div class="line">Transform list x into a heap, in-place, in linear time.</div><div class="line"></div><div class="line">heapq.heapreplace(heap, item)</div><div class="line">Pop and return the smallest item from the heap, and also push the new item. The heap size doesn’t change. If the heap is empty, IndexError is raised.</div><div class="line"></div><div class="line">This one step operation is more efficient than a heappop() followed by heappush() and can be more appropriate when using a fixed-size heap. The pop/push combination always returns an element from the heap and replaces it with item.</div><div class="line"></div><div class="line">The value returned may be larger than the item added. If that isn’t desired, consider using heappushpop() instead. Its push/pop combination returns the smaller of the two values, leaving the larger value on the heap.</div><div class="line"></div><div class="line">The module also offers three general purpose functions based on heaps.</div><div class="line"></div><div class="line">heapq.merge(*iterables)</div><div class="line">Merge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values.</div><div class="line"></div><div class="line">Similar to sorted(itertools.chain(*iterables)) but returns an iterable, does not pull the data into memory all at once, and assumes that each of the input streams is already sorted (smallest to largest).</div><div class="line"></div><div class="line">heapq.nlargest(n, iterable[, key])</div><div class="line">Return a list with the n largest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in the iterable: key=str.lower Equivalent to: sorted(iterable, key=key, reverse=True)[:n]</div><div class="line"></div><div class="line">heapq.nsmallest(n, iterable[, key])</div><div class="line">Return a list with the n smallest elements from the dataset defined by iterable. key, if provided, specifies a function of one argument that is used to extract a comparison key from each element in the iterable: key=str.lower Equivalent to: sorted(iterable, key=key)[:n]</div></pre></td></tr></table></figure></p>
<p><strong>Time complexity</strong><br>heapq.heapify(x): O(k)<br>heapq.heappush(heap, item): O(logk)<br>heapq.heappop(heap): O(logk)</p>
<p>对于 nsmallest 和 nlargest 的时间复杂度有点疑惑，找了些资料，就源代码而言，应该是 O(nlogt)，然而有一种说法是 O(t+n)，见 <a href="http://stackoverflow.com/questions/23038756/how-does-heapq-nlargest-work" target="_blank" rel="external">How does heapq.nlargest work?</a><br><a href="https://hg.python.org/cpython/file/3.4/Lib/heapq.py#l195" target="_blank" rel="external">source code</a>，2.7 和 3.4 这一部分是一样的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def nsmallest(n, iterable):</div><div class="line">    &quot;&quot;&quot;Find the n smallest elements in a dataset.</div><div class="line"></div><div class="line">    Equivalent to:  sorted(iterable)[:n]</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    if n &lt; 0:</div><div class="line">        return []</div><div class="line">    it = iter(iterable)</div><div class="line">    result = list(islice(it, n))</div><div class="line">    if not result:</div><div class="line">        return result</div><div class="line">    _heapify_max(result)</div><div class="line">    _heappushpop = _heappushpop_max</div><div class="line">    for elem in it:</div><div class="line">        _heappushpop(result, elem)</div><div class="line">    result.sort()</div><div class="line">    return result</div></pre></td></tr></table></figure></p>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="23-Merge-k-Sorted-Lists"><a href="#23-Merge-k-Sorted-Lists" class="headerlink" title="23. Merge k Sorted Lists"></a>23. Merge k Sorted Lists</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity.</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Find out the minimum value in a bunch of elements, we use minheap.</div><div class="line">    Time complexity for heap: heapify: O(k)   push/poll an elem: O(logk)</div><div class="line"></div><div class="line">    Space complexity: O(k)</div><div class="line">    Time complexity: O(k+avg(n)klogk)</div><div class="line">&apos;&apos;&apos;</div><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def mergeKLists(self, lists):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type lists: List[ListNode]</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        root=ListNode(0)</div><div class="line">        dummy=root</div><div class="line">        h=[]</div><div class="line">        for node in lists:</div><div class="line">            if node:</div><div class="line">                heapq.heappush(h,(node.val,node))</div><div class="line">        while h:</div><div class="line">            val,node=heapq.heappop(h)</div><div class="line">            dummy.next=ListNode(val)</div><div class="line">            dummy=dummy.next</div><div class="line">            if node.next:</div><div class="line">                heapq.heappush(h,(node.next.val,node.next))</div><div class="line">        return root.next</div><div class="line"></div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def mergeKLists(self, lists):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type lists: List[ListNode]</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        largeList=[]</div><div class="line">        for root in lists:</div><div class="line">            while root:</div><div class="line">                largeList.append(root.val)</div><div class="line">                root=root.next</div><div class="line">        largeList.sort()</div><div class="line">        print largeList</div><div class="line">        root=ListNode(0)</div><div class="line">        dummy=root</div><div class="line">        for n in largeList:</div><div class="line">            print dummy.val</div><div class="line">            dummy.next=ListNode(n)</div><div class="line">            dummy=dummy.next</div><div class="line">        return root.next</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 堆 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kNN 小结]]></title>
      <url>http://www.shuang0420.com/2016/12/26/kNN%E5%B0%8F%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>回顾传统 kNN 算法以及优化方法。<br><a id="more"></a></p>
<p>kNN 的基本思路，给定一个训练数据集，对新的输入 instance，在训练数据集中找到与之最邻近的 k 个 instance，然后看这 k 个 instance 的大多数属于哪个类，那么这个类就是输入 instance 的最终类别。</p>
<h1 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h1><p>这算是常识性的知识啦，不过每次都会提醒一下。对于数值型的特征，我们一般都会进行归一化，如将数值范围处理到 0 到 1 之间，以此来保证每个特征是同等重要的。</p>
<p>来看看下面的例子，如果不归一化，那么，第一列(数字差值最大)的属性对计算结果的影响最大(代入相似度距离计算公式)，然而我们不希望这样。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">40920	8.326976	0.953952	largeDoses</div><div class="line">14488	7.153469	1.673904	smallDoses</div><div class="line">26052	1.441871	0.805124	didntLike</div><div class="line">75136	13.147394	0.428964	didntLike</div><div class="line">38344	1.669788	0.134296	didntLike</div><div class="line">72993	10.141740	1.032955	didntLike</div><div class="line">35948	6.830792	1.213192	largeDoses</div><div class="line">42666	13.276369	0.543880	largeDoses</div><div class="line">......</div></pre></td></tr></table></figure></p>
<p>最简单的归一化做法 newValue=(oldValue-min)/(max-min)</p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def autoNorm(dataSet):</div><div class="line">    minVals = dataSet.min(0)  # select min value from columns</div><div class="line">    maxVals = dataSet.max(0) # select max value from columns</div><div class="line">    ranges = maxVals - minVals # max-min</div><div class="line">    m = dataSet.shape[0]</div><div class="line">    normDataSet = zeros(shape(dataSet))</div><div class="line">    normDataSet = dataSet - tile(minVals, (m, 1)) # oldValue-min</div><div class="line">    normDataSet = normDataSet / tile(ranges, (m, 1)) # division</div><div class="line">    return normDataSet, ranges, minVals</div></pre></td></tr></table></figure></p>
<p>当然，要记住在做分类的时候，也要先对特征进行归一化。</p>
<h1 id="相似度距离计算"><a href="#相似度距离计算" class="headerlink" title="相似度距离计算"></a>相似度距离计算</h1><p>怎么来判断邻近？如何来度量两个点之间的距离？距离选择很大程度上影响 kNN 的效果，因此它必须能足够的体现出样本间的相似和不同的程度，最常用的是欧式距离。</p>
<p>$$d(x,y):=\sqrt {(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2}=\sqrt {\sum^n_{i=1}(x_i-y_i)^2}$$</p>
<p>保存前 K 个点，可以用 <strong>最大堆(Max Heap)</strong> 实现。</p>
<p>根据具体的问题，距离也可以采用 <strong>余弦距离、海明距离、编辑距离</strong> 等等。</p>
<h1 id="k-值选取"><a href="#k-值选取" class="headerlink" title="k 值选取"></a>k 值选取</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/1.jpg" class="ful-image" alt="1.jpg">
<p>k 的选取是很重要的，看上面一个例子，蓝色和红色分别代表两个不同的类 B 和 R，绿色是输入 instance，我们要对其进行分类，可以发现，k 取 3 和 5 得到的分类结果是完全不同的。</p>
<ul>
<li>k=3，新的 instance 属于 R 类别，因为离它最近的 3 个 instance 是有 2 个属于 R，1 个属于 B。</li>
<li>k=5，新的 instance 属于 R 类别，因为离它最近的 5 个 instance 是有 2 个属于 R，3 个属于 B。</li>
</ul>
<p>所以，<strong>k 怎么选？</strong></p>
<p>事实上，通常来说，我们在一开始会选取一个较小的 k 值(k=1)，然后采取交叉验证(cross-validation)的方法，直到 k=n，取使交叉验证得到最好的结果的那个 k，经验上，k 小于数据集大小的平方根。</p>
<p><strong>k 值太小 or k 值太大</strong><br>k 值太小容易产生过拟合，因为它很容易学到噪声，比如说 k=1，那么就只用看和输入 instance 最邻近的一个 instance，举个例子吧<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/2.jpg" class="ful-image" alt="2.jpg"></p>
<p>而另一方面，k 值太大，那么意味着你的模型变得更加的简单，比如说 k=N(N为训练样本的个数)，那么无论输入的 instance 是什么类别，都会归到训练集中 instance 最多的那个类，也就是说，根本没有进行训练，只是简单的 count 而已，并没有利用训练集的其他大量的有用信息。</p>
<h1 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h1><p>KNN 是一种 instance-based method，对于未知和非正态分布的数据可以取得较高的分类准确率，优缺点如下：</p>
<p><strong>优点：</strong></p>
<ul>
<li>算法简单直观，易于实现</li>
<li>免训练，参数少</li>
<li>不需要产生额外的数据来描述规则，它的规则就是训练数据（样本）本身， 并不是要求数据的一致性问题，即可以存在噪音</li>
<li>虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。因此，采用这种方法可以较好地避免样本数量的不平衡问题，当然，在 k 值很大而样本又极度不平衡的情况下，结果就不妙了</li>
<li>最直接地利用了样本之间的关系，减少了类别特征选择不当对分类结果造成的不利影响，可以最大程度地减少分类过程中的误差项。对于一些类别特征不明显的类别而言，KNN法更能体现出其分类规则独立性的优势，使得分类自学习的实现成为可能</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>时空复杂度高，分类速度慢<br>  需要将所有训练样本首先存储起来，进行分类时实时进行计算处理，需要计算待分样本与训练样本库中每一个样本的相似度，才能求得与其最近的K个样本<br>  对于高维样本或样本集规模较大的情况，其时间和空间复杂度较高，时间代价为O(mn)，其中 m 为向量空间模型空间特征维数，n 为训练样本集大小   </li>
<li>样本库容量依赖性较强<br>  有不少类别无法提供足够的训练样本，产生分类误差</li>
<li>特征作用相同<br>  传统 KNN 认为每个属性的作用都是相同的(赋予相同权重)，而实际情况下，有些特征与分类是强相关的，有些特征与分类是弱相关的，还有一些特征(可能是大部分)与分类不相关。</li>
<li>K值的确定 KNN 算法必须指定 K 值，K 值选择不当则分类精度不能保证。</li>
</ul>
<h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><h2 id="加快分类速度"><a href="#加快分类速度" class="headerlink" title="加快分类速度"></a>加快分类速度</h2><p><strong>解决思路：</strong> 一是减少样本量，二是加快搜索 k 近邻。</p>
<h3 id="减少样本量"><a href="#减少样本量" class="headerlink" title="减少样本量"></a>减少样本量</h3><p>当训练样本集中样本数量太大时，可以从原始训练样本集中选择最优的子集进行 KNN 的寻找，这类方法主要包括 <strong>Condensing算法、WilSon 的 Editing 算法和 Devijver 的 MultiEdit 算法</strong>，Kuncheva 使用 <strong>遗传算法</strong> 在这方面也进行了一些研究。</p>
<h3 id="加快搜索-k-近邻"><a href="#加快搜索-k-近邻" class="headerlink" title="加快搜索 k 近邻"></a>加快搜索 k 近邻</h3><p>主要通过快速的搜索算法来实现，采用一定的方法加快搜索速度或减小搜索范围，如可以构造交叉索引表，利用匹配成功与否的历史来修改样本库的结构，使用样本和概念来构造层次或网络来组织训练样本。</p>
<p>常用的方法是先建立数据索引，然后再进行快速匹配。因为实际数据一般都会呈现出簇状的聚类形态，通过设计有效的索引结构可以大大加快检索的速度。索引树属于这一类，其基本思想就是对搜索空间进行层次划分。根据划分的空间是否有混叠可以分为 Clipping 和 Overlapping 两种。前者划分空间没有重叠，其代表就是 k-d 树；后者划分空间相互有交叠，其代表为 R 树。(这里只介绍k-d树)</p>
<h4 id="KD-Tree"><a href="#KD-Tree" class="headerlink" title="KD Tree"></a>KD Tree</h4><h5 id="构建-k-d-tree"><a href="#构建-k-d-tree" class="headerlink" title="构建 k-d tree"></a>构建 k-d tree</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">1.If Data-set为空，则返回空的k-d tree</div><div class="line">2.调用节点生成程序：</div><div class="line">    - 确定split域</div><div class="line">    对于所有描述子数据（特征矢量），统计它们在每个维上的数据方差。以SURF特征为例，描述子为64维，可计算64个方差。挑选出最大值，对应的维就是split域的值。数据方差大表明沿该坐标轴方向上的数据分散得比较开，在这个方向上进行数据分割有较好的分辨率</div><div class="line">    - 确定Node-data域</div><div class="line">    数据点集Data-set按其第split域的值排序。位于正中间的那个数据点被选为Node-data。此时新的Data-set&apos; = Data-set\Node-data（除去其中Node-data这一点）。</div><div class="line">3.dataleft = &#123;d属于Data-set&apos; &amp;&amp; d[split] ≤ Node-data[split]&#125;</div><div class="line">  Left_Range = &#123;Range &amp;&amp; dataleft&#125;</div><div class="line">  dataright = &#123;d属于Data-set&apos; &amp;&amp; d[split] &gt; Node-data[split]&#125;</div><div class="line">  Right_Range = &#123;Range &amp;&amp; dataright&#125;</div><div class="line">4.left = 由（dataleft，Left_Range）建立的k-d tree，即递归调用 createKDTree（dataleft，Left_Range）并设置 left 的 parent 域为 Kd；</div><div class="line">  right = 由（dataright，Right_Range）建立的k-d tree，即调用createKDTree（dataleft，Left_Range）并设置 right 的 parent 域为 Kd。</div></pre></td></tr></table></figure>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/4.jpg" class="ful-image" alt="4.jpg">
<p>如上图的 2 维数据，构建 KD Tree 过程：</p>
<ul>
<li>确定 split 域的首先该取的值。分别计算 x，y 方向上数据的方差得知x方向上的方差最大，所以 split 域值首先取0，也就是 x 轴方向；</li>
<li>确定 Node-data 的域值。根据 x 轴方向的值 2,4,5,7,8,9 排序选出中值为7，所以 Node-data = (7,2)。这样，该节点的分割超平面就是通过(7,2)并垂直于 split = 0(x轴)的直线x = 7；</li>
<li>确定左子空间和右子空间。分割超平面x = 7将整个空间分为两部分，如图2所示。x &lt; =  7的部分为左子空间，包含3个节点{(2,3)，(5,4)，(4,7)}；另一部分为右子空间，包含2个节点{(9,6)，(8,1)}。</li>
</ul>
<p>最后生成的 k-d 树。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/5.jpg" class="ful-image" alt="5.jpg"></p>
<h5 id="k-d-tree-寻找"><a href="#k-d-tree-寻找" class="headerlink" title="k-d tree 寻找"></a>k-d tree 寻找</h5><p>星号表示要查询的点(2.1,3.1)。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点(2,3)。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从(7,2)点开始进行二叉查找，然后到达(5,4)，最后到达(2,3)，此时搜索路径中的节点为&lt;(7,2)，(5,4)，(2,3)&gt;，首先以(2,3)作为当前最近邻点，计算其到查询点(2.1,3.1)的距离为0.1414，然后回溯到其父节点(5,4)，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以(2.1,3.1)为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入(5,4)节点右子空间中去搜索。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/6.jpg" class="ful-image" alt="6.jpg">
<p>再回溯到(7,2)，以(2.1,3.1)为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入(7,2)右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点(2,3)，最近距离为0.1414。</p>
<p>一个复杂点的例子如查找点为(2，4.5)。同样先进行二叉查找，先从(7,2)查找到(5,4)节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到(4,7)，形成搜索路径&lt;(7,2)，(5,4)，(4,7)&gt;，取(4,7)为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到(5,4)，计算其与查找点之间的距离为3.041。以(2，4.5)为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找。此时需将(2,3)节点加入搜索路径中得&lt;(7,2)，(2,3)&gt;。回溯至(2,3)叶子节点，(2,3)距离(2,4.5)比(5,4)要近，所以最近邻点更新为(2，3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点(2,3)，最近距离1.5。k-d树查询算法的伪代码如表3所示。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/kNN%E5%B0%8F%E7%BB%93/7.jpg" class="ful-image" alt="7.jpg">
<p>上述两次实例表明，当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间，导致检索过程复杂，效率下降。研究表明 N 个节点的 K 维 k-d 树搜索过程时间复杂度为：$O_{worst}=O(kN^{1-1/k})$。</p>
<p>关于 kd tree 的介绍来自于 <a href="http://www.cnblogs.com/eyeszjwang/articles/2429382.html" target="_blank" rel="external">k-d tree算法</a></p>
<h2 id="训练样本的维护"><a href="#训练样本的维护" class="headerlink" title="训练样本的维护"></a>训练样本的维护</h2><p>对训练样本库进行维护以满足 KNN 算法的需要，包括对训练样本库中的样本进行添加或删除。对样本库的维护并不是简单的增加删除样本，而是可采用适当的办法来保证空间的大小，如符合某种条件的样本可以加入数据库中，同时可以对数据库库中已有符合某种条件的样本进行删除。从而保证训练样本库中的样本提供 KNN 算法所需要的相对均匀的特征空间。</p>
<h2 id="相似度距离公式优化"><a href="#相似度距离公式优化" class="headerlink" title="相似度距离公式优化"></a>相似度距离公式优化</h2><p>为了改变传统 KNN 算法中特征作用相同的缺陷，可以对相似度的距离公式中给特征赋予不同权重，例如在欧氏距离公式中给不同特征赋予不同权重。特征的权重一般根据各个特征在分类中的作用设定，可根据特征在整个训练样本库中的所起的作用大小来确定权重，也可根据在训练样本的局部样本(靠近待测试样本的样本集合)中的分类作用确定权重。</p>
<h2 id="K-值确定"><a href="#K-值确定" class="headerlink" title="K 值确定"></a>K 值确定</h2><ul>
<li>K的选择往往通过大量独立的测试数据、多个模型来验证最佳的选择；</li>
<li>K值一般事先确定，也可以使用动态的，例如采用固定的距离指标，只对小于该指标的样本进行分析</li>
</ul>
<blockquote>
<p>参考链接：<br><a href="http://kesmlcv.blogspot.com/2013/08/knn.html" target="_blank" rel="external">KNN算法的优缺点和改进方法</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[新的开始]]></title>
      <url>http://www.shuang0420.com/2016/12/20/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/</url>
      <content type="html"><![CDATA[<p>一直提醒着自己，不要活成别人的样子，然而一天天，一年年，还是偏离了初衷，走着别人认为正确认为光明的路。<br><a id="more"></a><br>学金融、留学、做码农、刷题拼着进 google…</p>
<p>大抵是因为，不知道自己想要的究竟是啥样的，又觉着自己啥都无所谓，于是想着，那就跟着大流走吧，哪里最热就往哪里去，然后到了这里。</p>
<p>然而在这一学期终究是厌倦了。每天忙着做作业、刷题、找工作，唯独没有时间好好的学习。阅读列表里搁着一堆的待读文章也永远是待读的状态。</p>
<p>在这一刻回顾，只觉得茫茫然无所适从。太匆匆，太匆匆。做了很多题，写了很多算法，过的无比的“充实”，却不安心，好像学的是别人的，自己增长的不是知识，而只是经验。</p>
<p>更可怕的是，整夜整夜的失眠，觉着没什么记忆，如果有，也只是断层的，并不连贯。</p>
<p>并不满意自己现在的状态。也不想这样继续下去。</p>
<p>TVB 里说滥了的一句台词，做人呢，最重要就是开心，倒真是话糙理不糙。</p>
<p>可是，会笑，会闹，会折腾，并不意味着开心。</p>
<p>一直听着“既然来了美国，就争取留下来，再次也工作两年镀层金，回国起点就高了”“国外码农环境好，赚的多，又不累，还是多待几年”类似的话，听着听着就觉得，哎，好吧，既然你们都这么说，那我就试试。</p>
<p>然后就放了话，可是我只想进 google 怎么办，其它我都不喜欢。要不，进不了 google 我就回国去？</p>
<p>这句话现在看来也只能骗鬼了。你问为啥要去 google？因为它名气最大。就像纽约之于金融，硅谷之于码农。可是你再问下去，为什么？我却说不上来，大抵是进 google 是不会错的，不会有风险的。</p>
<p>付出努力了吗？</p>
<p>自然是有的，买了 leetcode 会员，刷了一些题，然而一直也并没有什么动力。潜意识里大概也觉着我是没那个水平进去的，进去了又如何呢？进不去又如何呢？好像也无所谓。进去了可能会有一瞬间的欢喜，然而又怎么样呢？进不去……大概会松一口气。</p>
<p>所以，你看，我也只不过想找个借口回国而已。</p>
<p>在这里终究是不安宁的，不是指走夜路总担心有个人突然拿着把枪出来的不安宁，而是觉着，被大家推着走。某个阶层，某个群体，好像总有着高度统一的价值观，什么是对，什么是错，什么时候该做什么事，界限分明像一本用了很多年的字典，真是令人沮丧。</p>
<p>之前和某个学心理学的朋友聊天，说到内心不平静，不安宁，怎么办。她说，一点点缩小自己的圈子。现在，我的圈子很小很小，才发现，缩小圈子或许只是一个表征，内在的核心是，专注于自身。自身的欢喜，自身的成长。不因外物而对自己施压，爱别人尊重别人的同时，也爱自己，尊重自己。相信自己的选择，喜欢自己的决定。</p>
<p>如果这个决定恰巧顺从了潮流，那么皆大欢喜，如果不是，那么，也要做好准备，因为逆行，意味着孤独。</p>
<p>索性人尽管不能离群索居，却只用对自己负责。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/flower.jpg" class="ful-image" alt="flower.jpg">
<p>接下来的时间，我会花很长的时间来培养好的习惯，早起、读书、健身。</p>
<p>会花两三天看一篇论文，庖丁解牛式。</p>
<p>会花一周做一个项目，争取优化到方方面面。</p>
<p>会去读“无用之书”，去沉淀去感受形而上的东西。</p>
<p>不会再因为 due 或者 final 或者面试轻易放弃这些重要而不紧急的事。</p>
<p>会争取将我的一点点努力，呈现在博客里。</p>
<p>技术是一件很美好的东西，生活也是。</p>
<p>相对于自然，地理不过是细节。</p>
<p>相对于生活，知识也不过是细节。</p>
<p>能让我们丰盛的，是情感，是体验。</p>
<p>记得很多年前，同学说我应该是那种坐在欧洲某家咖啡店的窗边静静翻着英文书的人。</p>
<p>他说错了。我不喜欢喝咖啡。也不喜欢看英文书。</p>
<p>嗯其它或许是对的。</p>
<p>我向往这种慢悠悠的日子。</p>
<p>一张一弛，文武之道。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/west.jpg" class="ful-image" alt="west.jpg">
]]></content>
      
        <categories>
            
            <category> 随笔 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Distributed Systems笔记－NFS、AFS、GFS]]></title>
      <url>http://www.shuang0420.com/2016/12/10/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/</url>
      <content type="html"><![CDATA[<p>CMU 95702 关于 NFS、AFS、GFS 的笔记。<br><a id="more"></a></p>
<h1 id="NFS-Network-File-System"><a href="#NFS-Network-File-System" class="headerlink" title="NFS(Network File System)"></a>NFS(Network File System)</h1><p><strong>目的：</strong></p>
<ul>
<li>Your files are available from any machine.</li>
<li>Distribute the files and we will not have to implement new protocols.</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>Defines a virtual C/S file system</li>
<li>Stateless</li>
<li>Uses RPC over TCP or UDP.</li>
</ul>
<p>NFS 的实质在于用户间计算机的共享。用户通过 NFS 客户端接入网络，可以访问同一网络中其它计算机系统的硬盘（该计算机为 NFS 服务端）。NFS 客户端可以 mount 远端文件系统的部分或全部到本地，访问这些文件系统就像访问在本地磁盘上的文件系统一样。</p>
<p>NFS 访问数据的速度以接近采用本地磁盘的速度为目标，NFS客户端的性能直接取决于服务端的性能和网络性能。如：</p>
<ul>
<li>网络的最大吞吐量</li>
<li>服务端硬件性能：网卡，磁盘等</li>
<li>服务端缓存大小，TCP/IP的配置</li>
<li>服务端服务实例的运行个数</li>
<li>客户端请求的网络文件数</li>
<li>客户端的系统性能<br>其它运行在客户或服务端上与NFS竞争资源的进程</li>
</ul>
<p>NFS客户端将用户级别命令转化为RPC；NFS服务端将RPC转换为用户级别命令。<br>NFS的主要缺点：文件服务器的定位对客户端非透明，即客户端需要知道服务端的确切地址（挂载点），这也导致了其可扩展性差，维护困难，优点是发展多年，Linux内核直接支持，使用简单方便。</p>
<h2 id="NFS-architecture"><a href="#NFS-architecture" class="headerlink" title="NFS architecture"></a>NFS architecture</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/nfsArch.jpg" class="ful-image" alt="nfsArch.jpg">
<h2 id="NFS-server-operations"><a href="#NFS-server-operations" class="headerlink" title="NFS server operations"></a>NFS server operations</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/nfsS1.jpg" class="ful-image" alt="nfsS1.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/nfsS2.jpg" class="ful-image" alt="nfsS2.jpg">
<p>-&gt; The directory and file operations are integrated into a single service.</p>
<h2 id="NFS-client"><a href="#NFS-client" class="headerlink" title="NFS client"></a>NFS client</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/nfsC.jpg" class="ful-image" alt="nfsC.jpg">
<h1 id="AFS-Andrew-File-System"><a href="#AFS-Andrew-File-System" class="headerlink" title="AFS(Andrew File System)"></a>AFS(Andrew File System)</h1><p><strong>目的：</strong></p>
<ul>
<li>Scalability</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>Modified from Coulouris</li>
<li>Cache<br>Whole files are cached in client nodes to reduce client server interactions -&gt; achieve scalability.<br>A client cache would typically hold several hundreds of files most recently used on that computer.<br>Permanent cache, surviving reboots.</li>
<li>Consider UNIX commands and libraries copied to the client.</li>
<li>Consider files only used by a single user.<br>These last two cases represent the vast majority of cases.</li>
<li>Gain: Your files are available from any workstation.</li>
<li>Principle: Make the common case fast.</li>
</ul>
<p><strong>Open file:</strong></p>
<ul>
<li>When the client tries to open a file<br>client cache is tried first<br>if not there, a server is located and the server is called for the file.</li>
<li>The copy is stored on the client side and is opened.</li>
<li>Subsequent reads and writes hit the copy on the client.</li>
</ul>
<p><strong>Close file:</strong></p>
<ul>
<li>When the client closes the file - if the files has changed it is sent back to the server. The client side copy is retained for possible more use.</li>
</ul>
<p>AFS(Andrew File System) 文件系统主要用于管理分部在不同网络节点上的文件。AFS 采用安全认证和灵活的访问控制提供一种分布式的文件和授权服务，该服务可以扩展到多个客户端。</p>
<p>AFS与NFS不同，AFS提供给用户的是一个完全透明，永远唯一的逻辑路径。因而其具有跨平台，分布式的特点。但是由于AFS使用本地文件系统来缓存最近被访问的文件块，访问一个在本地的AFS文件由于需要附加一些耗时的操作，比直接访问本地的其它文件要慢很多。AFS为读操作做了优化，写操作很复杂，是一个读快写慢的文件系统，不能提供很好的读写并发能力。</p>
<h2 id="AFS-architecture"><a href="#AFS-architecture" class="headerlink" title="AFS architecture"></a>AFS architecture</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/afsArch.jpg" class="ful-image" alt="afsArch.jpg">
<h2 id="Implementation-of-file-system-calls-in-AFS"><a href="#Implementation-of-file-system-calls-in-AFS" class="headerlink" title="Implementation of file system calls in AFS"></a>Implementation of file system calls in AFS</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/afsO.jpg" class="ful-image" alt="afsO.jpg">
<p><strong>File name space seen by clients of AFS</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/afsC.jpg" class="ful-image" alt="afsC.jpg"></p>
<p><strong>System call interception in AFS</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/afsI.jpg" class="ful-image" alt="afsI.jpg"></p>
<p><strong>The main components of the Vice service interface</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/afsCom.jpg" class="ful-image" alt="afsCom.jpg"></p>
<p>CMU’s Coda is an enhanced descendant of AFS<br>Very briefly, two important features are:<br>Disconnected operation for mobile computing.<br>Continued operation during partial network failures in server network.<br>During normal operation, a user reads and writes to the file system normally, while the client fetches, or “hoards”, all of the data the user has<br>listed as important in the event of network disconnection.<br>If the network connection is lost, the Coda client’s local cache serves data from this cache and logs all updates.<br>Upon network reconnection, the client moves to reintegration state; it sends logged updates to the servers. From Wikipedia</p>
<h1 id="GFS-Google-File-System"><a href="#GFS-Google-File-System" class="headerlink" title="GFS(Google File System)"></a>GFS(Google File System)</h1><p><strong>目的：</strong></p>
<ul>
<li>Scalability</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>Reliably with component failures.</li>
<li>Massively large files<br>Solve problems that Google needs solved – not a massive number of files but massively large files are common. Write once, append, read many times.</li>
<li>Streaming and no cache<br>Access is dominated by long sequential streaming reads and sequential appends. No need for caching on the client.</li>
<li>Throughput more important than latency.</li>
<li>Each file is mapped to a set of fixed size chunks(64Mb/chunk).</li>
<li>3 replicas<br>Each chunk is replicated on three different chunk servers.</li>
<li>Master and chunk servers<br>Each cluster has a single master and multiple (usually hundreds) of chunk servers.<br>The master knows the locations of chunk replicas.<br>The chunk servers know what replicas they have and are polled by the master on startup.</li>
</ul>
<p>Think of very large files each holding a very large number of HTML documents scanned from the web. These need read and analyzed.<br>This is not your everyday use of a distributed file system (NFS and AFS). Not POSIX.</p>
<h2 id="Google-physical-infrastructure"><a href="#Google-physical-infrastructure" class="headerlink" title="Google physical infrastructure"></a>Google physical infrastructure</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/gfsIns.jpg" class="ful-image" alt="gfsIns.jpg">
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/gfsArch.jpg" class="ful-image" alt="gfsArch.jpg">
<h2 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h2><h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><p>Suppose a client wants to perform a sequential read, processing a very large file from a particular byte offset.</p>
<ul>
<li>The client can compute the chunk index from the byte offset.</li>
<li>Client calls master with file name and chunk index.</li>
<li>Master returns chunk identifier and the locations of replicas.</li>
<li>Client makes call on a chunk server for the chunk and it is processed sequentially with no caching. It may ask for and receive several chunks.</li>
</ul>
<h3 id="Mutation"><a href="#Mutation" class="headerlink" title="Mutation"></a>Mutation</h3><p>Suppose a client wants to perform sequential writes to the end of a file.</p>
<ul>
<li>The client can compute the chunk index from the byte offset. This is the chunk holding End Of File.</li>
<li>Client calls master with file name and chunk index.</li>
<li>Master returns chunk identifier and the locations of replicas. One is designated as the primary.</li>
<li>The client sends all data to all replicas.The primary coordinates with replicas to update files<br>consistently across replicas.</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Distributed Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Federated Search]]></title>
      <url>http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。垂直数据库能获得更准确的搜索结果。那么对一个 query，我们可以放到合适的多个垂直数据库里检索，然后合并结果呈现给用户。<br><a id="more"></a><br>简单解释就是结果来自多个数据库。用图片表示<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/fs1.jpg" class="ful-image" alt="fs1.jpg"></p>
<p>当然，大多数数据库与我们的需求并不相关，所以可以略过。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/fs2.jpg" class="ful-image" alt="fs2.jpg"></p>
<p>Search portals 可以有不同的策略来处理不同类型的请求</p>
<ul>
<li>搜索非结构化数据<ul>
<li>将初始查询映射为对应的结构化查询</li>
<li>将查询定向到特定的搜索引擎<br>» 汽车，音乐，图片，视频…</li>
</ul>
</li>
<li>搜索结构化数据（数据库） - 例如，邮政编码，股票代码…<ul>
<li>调用服务或进程<ul>
<li>如，计算器，股票价格，航班跟踪…</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/fs3.jpg" class="ful-image" alt="fs3.jpg">
<h1 id="Constrains"><a href="#Constrains" class="headerlink" title="Constrains"></a>Constrains</h1><p><strong>不合作环境</strong></p>
<ul>
<li>没有特别的 federated search 的支持</li>
<li>资源不受信任</li>
</ul>
<p><strong>合作环境</strong></p>
<ul>
<li>资源支持公共协议/ API</li>
<li>资源是受信任的，能够提供准确的信息</li>
</ul>
<h1 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h1><ol>
<li>Resource representation(资源表达)。即获取每个数据库的信息</li>
<li>Resource selection(资源选择)。即对 resource/databases 进行排序，对每个查询选择少量资源进行检索。</li>
<li>Result-merging(结果合并)。即对来自不同搜索引擎／数据库对结果进行 merge，产生最终展现给用户的 ranking list。</li>
</ol>
<p>其中 1 是在线下完成的(offline)，2、3 是在查询时完成的。</p>
<h1 id="Resource-representation"><a href="#Resource-representation" class="headerlink" title="Resource representation"></a>Resource representation</h1><p><strong>如何表达资源的内容？</strong></p>
<ol>
<li>Bag of words: terms and frequencies</li>
<li>Sample queries: Queries that this resource is good for</li>
<li>Sample documents: Typical documents from this resource</li>
</ol>
<p>一个例子解释<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/rr1.jpg" class="ful-image" alt="rr1.jpg"></p>
<p><strong>资源的内容信息如何获取</strong></p>
<ul>
<li>通过 protocol 向资源发出请求</li>
<li>对 query log 请求相关性评价 (relevance assessments)</li>
<li>Query-based sampling: 提交查询，查看返回的内容</li>
</ul>
<p>下面主要介绍第三种 <strong>query-based sampling</strong></p>
<h2 id="Query-based-sampling"><a href="#Query-based-sampling" class="headerlink" title="Query-based sampling"></a>Query-based sampling</h2><p>过程</p>
<ol>
<li>选择一个初始查询</li>
<li>重复 N 次（例如，N = 100）<ul>
<li>向搜索引擎提交查询</li>
<li>下载一些结果（例如，2-4 条）</li>
<li>得到并更新 representation (words and frequencies)</li>
<li>从新的 representation 中随机选择 query term(s)形成新的查询</li>
</ul>
</li>
</ol>
<p><strong>为什么可行?</strong><br>因为词汇分布服从 Heaps’ Law</p>
<p><strong>如果第一条查询非常糟糕怎么办？</strong><br>如在一个医学领域的语料库里找 car，可能并没有结果(如果是 boolean ranking 的话)，那么就重新选择一个查询？</p>
<p><strong>抽样的大小有什么影响</strong><br>一条 query 取多少文档呢？实验决定吧，2-4 篇应该就够了</p>
<p><strong>在合作环境下随机采样</strong><br>只比 query-based sampling 好一点点</p>
<h1 id="Resource-selection"><a href="#Resource-selection" class="headerlink" title="Resource selection"></a>Resource selection</h1><p>两大类方法，无监督学习通常是通过对 P(ri|q) 排序来解决，监督算法通常通过分类解决。</p>
<h2 id="Unsupervised-Resource-Selection"><a href="#Unsupervised-Resource-Selection" class="headerlink" title="Unsupervised Resource Selection"></a>Unsupervised Resource Selection</h2><p><strong>Task:</strong> 给定查询 q，决定要搜索的资源(resources)</p>
<p>无监督方法将这个问题看作 resource ranking 的问题。</p>
<ol>
<li>估计 p(ri|q)</li>
<li>选择前 k 个资源<br>– 通常 k 是给出的<br>– 动态设置 k 是一个开放的研究问题</li>
</ol>
<p>两种具体方法，content-based methods 和 query-based methods</p>
<ul>
<li>Content-based methods(基于内容的方法)<br>– 对各个资源下 query 和 content 的相似度 $S(q,c)$ 来对资源进行排序<br>– 有不同的方法来实现，区分点在于：<br>» Representation type: bag of words vs. sampled documents<br>» Ranking algorithm: e.g., CORI</li>
<li>Query-based methods(基于查询的方法)<br>– 对 query，搜索各个资源下的 query log，找出 match 的 query，然后求当前 query 和历史 query 的相似度 $S(q,q_{past})$，对资源进行排序<br>– 之前用的并不多，因为缺少好的 query log</li>
</ul>
<h3 id="Content-based-methods"><a href="#Content-based-methods" class="headerlink" title="Content-based methods"></a>Content-based methods</h3><h4 id="Bag-of-words-method"><a href="#Bag-of-words-method" class="headerlink" title="Bag-of-words method"></a>Bag-of-words method</h4><p>基本思想是 <strong>将每个资源视为一个（非常大的）词袋(bag of words)</strong>，排序算法可以用 KL divergence 或者 query likelihood 等方法。<br>eg. CORI<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/cori.jpg" class="ful-image" alt="cori.jpg"></p>
<p>然而，Bag of words methods (e.g., CORI)是根据 resources 和 query 的相似度进行的排序，这并不是我们的初衷。这种方法 <strong>偏好具有较高 p(qi|Rj) 的资源</strong>，通常意义上也就是 homogeneous (通常很小)的资源。这并不是我们需要的，我们想要的是，选择能够返回更多相关文档的资源，sampled document method 更能达到这个目标。</p>
<p><strong>Bag of words (“large documents”) 方法的特点</strong></p>
<ul>
<li>大文档更好的代表了 resource representation</li>
<li>偏好具有较大比例相关内容的资源<br>– i.e., small or homogeneous resources</li>
<li>非常有效，很有竞争力</li>
</ul>
<h4 id="Sample-documents-method"><a href="#Sample-documents-method" class="headerlink" title="Sample documents method"></a>Sample documents method</h4><p><strong>基本思想：</strong> 对每个资源采样然后合并得到一个 centralized index，记录每个文档来自哪个 resource。<br>eg. ReDDE<br>给定一个查询</p>
<ol>
<li>搜索 centralized sample index 得到相关文档</li>
<li>取前 T 的文档</li>
<li>检查哪些资源提供了这些相关文档</li>
<li>估计每个资源中相关文档的数量<br>• 计算资源 i 中高于阈值 T 的文档数量 n<br>• n * resource_size / sample_size</li>
</ol>
<p>ReDDE 可以看作是 sample-based voting method，每个 top-ranked document 可以看作是对它属于的那个资源的一个投票。</p>
<p><strong>特点：</strong></p>
<ul>
<li>高召回率: 选择包含更多相关文档的资源</li>
<li>高准确率: 选择能够返回更多的出现在合并结果集 top T 的文档的资源</li>
<li>有很多算法的变种<br>– 来自更可靠/权威的资源的 sample 获得更多的投票<br>– 更相关的 sample 获得更多的投票</li>
</ul>
<h4 id="CORI-vs-ReDDE"><a href="#CORI-vs-ReDDE" class="headerlink" title="CORI vs ReDDE"></a>CORI vs ReDDE</h4><ul>
<li>ReDDE 相对而言更加准确<br>– 通常效果更好，几乎不会有更糟的情况</li>
<li>当各个 collection size 的分布是 skewed 时，ReDDE 的性能优于 CORI<br>– CORI 偏向小集合。他们更可能是同质的，它错过了大型，异质的 collections<br>– ReDDE 对大型 collection 的偏见没那么强</li>
<li>CORI 比 ReDDE 更有效<br>– 一个资源一个文档 vs. 一个资源多个文档</li>
</ul>
<h2 id="Supervised-Resource-Selection"><a href="#Supervised-Resource-Selection" class="headerlink" title="Supervised Resource Selection"></a>Supervised Resource Selection</h2><p>当我们有可用的训练数据时，就可以进行有监督的资源选择</p>
<ul>
<li>利用广泛的特征</li>
<li>将任务框架化为分类问题</li>
<li>让机器学习算法学习如何给不同的特征分配权重</li>
</ul>
<p><strong>Main issues</strong></p>
<ul>
<li>特征工程</li>
<li>获取训练数据</li>
<li>确定要选择的资源数</li>
</ul>
<h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p><strong>Task:</strong> 给定一个查询，选择一个或更多的 verticals</p>
<p>将问题定义为一个 “one-vs-all” 的分类任务</p>
<ul>
<li>对每个 vertical 都训练一个分类器</li>
<li>对 “no vertical” (web only) 也训练一个分类器</li>
<li>E.g., Arguello, et al (2009) 用了逻辑回归模型</li>
</ul>
<p><strong>典型的选择策略</strong></p>
<ul>
<li>选择一个具有最高置信度(confidence score)的分类器</li>
<li>选择 n 个最好的分类器</li>
<li>选择所有置信度分数高于 threshold 的分类器</li>
</ul>
<h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><p><strong>Query features</strong></p>
<ul>
<li>Boolean: keywords and regular expressions<br>– E.g., “weather”, “news”, “videos”, …</li>
<li>Geographic: Probabilities associated with geographic entities<br>– E.g., “Pittsburgh pizza”</li>
<li>Category: query’s affinity to a set of topic categories<pre><code>– E.g., “Cancun vacations”
</code></pre></li>
</ul>
<p><strong>Corpus Features</strong><br>ReDDE 是非常流行的资源排序算法</p>
<p><strong>Clarity (Cronen-Townsend, et al., 2002)</strong><br>Clarity 用来预测给定语料库上的查询的有效性，本质上是 top-ranked 文档和完整集合之间的 KL divergence，如果排名靠前的文档看起来像是 collection 的 random sample，搜索结果将会很差</p>
<p><strong>Query Log Features</strong></p>
<ul>
<li>人工评估/点击率</li>
</ul>
<p><strong>qlog feature</strong></p>
<ul>
<li>用查询日志中的 query 来构建语言模型 v</li>
<li>用 query likelihood 来建模 $P(q|v)=\prod_t^q P(t|v)$</li>
<li>p(t | v) is a smoothed MLE</li>
</ul>
<p><strong>Soft.ReDDE feature:</strong></p>
<ul>
<li>用查询日志中的 query 来构建语言模型 v</li>
<li>用 query 对外部的 collection 检索<br>– E.g., wikipedia</li>
<li>选择前 n 个文档</li>
<li>文档 di 的投票是 KLD (di || v)<br>– 与查询日志相似的文档具有更高的投票</li>
</ul>
<h1 id="Result-merging"><a href="#Result-merging" class="headerlink" title="Result merging"></a>Result merging</h1><p><strong>主要问题：</strong><br><strong>idf</strong><br>同一个 term，在不同数据库的 idf 不同怎么办？</p>
<p><strong>scores</strong><br>不同数据库的 doc score 不是可比的怎么办？</p>
<p><strong>＝&gt; Map resource-specific scores to resource-neutral scores</strong></p>
<p><strong>Result merging</strong></p>
<ul>
<li>用所有的 sampled documents 创建一个 index (offline)</li>
<li>从选定的资源里检索文档<br>– 因为资源并不提供文档分数，所以对每个文档计算 sim (q, d)<br>– 利用 index 的 corpus statistics 来计算每个文档的 authority score</li>
<li>score(d) = f ( sim (q, d), authority (d) )</li>
</ul>
<h2 id="Semi-Supervised-Learning"><a href="#Semi-Supervised-Learning" class="headerlink" title="Semi-Supervised Learning"></a>Semi-Supervised Learning</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/ss.jpg" class="ful-image" alt="ss.jpg">
<p>由资源 i 返回的一些文档可能同时出现在 sample index 里<br>对这些文档我们有两个分数</p>
<ul>
<li>Sample index (resource neutral) score</li>
<li>Resource i (resource-specific) score<br>这就是我们用来学习 f (resource-specific) = resource-neutral 的训练数据</li>
</ul>
<p>对这个所需的 “normalization” 用线性函数进行建模，因为线性模型可以利用少量的数据进行训练，而且 Ad-hoc CORI 的合并就是线性的<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Federated%20Search/ss2.jpg" class="ful-image" alt="ss2.jpg"></p>
<p>使用线性回归(linear regression)推导出权重</p>
<ul>
<li>f（resource-specific score）= a×dRi，j + b = resource-neutral score</li>
<li>每个(query, resource) pair 对应不同的函数</li>
</ul>
<p><strong>它有效么?</strong><br>目前为止是最好的选择，因为 mapping 是 query-specific 和 resource-specific 的。</p>
<p><strong>有没有足够的训练数据?</strong><br>通常有，因为 resource selection 和 sample index 可以用相同的文档。如果没有，下载 1-3 篇文档来产生训练数据。</p>
<h1 id="Large-Scale-Search-Architecture"><a href="#Large-Scale-Search-Architecture" class="headerlink" title="Large-Scale Search Architecture"></a>Large-Scale Search Architecture</h1><p>之前讲过 large-scale search architecture 的相关内容。我们可以把数据集分为两层 <strong>高价值</strong> 和 <strong>低价值</strong>，然后将每一层进行分区(shards)，再将分区分配给计算机。</p>
<p>事实上我们并不需要对每个查询都检索每一个分区，这样做成本很高，为什么不直接搜索最好/最合适的分区呢？</p>
<p>通常带来的结果是：</p>
<ul>
<li>高准确率 (尤其是 top ranks)</li>
<li>低召回率 (可能会错过许多有用的文档)<br>– 只搜索了少量资源，这也是广泛应用的障碍</li>
</ul>
<p><strong>所以分为两步:</strong></p>
<ol>
<li>资源定义(Resource definition)<ul>
<li>对文档分区</li>
</ul>
</li>
<li>资源选择(Resource selection)<ul>
<li>对特定 query 选择要搜索的分区</li>
</ul>
</li>
</ol>
<p>下面进行详细的介绍。</p>
<h2 id="Resource-definition"><a href="#Resource-definition" class="headerlink" title="Resource definition"></a>Resource definition</h2><p><strong>Goal:</strong> 搜索更少的分区得到更高的 recall</p>
<p><strong>Requirement:</strong><br>对于可能提交的任何 query，大多数的相关文档必须出现在同一个分区中</p>
<p><strong>How:</strong><br>把 closely-associated documents 也就是相似（相同主题）的文档归到同一个分区。</p>
<p><strong>Hypothesis:</strong> Closely-associated ≈ topically-related &amp; Each shard covers one topic/subject</p>
<p><strong>对给定的语料库，应该使用哪些 topics/subjects?</strong></p>
<ul>
<li>人工开发类别? – LCSH, DMOZ, Reuters, …</li>
<li>自动学习 corpus-specific topics? – Clustering, LDA, PLSA, …</li>
</ul>
<p>我们想要一个可以应用于任何语料库的解决方案，因此，需要能自动学习基于语料库的特定的主题。许多算法能产生语料库特定的“主题”：</p>
<ul>
<li>聚类: E.g., k-means clustering</li>
<li>主题模型: E.g., Latent Dirichlet Allocation (LDA)</li>
</ul>
<p>当然我们也要保证解决方案必须对 large collections 是有效的：</p>
<ul>
<li>从 collection 的一个随机样本里创建主题</li>
<li>将其余文档分配给最相似的主题</li>
</ul>
<h2 id="Resource-selection-1"><a href="#Resource-selection-1" class="headerlink" title="Resource selection"></a>Resource selection</h2><p><strong>Goal:</strong> 给定查询，选择最好的分区</p>
<p><strong>两类重要的资源选择算法:</strong></p>
<ul>
<li>Model-based: CORI, CRCS, Taily, …</li>
<li>Sample-based: ReDDE, SUSHI, Rank-S, …</li>
</ul>
<p>有很多好的算法</p>
<ul>
<li>Sample-based 的算法被认为是最有效的</li>
<li>在初始工作中常常使用 ReDDE 变种</li>
<li>Taily 和 Rank-S 也经常被研究/考虑</li>
</ul>
<p><strong>对每个分区采样:</strong><br>将样本存储到 sample index 里 =&gt; offline</p>
<p><strong>给定查询</strong></p>
<ul>
<li>搜索 sample index，得到 top documents</li>
<li>Top documents 对它们所在的分区投票</li>
<li>将 query 发送到得到最多投票的分区<br>=&gt; online</li>
</ul>
<h2 id="Cost"><a href="#Cost" class="headerlink" title="Cost"></a>Cost</h2><p>$C_{Total}$:</p>
<ul>
<li>处理 inverted list 的数量</li>
<li>用于资源选择</li>
<li>用于分区选择</li>
<li>I/O 和计算成本</li>
</ul>
<p>Selective search 分为两步：</p>
<ul>
<li>选择要搜索的分区（resource selection）</li>
<li>搜索所选的分区</li>
</ul>
<p>ReDDE 具有更高的计算成本</p>
<p><strong>Key techniques</strong></p>
<ol>
<li>Resource definition for selective search<br>– How to partition a large corpus effectively</li>
<li>Resource representation<br>– How to represent the contents of each resource<br>– Vocabulary-based, sample-documents-based, feature-based</li>
<li>Resource selection<br>– How to select the right resources for a particular query</li>
</ol>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Diversity]]></title>
      <url>http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。一个 query 可能表现了不同的信息需求，之前的相关性模型可能带来的结果是大多文档只能满足同一个信息需求，所以有些用户满意了，有些 user 抓狂了。我们希望检索到的 document 是能够 diverse 的，这样的话可以尽可能的满足不同用户的需求。这一章就讲了怎么实现 diversity。<br><a id="more"></a></p>
<h1 id="Relevance-based-ranking-model"><a href="#Relevance-based-ranking-model" class="headerlink" title="Relevance based ranking model"></a>Relevance based ranking model</h1><p>之前的 <strong>相关性模型(Relevance based ranking model)</strong> 的目的是：</p>
<ul>
<li>在每个 ranking 位置上最大化预期用户满意度</li>
<li>主要基于与原始查询的文本相似性</li>
</ul>
<p>我们基于了这样一个假设：</p>
<blockquote>
<p>each document is independent of other documents in the ranking</p>
</blockquote>
<p>它带来的检索结果是：</p>
<ul>
<li>主要关注最可能出现的 intent</li>
<li>适用于一些用户，但另一些用户可能不满意</li>
</ul>
<p>如果这条 query 清楚的表达了 user intent，那么这种方法能得到最佳的结果。然而事实往往是，一条 query 可能会有多种解释或者多种 user intents，如果我们返回的 documents 表达的都是错误的 intent 怎么办？用户得不到任何相关文档！这是相当糟糕的体验。</p>
<p>E.g.,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">multiple interpretations</div><div class="line">Query: discovery channel store</div><div class="line">– The Discovery Channel store homepage?</div><div class="line">– Discovery Channel store locations?</div><div class="line">– Toys and products sold by Discovery Channel stores?</div><div class="line">– Products based on the Animal Planet program?</div><div class="line"></div><div class="line">multiple tasks for the same interpretation</div><div class="line">Query: Carnegie Mellon University</div><div class="line">– Find the home page of CMU (Navigational)</div><div class="line">– Find the location of CMU  (Location)</div><div class="line">– Check recent news about CMU  (News)</div><div class="line">– Find pictures of CMU (Pictures)</div></pre></td></tr></table></figure></p>
<p>所以我们引入了 <strong>多样性</strong> 的概念。多样性是 <strong>健壮性(robustness)</strong> 和 <strong>相关性(relevance)</strong> 之间的权衡，它会</p>
<ul>
<li>减少最有可能的 intent 的相关性</li>
<li>覆盖多个 intent 增加鲁棒性</li>
</ul>
<p>下面描述了两个层次的多样性。</p>
<ol>
<li>原始查询的解释(Interpretation of the original query)<ul>
<li>这个查询是指什么？</li>
<li>用户想知道这个 query 的哪一个方面的信息？</li>
</ul>
</li>
<li>每个解释的不同任务(Different tasks for each interpretation)<ul>
<li>用户想要完成什么任务？图片？地图？视频？导航？</li>
</ul>
</li>
</ol>
<p>这一章我们集中讲 diversification of interpretations。</p>
<h1 id="Diversification-Algorithms"><a href="#Diversification-Algorithms" class="headerlink" title="Diversification Algorithms"></a>Diversification Algorithms</h1><p>主要有两类算法。<br><strong>Implicit</strong></p>
<ul>
<li>query intent 隐含在文档排名中</li>
<li>假设相似的文档涵盖了相似的 intent</li>
</ul>
<p><strong>Explicit</strong></p>
<ul>
<li>显性定义了 query intent</li>
<li>对文档重新排序，以便覆盖所有的 query intent</li>
</ul>
<h2 id="Implicit-Methods"><a href="#Implicit-Methods" class="headerlink" title="Implicit Methods"></a>Implicit Methods</h2><p><strong>一些特征：</strong></p>
<ul>
<li>不需要关于可能存在的 query intent 的先验知识 - 因此称为“隐式”</li>
<li>选择与查询匹配的文档然后重新排序，产生多样性排名</li>
<li>不偏好任何 query intent<ul>
<li>受欢迎的 intent 没有得到比 rare intent 更重要</li>
</ul>
</li>
</ul>
<h3 id="Maximum-Marginal-Relevance-MMR"><a href="#Maximum-Marginal-Relevance-MMR" class="headerlink" title="Maximum Marginal Relevance (MMR)"></a>Maximum Marginal Relevance (MMR)</h3><p><strong>主要思路：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">• Use the query to retrieve a ranking R of documents</div><div class="line">• Use MMR to rerank the top N documents (build a new ranking S)</div><div class="line">  – Select the 1st document based on how well it satisfies the query</div><div class="line">  – Select subsequent documents based on two criteria</div><div class="line">    » How well it satisfies the query</div><div class="line">    » How different it is from documents ranked above it</div></pre></td></tr></table></figure></p>
<p>MMR 是贪心算法(greedy algorithm)。在每一个 step，都选择一个文档插入最后的 ranking list。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/MMR.png" class="ful-image" alt="MMR.png"></p>
<p>R: The initial ranking<br>S: Documents already selected for the diverse ranking<br>  – Documents ranked above this position<br>Sim(di, dj): Similarity metric<br>  – E.g., vector space model or Jensen-Shannon Divergence</p>
<p>举个例子，假设$\lambda = 0.6$，初始排名及其相关性分数如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Initial Ranking</div><div class="line">d1, 0.80</div><div class="line">d2, 0.78</div><div class="line">d3, 0.76</div><div class="line">d4, 0.74</div><div class="line">d5, 0.72</div><div class="line">d6, 0.70</div></pre></td></tr></table></figure></p>
<p>第一步，选择与 query 最相关的文档 d1，并从 R 中移除 d1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Diversified Ranking</div><div class="line">d1</div></pre></td></tr></table></figure></p>
<p>计算剩余文档与 {d1} 的相似度和对应的多样性分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/step1.jpg" class="ful-image" alt="step1.jpg"></p>
<p>选择分数最高的文档作为多样性排名的第 2 篇文档，并从 R 中移除该文档<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Diversified Ranking</div><div class="line">d1</div><div class="line">d5</div></pre></td></tr></table></figure></p>
<p>分别计算剩余文档与已排序文档 {d1, d5} 的相似度，选相似度最大的值，计算 MMR 分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/step2.jpg" class="ful-image" alt="step2.jpg"></p>
<p>选择分数最高的文档作为多样性排名的第 3 篇文档，并从 R 中移除该文档<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Diversified Ranking</div><div class="line">d1</div><div class="line">d5</div><div class="line">d3</div></pre></td></tr></table></figure></p>
<p>分别计算剩余文档与已排序文档 {d1, d5, d3} 的相似度，选相似度最大的值，计算 MMR 分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/step3.jpg" class="ful-image" alt="step3.jpg"></p>
<p>选择分数最高的文档作为多样性排名的第 4 篇文档，并从 R 中移除该文档<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Diversified Ranking</div><div class="line">d1</div><div class="line">d5</div><div class="line">d3</div><div class="line">d6</div></pre></td></tr></table></figure></p>
<p>分别计算剩余文档与已排序文档 d1, d5, d3, d6 的相似度，选相似度最大的值，计算 MMR 分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/step4.jpg" class="ful-image" alt="step4.jpg"></p>
<p>选择分数最高的文档作为多样性排名的第 5 篇文档，并从 R 中移除该文档<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Diversified Ranking</div><div class="line">d1</div><div class="line">d5</div><div class="line">d3</div><div class="line">d6</div><div class="line">d2</div></pre></td></tr></table></figure></p>
<p><strong>Repeat until all documents in the initial ranking are added to the diversified ranking</strong></p>
<p>所以就有了最后的排名 {d1, d5, d3, d6, d2}</p>
<p>MMR 也会用于其它的任务如文本摘要，因为它是符合摘要的基本要求的，即相关性和多样性的权衡。摘要结果与原文的相关性越高，摘要就接近全文中心意思，而多样性则使摘要内容更加的全面。直观和简单是这种方法最大的优点。</p>
<h4 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h4><p>关于相似度的衡量。<br><strong>Anything-to-Anything Similarity</strong><br>Vector space 可以比较任意两个向量的相似度，应用广泛，如聚类。<br>BM25 不能用 anything-to-anything similarity，但可以用 language modeling framework</p>
<p><strong>Probabilistic Similarity</strong><br>也有很多专门来计算 probability distribution similarity 的方法。KL divergence 就是其中很受欢迎的一种，然而它是不对称的，Jensen-Shannon Divergence 则是 KL divergence 的一个 symmetric and smoothed 的版本。</p>
<p>$$JS(x||y)={1 \over 2} KL(x||M) + {1 \over 2}KL(y|M)$$<br>$$M={x+y \over 2}$$</p>
<p>放到两个 document 的比较里<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/JS.jpg" class="ful-image" alt="JS.jpg"></p>
<p>它也经常会用在 anything-to-anything similarity tasks 中。</p>
<h3 id="Learning-to-Rank-for-diversification"><a href="#Learning-to-Rank-for-diversification" class="headerlink" title="Learning to Rank for diversification"></a>Learning to Rank for diversification</h3><p>MMR 是无监督的，而 Learning-to-Rank 就是 MMR 的 supervised 版本。</p>
<p>如果直接用传统的 learning-to-rank 模型来优化一个多样性指标 (e.g., α-NDCG)，也就是用和相同模型、相同特征，但是用基于多样性的训练数据（e.g., pointwise, pairwise, or listwise training data），来学习怎样产生 diversity ranking，会发生什么？</p>
<ul>
<li>各种指标会得到不一致的结果</li>
<li>训练数据和测试数据会有不一样的 performance</li>
</ul>
<p><strong>Why?</strong><br>因为传统 Le2R 忽略了文档之间的相关性，它只考虑 document-query 的相关性特征及 query independent 特征（e.g., pagerank），而缺少多样性特征如：</p>
<ul>
<li>文档-文档的相似性特征</li>
<li>关于 sub-intents 的特征</li>
</ul>
<p>因此，机器学习算法难以找到适用于多样性模型的合适的权重，相反，多样性的训练数据还会让学习模型感到困惑。所以有了一个解决方案： <strong>Relational-LeToR（R-LeToR）</strong>，既考虑单个文档，又考虑文档与文档之间的关系，也就是说除传统 LeToR 需要的 <strong>relevance feature</strong> 之外，R-LeToR 还考虑了 <strong>relationship features</strong>，relationship 也就是相似度，计算的是候选文档与 higher-ranked documents 的相似度(和 MMR 的计算过程相似)</p>
<p>与传统 listwise Le2R 的区别是 ranking function 中多了 relational feature R：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/R_LTR.png" class="ful-image" alt="R_LTR.png"></p>
<h4 id="Relational-features"><a href="#Relational-features" class="headerlink" title="Relational features"></a>Relational features</h4><p>Relational features 由相关性函数得到，其输入是文档相似度。</p>
<p>文档间的相似度可以通过下面的方法计算：<br><strong> 文本相似度（Text similarity）:</strong><br>– E.g. KL-Divergence of document language models<br><strong> 主题相似度（Topic similarity）:</strong><br>– E.g. Cosine between docs’ topic distributions in topic models<br><strong> 类别相似度（Category similarity）:</strong><br>– E.g. Overlap of docs’ categories in a predefined ontology<br><strong> 链接相似度（Link similarity）:</strong><br>– E.g. whether docs have hyperlinks to each other<br><strong> URL similarity:</strong><br>– E.g. whether docs are from same website or not.</p>
<p>相关性函数可以是当前文档与 all higher-ranked doc 的：</p>
<ul>
<li>最小相似度</li>
<li>平均相似度</li>
<li>最大相似度</li>
</ul>
<h4 id="Relational-ListMLE"><a href="#Relational-ListMLE" class="headerlink" title="Relational-ListMLE"></a>Relational-ListMLE</h4><p>ListMLE 是一种 ListWise LeToR，将一个 query 返回的整个文档排序列表作为输入，直接对排序结果列表进行优化，训练得到一个最优的评分函数，损失函数是 likelihood loss。关于 LeToR 的更多类型，这一篇不多做介绍。</p>
<p>ListMLE 可以轻松地处理各种 relational features。回忆下 ListMLE 的 sequential assumption</p>
<ul>
<li>自上而下逐个挑选文档，这与 MMR 的 sequential assumption 相同</li>
<li>每个位置独立于先前的位置<br>与 ListMLE 不同的是，Relational-ListMLE 通过 relational features 打破了这种独立性假设</li>
</ul>
<p>Relational-ListMLE 的生成过程是：</p>
<ul>
<li>从候选文档中迭代产生新的排序</li>
<li>给定候选文档集合 Si = {d1, …, dn}<br>– 从 Si 中选择最适合出现在位置 i 上的文档 di<br>多提一句，p-ListMLE 也打破了这个假设</li>
</ul>
<p>Relational-ListMLE 的生成过程是：</p>
<ul>
<li>从候选文档中迭代产生新的 ranking</li>
<li>给定候选文档集合 Si = {d1, …, dn}<br>– 从 Si 中选择最适合出现在位置 i 上的文档 di</li>
</ul>
<h4 id="Learning-and-Ranking"><a href="#Learning-and-Ranking" class="headerlink" title="Learning and Ranking"></a>Learning and Ranking</h4><p>学习的过程和 ListMLE 完全相同，损失函数仍然是 likelihood loss，用 SGD 优化。</p>
<p>排序时，因为文档依赖于之前的位置，无法像 ListMLE 一样直接计算 ranking score，所以</p>
<ul>
<li>在 n 个位置的每一个位置上<br>» 从剩余的文档中选择 P(di|Si,w) 最高的文档<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/l2r.jpg" class="ful-image" alt="l2r.jpg">
» 更新与其他文档的相似性特征 (O(n))</li>
</ul>
<p>整个复杂度是 $O(n^2)$，而由于多样性算法是一种重排序方法，所以 n 不是很大，大多数情况下 n &lt;1000</p>
<h4 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h4><p>R-LeToR 是在 LeToR 基础上的一种最先进的技术，Relational-ListMLE 是唯一的一种监督的学习算法，因此可能比所有隐式和显式方法更好。然而正因为它是最近发表的，没有大量的证据，因此要小心使用。</p>
<h2 id="Explicit-Methods"><a href="#Explicit-Methods" class="headerlink" title="Explicit Methods"></a>Explicit Methods</h2><h3 id="Query-intents-subtopics-discovery"><a href="#Query-intents-subtopics-discovery" class="headerlink" title="Query intents (subtopics) discovery"></a>Query intents (subtopics) discovery</h3><p>怎么发现 query intents?</p>
<ul>
<li>通过分析搜索日志。下一章会展开。<br>非常挑战，因为 query intent 都很 personal，不同的人有不同的解释，也有很多种 possible subtopics。<br>通常用对 top retrieved documents 进行聚类或 topic modeling 等方法来进行，但是效果并不好。</li>
<li>用已有的经验<br>– 由 TREC 提供的 query intents<br>– 由商业搜索引擎的 query suggestions／related queries 推断出的 query intents</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Topic 1, type=faceted</div><div class="line">• Query: obama family tree</div><div class="line">• Description: Find information on President Barack Obama&apos;s family history, including genealogy, national origins, places and dates of birth, etc.</div><div class="line">• Subtopic, type=nav: Find the TIME magazine photo essay &quot;Barack Obama&apos;s Family Tree&quot;.</div><div class="line">• Subtopic, type=inf: Where did Barack Obama&apos;s parents and grandparents come from?</div><div class="line">• Subtopic, type=inf: Find biographical information on Barack Obama&apos;s mother.</div></pre></td></tr></table></figure>
<p>注意两种类型的主题<br><strong>Ambiguous:</strong> query 的不相关解释<br>– E.g., “michael jordan”, “avp”, “espn sports”</p>
<p><strong>Faceted:</strong> query 的相关解释<br>– E.g., “arizona game and fish”, “obama family tree”</p>
<h3 id="xQuAD"><a href="#xQuAD" class="headerlink" title="xQuAD"></a>xQuAD</h3><p>xQuAD(eXplicit Query Aspect Diversification (xQuAD))<br><strong>Inputs</strong><br>– An initial ranking<br>– 一组 query intents (e.g., from search engine suggestions)</p>
<p><strong>Observation:</strong> 单个文档可能覆盖多个 intents，尤其是对 faceted queries 而言。</p>
<p><strong>Key idea:</strong> 选择能够满足尽可能多的 uncovered intents 的文档<br>– Provide maximum coverage and minimum redundancy<br>– MMR did this implicitly<br>– xQuAD does it explicitly</p>
<p><strong>xQuAD characteristics</strong></p>
<ul>
<li>需要一组显性的 query intents</li>
<li>允许单个文档满足多个 query intents<br>– 这些文档更有可能有高排名</li>
<li>多样性排名 equally 覆盖了每一个 intent<br>– 也可以对 intent 加权，但实验表明 uniform weights 是最好的</li>
<li>每个 query q 都需要运行 q + {q1, …, qk} 这么多查询语句<br>– 计算量略大</li>
<li>目前可用的最有效的方法之一</li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><strong>Assumptions:</strong><br>• 每个查询 q 都有 {q1, …, qk} 这么多 query intents<br>• 这些 query intents 都是相互独立的</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/xquad.jpg" class="ful-image" alt="xquad.jpg">
<p>R: Initial ranking (produced by some other method)<br>S: Diversified ranking (initially empty)<br>τ: Desired length of diversified ranking<br>$\lambda$: Balance between relevance and diversity</p>
<ol>
<li>用相关性模型得到文档集合 R</li>
<li>计算文档分数，得到分数最高的文档 d*</li>
<li>从 R 中移除 d*</li>
<li>把 d* 加入 S</li>
</ol>
<p><strong>Selection criteria:</strong><br>relevance 和 diversity 的平衡<br>$$(1-\lambda)P(d|q) + \lambda P(d,\overline S|q)$$</p>
<p><strong>Diversity component:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/xquadd.jpg" class="ful-image" alt="xquadd.jpg"></p>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>E.g.,<br>Step 1:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/s1.jpg" class="ful-image" alt="s1.jpg"></p>
<p>Step 2:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/s2.jpg" class="ful-image" alt="s2.jpg"></p>
<p>Step 3:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/s3.jpg" class="ful-image" alt="s3.jpg"></p>
<p>Step 4:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/s4.jpg" class="ful-image" alt="s4.jpg"></p>
<p>Step 5:<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/s5.jpg" class="ful-image" alt="s5.jpg"></p>
<p>rank<br>xQuAD 偏好能够满足多个 intents 的文档<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/xquadb.jpg" class="ful-image" alt="xquadb.jpg"></p>
<h4 id="Weighting-queries"><a href="#Weighting-queries" class="headerlink" title="Weighting queries"></a>Weighting queries</h4><p>Santos, et al 研究了 weighting queries 的三种方法</p>
<ul>
<li>Uniform weights: 1 / |Q|</li>
<li>类似于 CRCS 资源排序算法的方法<br>– 与 ReDDE 相似<br>– 与 qi 匹配的 top-ranked documents 的数量相关</li>
<li>基于在商业搜索引擎中匹配的文档的相对数量</li>
</ul>
<p>在 TREC 2009 的数据中，Uniform 被认为是最有效的。</p>
<h4 id="Some-research-results"><a href="#Some-research-results" class="headerlink" title="Some research results"></a>Some research results</h4><p>Related queries 并没有什么用<br>Suggested queries 更加有效</p>
<h3 id="PM-2"><a href="#PM-2" class="headerlink" title="PM-2"></a>PM-2</h3><p>Proportionality Model 2 (PM-2)<br><strong>Inputs</strong><br>– An initial ranking<br>– 一组 query intents (e.g., from search engine suggestions)</p>
<p><strong>Observation:</strong> 用于发现 query intents 的算法，将发现许多稀少或不流行的 intents，高召回率。<br>– They shouldn’t get equal coverage in a diversified ranking</p>
<p><strong>Key idea:</strong> 每个 subtopic 的文档数量应与每个 subtopic 的 popularity 成正比</p>
<ul>
<li>最小化冗余</li>
<li>subtopic 的 coverage 可以更好地匹配用户需求</li>
</ul>
<p><strong>PM-2 characteristics</strong></p>
<ul>
<li>需要一组显性的 query intents</li>
<li>允许单个文档满足多个 query intents<br>– 这些文档更有可能有高排名</li>
<li>多样性排名按比例的覆盖每一个 intent<br>– 加权是可能的，但均匀的权重是最好的</li>
<li>每个 query q 都需要运行 q + {q1, …, qk} 这么多查询语句<br>可能计算量有点大</li>
<li>与 xQuAD 的相对性能取决于 datasets and explicit topics.</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><p>PM-2 的思想其实来源自选举制的比例代表制的一种方法：Sainte-Laguë method</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.jpg" class="ful-image" alt="pm2.jpg">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">• At each rank r</div><div class="line">  – 选择下一个必须覆盖的查询意图 qi，来保持排名中的意图比例覆盖率</div><div class="line">  – 选择一个覆盖了意图 qi 的文档 d </div><div class="line">  » 同时，文档 d 也可能涵盖其他查询意图</div><div class="line">  – 从原始排名 R 中删除文档 d</div><div class="line">  – 把 d 加入多样化排名S</div></pre></td></tr></table></figure>
<p><strong>怎么计算下一个必须覆盖的查询意图 qi？</strong><br>$$qt[i]={v_i\over 2s_i+1}$$</p>
<p>vi 是分配给这个意图的总文档数，实际等于 p(qi|q) x len(S)<br>si 代表已经分配给这个意图的文档，初始值为 0<br>最高的 quotient score 就对应下一个必须覆盖的意图</p>
<h4 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h4><p>E.g., 原始的相关性排名<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.0.jpg" class="ful-image" alt="pm2.0.jpg"></p>
<p><strong>假设：</strong><br>λ=0.6<br>p(qi|q)=0.5<br>Ranking depth=8</p>
<p>Step 0: 初始化变量<br>计算分配给各个意图的文档数 <strong>votes v[i]</strong><br>v[1] = 0.5×8 = 4<br>v[2] = 0.5×8 = 4</p>
<p>初始化 <strong>slots assigned s[i]</strong><br>s[1] = 0<br>s[2]=0</p>
<p>Step 1，计算 quotient score<br>qt[1] = 4 / (2×0+1) = 4<br>qt[2] = 4 / (2×0+1) = 4</p>
<p>选择分数高的意图作为下一个必须覆盖的意图<br>$i^* = 1$</p>
<p>计算 PM2 分数<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.d.jpg" class="ful-image" alt="pm2.d.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.1.jpg" class="ful-image" alt="pm2.1.jpg"></p>
<p>d2 wins!</p>
<p>Step 2<br><strong>slots assigned s[i]</strong><br>$$s[i]=s[i]+{P(d^*|q_i) \over \sum_qP(d^* |q_j)}$$<br>s[1] = 0+0.8/(0.8+0.1) = 0.89<br>s[2] = 0+0.1/(0.8+0.1) = 0.11</p>
<p><strong>Quotient scores qt[i]</strong><br>$$qt[i]={v_i\over 2s_i+1}$$<br>qt[1] = 4 / (2×0.89+1) = 1.44<br>qt[2] = 4 / (2×0.11+1) = 3.27</p>
<p><strong>selected intent</strong><br>$$argmax_i qt[i]$$<br>$i^* = 2$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.2.jpg" class="ful-image" alt="pm2.2.jpg">
<p>d5 wins!</p>
<p>Step 3<br>s[1] = 0.89+0.3/(0.3+0.8) = 1.16<br>s[2] = 0.11+0.8/(0.3+0.8) = 0.84</p>
<p>qt[1] = 4 / (2×1.16+1) = 1.20<br>qt[2] = 4 / (2×0.84+1) = 1.49</p>
<p>$i^*=2$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.3.jpg" class="ful-image" alt="pm2.3.jpg">
<p>d4 wins!</p>
<p>Step 4<br>s[1] = 1.16+0.2/(0.2+0.7) = 1.38<br>s[2] = 0.84+0.7/(0.2+0.7) = 1.62</p>
<p>qt[1] = 4 / (2×1.38+1) = 1.06<br>qt[2] = 4 / (2×1.62+1) = 0.95</p>
<p>$i^* = 1$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/pm2.4.jpg" class="ful-image" alt="pm2.4.jpg">
<p>d1 wins!</p>
<p>Repeat until all documents in the initial ranking are added to the diversified ranking</p>
<h3 id="xQuAD-vs-pm2"><a href="#xQuAD-vs-pm2" class="headerlink" title="xQuAD vs pm2"></a>xQuAD vs pm2</h3><p>xQuAD 选择涵盖多个 intents 的文档</p>
<ul>
<li>给需要覆盖的 intents 赋予较高的权重</li>
</ul>
<p>PM2 首先选择最需要覆盖的 intent</p>
<ul>
<li>然后选择一个覆盖这个 intent 的文档</li>
<li>如果这个文档能覆盖其它 intents，那么给它加分</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/vs.jpg" class="ful-image" alt="vs.jpg">
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>显式方法比 MMR 更有效。但需要一个 query intents 的外部来源，从网络搜索引擎获取的 intents 往往不能令人满意，因为它总是依赖于某一个组织。<br>xQuAD 和 PM-2 都是无监督的，效果不如有监督的模型 R-LeToR。</p>
<p>理想情况下，我们希望能结合这些方法的优点</p>
<ul>
<li>无需使用外部资源</li>
<li>充分利用 subtopics</li>
<li>充分利用监督学习</li>
</ul>
<p><strong>Methods of identifying query intents</strong></p>
<ul>
<li>Commercial search engine suggestions<br>– Query suggestions &gt; related queries</li>
<li>Still an open problem<br>– Very hard without search log</li>
</ul>
<p><strong>Characteristics</strong></p>
<ul>
<li>MMR: Implicit, unsupervised, penalizes redundancy</li>
<li>R-LeToR: Implicit, supervised, features to model redundancy</li>
<li>xQuAD: Explicit, unsupervised, penalizes redundancy</li>
<li>PM-2: Explicit, unsupervised, enforces proportionality</li>
</ul>
<p><strong>Performance:</strong><br>R-LeToR &gt; PM-2 ≈ xQuAD &gt; MMR</p>
<h1 id="Diversity-Evaluation-Metrics"><a href="#Diversity-Evaluation-Metrics" class="headerlink" title="Diversity Evaluation Metrics"></a>Diversity Evaluation Metrics</h1><h2 id="Precision-IA-k"><a href="#Precision-IA-k" class="headerlink" title="Precision-IA@k"></a>Precision-IA@k</h2><p>实际是 P@k 的变种。易于计算，易于理解。之前的模型里我们把 $q_i$ 当作 query 的第 i 个 term，而这里，$q_i$ 指代的是 query 的第 i 个 query intent。<br>假设 query q 有 n 个 query intents {q1, …, qn}，每个 intent 出现的概率是 p(qi | q)，一般来说，p(qi | q) 是均匀分布的，也就是 1/n 的概率，不需要 uniform。<br><strong>计算步骤：</strong></p>
<ol>
<li>采用某种方法对 query q 检索到的文档进行排序</li>
<li>对每个 intent qi 计算 $P@k_{qi}$</li>
<li>对 P@kqi 求平均得到 Precision-IA@k<br>$Precision-IA@k=\sum_{qi}P(qi|q)P@k_{qi}$</li>
</ol>
<p><strong>评价：</strong><br>通常一个好的策略是给满足了多个 intetns 的文档更高的优先级</p>
<ul>
<li>能提高 Precision-IA @ k</li>
<li>用一个文档满足多个 intent 是降低风险的有效方法</li>
</ul>
<h2 id="α-NDCG"><a href="#α-NDCG" class="headerlink" title="α-NDCG"></a>α-NDCG</h2><p>先来回顾下 NDCG 公式<br>$$NDCG@k = Z_k \sum_{i=1}^k{2^{R_i}-1 \over log(1+i)}$$</p>
<p>考虑了两点：</p>
<ul>
<li>Gain from a document worth R(k) at rank k(based on relevance): $G@k=2^{R(k)}$</li>
<li>Discount for selecting a document at rank k(based on rank): $D@G={1 \over log_2(1+k)}$</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>考虑了每个文档的位置</li>
<li>允许了多值(multi-valued)的相关性评估</li>
<li>忽略了排名的多样性</li>
</ul>
<p>它的变种 α-NDCG<br>Uses an intent-aware gain calculation<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/%CE%B1-NDCG.jpg" class="ful-image" alt="%CE%B1-NDCG.jpg"></p>
<p>The gain vector for α-NDCG discounts intent redundancy</p>
<ul>
<li>1-α controls the discount</li>
<li>If α=0.5<br>Value (j+1th reldoc, qi) = 0.5 ×Value (jth reldoc, qi)<br>(each document is worth 1⁄2 the previous document)</li>
</ul>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
            <tag> 多样化 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[EM算法]]></title>
      <url>http://www.shuang0420.com/2016/12/04/EM%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>CMU 10601 的课程笔记。EM 算法计算含有隐含变量的概率模型参数估计，能使用在一些无监督的聚类方法上。在 EM 算法总结提出以前就有该算法思想的方法提出，例如 HMM 中用的 Baum-Welch 算法。<br><a id="more"></a></p>
<h1 id="Mixtures-of-Gaussians"><a href="#Mixtures-of-Gaussians" class="headerlink" title="Mixtures of Gaussians"></a>Mixtures of Gaussians</h1><p>MLE 的通用步骤：<br>Steps:</p>
<ol>
<li>写出似然函数；</li>
<li>对似然函数取对数，并整理；</li>
<li>求导数，令导数为0，得到似然方程；</li>
<li>解似然方程，得到的参数即为所求；</li>
</ol>
<p>然而有的时候我们会陷入困境，因为似然方程没法求解，比如 Mixtures of Gaussians。</p>
<h2 id="One-Gaussian"><a href="#One-Gaussian" class="headerlink" title="One Gaussian"></a>One Gaussian</h2><p><strong>Given data:</strong> X1,…Xn<br><strong>Modeling assumption:</strong> Xi(independently) ~ Gaussian, $\sigma^2$=1/2<br><strong>LE</strong><br>$$P(x_1,..,x_n|\mu)={1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu)^2 \over 2 \sigma^2} $$</p>
<p><strong>MLE:</strong> $\hat \mu_{ML}= argmax \ L(x_1,..,x_n|\mu)$<br>$\mu_{ML} = {\sum_{i=1}^n \ x_i \over n} = \overline x_i$</p>
<h2 id="K-Gaussians"><a href="#K-Gaussians" class="headerlink" title="K Gaussians"></a>K Gaussians</h2><p><strong>Given data:</strong> X1,…Xn<br><strong>Modeling assumption:</strong> Xi(independently) ~ Mixture of K Gaussians    $\sigma^2$=1/2, $\mu_1,…\mu_k$<br><strong>Prior probability over Gaussians:</strong> $\lambda_1,…,\lambda_k \ 0 \le \lambda_j \le 1 \ \sum \lambda_j=1$, distribution of j is unknown, $\lambda_j, \mu_j$ is unknown<br><strong>LE</strong><br>$$<br>\begin{aligned}<br>P(x_1,..,x_n|\hat \lambda,\hat \mu,\sigma^2=0.5) &amp; = \prod_i^n[\sum_j^k P(Gaussian \ j \ was \ chosen)P(xi|Gaussian \ j \ was \ chosen)] \\<br> &amp; = \prod_i^n[\sum_j^k \lambda_j{1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu_j)^2 \over 2 \sigma_j^2}]<br>\end{aligned}<br>$$</p>
<p><strong>MLE:</strong> $(\hat \mu, \hat \lambda)_{ML}= argmax \ L(x1,..,xn|\hat \lambda,\hat \mu,\sigma^2=0.5)$</p>
<p><strong>How to find MLE?</strong></p>
<h2 id="“SOFT”-K-MEANS"><a href="#“SOFT”-K-MEANS" class="headerlink" title="“SOFT” K-MEANS"></a>“SOFT” K-MEANS</h2><p><strong>soft:</strong> our guesses are probabilities and taking values in [0,1]<br><strong>hard:</strong> represents a single best guess (taking values in {0,1} or {1,…k})</p>
<p>Special case of the EM Algorithm<br>$\sigma^2$=1/2</p>
<p><strong>初始化</strong><br>初始化分布参数θ, $\mu^{(0)}$,$\lambda^{(0)}$</p>
<p><strong>重复 EM 步骤直到收敛</strong><br><strong>1.E-step</strong><br>Fill in    the    missing    variables    with the expected values<br>根据参数初始值或上一次迭代的模型参数来计算出隐性变量的后验概率，其实就是隐性变量的期望。作为隐藏变量的现估计值：<br>$$w_j^{(i)}=P(z^{(i)}=j|x^{(i)};\mu,\lambda,\sigma^2=0.5) $$</p>
<p><strong>2.M-Step</strong><br>Regular    maximum    likelihood estimation    (MLE)    using    the    values computed    in the E step    and    the    values of    the    other    variables<br>将似然函数最大化以获得新的参数值 $\mu^{(l+1)}$,$\lambda^{(l+1)}$</p>
<ul>
<li>l: index over EM iterations</li>
<li>j: index of Gaussians</li>
<li>i: index of datapoints<br>$$\hat \lambda_j={\# j \ was \ chosen \over n}={\sum_{i=1}^n w_j^{(i)} \over n}$$<br>$$\hat \mu_j={\sum_{i=1}^n w_j^{(i)} x^{(i)} \over \sum_{i=1}^n w_j^{(i)}}$$</li>
</ul>
<p><strong>EM Algorithm guarantees:</strong><br>$$L(D|\theta^{[0]}) \le L(D|\theta^{[1]}) \le L(D|\theta^{[2]})…$$<br>This is a Non-decreasing function. And if the likelihood function is bounded, the sequence will converge. Here the example is bounded, because $\sigma$ is fixed.<br>It might end up with local optimal.</p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EM%E7%AE%97%E6%B3%95/draft.jpg" class="ful-image" alt="draft.jpg">
<p>第一次 iteration 的表格表示<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/EM%E7%AE%97%E6%B3%95/table.jpg" class="ful-image" alt="table.jpg"></p>
<h1 id="General-EM-algorithm"><a href="#General-EM-algorithm" class="headerlink" title="General EM algorithm"></a>General EM algorithm</h1><p><strong>X</strong> observed data<br><strong>Z</strong> unobserved ‘data’<br><strong>Complete data</strong> Y=(X,Z)<br><strong>Likelihood function</strong> $L(X,Z|\theta)$ known</p>
<p><strong>Problem:</strong> find MLE<br>$$<br>  \begin{aligned}<br>  \hat \theta_{ML} &amp; = argmax \ L(X|\theta)= argmax \sum_Z P(X,Z|\theta) \\<br>   &amp; = argmax \sum_Z P(Z|\theta)P(X|Z,\theta) \\<br>   &amp; = argmax \sum_Z P(X|\theta)P(Z|X,\theta)P(X|Z,\theta)<br>  \end{aligned}<br>$$</p>
<p><strong>Chicken-egg Problem</strong> with regard to P(Z),$\theta$<br>先有鸡还是先有蛋的问题。当我们知道了哪些 datapoints 属于同一个高斯分布的时候，我们才能够对这个分布的参数作出靠谱的预测，然而现在 datapoints 混在了一起，我们不知道哪个属于哪个，也就没办法估计参数。反过来，只有当我们对 K 个分布的参数作了准确的估计的时候，才知道哪些 datapoints 属于同一个高斯分布。<br>所以陷入了僵局，怎么办？不如就先随便整一个值出来，看你怎么变，然后我再根据你的变化调整我的变化，然后如此迭代着不断互相推导，最终就会收敛到一个解。这就是EM算法的基本思想。 -&gt; chicken stay, egg change</p>
<p><strong>EM solution</strong></p>
<ol>
<li>Initialize $\theta$ to some value</li>
<li>EM 方程<br>$$\theta^{[l+1]}=argmax \ E_{Z|\theta^l}[logP(Z,X|\theta)]$$<br>E 就是 expectation 的 E, write expression for E as function of $\theta$, expression: auxiliary function $Q(\theta^{[l]}|\theta)$<br>M 就是 argmax 的 M, find the MAX over $\theta$<br>=&gt;<br><strong>Simple EM solution:</strong><ul>
<li><strong>E-step:</strong> express MLE as function of X’s and Z’s<br>know the value of Z, what would the ML solution</li>
<li><strong>M-step:</strong> replace each Z with $E[Z|\theta]$</li>
</ul>
</li>
</ol>
<p><strong>EM guarantees</strong><br>$$L(X|\theta^{[l]}) \le L(X|\theta^{l+1})$$</p>
<p>If likelihood is bounded, the sequence will converge.</p>
<p><strong>Special but most commonly use:</strong><br>$L(X,Z|\theta)$ is a member of the exponential family</p>
<h1 id="适用情景"><a href="#适用情景" class="headerlink" title="适用情景"></a>适用情景</h1><p>When to use:</p>
<ul>
<li>Data is only partially observable</li>
<li>Unsupervised clustering(target value unobservable)</li>
<li>Supervised learning(some instance attributes unobservable)</li>
</ul>
<p>Some uses:</p>
<ul>
<li>Train Bayesian Belief Networks</li>
<li>Unsupervised clustering(AUTOCLASS)</li>
<li>Learning Hidden Markov Models</li>
</ul>
<blockquote>
<p>参考链接：<br><a href="http://blog.csdn.net/zouxy09/article/details/8537620" target="_blank" rel="external">从最大似然到EM算法浅解</a><br><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html" target="_blank" rel="external">（EM算法）The EM Algorithm</a><br><a href="http://blog.csdn.net/livecoldsun/article/details/40833829" target="_blank" rel="external"> EM（Expectation-Maximization）算法 </a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Graphical Models]]></title>
      <url>http://www.shuang0420.com/2016/12/04/Graphical%20Models/</url>
      <content type="html"><![CDATA[<p>CMU 10601 的课程笔记。EM 算法计算含有隐含变量的概率模型参数估计，能使用在一些无监督的聚类方法上。在 EM 算法总结提出以前就有该算法思想的方法提出，例如 HMM 中用的 Baum-Welch 算法。<br><a id="more"></a></p>
<h1 id="Graphical-Models"><a href="#Graphical-Models" class="headerlink" title="Graphical Models"></a>Graphical Models</h1><h2 id="Key-idea"><a href="#Key-idea" class="headerlink" title="Key idea"></a>Key idea</h2><ul>
<li>Conditional independence assumptions useful (but Naive Bayes is extrem)</li>
<li>Probabilistic graphical models are a joint probability distribution defined over a graph</li>
</ul>
<p>Two types of graphical models:</p>
<ul>
<li>Directed graphs (Bayesian Networks)</li>
<li>Undirected graphs (Markov Random Fields)</li>
</ul>
<p><strong>hard bias:</strong><br>Distribution shares conditional independent assumptions</p>
<h2 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h2><h3 id="Marginal-Independence"><a href="#Marginal-Independence" class="headerlink" title="Marginal Independence"></a>Marginal Independence</h3><p><strong>Definition:</strong>    X    is marginally    independent    of Y if<br>$$(\forall i,j)P(X=x_i,Y=y_j)=P(X=x_i)P(Y=y_j)$$</p>
<p>变种：<br>$(\forall i,j)P(X=x_i|Y=y_j)=P(X=x_i)$<br>$(\forall i,j)P(Y=y_j|X=x_i)=P(Y=y_j)$</p>
<h3 id="Conditional-Independence"><a href="#Conditional-Independence" class="headerlink" title="Conditional Independence"></a>Conditional Independence</h3><p><strong>Definition:</strong>    X    is conditional independent of    Y    given    Z, if    the    probability    distribution governing X is    independent    of the value of Y,    given    the    value    of Z<br>$$(\forall i,j,k)P(X=x_i|Y=y_j|Z=z_k)=P(X=x_i|Z=z_k)$$<br>即<br>$$P(X|Y,Z)=P(X|Z)$$</p>
<h1 id="DGM-Directed-Graphical-Models"><a href="#DGM-Directed-Graphical-Models" class="headerlink" title="DGM(Directed Graphical Models)"></a>DGM(Directed Graphical Models)</h1><p>DGM = Bayes Nets = Bayes Belief Networks<br>Directed graph = (potentially very large) set of conditional independent assumptions = factorization of joint probability(Likelihood function)</p>
<h2 id="Bayesian-Networks"><a href="#Bayesian-Networks" class="headerlink" title="Bayesian Networks"></a>Bayesian Networks</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/Bayesian.jpg" class="ful-image" alt="Bayesian.jpg">
<h2 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h2><p>Causality =&gt; DCM<br>cannot infer causality from DCM<br>eg. A-&gt;B-&gt;C or C-&gt;B-&gt;A gives same DCM  $C \bot A|B =&gt; A \bot C|B$</p>
<h2 id="Types-of-DGM"><a href="#Types-of-DGM" class="headerlink" title="Types of DGM"></a>Types of DGM</h2><h3 id="Eg-Chain-of-events"><a href="#Eg-Chain-of-events" class="headerlink" title="Eg. Chain of events"></a>Eg. Chain of events</h3><p>Cloudy -&gt; Rain -&gt; Wetgrass<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/Chain.jpg" class="ful-image" alt="Chain.jpg"><br>I(C;W)&gt;=0<br>I(C;W|R)=0 $C \bot W|R$</p>
<h3 id="Eg-Multiple-Effects"><a href="#Eg-Multiple-Effects" class="headerlink" title="Eg. Multiple Effects"></a>Eg. Multiple Effects</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/MultipleEffects.jpg" class="ful-image" alt="MultipleEffects.jpg">
<p>Cloudy -&gt; Rain<br>       -&gt; Sprinkler<br>I(R;S)&gt;=0 $R NOT \bot S$<br>I(R,S|C)=0 $R \bot S|C$  </p>
<h3 id="Eg-Multiple-Causes"><a href="#Eg-Multiple-Causes" class="headerlink" title="Eg. Multiple Causes"></a>Eg. Multiple Causes</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/MultipleCauses.jpg" class="ful-image" alt="MultipleCauses.jpg">
<p>Rain -&gt; Wetgrass<br>Sprinkler -&gt; Wetgrass<br>I(R;S)=0 $R \bot S$<br>I(R,S|W)&gt;=0 $R NOT \bot S|W$</p>
<p>“Explaining away”<br>look one of the factor explains the result</p>
<h2 id="Factoring-and-Count-parameters"><a href="#Factoring-and-Count-parameters" class="headerlink" title="Factoring and Count parameters"></a>Factoring and Count parameters</h2><p>Factoring 主要靠的是 chain rule 和 conditional independence assumptions，计算参数则主要看每个节点的父节点数。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/s.jpg" class="ful-image" alt="s.jpg"><br>without independent Assumptions<br>P(S,B,L,C,T,F)=P(S)P(B|S)P(L|S,B)P(C|S,B,L)P(T|S,B,L,C)P(F|S,B,L,C,T)<br><strong># of paras</strong> 1+2+4+8+16+32</p>
<p>with these assumptions, condition only on immediate parent<br>=P(S)P(B)P(L|S)P(C|S,B)P(T|S,B)P(T|L)P(F|S,L,C)<br><strong># of paras</strong> 1+1+2+4+2+8=18<br><strong>dominate factor:</strong> largest number of parents that any one node has</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>再具体一点分析，假设我们有一个警报系统，有以下元素组成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">An	alarm	system</div><div class="line">       B	–	Did	a	burglary	occur?</div><div class="line">       E	–	Did	an	earthquake	occur?</div><div class="line">       A	–	Did	the	alarm	go	off?</div><div class="line">       M	–	Mary	calls</div><div class="line">       J	–	John	calls</div></pre></td></tr></table></figure></p>
<ol>
<li><p>Factoring joint distributions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">P(M,J,A,B,E)	=		</div><div class="line">                P(M	|J,A,B,E)	P(J,A,B,E)	=</div><div class="line">                P(M	|	J,A,B,E)	P(J	|	A,B,E)	P(A|	B,E)	=		</div><div class="line">                P(M	|	J,A,B,E)	P(J	|	A,B,E)	P(A	|	B,E)	P(B,E)</div><div class="line">                P(M	|	J,A,B,E)	P(J	|	A,B,E)	P(A	|	B,E)P(B	|	E)P(E)</div></pre></td></tr></table></figure>
</li>
<li><p>Draw    Bayesian Network</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/fac1.jpg" class="ful-image" alt="fac1.jpg">
<p>对上图来说，共需要 31 个参数。<br>M: 2^4<br>J: 2^3<br>A: 2^2<br>B: 2^1<br>E: 2^0</p>
</li>
<li>Use knowledge of domain<br>-&gt; P(B)P(E)P(A|B,E)P(J|A)P(M|A)<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/dn.jpg" class="ful-image" alt="dn.jpg">
现在，需要 10 个参数（A:4, B:1, E:1, J:2, M:2），我们少用了 21 个参数！</li>
</ol>
<h2 id="Use"><a href="#Use" class="headerlink" title="Use"></a>Use</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/inf.jpg" class="ful-image" alt="inf.jpg">
<p>distribution posterior $P(Xt|Xo,\overline \theta)$<br>argmax $Xt = argmax \ P(Xt|Xo,\overline \theta)$</p>
<p><strong>Using the model</strong><br>Given the model (struct+params) input-&gt;output</p>
<p>Likelihood function twice<br>$P(X_k=0|X_1,..X_{k-1},X_{k+1}…X_p)$<br>$P(X_k=1|X_1,..X_{k-1},X_{k+1}…X_p)$<br>(if binary) normalize the two values, and gets the posterior</p>
<p>Condition on X1,X3,X5,X7<br>want to know: distribution over X2,X4,X6,X8<br>need to cal<br>$P(X_2=0,X_4=0,X_6=0,X_8=0,|X_1,..X_{k-1},X_{k+1}…X_p)$<br>$P(X_2=0,X_4=0,X_6=0,X_8=1,|X_1,..X_{k-1},X_{k+1}…X_p)$<br>…<br>$P(X_2=1,X_4=1,X_6=1,X_8=1,|X_1,..X_{k-1},X_{k+1}…X_p)$<br>cal all combinations, normalize, to get the posterior</p>
<p>exp(# of unobserved values)</p>
<h3 id="Eg-1-Computing-full-joints"><a href="#Eg-1-Computing-full-joints" class="headerlink" title="Eg.1 Computing full joints"></a>Eg.1 Computing full joints</h3><p>$P(B,\lnot E,A,J,\lnot M)$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/use.jpg" class="ful-image" alt="use.jpg"></p>
<h3 id="Eg-2-Compute-partial-joints"><a href="#Eg-2-Compute-partial-joints" class="headerlink" title="Eg.2 Compute partial joints"></a>Eg.2 Compute partial joints</h3><p>$P(B|J, \lnot M)$</p>
<p>$P(B|J, \lnot M)={P(B,J, \lnot M) \over P(B,J, \lnot M)+P(\lnot B,J,\lnot M)}$</p>
<p><strong>Compute $P(B,J, \lnot M)$</strong><br>Sum    all    instances    with these settings    (the sum is over the possible    assignments    to the other two variables,    E    and    A)<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/pj1.jpg" class="ful-image" alt="pj1.jpg"></p>
<p>有简化的方法如 Variable elimination，这里不展开。</p>
<h1 id="Things-we-do-with-regard-to-a-ML-framework"><a href="#Things-we-do-with-regard-to-a-ML-framework" class="headerlink" title="Things we do with regard to a ML framework"></a>Things we do with regard to a ML framework</h1><ol>
<li><p>Using it (“inference”)<br>DGM: DP, transforming the tree, approximate inference</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Graphical%20Models/inf.jpg" class="ful-image" alt="inf.jpg">
<p>distribution posterior $P(Xt|Xo,\overline \theta)$<br>argmax $Xt = argmax \ P(Xt|Xo,\overline \theta)$</p>
</li>
<li><p>Parameter learning: Learning/Deriving/Fitting the parameters (select weights/transition…) <strong>&lt;= soft bias</strong>/loss function/objective function/optimization function<br>from completed data -&gt; easy: relative frequency+smoothing<br>from incompleted data(some days may be missing) -&gt; hard: EM</p>
<p> for 1,2<br> UGM<br> consideration: largest number of click</p>
<p> DGM<br> consideration: max number of parents</p>
</li>
<li><p>Structure learning: Learning/Deriving/Selecting (select neurons,graph…) <strong>&lt;= hard bias</strong><br>the number of possible structure is huge<br>very very had, usually not enough data<br>use greedy, AIC, BIC</p>
</li>
</ol>
<h2 id="ML-Para-learning-in-DGM"><a href="#ML-Para-learning-in-DGM" class="headerlink" title="ML-Para learning in DGM"></a>ML-Para learning in DGM</h2><p>$\hat \theta_{ML} = argmax \ L(observed \ data|\overline \theta)$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    C R S W</div><div class="line">D1  0 0 0 1</div><div class="line">D2  0 1 1 1</div><div class="line">...</div><div class="line">Dn</div></pre></td></tr></table></figure></p>
<p>$P(C)={\# of \ C \over N}$<br>$P(S|C=1)={\#(C=1|S=1) \over \#(C=1)}$<br>Relative frequency + Smoothing</p>
<h2 id="Select-the-structure"><a href="#Select-the-structure" class="headerlink" title="Select the structure"></a>Select the structure</h2><p><strong>Model Hierarchy</strong><br>more and more expressive =&gt; complexity of model</p>
<p><strong>simple vs fitting tradeoff</strong><br>AIC: max<br>BIC: fewest para</p>
<p>CAL:<br>$\delta log L(D|\theta)$</p>
<h1 id="Conditional-Random-Fields"><a href="#Conditional-Random-Fields" class="headerlink" title="Conditional Random Fields"></a>Conditional Random Fields</h1><p>CRF = UGM Trained Discriminatively<br>$argmax \ L(labels|inputs,\theta)$</p>
<p><strong>generative modeling</strong><br>$\theta=argmax \ L(D|\theta)$<br>MAP=argmax P(|x)</p>
<p><strong>discriminative method</strong><br>$argmin \ ERR(classifier|D,\theta)$<br>better when training data is small</p>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - ReDDE Algorithm for Resource Selection]]></title>
      <url>http://www.shuang0420.com/2016/11/29/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/</url>
      <content type="html"><![CDATA[<p>对 Luo Si 和 Jamie Callan 的论文 <a href="http://boston.lti.cs.cmu.edu/classes/Papers/sigir03-lsi.pdf" target="_blank" rel="external">Relevant Document Distribution Estimation Method for Resource Selection</a> 做的一些笔记。之后会整理 Federated search 这一章的笔记。<br><a id="more"></a></p>
<h1 id="Distributed-information-retrieval"><a href="#Distributed-information-retrieval" class="headerlink" title="Distributed information retrieval"></a>Distributed information retrieval</h1><p>Distributed information retrieval 也叫做 Federated search，这种搜索引擎包含了多种搜索引擎或者数据库。论文表示 Distributed information retrieval 主要存在三个子问题。</p>
<ol>
<li>Resource representation。即获取每个数据库的信息</li>
<li>Resource ranking。即对 resource databases 进行排序，对每个 query 选择 a small number of resources。</li>
<li>Result-merging。即对来自不同搜索引擎／数据库对结果进行 merge，产生最终展现给用户的 ranking list。</li>
</ol>
<h1 id="Database-size-估计"><a href="#Database-size-估计" class="headerlink" title="Database size 估计"></a>Database size 估计</h1><p>Database size 是资源描述的重要组成部分。先前的研究表明，评估相似度时对数据库大小进行归一化对结果很重要。resource selection error 也通常是因为 normalize 不同大小的数据库中的 term frequency 时的不合理。</p>
<p>database size 有很多种表现形式：</p>
<ul>
<li>size of vocabulary</li>
<li>number of word occurrences</li>
<li>number of documents</li>
<li>…</li>
</ul>
<p>论文中的数据库大小用文档数量来表示。<br>目前有的估计 database size 的方法有 Capture-Recapture Method[11], gGIOSS[6], Iperirotis’s Hierarchical Database Sampling and Selection algorithm[8], D’Souza and Thom’s n-term indexing method[5], CORI resource selection algorithm[1,2] 等等。论文介绍了 Capture-Recapture Method，在此基础上提出了 Sample-Resample Method。</p>
<h2 id="Capture-Recapture-Method"><a href="#Capture-Recapture-Method" class="headerlink" title="Capture-Recapture Method"></a>Capture-Recapture Method</h2><p>主要思想是通过抽取两个 sample，基于样本独立的假设以及此基础上的概率知识 P(A|B)=P(A) 来推断 database size。</p>
<p>假设：一个 collection 里有两个（或更多）独立的 sample。</p>
<ul>
<li>N: population size</li>
<li>A: the event that an item is included in the first sample which is of size n1</li>
<li>B: the event that an item is included in the second sample which is of size n2</li>
<li>m2: the number of items that appeared in both samples</li>
</ul>
<p>于是有：</p>
<p>$$P(A)={n_1 \over N} \qquad (1)$$<br>$$P(B)={n_2 \over N} \qquad (2)$$<br>$$P(A|B)={m_2 \over n_2} \qquad (3)$$</p>
<p>因为这两个 sample 是独立的，所以有<br>$$P(A|B)=P(A) \qquad (4)$$</p>
<p>所以<br>$$\hat N={n_1n_2 \over m_2} \qquad (5)$$</p>
<p>通过随机发送 query 到数据库/搜索引擎然后从返回的 document ids 进行抽样，然后估计数据库大小。</p>
<p>论文指出，这种算法可能不现实，这主要是基于成本的考虑。比如说，当估计一个有 300,000 篇文档的数据库，采样过程使用 2000 条 query，每条 query 返回 1000 个 document ids，那么一共就有 2,000,000 (non-unique) documents 需要评估。这个 cost 无疑是巨大的。而我们希望用更低的成本来估计数据库大小。由此论文提出了 Sample-Resample Method。</p>
<h2 id="Sample-Resample-Method"><a href="#Sample-Resample-Method" class="headerlink" title="Sample-Resample Method"></a>Sample-Resample Method</h2><p>Capture-Recapture 算法需要和数据库进行多次交互，Sample-Resample 算法减少了交互的次数。</p>
<h3 id="相关前提假设"><a href="#相关前提假设" class="headerlink" title="相关前提假设"></a>相关前提假设</h3><p>假定 resource description 由 query-based sampling [13] 产生，每个 resource description list 列出了</p>
<ul>
<li>抽样的文档数 (number of documents sampled)</li>
<li>抽样文档中包含的 term (the terms contained in sampled documents)</li>
<li>抽样文档中包含各个 term 的文档数 (the number of sampled documents containing each term)</li>
</ul>
<p>并且，我们假定各个 search engine/database 列出了 match 当前 query 的所有文档数。</p>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>从当前数据库的 resource description 里随机抽取一个 term，然后把这个 term 当作 query (single-term query) 在当前 database 进行检索，这个过程又叫做 resampling。数据库会返回匹配这个 query 的文档数和一些排在前面的文档 (top-raked documents)。一些 notation 如下：</p>
<ul>
<li>Cj: database</li>
<li>Cj_sample: documents sampled from the database when the resource description was created</li>
<li>N_cj: size of Cj</li>
<li>N_cj_sample: size of Cj sample</li>
<li>qi: query term selected from the resouce description for Cj</li>
<li>df_qicj: number of documents in Cj that contain qi</li>
<li>df_qicj_samp: number of documents in Cj_sample that contain qi</li>
<li>A: the event that a document sampled from the database contains term qi</li>
<li>B: the event taht a document from the database contains qi</li>
</ul>
<p>然后，我们就能得到下面的概率。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/67.jpg" class="ful-image" alt="67.jpg">
<p><strong>假设:</strong> 从数据库里抽取的样本具有代表性，能够很好的描述整个数据库<br>在这个假设下，我们就可以推断出数据库大小。<br>=&gt; P(A)~P(B)<br>=&gt;</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/8.jpg" class="ful-image" alt="8.jpg">
<h2 id="Database-Size-Evaluation-Metrics"><a href="#Database-Size-Evaluation-Metrics" class="headerlink" title="Database Size Evaluation Metrics"></a>Database Size Evaluation Metrics</h2><p>很简单的公式。<br>$$AER={|N-N^\ast|\over N^\ast} \qquad (9)$$</p>
<ul>
<li>AER: absolute error ratio</li>
<li>$N^{\ast}$: actual databse size</li>
<li>N: estimate</li>
</ul>
<h2 id="Cost"><a href="#Cost" class="headerlink" title="Cost"></a>Cost</h2><p>重要的成本就是和搜索引擎进行的交互次数。</p>
<p>Liu 和 Yu 在 Capture-Recapture 算法[11]中假定数据库返回的排序列表有 1,000 个 document id。通常来说，像 AltaVista 和 Google 最初都只会返回 top 10 或 20 的结果。如果我们假定搜索引擎每页能返回 10-20 个结果，假定为 20，那么为了得到 1,000 个 docid，需要和数据库进行 50 次交互。当然，如果我们可以提前设定我们从 sample 里取多少条(top k documents)，并且搜索引擎可以让我们限定返回的结果数，那么就只需要和搜索引擎进行 1 次的交互啦，然而这个并不现实，一般搜索引擎不会提供这样的接口。</p>
<p>而 Sample-Resample 算法主要的成本来自于用于 resample 的 queries，每一个 resample query 都需要一次数据库交互，论文里对每个数据库只用了 5 个 resample queries，也就是 5 次交互。与 Capture-Recapture 算法相比 cost 大大降低。</p>
<h1 id="ReDDE-算法"><a href="#ReDDE-算法" class="headerlink" title="ReDDE 算法"></a>ReDDE 算法</h1><p>资源选择的目的是找出一个包含了许多相关文档的一小撮数据库。如果我们能知道这些相关文档在不同数据库的分布，那么就可以根据每个数据库有的相关文档的数量，给数据库一个 ranking，这个 ranking 叫做 relevance based ranking(RBR)。</p>
<p>数据库 Cj 里与 query q 相关的文档数：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/10.jpg" class="ful-image" alt="10.jpg">
<p>Ncj 是数据库 Cj 里的文档总数，可以用 Sample-Resample 的方法估计出来，是估计值我们可以用 $\hat N_{c_j}$ 来表示。 对于 P(di|Cj)，既然我们有一个 complete resource description，那么这个概率就是 1/Ncj。所以相关文档数就成了</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/11.jpg" class="ful-image" alt="11.jpg">
<p>现在就剩下了 P(rel|di)，给定一个文档求相关的概率。我们定义一个 centralized complete database，代表在分布式的 IR 系统里所有 available individual databases 的全集。P(rel|di)是给定一个文档，centralized complete database 返回的这篇文档与 query 相关的概率。这个概率分布可以用 step function 进行建模，也就是说在 ranked list 中排在 top k 的文档的 relevance probability 都是一个常量，其它所有文档的 relevance probability 都是 0。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/12.jpg" class="ful-image" alt="12.jpg">
<ul>
<li>Cq: query-independent constant</li>
<li>rank_central(di): the rank of document di in the centralized complete database</li>
<li>ratio: threshold, indicating how the algorithm focuses attention on different parts of the centralized complete DB ranking.</li>
</ul>
<p>在论文中，ratio 用了 0.003，相当于在一个文档总数为 1,000,000 的数据库里取前 3,000 文档。实验表明，ReDDE 算法在 0.002-0.005 间有比较好的效果。</p>
<p>然而，一个 centralized complete database 难以获取，我们只能建立 centralized sample database 来作为 centralized complete database 的 representative subset，这里的文档来自于在 database resource description 建立的基础上进行的 query-based sample。之前的研究表明，centralized sample database 对 result merging 的归一化文档分数非常有效。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/13.jpg" class="ful-image" alt="13.jpg">
<p>等式 12 和 13 代入 11 就可以计算出 $\hat Rel_q(j)$，到此，我们就可以通过 normalize 等式 11 的值来估计数据库分布，从而为数据库排名。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20ReDDE%20Algorithm%20for%20Resource%20Selection%20/14.jpg" class="ful-image" alt="14.jpg"></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>这里略过了实验，有兴趣的同学可以查看<a href="http://boston.lti.cs.cmu.edu/classes/Papers/sigir03-lsi.pdf" target="_blank" rel="external">原文</a>。主要是验证了 ReDDE 算法的有效性，并且提出了，提高资源选择不一定能提高检索准确率的结论。</p>
<p>如果我们有一个 centralized complete database，一个搜索算法会返回 TOP 100 的文档，然而这个结果集只在所有文档中占一个非常非常小的比例，ReDDE 算法需要评估更大的比例。比如说，如果 ratio 是 0.003，total testbed size 是 1,000,000 文档，尽管目标是检索 100 篇相关文档，然而资源选择算法会得到在 centralized complete database 的 TOP 3,000 篇文档，优化 TOP 100 和优化 TOP 3,000 并不是一回事儿。当然你可以选择降低 ratio，然而这样的话我们就只能在一个 sampled documents 很少的情况下做决策，使用 small ratio 会使一小部分数据库有 nonzero estimates.</p>
<p>随之而来的改进版 ReDDE 算法，考虑两个 ratio，一个 smaller ratio 一个 larger ratio，对于有 large enough estimation value 的数据库我们用 smaller ratio 来排序，而其他的数据库我们用 larger ratio 来排序，于是对每个数据库，我们用等式 14 可以得到两个 estimation values（DistRel_r1j，DistRel_r2j），r1 的经验值是 0.0005，r2 的经验值是 0.003，过程如下：</p>
<ol>
<li>Rank all the databases that have DistRel_r1j &gt;= backoff_Thres</li>
<li>For all the other databases rank them with the values DistRel_r2j.</li>
</ol>
<p>backoff_Thres 在实验中被设为 0.1。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>CORI 算法是非常有效的一种资源排序算法，然而对很多小的数据库和一部分非常大的数据库组合的环境下(比如公司或者政府环境)效果并不好，于是才引入了 ReDDE 算法。</p>
<p>ReDDE 算法的基本思想是通过使用估计的数据库大小和一个 centralized sample database 来估计 available databases 下相关文档的分布。实验表明 ReDDE 算法在 rank 1-10 下至少能和 CORI 算法一样有效，有些情况下甚至更有效，当然在有很多数据库被搜索的情况下 CORI 算法还是有着优势。</p>
<p>另一个结论是提高资源选择不一定能提高检索准确率。实验表明，ReDDE 算法产生的最终的文档排序和 CORI 算法产生的最终的文档排序至少一样准确，甚至更好。</p>
<p>ReDDE 算法的不足是它采用了常量来对文档相关性建模，这些常量本身可能是一个 weakness。训练数据能够自动来决定 testbed-specific parameter settings，来提高准确性和广泛性，这一点还在研究中。</p>
<p>论文也提出了估计数据库大小对算法 Sample-Resample，与 Capture-Recapture 算法相比，Sample-Resample 算法在各种数据库尤其是大型数据库下稳定性更强。</p>
<blockquote>
<p>REFERENCES<br>[1] J. Callan. (2000). Distributed information retrieval. In W.B.Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers. (pp. 127-150).<br>[2] J. Callan, W.B. Croft, and J. Broglio. (1995). TREC and TIPSTER experiments with INQUERY. Information Processing and Management, 31(3). (pp. 327-343).<br>[3] N. Craswell. (2000). Methods for distributed information retrieval. Ph. D. thesis, The Australian Nation University.<br>[4] A. Le Calv and J. Savoy. (2000). Database merging strategy based on logistic regression. Information Processing and Management, 36(3). (pp. 341-359).<br>[5] D. D’Souza, J. Thom, and J. Zobel. (2000). A comparison of techniques for selecting text collections. In Proceedings of the Eleventh Australasian Database Conference (ADC).<br>[6] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke.(1997). STARTS: Stanford proposal for internet metasearching. In Proceedings of the 20th ACM-SIGMOD International Conference on Management of Data.<br>[7] J.C. French, A.L. Powell, J. Callan, C.L. Viles, T. Emmitt, K.J. Prey, and Y. Mou. (1999). Comparing the performance of database selection algorithms. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.<br>[8] P. Ipeirotis and L. Gravano. (2002). Distributed search over the hidden web: Hierarchical database sampling and selection. In Proceedings of the 28th International Conference on Very Large Databases (VLDB).<br>[9] The lemur toolkit. <a href="http://www.cs.cmu.edu/~lemur" target="_blank" rel="external">http://www.cs.cmu.edu/~lemur</a><br>[10] InvisibleWeb.com. <a href="http://www.invisibleweb.com/" target="_blank" rel="external">http://www.invisibleweb.com/</a><br>[11] K.L. Liu, C. Yu, W. Meng, A. Santos and C. Zhang. (2001). Discovering the representative of a search engine. In Proceedings of 10th ACM International Conference on Information and Knowledge Management (CIKM).<br>[12] A.L. Powell, J.C. French, J. Callan, M. Connell, and C.L. Viles, (2000). The impact of database selection on distributed searching. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.<br>[13] L. Si and J. Callan. (2002). Using sampled data and regression to merge search engine results. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.<br>[14] J. Xu and J. Callan. (1998). Effective retrieval with distributed collections. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.<br>[15] S. Robertson and S. Walker. (1994). Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information  Retrieval.</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Bayesian Learning]]></title>
      <url>http://www.shuang0420.com/2016/11/28/Bayesian-Learning/</url>
      <content type="html"><![CDATA[<p>CMU 10601 的课程笔记。朴素贝叶斯思想即对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<br><a id="more"></a></p>
<h1 id="Generative-and-discriminative-algorithms"><a href="#Generative-and-discriminative-algorithms" class="headerlink" title="Generative and discriminative algorithms"></a>Generative and discriminative algorithms</h1><ul>
<li><p>Generative<br>try to model data, how data comes out, which disease cause with symptom with what probability - provide a profile for each disease<br>给定输入 x，生成模型可以给出输入和输出的联合分布 P(x,y)，生成模型的目标是求联合分布 P(x,y)，然后求出条件概率分布 P(Y|X) 作为预测的模型</p>
<ul>
<li>以朴素贝叶斯为例，生成模型的求解思路是：<strong>联合分布——-&gt;求解类别先验概率和类别条件概率</strong></li>
<li>联合分布能提供更多的信息，需要更多的样本和更多计算</li>
<li>收敛速度比较快，当样本数量较多时，生成模型能更快地收敛于真实模型。</li>
<li>生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。</li>
<li>e.g. Naive Bayes, HMM</li>
</ul>
</li>
<li><p>Discriminative<br>try to find mapping symptom -&gt; disease reverse order</p>
<ul>
<li>判别模型的求解思路是：<strong>条件分布——&gt;模型参数后验概率最大——-&gt;（似然函数参数先验）最大——-&gt;最大似然</strong></li>
<li>直接学习 P(Y|X)，节省计算资源，另外，需要的样本数量也少于生成模型</li>
<li>准确率往往较生成模型高</li>
<li>e.g. Logistic Regression</li>
<li>实践中多数情况下判别模型效果更好</li>
</ul>
</li>
</ul>
<h1 id="MLE-and-MAP"><a href="#MLE-and-MAP" class="headerlink" title="MLE and MAP"></a>MLE and MAP</h1><p><strong>MLE(maximum likelihood estimate):</strong> choose $\theta$ that maximize D probability of observed data.<br>$$\theta = argmax \ P(D|\theta)$$</p>
<p><strong>MAP(maximum a posterior estimate):</strong> choose $\theta$ that is most probable given prior probability and the data.<br>$$\theta = argmax \ P(\theta|D)=argmax \ {P(D|\theta)P(\theta) \over P(D)}$$</p>
<h1 id="Bayesian-statistics"><a href="#Bayesian-statistics" class="headerlink" title="Bayesian statistics"></a>Bayesian statistics</h1><h2 id="主旨"><a href="#主旨" class="headerlink" title="主旨"></a>主旨</h2><ul>
<li>Combine prior knowledge (prior probabilities) with observed data</li>
<li>Provides “gold standard” for evaluating other learning algorithms</li>
<li>Additional insight into Occam’s razor</li>
</ul>
<h2 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/bayesg.jpg" class="ful-image" alt="bayesg.jpg">
<p>要记住的是: today’s posterior is tomorrow’s prior</p>
<h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>$$P(h|D)= {P(D|h)P(h) \over P(D)}$$<br>P(h) = prior probability of hypothesis h<br>P(D) = prior probability of training data D<br>P(h|D) = probability of h given D<br>P(D|h) = probability of D given h</p>
<p>推导非常简单<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">P(h,D) = P(h)P(D|h)</div><div class="line">       = P(D)P(h|D)</div></pre></td></tr></table></figure></p>
<p>一般来说，prior 不能是 0，否则 posterior 也会是 0 了。</p>
<h2 id="Base-rate-neglect"><a href="#Base-rate-neglect" class="headerlink" title="Base rate neglect"></a>Base rate neglect</h2><p>Ignore prior in reasoning</p>
<blockquote>
<p>If presented with related base rate information (i.e. generic, general information) and specific information (information only pertaining to a certain case), the mind tends to ignore the former and focus on the latter.</p>
</blockquote>
<h2 id="MAP-Maximum-a-posteriori"><a href="#MAP-Maximum-a-posteriori" class="headerlink" title="MAP (Maximum a posteriori)"></a>MAP (Maximum a posteriori)</h2><p>判断 how well a hypothesis matches a data<br>$h \in H$<br>$$h_{MAP} = argmax P(h|D) = argmax {P(D|h)P(h) \over P(D)} = argmax P(D|h)P(h)$$</p>
<h2 id="Example-1-Cancer"><a href="#Example-1-Cancer" class="headerlink" title="Example 1. Cancer"></a>Example 1. Cancer</h2><p>通过例子来说明<br>问题是：这个患者是否有癌症<br>条件是：</p>
<ol>
<li>一个患者接受了一个检验，显示是阳性的(positive)。</li>
<li>这个检验返回的 correct positive 概率是 98%，correct negative 的概率是 97%。</li>
<li>人群中 0.008 的人可能会患这个癌症。</li>
</ol>
<p>首先用熟悉符号表示这个问题<br>Hypothesis: $H = {cancer, \lnot {cancer}}$<br>Prior: $\pi(cancer) = 0.008$, $\pi (\lnot cancer) = 0.992$<br>L(D|h)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">h   | pos  | neg</div><div class="line">c   | 0.98 | 0.02</div><div class="line">nc  | 0.03 | 0.97</div></pre></td></tr></table></figure>
<p>右上角的是 false negative (negative result which is false)，又叫 miss<br>左下角的是 false positive (positive result which is false)，又叫 alarm<br>miss 和 alarm 两者是一个 tradeoff，如果永远都说是 cancer，那么没有任何 miss，却有很多的 alarm</p>
<p>计算：<br>$$<br>  \begin{aligned}<br>  P(c|D=pos) &amp; = {\pi(c)L(pos|c) \over P(pos)} \\<br>  &amp; =  {\pi(c)L(pos|c) \over \sum_i P(pos|h_i)P(h_i)} \\<br>   &amp; = {0.008*0.98 \over 0.008*0.98+0.992*0.03} \\<br>   &amp; = 0.208<br>  \end{aligned}<br>$$</p>
<h2 id="Example-2-Cancer-cont"><a href="#Example-2-Cancer-cont" class="headerlink" title="Example 2. Cancer cont."></a>Example 2. Cancer cont.</h2><p>如果第二次检验的结果还是 positive，那么该患者患癌症的几率有多大？</p>
<h3 id="Method-1-Sequential-apply-of-Baye’s-Rules"><a href="#Method-1-Sequential-apply-of-Baye’s-Rules" class="headerlink" title="Method 1: Sequential apply of Baye’s Rules"></a>Method 1: Sequential apply of Baye’s Rules</h3><p>前提是 $D1 \bot D2 |h$<br>此时的 Posterior:<br>$\pi(cancer) = 0.208$, $\pi (\lnot cancer) = 0.792$<br>H, L(D|h) = SAME<br>D2 = pos<br>$$<br>  \begin{aligned}<br>  P(c|D2=pos) &amp; = {\pi(c)L(pos|c) \over P(pos)} \\<br>   &amp; = {0.208*0.98 \over 0.208*0.98+0.792*0.03} \\<br>   &amp; = 0.895<br>  \end{aligned}<br>$$</p>
<p>每多做一次结果为 positive 的 test，最后的概率都会越大。</p>
<h3 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a>Method 2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">h   |     ++    |     +-    |     -+    |    --     |</div><div class="line">c   | 0.98*0.98 | 0.98*0.02 | 0.02*0.98 | 0.02*0.02 |</div><div class="line">nc  | 0.03*0.03 | 0.03*0.97 | 0.97*0.03 | 0.97*0.97 |</div></pre></td></tr></table></figure>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Hypothesis H: a family of distributions indexed by $\ge $ 1 parameters</p>
<ul>
<li>subjective because of prior</li>
<li>returns a whole distribution<br>$\theta_{map} = argmax P(\theta|D)$</li>
</ul>
<p>Try to minimize MSE, $\theta_{mean}$<br>Try to minimize MAE, $\theta_{median}$</p>
<p><strong>Hard bias:</strong> choices of H family<br><strong>Soft bias:</strong> priors</p>
<h1 id="Frequentist-statistics"><a href="#Frequentist-statistics" class="headerlink" title="Frequentist statistics"></a>Frequentist statistics</h1><ul>
<li>no notion of prior</li>
<li>focus on likelihood L(D|h)</li>
</ul>
<h2 id="MLP-Maximum-likelihood-principle"><a href="#MLP-Maximum-likelihood-principle" class="headerlink" title="MLP (Maximum likelihood principle)"></a>MLP (Maximum likelihood principle)</h2><p>Given L() function family &amp; data, choose $$\theta = argmax \ L(D|\theta)$$</p>
<h2 id="Example-1-Binomial-distribution-X-B-n-p"><a href="#Example-1-Binomial-distribution-X-B-n-p" class="headerlink" title="Example 1. Binomial distribution X~B(n,p)"></a>Example 1. Binomial distribution X~B(n,p)</h2><p>X = {0,…n}<br>$$P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$$<br>Given n, and result of n flips -&gt; k heads, how to estimate P?</p>
<p><strong>简化</strong><br>$$<br>  \begin{aligned}<br>  P_{ML}  &amp; = argmax  \ L(k  \ heads  \ out  \ of  \ n  \ flips | P) \\<br>   &amp; = argmax \binom {n}{k}P^k(1-P)^{n-k}\\<br>   &amp; = argmax \  klogP + (n-k)log(1-P)<br>  \end{aligned}<br>$$</p>
<p><strong>求导</strong><br>=&gt; $\int {dll \over dp} =  k {1 \over p} - (n-k) {1 \over 1-p} = 0$<br>=&gt; ${k \over p} = {n-k \over 1-p}$<br>=&gt; $P_{ML} = {k \over n}$</p>
<h2 id="Example-2-Gaussian"><a href="#Example-2-Gaussian" class="headerlink" title="Example 2.Gaussian"></a>Example 2.Gaussian</h2><p>Fixed stdev, unkown mean X~N$(\mu \sigma^2)$<br>$$P(X=x)={1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu)^2 \over 2 \sigma^2}$$<br>Given $\sigma^2$ &amp; data x1,…xn $ \in$ IR, drawn i,i,d (independently, identical distributed)</p>
<p><strong>简化</strong><br>$$<br>\begin{aligned}<br>\hat \mu_{ML}  &amp; = argmax \ \prod_i^n {1 \over \sqrt{2 \pi \sigma^2}} e^{-(x_i-\mu)^2 \over 2 \sigma^2} \\<br> &amp; = argmax \sum_i^n {-(x_i - \mu)^2 \over 2 \sigma^2} \\<br> &amp; = argmin \sum_i^n (x_i-\mu)^2<br>\end{aligned}<br>$$</p>
<p><strong>求导</strong><br>=&gt; $\int {dll \over dp} =  \sum_{i=1}^n 2(x_i-\mu)= 0$<br>=&gt; $\sum_{i=1}^n x_i-n\mu$<br>=&gt; $\mu_{ML} = {\sum_{i=1}^n \ x_i \over n} = \overline x_i$<br>=&gt; sample mean = max likelihood gaussian true mean</p>
<h2 id="Example-3-Linear-regression"><a href="#Example-3-Linear-regression" class="headerlink" title="Example 3. Linear regression"></a>Example 3. Linear regression</h2><p>$$y = f(x_1,..x_n) + \epsilon$$</p>
<p>$$f(\overline x)=\sum_{j=1}^n \beta_jx_j$$<br>ASSUME</p>
<ul>
<li>$\epsilon$~N $(\mu, \sigma^2)$</li>
<li>$\epsilon = y - f(x_1,…x_n)$ ~ N $(\mu, \sigma^2)$</li>
</ul>
<p><strong>简化</strong><br>$$<br>\begin{aligned}<br>L(y_1, ….,y_n|x_1,…,x_n)  &amp; = \prod_i^n L(y_i|\overline x_i) \\<br> &amp; = \prod {1 \over 2 \pi \sigma^2} e^{-(y_i-f(\overline x_i)^2) \over 2 \sigma^2}<br>\end{aligned}<br>$$</p>
<p><strong>求导</strong><br>$\hat \beta = argmax \sum_{i=1}^n {-(y_i - \overline x_i)^2 \over 2 \sigma^2}$<br>  $= argmin \ \sum_{i=1}^n (y_i-f(\overline x_i)^2)$<br>  $= MSE(mean \ squared \ error)$</p>
<p>=&gt; MSE &lt;=&gt; Gaussian noise</p>
<h3 id="Special-case-in-Bayesian"><a href="#Special-case-in-Bayesian" class="headerlink" title="Special case in Bayesian"></a>Special case in Bayesian</h3><p>$\pi(\theta) = constant$  (uniform prior)<br>=&gt; $\theta_{MAP} = argmax \ \pi(\theta)L(D|\theta) = \theta_{ML}$</p>
<h3 id="How-to-estimate-P"><a href="#How-to-estimate-P" class="headerlink" title="How to estimate P?"></a>How to estimate P?</h3><p>我们希望 Estimator</p>
<ol>
<li>with enough data, should converge to the true value (asymptotical consistency)</li>
<li>fast converge to the right answer (efficiency)</li>
<li>low bias (usually no bias)<br>我们希望 $E[\hat \theta(D)]=\theta$。$bias(\hat \theta)=E[\hat \theta(D)]-\theta$ 这里我们想让它为 0，但是如果这是 ML inductive bias，我们不希望它为 0.</li>
<li>low variance (but can have high variants especially with little data)</li>
</ol>
<h1 id="MDL-minimum-length-description-principle"><a href="#MDL-minimum-length-description-principle" class="headerlink" title="MDL (minimum length description principle)"></a>MDL (minimum length description principle)</h1><p>Encode 数据。比如说我们要传递 1-100 万之间的质数，那我们只用告诉对方这是质数，而不用把所有的质数列出来，如果我们要传递的是除了 2 和 3 的质数，那么我们要告诉对方这是质数，而且里面没有  2 和 3。<br>放到 machine learning 里，假设双方都有一堆数据 X1..Xn(X有若干属性(x11,x22…xnn)，我们要通讯的是 Y 也就是 X 的标签，我们可以按顺序传送，这需要很多的 bits，或者可以建立一个 decision tree，这个 tree 可以 classify 大多数的数据，把 tree + exceptions 发送给对方就好，可以节省很多 bits。<br>$h_{MDL}$ = argmin (bits to describe h + bits to describe exceptions)</p>
<h1 id="Naive-Bayes-Classification"><a href="#Naive-Bayes-Classification" class="headerlink" title="Naive Bayes Classification"></a>Naive Bayes Classification</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>f: X -&gt; Y    &lt;=&gt;    P(Y |X )<br>X = ⟨X1,X2 …,Xn⟩   n 个属性<br>Y = boolean value    true or false</p>
<p>根据 Bayes rule, 来算 P(Y = yi|X)<br>$$P(Y=y_i|X=x_k)={P(X=x_k|Y=y_i)P(Y=y_i) \over \sum_j P(X=x_k|Y=y_j)P(Y=y_j)}$$</p>
<p>我们的目的是学习 P(Y|X)，来估计 P(X|Y) 和 P(Y)，用这些估计值，加上上面的 Bayes rule，就可以对新的 instance 分类。</p>
<h3 id="Unbiased-Learning-of-Bayes-Classifiers-is-Impractical"><a href="#Unbiased-Learning-of-Bayes-Classifiers-is-Impractical" class="headerlink" title="Unbiased Learning of Bayes Classifiers is Impractical"></a>Unbiased Learning of Bayes Classifiers is Impractical</h3><p>Unbiased Learning of Bayes Classifiers 是不实际的，因为它要估计的参数太多，计算量太大。<br>当 Y is boolean and X is a vector of n boolean attributes 的情况下，我们为了得到 P(X|Y) 需要的估计的参数有：</p>
<p>$$θ_{ij} ≡P(X =x_i|Y =y_j)$$</p>
<p>i takes on $2^n$ possible values (one for each of the possible vector values of X )<br>j takes on 2 possible values (boolean)<br> =&gt; $2^{n+1}$ parameters</p>
<p>For any fixed j, the sum over i of $θ_{ij}$ must be one.<br>i takes on $2^n-1$ possible values (one for each of the possible vector values of X )<br>j takes on 2 possible values (boolean)<br>=&gt; $2(2^n-1)$ parameters</p>
<p>To obtain reliable estimates of each of these parameters, we will need to observe each of these distinct instances multiple times!</p>
<p><strong>This is clearly unrealistic in most practical learning domains.</strong></p>
<p>For example, if X is a vector containing 30 boolean features, then we will need to estimate more than 3 billion parameters.</p>
<h3 id="Naive-Bayes-Algorithm"><a href="#Naive-Bayes-Algorithm" class="headerlink" title="Naive Bayes Algorithm"></a>Naive Bayes Algorithm</h3><p>Naive Bayes 的优势在于它能让我们对 P(X|Y) 建模时用更少的参数，$2(2^n-1)$ -&gt; 2n</p>
<h2 id="Naive-bayes-assumption-Independence-assumption"><a href="#Naive-bayes-assumption-Independence-assumption" class="headerlink" title="Naive bayes assumption (Independence assumption)"></a>Naive bayes assumption (Independence assumption)</h2><blockquote>
<p>Each $X_i$ is conditionally independent of each of the other $X_ks$ given Y, and also independent of each subset of the other $X_k$’s given Y.</p>
</blockquote>
<p>在这个假设下，我们举个例子，X = ⟨X1,X2⟩ 时，可以得到</p>
<p>$$<br>\begin{aligned}<br>P(X|Y)  &amp; = P(X1,X2|Y) \\<br> &amp; = P(X1|X2,Y)P(X2|Y) \\<br> &amp; = P(X1|Y)P(X2|Y)<br>\end{aligned}<br>$$</p>
<p>更加 general 的形式 <strong>Factor L(|Y)</strong><br>$$P(X1,X2…Xn|Y)=\prod_{i=1}^nP(X_i|Y)$$</p>
<p>同样的，Y and the Xi are boolean variables，这时候，我们只需要 2n 个参数来定义 $P(X_i = x_{ik}|Y = y_j)$!</p>
<p>首先我们产生 label，然后从 label 我们产生每一个属性。我们可以用下面的图来表示这种关系。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb.jpg" class="ful-image" alt="nb.jpg"></p>
<p>这个假设经常会被打破，然而即使是 horrible assumption，Naive Bayes 的效果也非常好。<br>tend to give extreme result</p>
<h2 id="Derivation-of-Naive-Bayes-Algorithm"><a href="#Derivation-of-Naive-Bayes-Algorithm" class="headerlink" title="Derivation of Naive Bayes Algorithm"></a>Derivation of Naive Bayes Algorithm</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb1.jpg" class="ful-image" alt="nb1.jpg">
<p>在 conditional independent assumption 下，可以继续得到<br>$$P(Y=y_k|X1…Xn)={P(Y=y_k)\prod_i P(X_i|Y=y_k) \over \sum_j P(Y=y_j)\prod_iP(Xi|Y=y_j)} \ (2)$$</p>
<p>$$Y \ &lt;= \ argmax \ {P(Y=y_k)\prod_i P(X_i|Y=y_k) \over \sum_j P(Y=y_j)\prod_iP(Xi|Y=y_j)}$$</p>
<p>简化下就是<br>$$Y \ &lt;= \ argmax \ P(Y=y_k)\prod_i P(X_i|Y=y_i=k) \ (3)$$</p>
<h2 id="Naive-Bayes-for-Discrete-Valued-Inputs"><a href="#Naive-Bayes-for-Discrete-Valued-Inputs" class="headerlink" title="Naive Bayes for Discrete-Valued Inputs"></a>Naive Bayes for Discrete-Valued Inputs</h2><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>n input attributes Xi each take on J possible discrete values<br>Y: a discrete variable taking on K possible values</p>
<p>Learning task is to estimate two sets of parameters.<br>$$\theta_{ijk}≡P(X_i=x_{ij}|Y=y_k)$$</p>
<p>这需要 nJK 个参数，对每个 j,k value 都满足  $1 = \sum_j \theta_{ijk}$，也就是 n(J-1)K 个参数。</p>
<p>另外我们要定义 prior probability over Y<br>$$\pi_k=P(Y=y_k)$$<br>需要 K-1 个参数。</p>
<h3 id="Estimate-parameters"><a href="#Estimate-parameters" class="headerlink" title="Estimate parameters"></a>Estimate parameters</h3><p>有两种估计参数的方法。Maximum likelihood 和 Bayesian MAP。</p>
<p><strong>MLE:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb6.jpg" class="ful-image" alt="nb6.jpg"></p>
<p>这种情况下有一个危险，当没有 example 符合条件的情况下，$\theta$ 可能为 0，所以我们需要 smooth。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb7.jpg" class="ful-image" alt="nb7.jpg"></p>
<p>J: the number of distinct values Xi can take on<br>l: determines the strength of this smoothing</p>
<p>同样的，估计 $\pi_k$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/nb9.jpg" class="ful-image" alt="nb9.jpg"></p>
<p>K: the number of distinct values Y can take on<br>l: determines the strength of the prior assumptions relative to the observed data D.</p>
<p><strong>MAP classifier:</strong><br>$Vmap = argmax P(v|a_1=x_1…a_n=x_n)=argmax P(v)L(a_1=x_1…a_n=x_n|v)$</p>
<p>为了估计 prior，我们需要 O(k) 个 parameter，这一般不是问题。<br>估计 L 的概率，我么需要 EXP(n) 个 parameter。</p>
<p>通过 Factor analysis 来减少 parameters</p>
<h2 id="Naive-Bayes-for-Continuous-Inputs"><a href="#Naive-Bayes-for-Continuous-Inputs" class="headerlink" title="Naive Bayes for Continuous Inputs"></a>Naive Bayes for Continuous Inputs</h2><p>当输入是连续的情况下，我们同样可以用(2)(3)来设计 Naive Bayes classifier，然而，由于 Xi 是连续的，我们必须用不同的方法来表示 P(Xi|Y) 的分布。通常的方法是假设 Xi 服从 Gaussian 分布，于是我们要定义 mean 和 standard deviation。</p>
<p>$$\mu_{ik}=E[X_i|Y=y_k]$$</p>
<p>$$\sigma^2_{ik}=E[(X_i-\mu_{ik})^2|Y=y_k]$$<br>这样我们需要估计 2nK 个参数。</p>
<p>prior:<br>$$\pi_k=P(Y=y_k)$$</p>
<h2 id="Squashing"><a href="#Squashing" class="headerlink" title="Squashing"></a>Squashing</h2><p>因为 independency 的假设，我们会得到非常极端的数值，如真实概率是 0.7, NB 出来的结果可能是 0.99999，真实概率是 0.2，NB 出来的结果可能是 0.0000001。<br>$$P^{\alpha}(y) \over \sum_i^n P^{\alpha}(y)$$</p>
<h2 id="Naive-Bayes-Classifier-vs-Logistic-Regression"><a href="#Naive-Bayes-Classifier-vs-Logistic-Regression" class="headerlink" title="Naive Bayes Classifier vs Logistic Regression"></a>Naive Bayes Classifier vs Logistic Regression</h2><ul>
<li>LR is discriminative classifier as LR directly estimates the parameters of P(Y|X). We can view the distribution P(Y|X) as directly discriminating the value of the target value Y for any given instance X.</li>
<li>NB is generative classifier as NB directly estimates parameters for P(Y) and P(X|Y). We can view the distribution P(X|Y) as describing how to generat random instances X conditioned on the target attribute Y.</li>
<li>NB is with greater bias but lower variance than LR.</li>
</ul>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Naive_Bayes_Learn(examples)<br> For each target value vj<br>  $\hat P(vj) &lt;- estimate P(vj)$<br>    For each attribute value ai of each attribute a<br>      $\hat P(ai|vj) &lt;- estimate P(ai|vj)$</p>
<p><strong>Classifier_New_Instance(x)</strong><br>  $Vnb = argmax \hat P(vj)\prod \hat P(ai|vj)$</p>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>new instance</p>
<outlk =="" sun,="" temp="cool," humid="high," wind="st">

<p>$$Vnb = argmax \hat P(vj)\prod \hat P(ai|vj)$$</p>
<p>P(y)P(sun|y)P(cool|y)P(high|y)P(strong|y)=0.005<br>P(n)P(sun|n)P(cool|n)P(high|n)P(strong|n)=0.021</p>
<h2 id="Learn-Naive-Bayes-Text"><a href="#Learn-Naive-Bayes-Text" class="headerlink" title="Learn Naive Bayes Text"></a>Learn Naive Bayes Text</h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Bayesian-Learning/algorithm.jpg" class="ful-image" alt="algorithm.jpg">
<h3 id="python-代码"><a href="#python-代码" class="headerlink" title="python 代码"></a>python 代码</h3><p>运行命令</p>
<pre>
python nb.py split.train split.test</pre>

<p>split.train / split.test 每行是一个 training file 文件名，有两个类别，con 和 lib。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">con10.txt</div><div class="line">con11.txt</div><div class="line">con1.txt</div><div class="line">con46.txt</div><div class="line">con48.txt</div><div class="line">con6.txt</div><div class="line">lib16.txt</div><div class="line">lib25.txt</div><div class="line">lib28.txt</div><div class="line">lib29.txt</div><div class="line">lib2.txt</div><div class="line">lib30.txt</div><div class="line">lib31.txt</div><div class="line">con25.txt</div><div class="line">con39.txt</div></pre></td></tr></table></figure></p>
<p>training file 每行是代表这个文件名类别的单词<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Random</div><div class="line">Jottings</div><div class="line">October</div><div class="line">30</div><div class="line">2008</div><div class="line">And</div><div class="line">you</div></pre></td></tr></table></figure></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">import math</div><div class="line">import sys</div><div class="line">from collections import Counter</div><div class="line">from sets import Set</div><div class="line"></div><div class="line"># collect all distinct words and other tokens in examples</div><div class="line"></div><div class="line"></div><div class="line">def initiate(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter</div><div class="line">    conCounter, libCounter = Counter(), Counter()</div><div class="line">    conDoc, libDoc = 0, 0</div><div class="line">    f = open(file, &apos;r&apos;)</div><div class="line">    files = f.readlines()</div><div class="line">    for file in files:</div><div class="line">        assert file.startswith(&apos;con&apos;) or file.startswith(&apos;lib&apos;)</div><div class="line">        if file.startswith(&apos;con&apos;):</div><div class="line">            conDoc += 1</div><div class="line">            conCounter = readTrain(file.strip(), conCounter, libCounter)</div><div class="line">        else:</div><div class="line">            libDoc += 1</div><div class="line">            libCounter = readTrain(file.strip(), libCounter, conCounter)</div><div class="line"></div><div class="line"></div><div class="line">def readTrain(trainFile, tokenDict, secondDict):</div><div class="line">    f = open(trainFile, &apos;r&apos;)</div><div class="line">    tokens = [line.strip().lower() for line in f]</div><div class="line">    for token in tokens:</div><div class="line">        tokenDict[token] += 1</div><div class="line">        if token not in secondDict:</div><div class="line">            secondDict[token] = 0</div><div class="line">    return tokenDict</div><div class="line"></div><div class="line"># calculate the requeired P(vj) and P(wk|vj)probability terms</div><div class="line"></div><div class="line"></div><div class="line">def train(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter, priorCon, priorLib</div><div class="line">    initiate(file)</div><div class="line">    # P(vj) = docsj/examples</div><div class="line">    priorCon = math.log(conDoc / float(conDoc + libDoc))</div><div class="line">    priorLib = math.log(libDoc / float(conDoc + libDoc))</div><div class="line">    # total number of words in Vocabulary</div><div class="line">    vocLen = len(Set(conCounter.keys()).union(Set(libCounter.keys())))</div><div class="line">    # P(wk|vj) = (nk+1)/(n+vocLen)</div><div class="line">    conN = sum(conCounter.values()) + vocLen</div><div class="line">    libN = sum(libCounter.values()) + vocLen</div><div class="line">    conDefault = math.log(1.0 / conN)</div><div class="line">    libDefault = math.log(1.0 / libN)</div><div class="line">    for key in conCounter:</div><div class="line">        conCounter[key] = math.log(</div><div class="line">            float(conCounter[key] + 1) / conN)</div><div class="line">    for key in libCounter:</div><div class="line">        libCounter[key] = math.log(</div><div class="line">            float(libCounter[key] + 1) / libN)</div><div class="line"></div><div class="line"></div><div class="line">def test(file):</div><div class="line">    global conDoc, libDoc, conCounter, libCounter, priorCon, priorLib</div><div class="line">    testFile = open(file)</div><div class="line">    testNum, correct = 0, 0</div><div class="line">    for line in testFile:</div><div class="line">        testNum += 1</div><div class="line">        libProb = priorLib</div><div class="line">        conProb = priorCon</div><div class="line">        conProb, libProb = readTest(line.strip(), conProb, libProb)</div><div class="line">        # print conProb,libProb</div><div class="line">        if conProb &gt; libProb:</div><div class="line">            print &apos;C&apos;</div><div class="line">            if line.startswith(&apos;con&apos;):</div><div class="line">                correct += 1</div><div class="line">        else:</div><div class="line">            print &apos;L&apos;</div><div class="line">            if line.startswith(&apos;lib&apos;):</div><div class="line">                correct += 1</div><div class="line">    accuracy = correct / float(testNum)</div><div class="line">    print &quot;Accuracy: %.04f&quot; % accuracy</div><div class="line"></div><div class="line"></div><div class="line">def readTest(file, conProb, libProb):</div><div class="line">    global conCounter, libCounter</div><div class="line">    f = open(file, &apos;r&apos;)</div><div class="line">    tokens = [line.strip().lower() for line in f]</div><div class="line">    for token in tokens:</div><div class="line">        if token in conCounter:</div><div class="line">            conProb += conCounter[token]</div><div class="line">        if token in libCounter:</div><div class="line">            libProb += libCounter[token]</div><div class="line">    return conProb, libProb</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    trainFile = sys.argv[1]</div><div class="line">    testFile = sys.argv[2]</div><div class="line">    train(trainFile)</div><div class="line">    test(testFile)</div></pre></td></tr></table></figure></p>
<h1 id="Error-and-test-of-hypothesis"><a href="#Error-and-test-of-hypothesis" class="headerlink" title="Error and test of hypothesis"></a>Error and test of hypothesis</h1><h2 id="True-error-of-hypothesis"><a href="#True-error-of-hypothesis" class="headerlink" title="True error of hypothesis"></a>True error of hypothesis</h2><p>sample error 和 true error 是不一样的。从 X 中抽取的样本 S，hypothesis h 关于 S 的 sample error 是 h 的实例在 S 中所在的比例，true error 也就是 true error 指的是对按某个分布随机抽取的实例，h 对它错误分类的概率。</p>
<p>h was wrong on 3/20 TEST ~ D<br>True error rate of $P(h(x)) \neq c(x)$   x ~ D</p>
<p>可能样本里只有一个 instance 错误，但是真实中会有很多这样的 input。</p>
<p>如果 $\hat \theta_{error} = 0.15$</p>
<h2 id="Compare-two-hypothesis"><a href="#Compare-two-hypothesis" class="headerlink" title="Compare two hypothesis"></a>Compare two hypothesis</h2><p>计算 sample error 之间的差异，通过统计学理论计算。<br>$Err(h_A) &gt;&lt;= Err(h_B)$ =&gt; Err(h_B) - Err(h_A) &gt; 0?<br>Depend on sample</p>
<h2 id="Compare-learning-algorithms"><a href="#Compare-learning-algorithms" class="headerlink" title="Compare learning algorithms"></a>Compare learning algorithms</h2><p>La: training set -&gt; $h_A$<br>Lb: training set -&gt; $h_b$</p>
<p>Compare h_A(test set y)  h_B(test set y)</p>
<p>better on this training set<br>avg performance of La is better than Lb</p>
<p>True error, E[Err_{test~D}[$L_A$(train ~ D)]]  this is hypothesis</p>
</outlk>]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hidden-Markov-Models]]></title>
      <url>http://www.shuang0420.com/2016/11/26/Hidden-Markov-Models/</url>
      <content type="html"><![CDATA[<p>CMU 10601 的课程笔记。上一章讲了 Bayesian networks，我们发现它对 joint distributions 建模很有用，然而却不能解释 temporal/sequence models，也没法解释循环问题。所以需要引入另一个模型，隐马尔可夫模型（Hidden Markov Model，HMM）。HMM 用来描述一个含有隐含未知参数的马尔可夫过程。难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来作进一步的分析。<br><a id="more"></a></p>
<h1 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h1><p>要素</p>
<ul>
<li>States: 所有可能出现的状态。$S={S_0, S_1, …S_n}$</li>
<li>Transition probabilities: 状态和状态间转换的概率。$P(q_t=S_i|q_{t-1}=S_j)$</li>
</ul>
<p>假设<br><strong>First order markov assumption</strong><br>Transition probability depends only on current state<br>即 $P(q_t=S_i|q_{t-1}=S_j,q_{t-2}=S_k) = P(q_t=S_i|q_{t-1}=S_j) = a_{ji}$<br>$a_{ji} &gt;=0 \ \forall j,i$ 且 $\sum_{i=0}^N a_{ji}=1 \ \forall j$<br>即 $q_t \bot q_j|q_{t-1} \ j&lt;t-1$<br>即 Markov Process only remembers one state at a time 不具备记忆特质（memorylessness），条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关</p>
<p>$P(q_1….q_T) = \prod P(q_t|q1….q_{t-1}) = \prod P(q_t|q_{t-1})$</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/mm.jpg" class="ful-image" alt="mm.jpg">
<h1 id="隐马尔可夫-HMM"><a href="#隐马尔可夫-HMM" class="headerlink" title="隐马尔可夫(HMM)"></a>隐马尔可夫(HMM)</h1><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><ul>
<li>signal processing</li>
<li>speech processing</li>
<li>low level NLP: eg. POS tagging, phrase chunking, target information extraction</li>
</ul>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>用一个例子来描述吧，网上有一篇帖子讲的非常好，这里就直接引用过来了。</p>
<p>假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为 D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为 D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为 D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/die1.jpg" class="ful-image" alt="die1.jpg">
<p>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是 1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4</p>
<p>这串数字叫做可见状态链(observations)。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8 (states)</p>
<p>一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6 的下一个状态是 D4，D6，D8 的概率都是1/3。D4，D8的下一个状态是 D4，D6，D8 的转换概率也都一样是 1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6 后面不能接 D4，D6 后面是 D6 的概率是0.9，是 D8 的概率是 0.1。这样就是一个新的 HMM。</p>
<p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生 1 的输出概率是 1/6。产生 2，3，4，5，6 的概率也都是 1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是 1/2，掷出来是 2，3，4，5，6 的概率是 1/10。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/die2.jpg" class="ful-image" alt="die2.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/die3.jpg" class="ful-image" alt="die3.jpg">
<h2 id="要素"><a href="#要素" class="headerlink" title="要素"></a>要素</h2><p>定义一些 notation。</p>
<ul>
<li>States(S): 隐含状态。$S={S_0, S_1, …S_n}$</li>
<li>Transition probabilities(A): 状态和状态间转换的概率P(Si-&gt;Sj)。<br>$P(q_t=S_i|q_{t-1}=S_j)=a_{ji}$<br>n*n matrix of transition probabilities</li>
<li>Emission probabilities(B) / Output prob distribution (at state j for symbol k): 发射概率（隐状态表现为显状态的概率）。状态 j 下，表现为 k 的概率。<br>$P(y_t=O_k|q_t=S_j)=b_j(k)$<br>n*m matrix of emission probabilities</li>
<li>Observations: 观测序列(可见状态)。$O={O_0, O_1, …O_n}$</li>
<li>Prior: 初始概率（隐状态）</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/hmm.jpg" class="ful-image" alt="hmm.jpg">
<h2 id="Probabilistic-graphical-models"><a href="#Probabilistic-graphical-models" class="headerlink" title="Probabilistic graphical models"></a>Probabilistic graphical models</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/gm.jpg" class="ful-image" alt="gm.jpg">
<h1 id="HMM-的问题与解决"><a href="#HMM-的问题与解决" class="headerlink" title="HMM 的问题与解决"></a>HMM 的问题与解决</h1><p>对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用 HMM 模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。和 HMM 模型相关的算法主要分为三类，分别解决三种问题。</p>
<h2 id="Evaluation-Problem"><a href="#Evaluation-Problem" class="headerlink" title="Evaluation Problem"></a>Evaluation Problem</h2><ul>
<li>Problem: Compute probability of observation sequence given a model $P(O|\lambda)$<br>Given $\lambda, \overline O=O1,…Ot, =&gt; Calculate  \ P(\overline O|\lambda)$<br>知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。</li>
<li>Solution: Forward Algorithm, Viterbi Algorithm</li>
</ul>
<p>问这个问题的目的在于检测观察到的结果是否和已知的模型吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。</p>
<h2 id="Decoding-Problem"><a href="#Decoding-Problem" class="headerlink" title="Decoding Problem"></a>Decoding Problem</h2><ul>
<li>Problem: Find the state sequence Q which maximizes<br>Given $\lambda = (A,B), \overline O=O1,…Ot, =&gt; Calculate $$\overline Q_{MAP}=argmax P(\overline Q|\overline O,\lambda)=argmax {P(\overline Q,\overline O|\lambda) \over P(\overline O|\lambda)} = argmax P(\overline Q,\overline O|\lambda)$<br>知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。</li>
<li>Solution: Viterbi Algorithm</li>
</ul>
<p>这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2.第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。</p>
<h2 id="Training-Learning-Problem"><a href="#Training-Learning-Problem" class="headerlink" title="Training(Learning) Problem"></a>Training(Learning) Problem</h2><ul>
<li>Problem: Adjust model parameters to maximize probability of observed sequences<br>Given $\overline O=O1…Ot$ =&gt; Estimate $\hat \lambda_{ML} = argmax P(\overline O|\lambda)$<br>知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。</li>
<li>Solution: Forward-Backward Algorithm</li>
</ul>
<p>这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道 HMM 模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。</p>
<p>Estimate A, B<br>$a_{ij_{ML}} = {\#(i-&gt;j) \over \#(i)}$ + SMOOTHING<br>$b_{j(Ok)_{ML}} = {\#(j \quad AND \quad Ok) \over \#(j)}$ + SMOOTHING</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="Forward-Algorithm-前向算法"><a href="#Forward-Algorithm-前向算法" class="headerlink" title="Forward Algorithm 前向算法"></a>Forward Algorithm 前向算法</h3><p>$\alpha = P(O_1 O_2 … O_t, q_t = S_j|\lambda)$<br>基本逻辑是：穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，把所有算出来的概率相加，得到的总概率就是我们要求的结果。<br>我们还需要一个 prior, 来表示 no states before 的情况。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/graph.jpg" class="ful-image" alt="graph.jpg"></p>
<p>$P(\overline O|\lambda)=a_{q0q1}b_{q1(y1)}a_{q1q2}b_{q2(y2)}…a_{q_{t-1}qt}b_{qt(yt)}$<br>即 $P(\overline O|\lambda)=\sum_QP(\overline O,\overline Q|\lambda)$</p>
<p>递归的方法来计算 $\alpha$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/alpha.jpg" class="ful-image" alt="alpha.jpg"></p>
<p>图解<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/forwardStep.jpg" class="ful-image" alt="forwardStep.jpg"></p>
<p>例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/forward.jpg" class="ful-image" alt="forward.jpg"></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line">from math import *</div><div class="line">from logsum import log_sum</div><div class="line">import sys</div><div class="line"></div><div class="line"></div><div class="line">def initiate(transFile, emitFile, priorFile):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    states, trans = probFileHandler(transFile)</div><div class="line">    noUse, emit = probFileHandler(emitFile)</div><div class="line">    prior = priorFileHandler(priorFile)</div><div class="line">    # print &apos;states &apos;, states</div><div class="line">    # print &apos;trans &apos;, trans</div><div class="line">    # print &apos;prior &apos;, prior</div><div class="line"></div><div class="line"></div><div class="line">def probFileHandler(myFile):</div><div class="line">    # states S</div><div class="line">    S = []</div><div class="line">    # transition or emit probability</div><div class="line">    P = dict()</div><div class="line">    # trans or emit</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        S.append(words[0])</div><div class="line">        d = dict([[words[i].split(&apos;:&apos;)[0], log(float(words[i].split(&apos;:&apos;)[1]))]</div><div class="line">                  for i in range(1, len(words))])</div><div class="line">        P[words[0]] = d</div><div class="line">    return S, P</div><div class="line"></div><div class="line"></div><div class="line">def priorFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    prior = dict()</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        prior[words[0]] = log(float(words[1]))</div><div class="line">    return prior</div><div class="line"></div><div class="line"></div><div class="line">def forward(word, alpha):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    res = []</div><div class="line">    # initial state</div><div class="line">    if not alpha:</div><div class="line">        for i in range(len(states)):</div><div class="line">            curState = states[i]</div><div class="line">            # use prior as trans_ji, at[j]*aji, use logsum</div><div class="line">            res.append(prior[curState] + emit[curState][word])</div><div class="line">    # if not initial state, iterate</div><div class="line">    else:</div><div class="line">        for i in range(len(states)):</div><div class="line">            curState_i = states[i]</div><div class="line">            for j in range(len(states)):</div><div class="line">                curState_j = states[j]</div><div class="line">                trans_ji = trans[curState_j][curState_i]</div><div class="line">                if j == 0:</div><div class="line">                    curRes = alpha[j] + trans_ji</div><div class="line">                else:</div><div class="line">                    # sum up all at[j]*aji, use logsum</div><div class="line">                    curRes = log_sum(curRes, alpha[j] + trans_ji)</div><div class="line">            # generate emit probability at t+1 at state i and multiply</div><div class="line">            curRes += emit[curState_i][word]</div><div class="line">            res.append(curRes)</div><div class="line">    return res</div><div class="line"></div><div class="line"></div><div class="line">def devFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        alpha = []</div><div class="line">        for word in words:</div><div class="line">            alpha = forward(word, alpha)</div><div class="line">        res = alpha[0]</div><div class="line">        # P(O|lambda)=alphaT*(endingStateN)</div><div class="line">        for i in range(1, len(alpha)):</div><div class="line">            res = log_sum(res, alpha[i])</div><div class="line">        print res</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    devFile = sys.argv[1]</div><div class="line">    transFile = sys.argv[2]</div><div class="line">    emitFile = sys.argv[3]</div><div class="line">    priorFile = sys.argv[4]</div><div class="line">    initiate(transFile, emitFile, priorFile)</div><div class="line">    devFileHandler(devFile)</div></pre></td></tr></table></figure></p>
<h3 id="Backward-Algorithm"><a href="#Backward-Algorithm" class="headerlink" title="Backward Algorithm"></a>Backward Algorithm</h3><p>$\beta_t(i)=P(O_{t+1}O_{t+2}…O_T|q_t=S_i, \lambda)$</p>
<p>同样，递归的方法来计算 $\beta$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/beta.jpg" class="ful-image" alt="beta.jpg"></p>
<p>例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/backward.jpg" class="ful-image" alt="backward.jpg"></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line">from math import *</div><div class="line">from logsum import log_sum</div><div class="line">import sys</div><div class="line"></div><div class="line"></div><div class="line">def initiate(transFile, emitFile, priorFile):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    states, trans = probFileHandler(transFile)</div><div class="line">    noUse, emit = probFileHandler(emitFile)</div><div class="line">    prior = priorFileHandler(priorFile)</div><div class="line">    # print &apos;states &apos;, states</div><div class="line">    # print &apos;trans &apos;, trans</div><div class="line">    # print &apos;prior &apos;, prior</div><div class="line"></div><div class="line"></div><div class="line">def probFileHandler(myFile):</div><div class="line">    # states S</div><div class="line">    S = []</div><div class="line">    # transition or emit probability</div><div class="line">    P = dict()</div><div class="line">    # trans or emit</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        S.append(words[0])</div><div class="line">        d = dict([[words[i].split(&apos;:&apos;)[0], log(float(words[i].split(&apos;:&apos;)[1]))]</div><div class="line">                  for i in range(1, len(words))])</div><div class="line">        P[words[0]] = d</div><div class="line">    return S, P</div><div class="line"></div><div class="line"></div><div class="line">def priorFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    prior = dict()</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        prior[words[0]] = log(float(words[1]))</div><div class="line">    return prior</div><div class="line"></div><div class="line"></div><div class="line">def backward(word, beta):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    res = []</div><div class="line">    for i in range(len(states)):</div><div class="line">        curState_i = states[i]</div><div class="line">        for j in range(len(states)):</div><div class="line">            curState_j = states[j]</div><div class="line">            # transpose matrix!!!!</div><div class="line">            trans_ij = trans[curState_i][curState_j]</div><div class="line">            if j == 0:</div><div class="line">                curRes = beta[j] + trans_ij + emit[curState_j][word]</div><div class="line">            else:</div><div class="line">                # sum up all at[j]*aji*emit[j], use logsum</div><div class="line">                curRes = log_sum(</div><div class="line">                    curRes, beta[j] + trans_ij + emit[curState_j][word])</div><div class="line">        res.append(curRes)</div><div class="line">    return res</div><div class="line"></div><div class="line"></div><div class="line">def devFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        # initialize, all states could be ending states</div><div class="line">        beta = [0.0 for i in range(len(states))]</div><div class="line"></div><div class="line">        for i in range(len(words) - 1, 0, -1):</div><div class="line">            beta = backward(words[i], beta)</div><div class="line"></div><div class="line">        res = prior[states[0]] + emit[states[0]][words[0]] + beta[0]</div><div class="line"></div><div class="line">        for i in range(1, len(beta)):</div><div class="line">            res = log_sum(res, prior[states[i]] +</div><div class="line">                          emit[states[i]][words[0]] + beta[i])</div><div class="line">        print res</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    devFile = sys.argv[1]</div><div class="line">    transFile = sys.argv[2]</div><div class="line">    emitFile = sys.argv[3]</div><div class="line">    priorFile = sys.argv[4]</div><div class="line">    initiate(transFile, emitFile, priorFile)</div><div class="line">    devFileHandler(devFile)</div></pre></td></tr></table></figure></p>
<h3 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h3><p>Viterbi 被广泛应用到分词，词性标注等应用场景。和 Forward algorithm 差不多，两点改变：</p>
<ol>
<li>把 SUM 改成 MAX。</li>
<li>记录 MAX 的h状态。</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/vi.jpg" class="ful-image" alt="vi.jpg">
<p>例子<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/vie.jpg" class="ful-image" alt="vie.jpg"></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">from math import *</div><div class="line">from logsum import log_sum</div><div class="line">import sys</div><div class="line">import copy</div><div class="line"></div><div class="line"></div><div class="line">def initiate(transFile, emitFile, priorFile):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    states, trans = probFileHandler(transFile)</div><div class="line">    noUse, emit = probFileHandler(emitFile)</div><div class="line">    prior = priorFileHandler(priorFile)</div><div class="line">    # print &apos;states &apos;, states</div><div class="line">    # print &apos;trans &apos;, trans</div><div class="line">    # print &apos;prior &apos;, prior</div><div class="line"></div><div class="line"></div><div class="line">def probFileHandler(myFile):</div><div class="line">    # states S</div><div class="line">    S = []</div><div class="line">    # transition or emit probability</div><div class="line">    P = dict()</div><div class="line">    # trans or emit</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        S.append(words[0])</div><div class="line">        d = dict([[words[i].split(&apos;:&apos;)[0], log(float(words[i].split(&apos;:&apos;)[1]))]</div><div class="line">                  for i in range(1, len(words))])</div><div class="line">        P[words[0]] = d</div><div class="line">    return S, P</div><div class="line"></div><div class="line"></div><div class="line">def priorFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    prior = dict()</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        prior[words[0]] = log(float(words[1]))</div><div class="line">    return prior</div><div class="line"></div><div class="line"></div><div class="line">def viterbi(word, alpha, resPath):</div><div class="line">    global states, trans, emit, prior</div><div class="line">    res = []</div><div class="line">    curPath = []</div><div class="line">    # initial state</div><div class="line">    if alpha == []:</div><div class="line">        for i in range(len(states)):</div><div class="line">            curState = states[i]</div><div class="line">            # use prior as trans_ji, at[j]*aji, use logsum</div><div class="line">            res.append(prior[curState] + emit[curState][word])</div><div class="line">            curPath.append([i])</div><div class="line">    # if not initial state, iterate</div><div class="line">    else:</div><div class="line">        for i in range(len(states)):</div><div class="line">            curState_i = states[i]</div><div class="line">            curMaxIndex = 0</div><div class="line">            for j in range(len(states)):</div><div class="line">                curState_j = states[j]</div><div class="line">                trans_ji = trans[curState_j][curState_i]</div><div class="line">                if j == 0:</div><div class="line">                    curMax = alpha[j] + trans_ji + emit[curState_i][word]</div><div class="line">                else:</div><div class="line">                    # sum up all at[j]*aji, use logsum</div><div class="line">                    curRes = alpha[j] + trans_ji + emit[curState_i][word]</div><div class="line">                    if curMax &lt; curRes:</div><div class="line">                        curMax = curRes</div><div class="line">                        curMaxIndex = j</div><div class="line">            # generate emit probability at t+1 at state i and multiply</div><div class="line">            #curMax += emit[curState_i][word]</div><div class="line">            res.append(curMax)</div><div class="line">            # choose max</div><div class="line">            curPath.append(copy.deepcopy(resPath[curMaxIndex]))</div><div class="line">            curPath[i].append(i)</div><div class="line">    return res, curPath</div><div class="line"></div><div class="line"></div><div class="line">def devFileHandler(myFile):</div><div class="line">    f = open(myFile)</div><div class="line">    for line in f:</div><div class="line">        words = line.strip().split()</div><div class="line">        alpha = []</div><div class="line">        resPath = []</div><div class="line">        for word in words:</div><div class="line">            alpha, resPath = viterbi(word, alpha, resPath)</div><div class="line">        final = resPath[alpha.index(max(alpha))]</div><div class="line">        for i in range(len(words)):</div><div class="line">            print words[i] + &apos;_&apos; + states[final[i]],</div><div class="line">        print</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    devFile = sys.argv[1]</div><div class="line">    transFile = sys.argv[2]</div><div class="line">    emitFile = sys.argv[3]</div><div class="line">    priorFile = sys.argv[4]</div><div class="line">    initiate(transFile, emitFile, priorFile)</div><div class="line">    devFileHandler(devFile)</div></pre></td></tr></table></figure></p>
<h3 id="Forward-Backward-Algorithm"><a href="#Forward-Backward-Algorithm" class="headerlink" title="Forward-Backward Algorithm"></a>Forward-Backward Algorithm</h3><p>算法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/fb_alg.jpg" class="ful-image" alt="fb_alg.jpg"></p>
<p>计算 $\alpha$, $\beta$,$\Xi$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/fb1.jpg" class="ful-image" alt="fb1.jpg"></p>
<p>时间复杂度 O(N<em>N</em>T)</p>
<p>Baum-Welch Reestimation<br>估计 $\overline \lambda$</p>
<p>$$\overline a_{ij} = {expected \ number \ of \ trans \ from  \ Si \ to \ Sj \over expected \ number \ of \ trans \ from  \ Si}$$<br>=&gt;<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/a.jpg" class="ful-image" alt="a.jpg"></p>
<p>$$\overline b_{j(k)} =  {expected \ number \ of \ times \ in \ state \ j \ with \ symbol \ k \over expected \ number \ of \ times \ in \ state \ j } $$<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Hidden-Markov-Models/b.jpg" class="ful-image" alt="b.jpg"></p>
<blockquote>
<p><a href="http://transcoder.baidu.com/from=844b/bd_page_type=1/ssid=0/uid=0/pu=usm%402%2Csz%401320_2001%2Cta%40iphone_1_8.4_3_600/baiduid=CBD7FA29AA4245050D07B584C50F010C/w=0_10_/t=iphone/l=3/tc?ref=www_iphone&amp;lid=3007046878162640498&amp;order=3&amp;fm=alop&amp;tj=www_normal_3_0_10_title&amp;vit=osres&amp;m=8&amp;srd=1&amp;cltj=cloud_title&amp;asres=1&amp;nt=wnor&amp;title=%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82HMM%28%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%29-skyme-%E5%8D%9A%E5%AE%A2%E5%9B%AD&amp;dict=30&amp;w_qd=IlPT2AEptyoA_yiRASq&amp;sec=16841&amp;di=9e95f6fbe62aeac3&amp;bdenc=1&amp;tch=124.784.240.607.1.45488462&amp;nsrc=IlPT2AEptyoA_yixCFOxXnANedT62v3IEQGG_ytK1DK6mlrte4viZQRAUTjmLXrTUS4dgTCctRcIwXOc0nhunM5X&amp;eqid=29bb2d3423d4de00100000065833361c&amp;wd=&amp;clk_info=%7B%22srcid%22%3A%221599%22%2C%22tplname%22%3A%22www_normal%22%2C%22t%22%3A1479751205634%2C%22xpath%22%3A%22div-a-h3%22%7D" target="_blank" rel="external">一文搞懂HMM（隐马尔可夫模型）</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[深度学习-链式反向梯度传导]]></title>
      <url>http://www.shuang0420.com/2016/11/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E9%93%BE%E5%BC%8F%E5%8F%8D%E5%90%91%E6%A2%AF%E5%BA%A6%E4%BC%A0%E5%AF%BC/</url>
      <content type="html"><![CDATA[<p>反向梯度传播实例。<br><a id="more"></a></p>
<p><strong>计算顺序：</strong> 从 loss 向输入传播<br><strong>导数存储：</strong> 每层的导数 $(\Delta y, \Delta x)$ 结果进行存储，用于下一层导数的计算。</p>
<p>假设 $loss = ax_{1,t}+bx_{2,t}+cx_{3,t}+d-y$，当前参数 $m_0 = [a_0, b_0,c_0,d_0]$，那么计算梯度 $\Delta m=[x_{1,t}, x_{2,t}, x_{3,t}, 1]$，也就是分别求偏导。参数的更新为 $m:=m-\eta \Delta m$</p>
<p>下面的例子，黑色加深部分是前向计算的结果，我们的目的是求反向传播梯度。(原谅我 poor hand-writting :( )</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E9%93%BE%E5%BC%8F%E5%8F%8D%E5%90%91%E6%A2%AF%E5%BA%A6%E4%BC%A0%E5%AF%BC/1.jpg" class="ful-image" alt="1.jpg">
<p><strong>小结：</strong></p>
<ol>
<li>随机初始化参数</li>
<li>开启循环：t=0,1,2…<ul>
<li>带入数据求出结果 $\hat y_t$</li>
<li>与真值比较得到 $loss=y-\hat y_t$</li>
<li>对各个变量求导得到 $\Delta m$</li>
<li>更新变量 m</li>
<li>如果 loss 足够小或者 t 循环结束，停止</li>
</ul>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> 反向传播 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 动态规划]]></title>
      <url>http://www.shuang0420.com/2016/11/12/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      <content type="html"><![CDATA[<p>我们再次强调：动态编程的核心在于，如果在一个问题的解决方案中，子问题被重复计算，那么就可以利用记录中间结果，达到用空间换取时间的目的。<br><a id="more"></a></p>
<h1 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def __init__(self):</div><div class="line">        self.cache = dict()</div><div class="line"></div><div class="line">    def f(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n in self.cache:</div><div class="line">            return self.cache[n]</div><div class="line">        if n == 0 or n == 1:</div><div class="line">            return n</div><div class="line">        self.cache[n] = self.f(n - 1) + self.f(n - 2)</div><div class="line">        return self.cache[n]</div></pre></td></tr></table></figure>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="70-Climbing-Stairs"><a href="#70-Climbing-Stairs" class="headerlink" title="70. Climbing Stairs"></a>70. Climbing Stairs</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>You are climbing a stair case. It takes n steps to reach to the top.</p>
<p>Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Dynamic programming.</div><div class="line"></div><div class="line">Induction Rule:</div><div class="line">T(n)=T(n-1)+T(n-2)</div><div class="line"></div><div class="line">Base cases:</div><div class="line">if n &lt;= 0, then the number of ways should be zero.</div><div class="line">if n == 1, then there is only way to climb the stair.</div><div class="line">if n == 2, then there are two ways to climb the stairs. One solution is one step by another; the other one is two steps at one time.</div><div class="line"></div><div class="line">The key intuition to solve the problem is that given a number of stairs n, if we know the number ways to get to the points [n-1] and [n-2] respectively, denoted as n1 and n2 , then the total ways to get to the point [n] is n1 + n2. Because from the [n-1] point, we can take one single step to reach [n]. And from the [n-2] point, we could take two steps to get there. There is NO overlapping between these two solution sets, because we differ in the final step.</div><div class="line"></div><div class="line">Now given the above intuition, one can construct an array where each node stores the solution for each number n. Or if we look at it closer, it is clear that this is basically a fibonacci number, with the starting numbers as 1 and 2, instead of 1 and 1.</div><div class="line"></div><div class="line">Recursive method:</div><div class="line">- cache, time complexity: O(n), space complexity: O(n)</div><div class="line"></div><div class="line">Follow up:</div><div class="line">- space complexity: O(1)</div><div class="line">    use two pointers</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def __init__(self):</div><div class="line">        self.cache = dict()</div><div class="line"></div><div class="line">    # Recursive method</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def climbStairs(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n &lt;= 0: return 0</div><div class="line">        if n in self.cache:</div><div class="line">            return self.cache[n]</div><div class="line">        if n==1 or n==2: return n</div><div class="line">        self.cache[n] = self.climbStairs(n-1) + self.climbStairs(n-2)</div><div class="line">        return self.cache[n]</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    # two pointers</div><div class="line">    def climbStairs(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n &lt;= 0: return 0</div><div class="line">        if n == 1 or n == 2: return n</div><div class="line">        pre,cur = 1, 2</div><div class="line">        for i in range(2,n):</div><div class="line">            pre, cur = cur, pre+cur</div><div class="line">        return cur</div></pre></td></tr></table></figure>
<p>##</p>
<h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Find the contiguous subarray within an array (containing at least one number) which has the largest sum.</p>
<p>For example, given the array [-2,1,-3,4,-1,2,1,-5,4],<br>the contiguous subarray [4,-1,2,1] has the largest sum = 6.</p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- subarray: 2 pointers: head,tail</div><div class="line">- any repeated work? no   any meaningless work? yes    check the sum-array and we can find that the non-max-sum either subtract one more number or miss one more addition</div><div class="line">    that is, for each element in the array, we have two options, either start from this element, or continue to sum up, no other options, and we choose maximum value of these two numbers.</div><div class="line">    --&gt;</div><div class="line">    cur_sum=max(cur_sum,nums[start]+cur_sum),</div><div class="line">    max_sum=(cur_sum,max_sum)</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def maxSubArray(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        cur_sum,max_sum=nums[0],nums[0]</div><div class="line">        for start in range(1,len(nums)):</div><div class="line">            cur_sum=max(nums[start],nums[start]+cur_sum)</div><div class="line">            max_sum=max(cur_sum,max_sum)</div><div class="line">        return max_sum</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # brute-force, time limit exceeded</div><div class="line">    def maxSubArray(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)==0:</div><div class="line">            return 0</div><div class="line">        global_sum=nums[0]</div><div class="line">        for i in range(0,len(nums)-1):</div><div class="line">            part_sum=nums[i]</div><div class="line">            if part_sum&gt;global_sum:</div><div class="line">                global_sum=part_sum</div><div class="line">            for j in range(i+1,len(nums)):</div><div class="line">                part_sum+=nums[j]</div><div class="line">                if part_sum&gt;global_sum:</div><div class="line">                    global_sum=part_sum</div><div class="line">        part_sum=nums[-1]</div><div class="line">        if part_sum&gt;global_sum:</div><div class="line">            global_sum=part_sum</div><div class="line">        return global_sum</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="204-Count-Primes"><a href="#204-Count-Primes" class="headerlink" title="204. Count Primes"></a>204. Count Primes</h2><h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h3><p>Description:</p>
<p>Count the number of prime numbers less than a non-negative number, n.</p>
<h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Primitive idea:</div><div class="line">- for each number i from 2-n, exclusive, check if i can be divided by a natural number that is from 2-(i-1), inclusive</div><div class="line">- Time complexity: O(n^2)</div><div class="line"></div><div class="line">Followup:</div><div class="line">- repeated work? yes, for each number larger than 2, we divided it by 2, for each number larger than 3, we divided it by 3....</div><div class="line">- think of it in a reverse way, we can enumerate the multiples of p by counting to n from 2p in increments of p, and mark them in the list as non-prime;</div><div class="line"></div><div class="line">- Time complexity: O(nlog(logn))  </div><div class="line">The inner loop does n/i steps, where i is prime =&gt; the whole complexity is sum(n/i) = n * sum(1/i). According to prime harmonic series, the sum (1/i) where i is prime is log (log n). In total, O(n*log(log n)).</div><div class="line">- Space complexity: O(n)</div><div class="line"></div><div class="line">[Divergence of the sum of the reciprocals of the primes</div><div class="line">](https://en.wikipedia.org/wiki/Divergence_of_the_sum_of_the_reciprocals_of_the_primes)</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def countPrimes(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n&lt;2: return 0</div><div class="line">        isPrime=[True]*n</div><div class="line">        isPrime[0:2]=[False]*2</div><div class="line">        for i in range(2,int(n**0.5)+1):</div><div class="line">            if isPrime[i]:</div><div class="line">                isPrime[i*2:n:i]=[False]*((n-1-i*2)/i+1)</div><div class="line">        return sum(isPrime)</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def countPrimes(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n&lt;2: return 0</div><div class="line">        count = 0</div><div class="line">        for i in range(2, n):</div><div class="line">            if self.isPrime(i):</div><div class="line">                count+=1</div><div class="line">        return count</div><div class="line"></div><div class="line"></div><div class="line">    def isPrime(self,n):</div><div class="line">        for i in range(2,n):</div><div class="line">            if n % i == 0:</div><div class="line">                return False</div><div class="line">        return True</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="96-Unique-Binary-Search-Trees"><a href="#96-Unique-Binary-Search-Trees" class="headerlink" title="96. Unique Binary Search Trees"></a>96. Unique Binary Search Trees</h2><h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem"></a>Problem</h3><p>Given n, how many structurally unique BST’s (binary search trees) that store values 1…n?</p>
<p>For example,<br>Given n = 3, there are a total of 5 unique BST’s.</p>
<h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92//leetcode95.jpg" class="ful-image" alt="/leetcode95.jpg">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Taking 1~n as root respectively:</div><div class="line">1 as root: # of trees = F(0) * F(n-1)  // F(0) == 1</div><div class="line">2 as root: # of trees = F(1) * F(n-2)</div><div class="line">3 as root: # of trees = F(2) * F(n-3)</div><div class="line">...</div><div class="line">n-1 as root: # of trees = F(n-2) * F(1)</div><div class="line">n as root:   # of trees = F(n-1) * F(0)</div><div class="line"></div><div class="line">So, the formulation is:</div><div class="line">F(n) = F(0) * F(n-1) + F(1) * F(n-2) + F(2) * F(n-3) + ... + F(n-2) * F(1) + F(n-1) * F(0)</div><div class="line"></div><div class="line"># Recursive method</div><div class="line">    - count(n)=sum(count(i)*count(n-i-1))</div><div class="line">    - use cache</div><div class="line">    - time complexity: O(n!)-&gt;O(n)</div><div class="line"></div><div class="line"># No need to use recursive. Memorized Search -&gt; Dynamic Programming</div><div class="line">    Inductive rule:</div><div class="line">        count[n]=sum(count[i]*count[n-i-1])</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # Recursive method</div><div class="line">    def numTrees(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n&lt;=0: return 0</div><div class="line">        return self.count(n,dict())</div><div class="line"></div><div class="line"></div><div class="line">    def count(self,n,cache):</div><div class="line">        if n==0 or n==1: return 1</div><div class="line">        if n in cache:</div><div class="line">            return cache[n]</div><div class="line">        cache[n] = sum([self.count(i,cache)*self.count(n-i-1,cache) for i in range(0,n)])</div><div class="line">        return cache[n]</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def numTrees(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n &lt;= 0:</div><div class="line">            return 0</div><div class="line">        count = [0] * (n + 1)</div><div class="line">        count[0] = 1</div><div class="line">        for i in range(1, n + 1):</div><div class="line">            for j in range(0, i):</div><div class="line">                count[i] += count[j] * count[i - j - 1]</div><div class="line">        return count[n]</div></pre></td></tr></table></figure>
<h2 id="95-Unique-Binary-Search-Trees-II"><a href="#95-Unique-Binary-Search-Trees-II" class="headerlink" title="95. Unique Binary Search Trees II"></a>95. Unique Binary Search Trees II</h2><h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem"></a>Problem</h3><p>Given an integer n, generate all structurally unique BST’s (binary search trees) that store values 1…n.</p>
<p>For example,<br>Given n = 3, your program should return all 5 unique BST’s shown below.</p>
<p>   1         3     3      2      1<br>    \       /     /      / \      \<br>     3     2     1      1   3      2<br>    /     /       \                 \<br>   2     1         2                 3</p>
<h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">recursive down</div><div class="line">    pass all the nodes that can be selected as a new root down</div><div class="line">        leftTree = self.helper(start, rootVal - 1)</div><div class="line">        rightTree = self.helper(rootVal + 1, end)</div><div class="line"></div><div class="line">return up</div><div class="line">    all possible trees under current root</div><div class="line"></div><div class="line">current level</div><div class="line">    build all the possible trees (different combination of left and right treelist )</div><div class="line">    - for root i in leftTree and for root j in right tree, build a new root and connect with left and right subtree.</div><div class="line">&apos;&apos;&apos;</div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def generateTrees(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: List[TreeNode]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n &lt;= 0:</div><div class="line">            return []</div><div class="line">        return self.helper(1, n)</div><div class="line"></div><div class="line">    def helper(self, start, end):</div><div class="line">        # base case</div><div class="line">        if start &gt; end:</div><div class="line">            return [None]</div><div class="line">        res = []</div><div class="line">        for rootVal in range(start, end + 1):</div><div class="line">            leftTree = self.helper(start, rootVal - 1)</div><div class="line">            rightTree = self.helper(rootVal + 1, end)</div><div class="line">            for i in leftTree:</div><div class="line">                for j in rightTree:</div><div class="line">                    root = TreeNode(rootVal)</div><div class="line">                    root.left = i</div><div class="line">                    root.right = j</div><div class="line">                    res.append(root)</div><div class="line">        return res</div></pre></td></tr></table></figure>
<h2 id="72-Edit-Distance"><a href="#72-Edit-Distance" class="headerlink" title="72. Edit Distance"></a>72. Edit Distance</h2><h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem"></a>Problem</h3><p>Given two words word1 and word2, find the minimum number of steps required to convert word1 to word2. (each operation is counted as 1 step.)</p>
<p>You have the following 3 operations permitted on a word:</p>
<p>a) Insert a character<br>b) Delete a character<br>c) Replace a character</p>
<h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Recursion Rule -&gt; Memorized Search -&gt; DP</div><div class="line">Time complexity: O(len1*len2)</div><div class="line">Space complexity: O(len1*len2)</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # Memorized Search</div><div class="line">    def minDistance(self, word1, word2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type word1: str</div><div class="line">        :type word2: str</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not word1 and not word2: return 0</div><div class="line">        if len(word1)==0: return len(word2)</div><div class="line">        if len(word2)==0: return len(word1)</div><div class="line">        return self.match(word1,word2,0,0,[[0 for col in range(len(word2))] for row in range(len(word1))])</div><div class="line"></div><div class="line"></div><div class="line">    def match(self,word1,word2,i,j,count):</div><div class="line">        # base case</div><div class="line">        if i==len(word1):</div><div class="line">            return len(word2)-j</div><div class="line">        if j==len(word2):</div><div class="line">            return len(word1)-i</div><div class="line">        # base case end</div><div class="line">        if count[i][j]!=0:</div><div class="line">            return count[i][j]</div><div class="line">        if word1[i]==word2[j]:</div><div class="line">            res=self.match(word1,word2,i+1,j+1,count)</div><div class="line">        else:</div><div class="line">            # insert</div><div class="line">            insert=self.match(word1,word2,i,j+1,count)</div><div class="line">            # delete</div><div class="line">            delete=self.match(word1,word2,i+1,j,count)</div><div class="line">            # replace</div><div class="line">            replace=self.match(word1,word2,i+1,j+1,count)</div><div class="line">            res=min(insert,delete,replace)+1</div><div class="line">        count[i][j]=res</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def minDistance(self, word1, word2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type word1: str</div><div class="line">        :type word2: str</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not word1 and not word2:</div><div class="line">            return 0</div><div class="line">        if len(word1) == 0:</div><div class="line">            return len(word2)</div><div class="line">        if len(word2) == 0:</div><div class="line">            return len(word1)</div><div class="line">        count = [[0 for col in range(len(word2) + 1)]</div><div class="line">                 for row in range(len(word1) + 1)]</div><div class="line">        # initialize</div><div class="line">        for i in range(len(word1) + 1):</div><div class="line">            count[i][0] = i</div><div class="line">        for j in range(len(word2) + 1):</div><div class="line">            count[0][j] = j</div><div class="line">        for i in range(len(word1)):</div><div class="line">            for j in range(len(word2)):</div><div class="line">                if word1[i] == word2[j]:</div><div class="line">                    count[i + 1][j + 1] = count[i][j]</div><div class="line">                else:</div><div class="line">                    count[i + 1][j + 1] = min(count[i + 1][j], count[i][j + 1], count[i][j]) + 1</div><div class="line">        return count[-1][-1]</div></pre></td></tr></table></figure>
<h2 id="303-Range-Sum-Query-Immutable"><a href="#303-Range-Sum-Query-Immutable" class="headerlink" title="303. Range Sum Query - Immutable"></a>303. Range Sum Query - Immutable</h2><h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem"></a>Problem</h3><p>Given an integer array nums, find the sum of the elements between indices i and j (i ≤ j), inclusive.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Given nums = [-2, 0, 3, -5, 2, -1]</div><div class="line"></div><div class="line">sumRange(0, 2) -&gt; 1</div><div class="line">sumRange(2, 5) -&gt; -1</div><div class="line">sumRange(0, 5) -&gt; -3</div></pre></td></tr></table></figure></p>
<p>Note:<br>You may assume that the array does not change.<br>There are many calls to sumRange function.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">class NumArray(object):</div><div class="line">    def __init__(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        :type nums: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.sumArray=[0]</div><div class="line">        cur=0</div><div class="line">        for i in nums:</div><div class="line">            cur+=i</div><div class="line">            self.sumArray.append(cur)</div><div class="line"></div><div class="line">    def sumRange(self, i, j):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        sum of elements nums[i..j], inclusive.</div><div class="line">        :type i: int</div><div class="line">        :type j: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return self.sumArray[j+1]-self.sumArray[i]</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># Your NumArray object will be instantiated and called as such:</div><div class="line"># numArray = NumArray(nums)</div><div class="line"># numArray.sumRange(0, 1)</div><div class="line"># numArray.sumRange(1, 2)</div></pre></td></tr></table></figure>
<h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure>
<h2 id="304-Range-Sum-Query-2D-Immutable"><a href="#304-Range-Sum-Query-2D-Immutable" class="headerlink" title="304. Range Sum Query 2D - Immutable"></a>304. Range Sum Query 2D - Immutable</h2><h3 id="Problem-7"><a href="#Problem-7" class="headerlink" title="Problem"></a>Problem</h3><p>Given a 2D matrix matrix, find the sum of the elements inside the rectangle defined by its upper left corner (row1, col1) and lower right corner (row2, col2).</p>
<p>Range Sum Query 2D<br>The above rectangle (with the red border) is defined by (row1, col1) = (2, 1) and (row2, col2) = (4, 3), which contains sum = 8.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Given matrix = [</div><div class="line">  [3, 0, 1, 4, 2],</div><div class="line">  [5, 6, 3, 2, 1],</div><div class="line">  [1, 2, 0, 1, 5],</div><div class="line">  [4, 1, 0, 1, 7],</div><div class="line">  [1, 0, 3, 0, 5]</div><div class="line">]</div><div class="line"></div><div class="line"></div><div class="line">sumRegion(2, 1, 4, 3) -&gt; 8</div><div class="line">sumRegion(1, 1, 2, 2) -&gt; 11</div><div class="line">sumRegion(1, 2, 2, 4) -&gt; 12</div></pre></td></tr></table></figure></p>
<p>Note:<br>You may assume that the matrix does not change.<br>There are many calls to sumRegion function.<br>You may assume that row1 ≤ row2 and col1 ≤ col2.</p>
<h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    brute-force: O(n^2) time</div><div class="line">    use idea from Range Sum Query - Immutable, for each row, store current sum value of all column value, takes O(n) time to set up, and for sumRange, loop the row and calculate the sum of rows result, also takes O(n) time</div><div class="line">    improve: store current sum value of all values on the left and above of current value, takes O(n^2) to setup and O(1) time to get sumRange</div><div class="line">&apos;&apos;&apos;</div><div class="line">&apos;&apos;&apos;</div><div class="line">O(n^2) setup and O(1) sumRegion</div><div class="line">&apos;&apos;&apos;</div><div class="line">class NumMatrix(object):</div><div class="line">    def __init__(self, matrix):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        :type matrix: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not matrix: return</div><div class="line">        self.m=len(matrix)</div><div class="line">        self.n=len(matrix[0])</div><div class="line">        self.matrix=[[0]*(self.n+1) for i in range(self.m+1)]</div><div class="line">        for row in range(self.m):</div><div class="line">            cur=0</div><div class="line">            for col in range(self.n):</div><div class="line">                cur+=matrix[row][col]</div><div class="line">                self.matrix[row+1][col+1]=cur</div><div class="line">        for col in range(self.n+1):</div><div class="line">            cur=0</div><div class="line">            for row in range(self.m+1):</div><div class="line">                cur+=self.matrix[row][col]</div><div class="line">                self.matrix[row][col]=cur</div><div class="line"></div><div class="line">    def sumRegion(self, row1, col1, row2, col2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        sum of elements matrix[(row1,col1)..(row2,col2)], inclusive.</div><div class="line">        :type row1: int</div><div class="line">        :type col1: int</div><div class="line">        :type row2: int</div><div class="line">        :type col2: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.matrix: return 0</div><div class="line">        return self.matrix[row2+1][col2+1]-self.matrix[row1][col2+1]-(self.matrix[row2+1][col1]-self.matrix[row1][col1])</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">O(n) setup and O(n) sumRegion</div><div class="line"></div><div class="line">class NumMatrix(object):</div><div class="line">    def __init__(self, matrix):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        :type matrix: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.matrix=[[0] for i in range(len(matrix))]</div><div class="line">        for row in range(len(matrix)):</div><div class="line">            cur=0</div><div class="line">            for col in matrix[row]:</div><div class="line">                cur+=col</div><div class="line">                self.matrix[row].append(cur)</div><div class="line"></div><div class="line">    def sumRegion(self, row1, col1, row2, col2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        sum of elements matrix[(row1,col1)..(row2,col2)], inclusive.</div><div class="line">        :type row1: int</div><div class="line">        :type col1: int</div><div class="line">        :type row2: int</div><div class="line">        :type col2: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        cur=0</div><div class="line">        for row in xrange(row1,row2+1):</div><div class="line">            cur+=self.matrix[row][col2+1]-self.matrix[row][col1]</div><div class="line">        return cur</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># Your NumMatrix object will be instantiated and called as such:</div><div class="line"># numMatrix = NumMatrix(matrix)</div><div class="line"># numMatrix.sumRegion(0, 1, 2, 3)</div><div class="line"># numMatrix.sumRegion(1, 2, 3, 4)</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 动态规划 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Personalization]]></title>
      <url>http://www.shuang0420.com/2016/11/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。关于个性化搜索引擎。<br><a id="more"></a></p>
<p>目前，有三种实现个性化搜索引擎的方法。</p>
<ul>
<li>Topic-based personalization</li>
<li>Long-term vs. short-term personalization</li>
<li>Personalization for typical vs. atypical information needs</li>
</ul>
<p><strong>基本逻辑：</strong></p>
<ul>
<li>Representation: 描述用户的兴趣/偏好</li>
<li>Learning: 从数据中学习兴趣/偏好</li>
<li>Ranking: 在检索算法中使用兴趣/偏好</li>
</ul>
<h1 id="Topic-based-personalization"><a href="#Topic-based-personalization" class="headerlink" title="Topic-based personalization"></a>Topic-based personalization</h1><p><strong>假设：</strong> 用户的长期兴趣(high-level topics)可以从训练数据中得到</p>
<p><strong>预处理：</strong></p>
<ul>
<li>在建立索引前，每篇文档都被分配了 [0…n] 之间的某个类别；</li>
<li>在建立索引的时候，类别被当作 feature 或者 metadata 保存起来</li>
</ul>
<p><strong>User representation:</strong></p>
<ul>
<li>根据用户的长期检索历史对用户建模</li>
<li>用户模型是在类别标签上的一个概率分布<br>arts/movies: 1.1%, arts/television: 0.2%, arts/music: 2%, …</li>
<li>第 n 个用户的训练数据: p (q1, c1), p(q1, c2), … p(q2, c1) …<br>p(qi, cj) = p(d is  clicked &amp; d in category cj | q)</li>
</ul>
<p><strong>对排序列表进行 rerank，top n 的文档是两个分数的组合:</strong></p>
<ul>
<li>原始文档分数</li>
<li>文档类别和用户兴趣类别的匹配分数</li>
</ul>
<p>Bing 用 25 天的数据，20 天数据做训练数据，5 天数据做测试数据，一共 102,417 条查询和 54，581 个用户，实验结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Bing vs. Personalized Bing</div><div class="line">• Metric: Mean reciprocal rank (MRR)</div><div class="line">• ODP classifier accuracy: 60% Micro-averaged F1, 86% coverage</div><div class="line">• Effect of personalization</div><div class="line">  – 1-2% improvement in overall MRR</div><div class="line">  – 17-18% improvement in MRR for results that change position</div><div class="line">• Effect of personalization on acronyms</div><div class="line">  – 5% improvement in overall MRR</div><div class="line">  – 17-22% improvement in MRR for results that change position</div></pre></td></tr></table></figure></p>
<h1 id="Long-term-vs-short-term-personalization"><a href="#Long-term-vs-short-term-personalization" class="headerlink" title="Long-term vs. short-term personalization"></a>Long-term vs. short-term personalization</h1><p>个性化可以基于三种类型的信息：</p>
<ul>
<li>Historic: 从用户长期的检索记录中获得的信息</li>
<li>Session: 用户一个 search session 的信息</li>
<li>Combination of historic and session: 两者合并的信息</li>
</ul>
<p>可以把这三方面的信息看作用户历史记录的三个不同的 view。</p>
<h2 id="Features"><a href="#Features" class="headerlink" title="Features:"></a>Features:</h2><p><strong>View features</strong></p>
<ul>
<li>Cosine between view and document topic categories</li>
<li>Cosine between matching queries (and subsets, and supersets)<br>and document topic categories</li>
<li>url click count</li>
<li>url click counts for matching queries (and subsets and<br>supersets)</li>
<li>Number of queries</li>
<li>Number of sessions with this query</li>
<li>Number of subset queries</li>
<li>Number of superset queries</li>
</ul>
<p><strong>Query features</strong></p>
<ul>
<li>Ambiguity measures: Click entropy, topic entropy<br>– How much do people click on different pages or topics for<br>this query?</li>
<li>Difficulty measures: Position in session, length, frequency</li>
<li>Document rank (not personalized)</li>
</ul>
<h2 id="User-profile"><a href="#User-profile" class="headerlink" title="User profile:"></a>User profile:</h2><ul>
<li>User topic entropy</li>
<li>User query (and subset and superset) entropy</li>
<li>User position entropy, user query position entropy</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验:"></a>实验:</h2><ul>
<li>特征：每个 view 有 38 个特征，一共 102 个特征</li>
<li>数据集：2011年 7-8 月的搜索日志</li>
<li>方法：对原始结果集的前 10 篇文档重新排序，用 LambdaMART 算法(pairwise LeToR)，然后进行相关性评估。</li>
</ul>
<p><strong>Value of each view:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/viewVal.jpg" class="ful-image" alt="viewVal.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/viewVal2.jpg" class="ful-image" alt="viewVal2.jpg"></p>
<p><strong>Effect of personalization:</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/peff.jpg" class="ful-image" alt="peff.jpg"></p>
<p><strong>结论：</strong></p>
<ul>
<li>Historic 信息在 session 早期的作用比较大</li>
<li>Session 信息在 session 晚期的作用比较大ssion</li>
<li>Personalization 在 session 晚期的作用越来越弱<br>－ 可能是因为用户的查询语句更加优化了</li>
<li>Personalization 在 session 晚期能够影响更多的 query</li>
</ul>
<h1 id="Personalization-for-typical-vs-atypical-information-needs"><a href="#Personalization-for-typical-vs-atypical-information-needs" class="headerlink" title="Personalization for typical vs. atypical information needs"></a>Personalization for typical vs. atypical information needs</h1><p>很多个性化的技术假定 user profile 很少变化，然而当用户搜索 atypical 信息时就有问题了。</p>
<p><strong>Detect atypical sessions:</strong></p>
<ul>
<li>创建 long-term user profile</li>
<li>衡量 profile 和 session 的 divergence<ul>
<li>Divergence of each session feature from this user’s historical norms</li>
<li>Cosine distance between session and historical vocabularies</li>
<li>Cosine distance between session and historical topic categories</li>
</ul>
</li>
</ul>
<p>然后收集特征，继续按之前 Long-term vs. short-term personalization 的方法来进行实验，最有效的特征：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Personalization/features.jpg" class="ful-image" alt="features.jpg"></p>
<p>待更新。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Search logs]]></title>
      <url>http://www.shuang0420.com/2016/11/04/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。这一章开始的很多内容都是从论文的研究报告而来。重点是第四部分，怎样对 search log 划分 session，有哪些 feature 可以作为划分 session 的依据。<br><a id="more"></a></p>
<h1 id="User-information-collection"><a href="#User-information-collection" class="headerlink" title="User information collection"></a>User information collection</h1><p>大多数搜索引擎会保存每一次搜索的信息，主要有下面的内容：</p>
<ul>
<li>query</li>
<li>timestamp</li>
<li>IP address of the search client</li>
<li>possibly an id recorded in a cookie or obtained another way</li>
<li>information about the operating system and browser</li>
<li>clickthrough information</li>
</ul>
<p>目前公开的 web search logs</p>
<ul>
<li>The Excite log(1997)<br>18,113 users, 51,473 queries</li>
<li>The AOL Log(2006)<br>$&gt;$ 650,000 users, $&gt;$ 20 million queries</li>
<li>Alta Vista(1999)<br>285 million users, 1 billion queries</li>
<li>Alta Vista(2001)<br>$&gt;$ 7 million queries</li>
</ul>
<h2 id="Query-characteristics"><a href="#Query-characteristics" class="headerlink" title="Query characteristics"></a>Query characteristics</h2><h3 id="Query-Length-Average-terms-per-query-3-5-terms"><a href="#Query-Length-Average-terms-per-query-3-5-terms" class="headerlink" title="Query Length: Average terms per query: 3.5 terms"></a>Query Length: Average terms per query: 3.5 terms</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/queryLen.jpg" class="ful-image" alt="queryLen.jpg">
<h3 id="Who-queries"><a href="#Who-queries" class="headerlink" title="Who queries"></a>Who queries</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/whoQueries.jpg" class="ful-image" alt="whoQueries.jpg">
<h3 id="Query-Frequency-Query-Frequency-Follows-a-Power-Law"><a href="#Query-Frequency-Query-Frequency-Follows-a-Power-Law" class="headerlink" title="Query Frequency: Query Frequency Follows a Power Law"></a>Query Frequency: Query Frequency Follows a Power Law</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/queryFreq.jpg" class="ful-image" alt="queryFreq.jpg">
<p><strong>Power Law</strong><br>$$Frequency(q) = K*Rank(q)^{- \alpha}$$</p>
<ul>
<li>K: positive constant</li>
<li>Rank(q): popularity rank(r=1 is the most popular)</li>
<li>$\alpha$: constant, about 2.4 for the Excite query log</li>
</ul>
<p>和 Zipf’s law 一样的 shape，但不一样的斜率。</p>
<p>一小部分的 query 非常常见，而大部分的 query 非常少见。 =&gt; 通过 <strong>缓存</strong> 来提高搜索效率。</p>
<h3 id="Vary-over-time"><a href="#Vary-over-time" class="headerlink" title="Vary over time"></a>Vary over time</h3><p>最常见的 query 总是在不断变化<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/queryFreqVary.jpg" class="ful-image" alt="queryFreqVary.jpg"></p>
<p>来自 google 的统计数据</p>
<ul>
<li>每天的 query 中有 20% 在之前从未出现过。(占每天所有 unique query 的 50%)</li>
<li>8% 的 query 是名字(names)</li>
</ul>
<h1 id="Model-query"><a href="#Model-query" class="headerlink" title="Model query"></a>Model query</h1><ul>
<li>Query topics: What<br>比如说在雅虎目录下的分类。</li>
<li>User demographics: Who<br>如用户的年龄、性别（用户提供）、收入、教育程度（从邮编等信息推断）等。</li>
<li>Session characteristics: How<br>eg. session length, number of queries/session, % of queries with low/high click entropy</li>
</ul>
<h1 id="Represent-users"><a href="#Represent-users" class="headerlink" title="Represent users"></a>Represent users</h1><p>需要用到的工具: Pseudo documents, clustering, control vocabulary<br><strong>How to represent users？</strong></p>
<ul>
<li>在 “what” 维度上对用户进行聚类</li>
<li>在 (“who”,”how”) 维度上进一步 investigate groups</li>
<li>根据 group 的显著特征对其打上标签</li>
</ul>
<p>根据用户的 query type 来对用户进行聚类：</p>
<ol>
<li>从日志中得到 (user,query) pairs</li>
<li>为用户创建 pseudo documents<br>标题: user id<br>内容: 每条 query 的前 10 篇文档对应的 Yahoo! 分类目录</li>
<li>计算相似度<br>e.g. JS divergence, cosine</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Search%20logs/pseudo.jpg" class="ful-image" alt="pseudo.jpg">
<h2 id="Types-of-users"><a href="#Types-of-users" class="headerlink" title="Types of users"></a>Types of users</h2><p>这一部分主要是介绍不同类型的用户特征，这些聚类结果有什么用？=&gt; 为下一步的个性化搜索引擎做准备。</p>
<h3 id="Informational-Users"><a href="#Informational-Users" class="headerlink" title="Informational Users"></a>Informational Users</h3><p><strong>How:</strong><br>• More likely to issue non-navigational queries<br>• Less likely to have single-click sessions<br>• More likely to use query suggestions</p>
<p><strong>What:</strong><br>• Wide range of topics<br>• Little interest in adult content</p>
<p><strong>Who:</strong><br>• More likely to be well-educated<br>• More likely to have above-average income</p>
<h3 id="Navigational-Users"><a href="#Navigational-Users" class="headerlink" title="Navigational Users"></a>Navigational Users</h3><p><strong>How:</strong><br>• More likely to issue navigational queries<br>• More likely to have single-click sessions<br>• Less likely to use query suggestions</p>
<p><strong>What:</strong><br>• Dominated by popular websites (Facebook, YouTube, Craigslist) Who<br>• Mostly representative of the entire population</p>
<h3 id="Transactional-Users"><a href="#Transactional-Users" class="headerlink" title="Transactional Users"></a>Transactional Users</h3><p><strong>How:</strong><br>• Somewhat similar to navigational users. But, multiple sites can perform the transaction<br>• Diverse clicks<br>• Short interaction with search engine What<br>• Shopping, adult content, gaming</p>
<p><strong>Who:</strong><br>• Depends heavily on the type of transaction<br>• Topic “recreation/games” associated with low income &amp; education</p>
<h2 id="聚类结果-Selected-groups"><a href="#聚类结果-Selected-groups" class="headerlink" title="聚类结果 - Selected groups"></a>聚类结果 - Selected groups</h2><p><strong>Baby boomers:</strong><br>• 50 years old<br>• Interested in finance<br>• Simple navigational queries related to online banking</p>
<p><strong>Challenged youth:</strong><br>• Average age of 34<br>• Low-income neighborhoods with low-level of education<br>• Interested in music<br>• Navigational sessions</p>
<p><strong>Liberal females:</strong><br>• Mostly female from areas that voted Democratic<br>• Shopping queries<br>• Long sessions (browsing and comparison)</p>
<p><strong>White conservatives:</strong><br>• Mostly male from areas that voted Republican<br>• Interested in automotive, business, home &amp; garden</p>
<p><strong>Older users:</strong> Health / diseases &amp; conditions, gambling, travel<br><strong>People in their late 20s:</strong> Health / fitness, reproductive health<br><strong>Younger people:</strong> Games, education<br><strong>Low income:</strong> Music, comics &amp; animation, military<br><strong>Asian descent:</strong> Computers &amp; internet, programming &amp; development</p>
<p><strong>Some topics typically receive few clicks:</strong><br>• News &amp; media, society &amp; culture, computers &amp; internet People are more likely to click on suggestions for some topics<br>• Health, science, arts</p>
<p><strong>People with higher educational levels:</strong><br>• Tend to have shorter sessions<br>• Click on query suggestions less often<br>• Are more likely to submit tail queries</p>
<h1 id="Segmenting-search-logs-into-sessions"><a href="#Segmenting-search-logs-into-sessions" class="headerlink" title="Segmenting search logs into sessions"></a>Segmenting search logs into sessions</h1><p>通常，我们会把 search log 划分为一个个基于 information need 的 dialogue<br>dialogue 形式，实际就是用户发出初始查询，搜索引擎给出结果，用户不满意，重新修改查询语句再次搜索，然后得到新的结果，不断循环的过程。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Person: query</div><div class="line">Engine: search results</div><div class="line">Person: reformulated query</div><div class="line">Engine: new search results ...</div></pre></td></tr></table></figure></p>
<p>那么问题来了，怎么来区分这些不同的 dialogues 呢？<br>我们定义 information need = a search session(dialogue)，最简单粗暴划分 session 的方法就是通过时间，比如说三十分钟内一系列的用户行为就算作一个 session。</p>
<p>划分 session 可以考虑的因素：</p>
<ul>
<li>Time: Same session iff |timestamp (q2) – timestamp| (q1) &lt; ∆<br>often ∆ = 30 minutes</li>
<li>Common term: Same session iff q1 ∩ q2 ≠ Ø<br>Probably high Precision, low Recall</li>
<li>Rewrite classes: Common reformulation patterns<br>E.g., term added, deleted, or replaced<br>Probably high Precision, low Recall</li>
<li>Edit distance</li>
<li>Co-occurrence<br>e.g., PMI, Chi-square of queries in a query log</li>
<li>Same clicks<br>queries have co-occurring clicks in a query log</li>
<li>Document categories<br>ODP or Yahoo page category overlap of top 10 results</li>
<li>Same/overlap retrieved documents<br>JSD similarity of top 10 results<br>cosine distance among results</li>
<li>Classifiers<br>…</li>
</ul>
<p>训练一个分类器是最好的方法，对任意一对 queries 大概有 95-97% 的准确率。然而 heuristics 也不差，尤其是 edit distance 和 cosine distance among results 都非常有效，时间尺度通常会和其他 heuristic 合用，效果更佳。</p>
<p>各种 heuristic 都可以作为分类器的 Features</p>
<ul>
<li>Temporal<br>– ≤ {5, 30, 60, 120} minutes, ∆ time, are_sequential</li>
<li>Edit distance<br>– Several character and token-based metrics</li>
<li>Query log<br>– Various types of (q1, q2) co-occurrence in a larger query log</li>
<li>Web search<br>– Cosine distance of top 50 search results for each query<br>(“prisma”)</li>
</ul>
<p>我们训练分类器一般需要完成两个任务</p>
<ul>
<li>Boundary task: Given a pair of sequential queries</li>
<li>Same task: Given a pair of queries</li>
</ul>
<h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>用户的信息需求可能是跨越了几天甚至是几周的，比如用户写论文、找学校信息等，可能出现的情况是：</p>
<ul>
<li>用户有交叉的任务<br>如查 paper 的时候顺便查下中午吃什么</li>
<li>用户把一个任务分成若干个子任务分阶段完成<br>比如今天查为什么，明天查怎么做</li>
<li>子任务可能看起来是独立的实际确实相关的</li>
</ul>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Authority Metrics]]></title>
      <url>http://www.shuang0420.com/2016/11/04/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。这一章 Authority Metrics（权威指数），用来判断哪些来源的信息更值得信任。介绍 PageRank、Topic-Sensitive PageRank(TSPR)、T-Fresh、Hyperlink-Induced Topic Search(HITS) 四种指数计算方式。<br><a id="more"></a></p>
<h1 id="链接分析"><a href="#链接分析" class="headerlink" title="链接分析"></a>链接分析</h1><p>链接分析思想最早起源于引文分析领域，通过分析文献之间的引用模式来量化学术论文的影响力。文献引用代表某篇学术论文对所引用论文的权威度的认可。类似的，对于 Web 网页，我们把超链接看成是一个网页对另一个网页的权威度的认可。当然，并不是所有引用都代表权威度认可，因为某个人可以建立多个 web 网页来指向同一个目标网页，人工来提高目标网页入链数。这种链接通常叫垃圾链接或链接作弊（link spam）。尽管如此，引用现象的普遍性和可靠性足以使搜索引擎通过精妙的链接分析方法推导出有用的排序因子。链接分析也被证明是在 web 采集中选择下一个采集网页的非常好的方法之一。给定查询下，链接分析结果已经成为 Web 搜索引擎在计算某个网页的组合得分中的一个因子。PageRank 和 HITS 就是常用的两种链接分析方法。</p>
<h2 id="基本假设"><a href="#基本假设" class="headerlink" title="基本假设"></a>基本假设</h2><ol>
<li>指向页面 B 的锚文本是对 B 的一个很好的描述。</li>
<li>A 到 B 的超链接表示 A 的作者对 B 的认可。</li>
</ol>
<h2 id="锚文本"><a href="#锚文本" class="headerlink" title="锚文本"></a>锚文本</h2><h3 id="什么是锚文本"><a href="#什么是锚文本" class="headerlink" title="什么是锚文本"></a>什么是锚文本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;a href=&quot;http://www.acm.org/jacm/&quot;&gt;Journal of the ACM.&lt;/a&gt;</div></pre></td></tr></table></figure>
<p>这个 HTML 代码片段给出了一个指向 Journal of the ACM 的链接。链接指向 <a href="http://www.acm.org/jacm/，锚文本是" target="_blank" rel="external">http://www.acm.org/jacm/，锚文本是</a> Journal of the ACM。</p>
<h3 id="锚文本的作用"><a href="#锚文本的作用" class="headerlink" title="锚文本的作用"></a>锚文本的作用</h3><p>Web 上随处可见的现象是，很多网页的内容并不包含对自身的精确描述，尤其是公司网页，因为它们往往是用作商业宣传而不是介绍公司内容。如 IBM 是计算机制造商，但公司主页 (www.ibm.com) 的 HTML 代码的任何地方都不包含词项 computer。这时候，锚文本的作用就体现出来了。锚文本的词项可以作为索引目标网页的词项，也就是说 computer 的 inverted list 包含了 www.ibm.com，这时再用一个特别的指示器表示这些词项在锚文本而不是网页内部，网页评分时也基于 term frequency 计算锚文本的 term weight，在多个锚文本中高频出现的词项(如 Click 和 here)会收到惩罚。<br>除了锚文本，锚文本周围窗口的文本(extended anchor text)，通常也可以当成锚文本来使用。</p>
<h3 id="副作用"><a href="#副作用" class="headerlink" title="副作用"></a>副作用</h3><p>网站可以通过构造具有误导性的锚文本来指向自己，以此提高在某些 query term 的排名。</p>
<h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><p>PageRank 是 query-independent 的，也就是说与用户输入的查询无关。PageRank 高的网页并不代表一定适合某个特定的 query。如 kanye west wikipedia 网页有很高的 PageRank，但并不适合 “obama family tree” 这个特定的 query。</p>
<h2 id="Random-Walk-Algorithm"><a href="#Random-Walk-Algorithm" class="headerlink" title="Random Walk Algorithm"></a>Random Walk Algorithm</h2><p>PageRank 可以被看作是一个随机游走的算法。基本思想是模拟一个悠闲的上网者，上网者首先随机选择一个网页打开，然后在这个网页上呆了几分钟后，跳转到该网页所指向的链接，这样无所事事、漫无目的地在网页上跳来跳去，PageRank 就是估计这个悠闲的上网者分布在各个网页上的概率。</p>
<ol>
<li>随机选一个网页 A 。</li>
<li>网页 A 有 n 条出链（outlinks），这时候有两种方法进行下一步。<ul>
<li>随机在出链中选一个到下一个网页。</li>
<li>随机从所有 web 网页里选一个其它网页(teleport)。（因为网页 A 可能没有出链）</li>
</ul>
</li>
<li>不断重复以上过程。</li>
</ol>
<p>随机游走中访问越频繁的网页越重要。<br>计算方法<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/PageRank.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/PageRank.jpg"></p>
<h2 id="Voting-Algorithm"><a href="#Voting-Algorithm" class="headerlink" title="Voting Algorithm"></a>Voting Algorithm</h2><p>PageRank 也可以被看作是一个 Voting algorithm。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">While (Not done)</div><div class="line">  For each page p</div><div class="line">    p votes for each page that it links to</div></pre></td></tr></table></figure></p>
<p>再具体些<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">For each page p</div><div class="line">  current PR = 1 / |C|    C: Number of nodes in the graph next PR = 0</div><div class="line">While (Not done)</div><div class="line">  For each page p</div><div class="line">    use p’s current PR to update the next PR of each outlink page</div><div class="line">  For each page p</div><div class="line">    current PR = next PR</div><div class="line">    next PR = 0</div></pre></td></tr></table></figure></p>
<ul>
<li>每次循环，网页 Pi 可以左 PR(Pi) 次投票。</li>
<li>把投票平均分发给每一个它指向的网页。</li>
<li>如果两个网页在第 i 次循环中有相同的 PageRank:<br>PR(p1)=0.4, p1 有 2 个 outlinks, 那么 p1 的每一个 vote 是 0.4/2=0.2<br>PR(p2)=0.4, p2 有 4 个 outlinks, 那么 p2 的每一个 vote 是 0.4/4=0.1</li>
</ul>
<p>在每一个循环中, PR 的计算方法都如之前的图示。</p>
<h2 id="d-的选择"><a href="#d-的选择" class="headerlink" title="d 的选择"></a>d 的选择</h2><p>假定 A 有两条出链，指向 B 和 C，B 和 C 各自有一条出链，都指向 A。<br>如果 d=0.5，那么经历 14 次循环 PR 会 converge<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/0.5.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/0.5.jpg"></p>
<p>如果 d=0.85，那么经历 58 次循环 PR 会 converge<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/0.85.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/0.85.jpg"></p>
<h2 id="Transformation-of-PageRank"><a href="#Transformation-of-PageRank" class="headerlink" title="Transformation of PageRank"></a>Transformation of PageRank</h2><p>有些 PageRank varies over a wide range，我们可以把这个 range 进行压缩和转化。如<br>$$PR_T = log_{10}(PR)+11$$</p>
<p>效果是<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/PR_trans.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/PR_trans.jpg"></p>
<p>Google 的 PageRank 范围是 1-10，我们通常说的 PageRank 都是经过这种转化的 PageRank。</p>
<h2 id="怎样产生-high-PageRank"><a href="#怎样产生-high-PageRank" class="headerlink" title="怎样产生 high PageRank"></a>怎样产生 high PageRank</h2><ul>
<li>有很多入链</li>
<li>有很多来自具有高 PageRank 网页的入链</li>
<li>入链有很少的出链<br>因为在每一次 propagation 时，一个网页的 PR 会被它的出链平分，所以一个有很多出链的入链网页并不是非常有帮助。</li>
</ul>
<h2 id="PageRank-的其它问题"><a href="#PageRank-的其它问题" class="headerlink" title="PageRank 的其它问题"></a>PageRank 的其它问题</h2><ul>
<li>同一个站点的网页链接算不算？</li>
<li>新的网页的 PR 怎么算？</li>
<li>怎么处理 sinks（没有出链的网页）?</li>
<li>怎么处理 link farms 和链接交换？</li>
</ul>
<p>还有一个问题就是 PR 是 topic-independent 的，一个网页可能有很高的 PR 但是对某个 query 来说却是一个很坏的选择。Topic-Sensitive PageRank(TSPR) 可以解决这个问题。</p>
<h1 id="Topic-Sensitive-PageRank-TSPR"><a href="#Topic-Sensitive-PageRank-TSPR" class="headerlink" title="Topic-Sensitive PageRank(TSPR)"></a>Topic-Sensitive PageRank(TSPR)</h1><p>之前考虑的 PageRank 是等概率跳到一个随机网页的情况，关于 topic PageRank，我们考虑的是非等概率跳到一个随机网页的情况，计算的是基于特定兴趣/主题的 PageRank。如一个体育迷希望有关体育主题的网页的排名高于非体育主题的网页，假定这些有关体育的网页在 web 图中彼此相近，那么随机游走过程中，一个喜欢体育网页的人就可能在这类网页上停留大量的时间，因此，体育类网页的稳态分布概率被提升。</p>
<p>主要逻辑是</p>
<ul>
<li>在索引过程中，每一个网页都被自动分配一个 topic (用文本分类的方法)。</li>
<li>为每一个网页计算一个 topic-specific 的 PageRank。</li>
<li>每一个 query 都被分配一个 topic，根据用户在这个 search session 中浏览的网页来确定 topic。</li>
<li>对每一个页面，只考虑 topic-related PageRank，如 $PR_{sports}(d)$</li>
</ul>
<p><strong>注：</strong> DMOZ 是一个著名的开放式分类目录，有很多可以用来做 training data 的网页。可以从 top-level 中定义 topics。</p>
<h1 id="T-Fresh"><a href="#T-Fresh" class="headerlink" title="T-Fresh"></a>T-Fresh</h1><p>PageRank rewards older web pages。old web 网页有更多的时间来积累 inlinks，有些 link 可能是来自已经废弃不用的网页，然而用户可能更偏好新的网页。<br>网页快照提供了 freshness 的线索。</p>
<ul>
<li>Page freshness：网页最近什么时候被更新过。Page text(↑)，url(↑)，anchor text(↑↑)，new link(↑↑↑)</li>
<li>Link freshness：有多少 inlinks 来自 fresh page。</li>
<li>Decay freshness measures exponentially.</li>
</ul>
<p>follow a link 的概率取决于网页的新鲜度。<br>有 fresh inlinks 的 fresh page 有更高的 authority score。</p>
<p><strong>BM2500+T-Fresh vs BM2500+PageRank</strong><br>Relevance: +8% in P@10，+11-30% in NDCG@k<br>Freshness: +8% in P@10，+10-12% in NDCG@k</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/BM2500.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/BM2500.jpg">
<p>其它相同作用的 model 如 TimedPageRank, T-Rank, BuzzRank, TemporalPageRank。</p>
<h1 id="Hyperlink-Induced-Topic-Search-HITS"><a href="#Hyperlink-Induced-Topic-Search-HITS" class="headerlink" title="Hyperlink-Induced Topic Search(HITS)"></a>Hyperlink-Induced Topic Search(HITS)</h1><p>在泛主题搜索（broad-topic search），也就是 informational needs 时，主要有两种非常有用的网页结果。</p>
<ul>
<li>Authority 网页：一些权威性的网页。</li>
<li>Hub 网页：导航型网页，本身不是权威型网页，而是对某个主题感兴趣的人花时间编辑整理出的权威型网页列表。可以通过这种导航型网页来帮我们找到权威型网页。</li>
</ul>
<p>一个好的 hub 网页会同时指向多个好的 authority 网页，而一个好的 authority 网页同时会被多个好的 hub 网页所指向。给定某个查询，我们对每个网页给出两个得分，一个是 hub 值，一个是 authority 值，所以对任何一个查询，我们都能得到两个排序结果列表。所以，我们其实可以给出一个 hub 值和 authority 值的循环定义，然后通过迭代计算求解。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/HITS.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/HITS.jpg">
<p>并不是对整个 web 计算 Hubs 和 Authority 分数。而是对一个小的 set 来进行计算，set 的创建方法如下：</p>
<ul>
<li>收集 query q 的 top n 网页构成 the root set</li>
<li>用 root set 的入链和出链扩充这个 set。</li>
</ul>
<p>之后在这个网页子集上来计算 hub 和 authority 值。这个 set 的好处是有一个 strong, query-specific focus，而且 set 规模相对较小，大概只有 200 个网页。然而这些 score 必须在 query time 时进行计算。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/rootset.jpg" class="ful-image" alt="Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/rootset.jpg"></p>
<p><strong>HITS 在大规模的搜索引擎中不会被使用</strong></p>
<ul>
<li>计算 spam 要比计算 PageRank 简单</li>
<li>创建一个有 high hub score 的网页很简单</li>
<li>运行成本比 PageRank 高。</li>
</ul>
<p>HITS 通常用于别的目标。如找 communities 或是找一个 community 里的专家，因为它们会有 tightly-bound hubs 和 authorities。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Document Structure]]></title>
      <url>http://www.shuang0420.com/2016/11/04/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。文档的表示形式不只词袋一种，它可以有 fields，有 hierarchical structure(XML)，multiple representations of meaning, priors 等形式，这一章讲的就是检索模型怎么来处理这些复杂的文档表达。<br><a id="more"></a></p>
<h1 id="Multiple-representations-of-meaning"><a href="#Multiple-representations-of-meaning" class="headerlink" title="Multiple representations of meaning"></a>Multiple representations of meaning</h1><blockquote>
<p>Fields can be used to provide different representations of the same information</p>
</blockquote>
<p>三种表达方法。</p>
<ul>
<li><strong>Vector space</strong><br>– 每个 representation 提供独立的 ranking score<br>– 对每个分数求平均来得到一个最终的 ranking</li>
<li><strong>Okapi</strong><br>– Representations 以不同的方式来表达相同含义<br>– 合并 representations 来得到一个更好的 representation</li>
<li><strong>Indri</strong><br>– 每个 representation 提供一个 p(t|d) 的估计<br>– 有多种方法来合并这些估计</li>
</ul>
<h2 id="Vector-space"><a href="#Vector-space" class="headerlink" title="Vector space"></a>Vector space</h2><p><strong>Option 1:</strong> 一个词袋模型，包含了各种 filed 词汇<br>E.g., iPad::inlink, iPad::title, iPad::body, …</p>
<p>这让 length normalization 变得更为复杂，比如说 inlink text 可能会和 title 或者 body text 混在一起。一般不用这种方法。</p>
<p><strong>Option 2:</strong> Several vector spaces<br>$w_{title} *sim(query,title_i) +$<br>$w_{body} *sim(query,body_i) +$<br>$w_{inlink} *sim(query,inlink_i) +$<br>$w_{url} *sim(query,url_i) $</p>
<p>这种方法易于管理，也易于扩展，Lucene 也采用了这种方法，然而关于怎么设置 weights，并没有什么 guidance。</p>
<h2 id="Okapi"><a href="#Okapi" class="headerlink" title="Okapi"></a>Okapi</h2><blockquote>
<p>Evidence from each field should be weighted differently</p>
</blockquote>
<p>一些 field 可能有更好的表达能力，如 inlink text，一些 field 可能更为冗长，如 body，所以不同的 fields 我们应该区别对待。</p>
<p><strong>One solution</strong><br>• 把 query 看作 |F| 个词袋<br>• 对 query 匹配每个 field，然后 add the score</p>
<p>然而 … 这打破了 BM25’s saturation assumption。在 |F| 个 field 里中出现了一次的字段，比在一个 field 里出现了|F|次的字段，有更大的影响，这并不是我们想要的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5 fields × 1 occurrence = 5 × 0.4</div><div class="line">1 field × 5 occurrences = 0.77</div></pre></td></tr></table></figure></p>
<p><strong>Solution:</strong> 用一个复合加权的 representation，一篇文档就用一个词袋</p>
<p>$$tf_t=\sum_fw_ftf_{t,f}$$<br>$$doclen=\sum_fw_fdoclen_f$$</p>
<p>F: the set of fields</p>
<p>-&gt; 然后用标准的 BM25<br>但是 … BM25 有常量，可能我们需要 tune them on a per-field basis，可以用 BM25F。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/BM25F.jpg" class="ful-image" alt="BM25F.jpg"><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/exampleW.jpg" class="ful-image" alt="exampleW.jpg"></p>
<p><strong>特性：</strong></p>
<ul>
<li>一个 vocabulary 涵盖了所有的 fields<br>– 所以一个 term 有一个 global idf, 而不是一个 field-specific idf<br>– 对各种文档都有适用么?<pre><code>» E.g., 专利，医疗记录等
</code></pre></li>
<li>Field-specific tuning</li>
<li>constants 的影响很难理解</li>
</ul>
<h2 id="Indri"><a href="#Indri" class="headerlink" title="Indri"></a>Indri</h2><p>Indri 模型。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/indri.jpg" class="ful-image" alt="indri.jpg"></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/andSum.jpg" class="ful-image" alt="andSum.jpg">
<p>#AND 和 #WAND 用来合并独立的概率<br>#SUM 和 #WSUM 用不同方法来估计同一个概率</p>
<h1 id="Hierarchical-structure-XML"><a href="#Hierarchical-structure-XML" class="headerlink" title="Hierarchical structure(XML)"></a>Hierarchical structure(XML)</h1><p>Hierarchical structure 的文档通常用 Bayesian inference networks 来解决，如 indri。<br>首先对 flat 和 hierarchical structure 做一个区分。<br><strong>Flat</strong><br>元素之间是相互独立的，一个 term 只在包含了它的那个元素（通常是一个）里出现。<br>检索目的通常是一篇文档。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/1.jpg" class="ful-image" alt="1.jpg"><br><strong>Hierarchical</strong><br>元素之间是相互关联的，通常是包含的关系，一个 term 在所有包含了它的元素里出现。<br>检索目的可以是文档，也可以是元素。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/2.jpg" class="ful-image" alt="2.jpg"></p>
<h2 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h2><p><strong>检索什么样的 element?</strong><br>• Document? Encounter? Diagnosis?</p>
<p><strong>用怎样的 corpus statistics</strong><br>• Document vs. element</p>
<p><strong>怎样组合来自不同 elements 的 evidence</strong><br>• Can we prefer patients that have several matching encounters?</p>
<p><strong>Exact-match vs. best-match document structure</strong><br>• 可能 query 并不能完全匹配文档结构</p>
<h2 id="Ranking-elements"><a href="#Ranking-elements" class="headerlink" title="Ranking elements"></a>Ranking elements</h2><p><strong>One option: Use Jelinek-Mercer smoothing</strong><br>$$P(q_i|e)=(1-\lambda)P_{MLE}P(q_i|e)+ \lambda P_{MLE}P(q_i|C)$$</p>
<p>这种方法可能带来的问题是：</p>
<ol>
<li>文档结构不规范，这种情况经常在 web 文档中出现，如果文档里本来包含了 query term 然而因为文档不规范而找不到相应的 element，那么 score 可能为 0。</li>
<li>query 可能太严格。<br>– #AND[title](iphone)：不匹配 “Apple Cuts Phone Price”<br>– #AND[paragraph](solutions to poverty)： “poverty” 和 “solutions” 可能出现在不同的段落里</li>
</ol>
<p>解决方案是，多加一个 smoothing。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/formula.jpg" class="ful-image" alt="formula.jpg"></p>
<h2 id="Multiple-elements"><a href="#Multiple-elements" class="headerlink" title="Multiple elements"></a>Multiple elements</h2><p>这里我们探讨右图的情况。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/multiple.jpg" class="ful-image" alt="multiple.jpg"></p>
<p>两种方法 Aggregation OR Combination。</p>
<ul>
<li>Aggregation: Combine inverted lists</li>
<li>Combination: Combine scores</li>
</ul>
<h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><p>Indri 允许在 query 里指定 aggregation type</p>
<ul>
<li>term.element<br>– 如果没有指定 element, 那么默认 element 为 Document<br>– Example: apple.inlink</li>
<li>结果: 一个 inverted list 包含了一个 element 的所有 instance</li>
</ul>
<p><strong>Example</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#wand( #wsum (0.3 apple.title 0.2 apple.inlink 0.5 apple.body ) #wsum (0.3 ipad.title 0.2 ipad.inlink 0.5 ipad.body ) )</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/ainlink.jpg" class="ful-image" alt="ainlink.jpg">
<p><strong>问题：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#AND( breast.paragraph cancer.paragraph treatment.paragraph )</div></pre></td></tr></table></figure></p>
<p>对这一个查询，用 Aggregation 的话以下两篇文档会得到一样的分数，然而其实 D2 要比 D1 更相关。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/ap.jpg" class="ful-image" alt="ap.jpg"></p>
<h3 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a>Combination</h3><p>用 #MAX, #SUM, #OR 等 query operators 来合并 element scores。<br>首先我们来认识一下 query。<br>E.g.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#SUM[document] (#AND[sentence] (breast cancer treatment) )</div></pre></td></tr></table></figure></p>
<p>最外面的 element 是 document, 所以对 documents 进行排序</p>
<ol>
<li>对每个 sentence 进行 #AND (breast cancer treatment) 运算，结果是一个 (sentence, score) 列表</li>
<li>对上一步产生的结果 #SUM 运算，结果是一个 (document, score) 列表</li>
</ol>
<p>E.g.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#SUM[paragraph] (#AND[sentence] (breast cancer treatment) )</div></pre></td></tr></table></figure></p>
<p>最外面的 element 是 paragraph, 所以对 paragraphs 进行排序</p>
<ol>
<li>对每个 sentence 进行 #AND (breast cancer treatment) 运算，结果是一个 (sentence, score) 列表</li>
<li>对上一步产生的结果 #SUM 运算，结果是一个 (paragraph, score) 列表</li>
</ol>
<p>来考虑下不同的 combine query operator 产生的影响。</p>
<p><strong>#MAX[document]</strong> (#AND[sentence](breast cancer treatment) )<br>– 只考虑 best sentence<br><strong>#SUM[document]</strong> (#AND[sentence](breast cancer treatment) )<br>– poorly matching sentences 会让分数变低<br><strong>#OR[document]</strong> (#AND[sentence](breast cancer treatment) )<br>– 偏好有更多匹配的 sentences 的文档</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/c4.jpg" class="ful-image" alt="c4.jpg">
<p>• #MAX considers them equal<br>• #AND prefers C4<br>• #OR prefers C1<br>• #AVERAGE prefers C4</p>
<h3 id="Comparsion"><a href="#Comparsion" class="headerlink" title="Comparsion"></a>Comparsion</h3><p>假定目标是检索有 paragraphs 讨论 breast cancer treatment 的文档<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/compare.jpg" class="ful-image" alt="compare.jpg"><br><strong>Combination:</strong> Partial credit for sections that partially match<br><strong>Aggregation:</strong> The terms might not all be in the same section</p>
<h2 id="Extents"><a href="#Extents" class="headerlink" title="Extents"></a>Extents</h2><p>Using extents as evidence<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#AND(</div><div class="line">  #OR[document](</div><div class="line">      #AND[sentence](iraq war))</div><div class="line">  bush</div><div class="line">  #NEAR/1(exit strategy) )</div></pre></td></tr></table></figure></p>
<p>• 检索的是 document<br>• <strong>偏好</strong> ‘iraq’ and ‘war’ 在同一句话里的文档<br>– 出现在 title 里会让分数有稍稍的提高，因为有 smoothing</p>
<p>Using extents as a constraint<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#AND (</div><div class="line">  iraq.sentence</div><div class="line">  war.sentence</div><div class="line">  bush</div><div class="line">  #NEAR/1(exit strategy) )</div></pre></td></tr></table></figure></p>
<p>• 检索的是 document<br>• <strong>要求</strong> ‘iraq’ and ‘war’ 必须出现在同一句话的文档<br>  – 出现在文档的其它部分并不会有任何帮助</p>
<h2 id="HLT-Applications"><a href="#HLT-Applications" class="headerlink" title="HLT Applications"></a>HLT Applications</h2><p>文档结构可以由 annotation processes 产生<br>• E.g., named entity annotators, semantic role labelers, …</p>
<p>Retrieval of elements, using semantic role annotations<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#and[sentence](</div><div class="line">  #and[target_verb]( Loves</div><div class="line">      #and[./agent]( John )</div><div class="line">      #and[./patient]( Mary ) ) )</div></pre></td></tr></table></figure></p>
<p>• ./element specifies that element is a child of the parent field</p>
<h1 id="Table-retrieval"><a href="#Table-retrieval" class="headerlink" title="Table retrieval"></a>Table retrieval</h1><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/table.jpg" class="ful-image" alt="table.jpg">
<p>实际就是一个把 table 转化为 structure document 的问题。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Structure/table2.jpg" class="ful-image" alt="table2.jpg"></p>
<h1 id="Summaries"><a href="#Summaries" class="headerlink" title="Summaries"></a>Summaries</h1><p><strong>Fields</strong><br>• 用于 independent evidence (author, title, journal, …)<br>• 用于 multiple representations (url, title, body, …)<br>• 了解差异<br>• 了解各个检索模型如何支持这些 fields<br>• 了解在 query 中如何使用这些 fields</p>
<p><strong>Hierarchical structure</strong><br>• 怎样在多个 elements 都有匹配时 combine evidence<br>• Exact-match vs. best-match document structure<br>• 了解 Indri 怎样支持这些 elements<br>• 了解在 query 中如何使用这些 elements</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Distributed Systems笔记－middlewares]]></title>
      <url>http://www.shuang0420.com/2016/11/02/Web-service-middlewares/</url>
      <content type="html"><![CDATA[<p>CMU 95702 Distributed Systems 笔记。简单介绍分布式系统中解决 interoperability concern 的几种方案 Cobra’s CDR, Java serialization 和 XML/JSON。这章整理的比较简单。<br><a id="more"></a></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-service-middlewares/Web-service-middlewares/middlewarelayer.jpg" class="ful-image" alt="Web-service-middlewares/middlewarelayer.jpg">
<p>一言以蔽之，middleware 是为了更好的与 remote server 交流。</p>
<h1 id="Interoperability-concern"><a href="#Interoperability-concern" class="headerlink" title="Interoperability concern"></a>Interoperability concern</h1><p>分布式系统里的互操作性问题。</p>
<ul>
<li>Big/Little Endian byte ordering may differ</li>
<li>Floating point representation may differ</li>
<li>Binary vs Unicode<br>如果 j=3, binary 表示就是 00…011，而 unicode 表示是 0000000000110011，如果两端没有达成一致，那么就会出错。<br>The receiver had better know which one we are using。</li>
</ul>
<p>假设我们用 C++ 写了 TCP server，那么我们可以写个 JAVA TCP connection 来连接 server 吗？<strong>可以！</strong><br>C++ 和 JAVA 都知道怎么 open 一个 TCP connection。</p>
<p>假设 client 把一个 java object 发给了 server，这个 object 的内容可以重新被封装成 c++ 的 object 吗？<strong>不可以！</strong></p>
<h1 id="三种解决方案"><a href="#三种解决方案" class="headerlink" title="三种解决方案"></a>三种解决方案</h1><h2 id="CORBA’s-CDR"><a href="#CORBA’s-CDR" class="headerlink" title="CORBA’s CDR"></a>CORBA’s CDR</h2><p>双方都知道 message 的 data type 和 order。双方在交流前都有一个 IDL(Interface description language 接口描述语言)，这和 google 的 protocol buffers 差不多。XML, XSDL, WSDL 都可以作为 IDL。</p>
<p>如下面一段 C 的代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">struct Person &#123;</div><div class="line">  string name;</div><div class="line">  string place;</div><div class="line">  long year;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-service-middlewares/Web-service-middlewares/corba_cdr.jpg" class="ful-image" alt="Web-service-middlewares/corba_cdr.jpg">
<p>我们可以让 CORBA Interface Compiler 来做合适的 marshalling 和 unmarshalling operation，无论是 C 还是 JAVA。<br>CORBA’s CDR 的特点是 - 非常快！所以传送的信息不包括 data type，只有表格中的右边一栏数据。</p>
<h2 id="Java-serialization"><a href="#Java-serialization" class="headerlink" title="Java serialization"></a>Java serialization</h2><p>Java’s serialization 本身可以用来 marshal 和 unmarshal，所以并不需要 IDL。双方事先也不知道 data type。</p>
<p>如下面一段 Java 的代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">public class Person implements Serializable&#123;</div><div class="line">  string name;</div><div class="line">  string place;</div><div class="line">  long year;</div><div class="line">  public Person(String nm,place,year) &#123;</div><div class="line">    nm=name;this.place=place;this.year=year;</div><div class="line">  &#125;</div><div class="line">  // more methods</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-service-middlewares/Web-service-middlewares/java.jpg" class="ful-image" alt="Web-service-middlewares/java.jpg">
<p>Java 序列化的特点是有很多 data (如 class name, version number, data type 等)来 describe 真正的 data。</p>
<h2 id="Web-Service-use-of-XML"><a href="#Web-Service-use-of-XML" class="headerlink" title="Web Service use of XML"></a>Web Service use of XML</h2><p>格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;p:person xmlns:p=“http://www.andrew.cmu.edu/~mm6”&gt;</div><div class="line">  &lt;p:name&gt;Smith&lt;/p:name&gt;</div><div class="line">  &lt;p:place&gt;London&lt;/p:place&gt;</div><div class="line">  &lt;p:year&gt;1934&lt;/p:year&gt;</div><div class="line">&lt;/p:person&gt;</div></pre></td></tr></table></figure></p>
<ul>
<li>相对前两种方法来说会比较慢。因为它是 text 形式而前两种方法是 binary 形式。</li>
<li>HTTP header 需要声明 Content-Type: text/xml; charset: ISO-8859-1</li>
<li>可以表示任何 binary message，因为 binary data（图片和其它加密的元素）可以被表示成 Base64</li>
<li>必须遵循 XSDL 的语法。</li>
<li>支持各平台。</li>
</ul>
<h2 id="Web-Service-use-of-JSON"><a href="#Web-Service-use-of-JSON" class="headerlink" title="Web Service use of JSON"></a>Web Service use of JSON</h2><p>格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&#123; “person” : &#123; “name” : “Smith”</div><div class="line">“place”:”London”</div><div class="line">“year”:”1934”&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li>可以表示任何 binary message，因为 binary data（图片和其它加密的元素）可以被表示成 Base64</li>
<li>必须遵循 JSON 的语法。</li>
</ul>
<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><ul>
<li>Marshalling and external data representation<br>binary, xml/json text</li>
<li>Interoperability<br>corba flexibility, java requires both sides, xml/json interoperable</li>
<li>Security</li>
<li>Reliability<br>TCP: reliable as it checks if the message is arrived<br>UDP: not reliable</li>
<li>Performance<br>corba &gt; java &gt; xml/json(package and unpackage)</li>
<li>Remote references</li>
<li>Full OOP</li>
<li>Describe how the protocols of the internet allow for heterogeneity</li>
<li>Describe how middleware allows for heterogenity<br>hides low level implementation</li>
</ul>
<h1 id="Pass-pointers"><a href="#Pass-pointers" class="headerlink" title="Pass pointers"></a>Pass pointers</h1><p>在分布式的 OOP 中，我们需要传送 pointers，包括以下信息。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-service-middlewares/Web-service-middlewares/object_reference.jpg" class="ful-image" alt="Web-service-middlewares/object_reference.jpg"></p>
<h1 id="UDP-Based-Request-Reply-Protocol"><a href="#UDP-Based-Request-Reply-Protocol" class="headerlink" title="UDP Based Request-Reply Protocol"></a>UDP Based Request-Reply Protocol</h1><p>直接上图和代码。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-service-middlewares/Web-service-middlewares/UDP.jpg" class="ful-image" alt="Web-service-middlewares/UDP.jpg"></p>
<p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Client side:</div><div class="line">public byte[] doOperation (RemoteObjectRef o, int methodId, byte[] arguments)</div><div class="line">sends a request message to the remote object and returns the reply.</div><div class="line">The arguments specify the remote object, the method to be invoked and the</div><div class="line">arguments of that method.</div><div class="line"></div><div class="line">Server side:</div><div class="line">public byte[] getRequest ();</div><div class="line">acquires a client request via the server port.</div><div class="line"></div><div class="line">coolOperation</div><div class="line">select object, execute, method</div><div class="line"></div><div class="line">public void sendReply (byte[] reply, InetAddress clientHost, int clientPort);</div><div class="line">sends the reply message reply to the client at its Internet address and port.</div></pre></td></tr></table></figure></p>
<h2 id="Failure-model"><a href="#Failure-model" class="headerlink" title="Failure model"></a>Failure model</h2><p>doOperation 可能在 waiting 的时候 timeout，我们要做什么？</p>
<ul>
<li>返回给 caller 一个错误信息</li>
<li>response 可能会丢失，所以我们告诉 client 让 client try and try 直到确认服务器挂了。这带来的结果是 client 可能会收到同样的信息。</li>
</ul>
<h3 id="Handle-duplicates"><a href="#Handle-duplicates" class="headerlink" title="Handle duplicates"></a>Handle duplicates</h3><p>根据 client 的 acknowledgement 来清空历史。</p>
<h2 id="Request-Reply-Message-Structure"><a href="#Request-Reply-Message-Structure" class="headerlink" title="Request-Reply Message Structure"></a>Request-Reply Message Structure</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">messageType: int (0=Request, 1=Reply)</div><div class="line">requestId: int</div><div class="line">objectReference: RemoteObjectRef</div><div class="line">methodId: int or Method</div><div class="line">argument: array of bytes</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Distributed Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Web Service </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Distributed Systems笔记－Cryptographic Protocols]]></title>
      <url>http://www.shuang0420.com/2016/11/02/Cryptographic-Protocols/</url>
      <content type="html"><![CDATA[<p>CMU 95702 Distributed Systems 笔记。简单介绍几种加密、签名方式。<br><a id="more"></a></p>
<p><a href="http://www.shuang0420.com/2016/10/02/AES%20%E5%92%8C%20RSA%E7%AC%94%E8%AE%B0/">AES 和 RSA 笔记</a> 的续章。</p>
<h1 id="Scenario-1-Like-WWII-和-TEA"><a href="#Scenario-1-Like-WWII-和-TEA" class="headerlink" title="Scenario 1 (Like WWII 和 TEA)"></a>Scenario 1 (Like WWII 和 TEA)</h1><ol>
<li>双方共享一把密钥。</li>
<li>A 用密钥对信息加密。$E(K_AB,M_i)$，发送给 B</li>
<li>B 用 $D(K_AB,{M_i}K_AB)$ 解密读取信息。</li>
</ol>
<p><strong>问题是：</strong><br>双方如何同步密钥？<br>怎么确定 B 收到的 ${M_i}K_AB$ 不是 replay of an old message?</p>
<h1 id="Scenario-2-Like-Kerberos"><a href="#Scenario-2-Like-Kerberos" class="headerlink" title="Scenario 2 (Like Kerberos)"></a>Scenario 2 (Like Kerberos)</h1><ol>
<li>A 向第三方 S 索要一张和 B 通话的 ticket。</li>
<li>S 知道 A 的 password 所以他可以计算 $K_A$</li>
<li>S 发送给 A $\{\{Ticket\}K_B,K_{AB}\}, K_A$</li>
<li>A 知道自己的 password 所以可以计算 $K_A$，注意 A 的 password 不会在网络中传输。</li>
<li>A 可以计算出 $K_{AB}$ 和 $\{Ticket\}K_B$</li>
<li>A 向 B 发送一个读的请求，发送的信息是 $\{Ticket\}K_B$,Alice,Read</li>
<li>B 用 $K_B$ 来读取 Ticket 的内容，Ticket 的内容是 $K_{AB}$，Alice</li>
<li>A、B 可以用 session key 来交流了。</li>
</ol>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Cryptographic-Protocols/Cryptographic-Protocols/scenario2.jpg" class="ful-image" alt="Cryptographic-Protocols/scenario2.jpg">
<p>可以防止 replay，但 <strong>问题是:</strong></p>
<ul>
<li>难以 scale,S 必须知道 $K_A$, $K_B$,…</li>
<li>S 是唯一可能导致失败的因素。</li>
</ul>
<p>Kerberos 这一名词来源于希腊神话“三个头的狗——地狱之门守护者”系统设计上采用客户端/服务器结构与DES加密技术，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止 repla y攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统。</p>
<h2 id="Needham-Schroeder-protocol"><a href="#Needham-Schroeder-protocol" class="headerlink" title="Needham-Schroeder protocol"></a>Needham-Schroeder protocol</h2><p>这层协议是 Kerberos 的基础，在这之后，Alice and Bob share a secret (KAB)</p>
<h1 id="Scenario-3-Authentication"><a href="#Scenario-3-Authentication" class="headerlink" title="Scenario 3 (Authentication)"></a>Scenario 3 (Authentication)</h1><p>数字签名，用私钥签名，公钥解密。注意公钥加密比私钥慢 100-1000倍。<br>很难找到 digest(M1) == digest(M2)</p>
<ol>
<li>A 发送 Message＋用密钥加密的 Message 的 digest。{Digest(M)}$K_Apriv$</li>
<li>B 收到签名的文件，取出 Message，计算 Message 的 digest。</li>
<li>B 用 A 的公钥 $K_Apub$ 解密 {Digest(M)}$K_Apriv$ 然后和自己算的 digest 比较，如果匹配，签名验证。</li>
</ol>
<p><strong>问题：</strong><br>如果 A 说他没有签名？说自己的私钥泄漏了？只要 A、B 互相信任，还是有用的。</p>
<h1 id="Scenario-4-Like-SSL"><a href="#Scenario-4-Like-SSL" class="headerlink" title="Scenario 4 (Like SSL)"></a>Scenario 4 (Like SSL)</h1><ol>
<li>A 和 B 想要建立一个共享的密钥</li>
<li>A 拿到 B 的公钥，这个公钥被可信任的第三方 T 签名认证了，所以这个公钥确实是 B 的。</li>
<li>A 确认第三方 T 对 $K_Bpub$ 签名了。怎么确认？A 和 T 都有 B 的 public key，T 把 $K_Bpub$ 加密后给 A，A 对其进行解密然后比对自己手上的 B 的 public key 看是不是一致。</li>
<li>A 生成了 $K_{AB}$ 并用 $K_Bpub$ 加密。</li>
<li>B 有很多公钥所以 A 发送公钥的名字。</li>
<li>A 发送了 key name $\{K_{AB}\}K_Bpub$</li>
<li>B 用这个 key name 选择了对应的私钥并计算 $\{\{K_{AB}\}K_Bpub\}K_Bpriv == K_{AB}$</li>
</ol>
<p>最后 A 和 B 共享了对称的钥匙 $K_{AB}$</p>
<p><strong>问题：</strong><br>在 A 第一次得到 B 的公钥时（A 认为这是 B 的公钥，然而这并不是，这是 C 也经过第三方 T 签名认证的公钥。</p>
<p>TLS 和这个相似<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Cryptographic-Protocols/Cryptographic-Protocols/TLS_config.jpg" class="ful-image" alt="Cryptographic-Protocols/TLS_config.jpg"></p>
<h1 id="Message-Authentication-Codes-MACs"><a href="#Message-Authentication-Codes-MACs" class="headerlink" title="Message Authentication Codes(MACs)"></a>Message Authentication Codes(MACs)</h1><p>对称加密生成的数字签名。<br>双方都有 Key(K)，sender 把 Key(K) 通过 MAC 算法加密后连同 message 一起给 receiver，receiver 比对收到的 MAC 和自己用 MAC 算法对自己这里的 Key(K) 加密后的 MAC 是否一致，如果一致，那么信息真实性和完整性就得到了证实。</p>
<p>用于数字签名，双方都算了一遍 MAC</p>
<h1 id="JAVA-里的-keystore-和-truststore"><a href="#JAVA-里的-keystore-和-truststore" class="headerlink" title="JAVA 里的 keystore 和 truststore"></a>JAVA 里的 keystore 和 truststore</h1><p>keystore: 存了公钥、私钥、证书<br>truststore：存了公钥，只能存 server 发过来的东西</p>
]]></content>
      
        <categories>
            
            <category> Distributed Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Web Service </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Distributed Systems笔记－Web Service Design Patterns]]></title>
      <url>http://www.shuang0420.com/2016/11/02/Web-Service-Design-Patterns/</url>
      <content type="html"><![CDATA[<p>CMU 95702 Distributed Systems 笔记。简单介绍 XML-RPC、SOAP、REST 三种 web 服务实现方案以及 RPC、Message、Resource 三种 patterns。<br><a id="more"></a></p>
<h1 id="Web-服务实现方案"><a href="#Web-服务实现方案" class="headerlink" title="Web 服务实现方案"></a>Web 服务实现方案</h1><p>主流的 Web 服务实现方案有以下三种，因为 XML-RPC 逐渐被 SOAP 取代，所以也可以说，主流的 Web 服务实现方案只有 REST 和 SOAP 两种。</p>
<ul>
<li>REST：表征状态转移（Representational State Transfer</li>
<li>SOAP：简单对象访问协议（Simple Object Access Protocol）</li>
<li>XML-RPC：远程过程调用（Remote procedure call，RPC)</li>
</ul>
<h2 id="XML-RPC"><a href="#XML-RPC" class="headerlink" title="XML-RPC"></a>XML-RPC</h2><p>XML-RPC 是一个远程过程调用（remote procedure call，RPC)的分布式计算协议，通过XML将调用函数封装，并使用 HTTP 协议作为传送机制。后来在新的功能不断被引入下，这个标准慢慢演变成为今日的 SOAP 协定。XML-RPC 协定是已登记的专利项目。XML-RPC 透过向装置了这个协定的服务器发出HTTP请求。发出请求的用户端一般都是需要向远端系统要求呼叫的软件。</p>
<p>eg. Long-lived image<br>如果需要经常把大的图片传到前端，那么可以把图片的 cache-control 设置的大一些，如 30 天，如果需要更新图片，那么上传一张新的图片到新的 URI，然后再改变 HTML，指向新的 URI。</p>
<p>HTML<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;img src=&apos;/image/big-image.jpg&apos;\&gt;</div></pre></td></tr></table></figure></p>
<p>Server<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">HTTP/1.1 200 Ok</div><div class="line">Date: Thu, 15 Aug 2008 23:26:31 GMT</div><div class="line">Server: Apache</div><div class="line">Content-Length: 50753</div><div class="line">Cache-Control: max-age=259200</div></pre></td></tr></table></figure></p>
<p>HTML<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;img src=&apos;/image/big-image2.jpg&apos;&gt;&lt;/pre&gt;</div></pre></td></tr></table></figure></p>
<h2 id="SOAP"><a href="#SOAP" class="headerlink" title="SOAP"></a>SOAP</h2><p>SOAP(Simple Object Access Protocol) 是一套完整的实现 Web 服务的解决方案。</p>
<p>SOAP 方式的 Web 服务中的 Web 服务描述语言（WSDL）和简单对象访问协议（SOAP）一起构成了 SOAP 方式下的 Web 服务的结构单元。客户端通过 WSDL 可以了解 Web 服务公开了那些可以被执行的方法以及 Web 服务可以发送或接收的消息格式（解决了公布访问资源方法的问题）。客户端按照 SOAP 将调用位于远程系统上的服务所需信息序列化为消息（解决了如何调用远程方法的问题）。注意 WSDL 描述的服务以及SOAP消息都是符合统一标准的，都是机器可读的.</p>
<p>WSDL 基于 XML 格式，用来描述 Web 服务。WSDL 文档可以看成是客户端和服务器之间的一个协约。使用 WSDL 工具，你可以自动处理这个过程，几乎不用手工编写代码就能够让应用程序整合新的服务。因此 WSDL 是 Web 服务体系结构的基础，因为它提供了一个通用语言，用来描述服务和整合这些服务的平台。</p>
<p>SOAP 本身提供了与 Web 服务交换信息的方法。SOAP 是序列化调用位于远程系统上的服务所需信息的标准方法，这些信息可以使用一种远程系统能够读懂的格式通过网络发送到远程系统，而不必关心远程系统运行于何种平台或者使用何种语言编写。SOAP 以 XML 格式提供了一个简单、轻量的用于在分散或分布环境中交换结构化和类型信息的机制。实际上它通过提供一个有标准组件的包模型和在模块中编码数据的机制，定义了一个简单的表示应用程序语义的机制。</p>
<p>用一个简单的例子来说明 SOAP 使用过程，一个 SOAP 消息可以发送到一个具有 Web Service 功能的 Web 站点，例如，一个含有房价信息的数据库，消息的参数中标明这是一个查询消息，此站点将返回一个 XML 格式的信息，其中包含了查询结果（价格，位置，特点，或者其他信息）。由于数据是用一种标准化的可分析的结构来传递的，所以可以直接被第三方站点所利用。</p>
<h2 id="REST"><a href="#REST" class="headerlink" title="REST"></a>REST</h2><p>表征状态转移（Representional State Transfer），是 Roy Fielding（ HTTP规范的主要编写者之一）博士在2000年他的博士论文中提出来的一种软件架构风格。它并不是一个标准，而是通过表征（Representional）来描述传输状态的一种原则。其宗旨是从资源的角度来观察整个网络，分布在各处的资源由URI确定，而客户端的应用通过 URI 来获取资源的表征。获得这些表征致使这些应用程序转变了其状态。随着不断获取资源的表征，客户端应用不断地在转变着其状态。</p>
<p>REST 中没有用于描述资源（服务）列表，资源元数据的类似于WSDL的东西。所以我们需要其他策略去代替 WSDL 实现“公布访问资源方法的问题”。</p>
<p>由于没有类似于 SOAP 的权威性协议作为规范，因此各个网站的REST实现都自有一套，也正是因为这种各自实现的情况，在性能和可用性上会大大高于 SOAP 发布的 web service，但细节方面有太多没有约束的地方，其统一通用方面远远不及 SOAP。</p>
<p>举个例子：假设A组织，B组织都实现了Restful API来通过工号查询人员信息，因为没有统一的规范。</p>
<p>A的API 可能是这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://A/api/person/001</div></pre></td></tr></table></figure></p>
<p>B的API 可能是这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://A/api/person/id=001</div></pre></td></tr></table></figure></p>
<p>第三方客户端在实现远程调用的时候就必须考虑这些API的差异，分别查看A，B的API文档。</p>
<p>如果有个权威性协议作为规范做指导，规定这个API应该实现成下面这样，那么第三方客户端也只需按照这个标准去调用远程API，而不用查看A，B的API文档：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http://A/api/person/&#123;001&#125;</div></pre></td></tr></table></figure></p>
<p>而 OData 就是这样的一个设计和使用 Restful API 的权威性协议. OData 定义了一些标准规则（像一个接口定义一堆方法一样），实现 Restful API 时候，必须实现这些标准规则（就像实现一个接口必须实现其所有方法一样）。第三方就可以根据 Odata 协议定义的规则去访问 Restful API。</p>
<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><p>Resources</p>
<ul>
<li>URI<br>REST 的一个重要原则是  Addressability，每一个资源都有一个 URI。格式为 scheme://host:port/path?queryString#fragment。scheme 可以是 http、ftp、https 等。</li>
<li>Uniform Interface<br>methods，用 http 的若干方法来操作资源。<br>representation，提供了多种 formats 来表述网页，如 xml, json 等。http 用 content-type header 来定义格式。</li>
</ul>
<p>Protocol</p>
<ul>
<li>client-server<br>客户和服务器之间通过一个统一的接口来互相通讯。</li>
<li>stateless<br>每一个 request 都是独立的，每次发送请求时客户端都需要提供足够的信息，服务端并不会保存有关客户的任何状态。</li>
<li>cacheable<br>REST 的系统能恰当对请求进行缓存，尽量减少服务器和客户端之间的信息传输来提高性能</li>
<li>layered(intermediaries)<br>客户端并不会固定和一个服务器打交道</li>
</ul>
<p><strong>HTTP method 的补充：</strong></p>
<ul>
<li>GET - safe, idempotent, cacheable</li>
<li>PUT - idempotent</li>
<li>DELETE - idempotent</li>
<li>HEAD - safe, idempotent</li>
<li>POST</li>
</ul>
<p>cacheable：response 可以被缓存<br>idempotent：该操作可以被执行多次<br>safe：该操作并没有副作用（不会影响别的操作）</p>
<h3 id="HATEOAS"><a href="#HATEOAS" class="headerlink" title="HATEOAS"></a>HATEOAS</h3><p>REST 另一个主要内容是 HATEOAS。HATEOS 用中文解释就是 <strong>超文本作为状态转移的引擎</strong>，这是一个 late binding 的例子。用户在浏览器输入 URL 向该资源发起一个 HTTP GET 请求，服务器会返回 response，在这个 response 中包含了下一步你该去哪里的信息，你可以在这个 response 中找到对其它资源的引用：链接、图片、脚本等。也就是说，一个典型的REST服务不需要额外的文档标示通过哪些URL访问特定类型的资源，而是通过服务端返回的响应来标示到底能在该资源上执行什么样的操作。一个REST服务的客户端也不需要知道任何有关哪里有什么样的资源这种信息。</p>
<p>举例来说，一个客户端可能会接收一个我们上面所描述的用于报表服务的主RESTful服务的引用：</p>
<pre>http://company1.com/report/</pre>

<p>如果是通过浏览器发出的请求，可能会返回一个包含如下引用的HTML文档：</p>
<pre>http://company1.com/report/sales</pre>

<p>用户可以点击进入并找到可浏览的年份列表。要说明的是浏览器对于URL的结构并没有特别的认知，但它知道如何分析结果并以用户可以浏览的结果返回内容。</p>
<p>对于其它的MIME类型道理也是一样，比如以XML的格式请求2009年的季度报表：</p>
<pre>http://company1.com/reports/sales/2009/qtr</pre>

<p>可能得到：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;reports&gt;</div><div class="line">    &lt;description&gt;2009 Quarterly Reports&lt;/description&gt;</div><div class="line">    &lt;report name=&quot;First Quarter&quot; src=&quot;http://company1.com/reports/sales/2009/qtr/1&quot;/&gt;</div><div class="line">    &lt;report name=&quot;Second Quarter&quot; src=&quot;http://company1.com/reports/sales/2009/qtr/2&quot;/&gt;</div><div class="line">    &lt;report name=&quot;Third Quarter&quot; src=&quot;http://company1.com/reports/sales/2009/qtr/3&quot;/&gt;    </div><div class="line">&lt;/reports&gt;</div></pre></td></tr></table></figure></p>
<p>可以将 URL 想成是贯穿信息空间的向量。每一个层次都进一步的将你指向最终的资源。不同的路径可能产生同样的结果。客户端需要知道如何分析这些结果，但通过对响应给出可识别的类型，我们可以触发合适的分析器。这一结构可通过爬虫降序的从引用来抓取，或者以某种接口展现给用户浏览。一个 RESTful 接口成为了客户端通过基于已知来请求信息的方式。它们以一个已知或者已发现的点作为开始，就像你浏览web一样来浏览信息。</p>
<p>这就是 HATEOS 所指代的。应用的状态在超文本响应中被转移和发现。就像浏览器需要知道HTML、图片、声音文件等等一样，一个RESTful客户端也需要知道如何分析解析资源引用的结果。然而，整个过程是简单、受约束、可伸缩并且灵活的——这正是我们所期望的网络软件系统的属性。</p>
<p>许多人搭建的”RESTful”系统都要求客户端事先知道URL的每一层次的意义。如果服务端将信息重组了，这些系统的客户端就会崩溃。真正体现了HATEOS的客户端与它们所通讯的服务器之间才更加能做到松耦合。</p>
<p>更多见<a href="http://www.infoq.com/cn/articles/roa-rest-of-rest/" target="_blank" rel="external">面向资源的架构：REST的另一面</a></p>
<h3 id="Linked-Services-Pattern"><a href="#Linked-Services-Pattern" class="headerlink" title="Linked Services Pattern"></a>Linked Services Pattern</h3><p>是 HATEOAS 的核心。只发布一部分 root web services 的地址，在每个 response 中返回相关服务的地址，让客户端通过这种 response 来发现之后的 URI。</p>
<blockquote>
<p>Only publish the addresses of a few root web services. Include the addresses of related services in each response. Let clients parse responses to discover subsequent service URIs.</p>
</blockquote>
<h3 id="Network-performance"><a href="#Network-performance" class="headerlink" title="Network performance"></a>Network performance</h3><p>客户端 -&gt; 服务端。</p>
<pre>client - proxy - gateways - origin server</pre>

<p>REST 的优势。</p>
<ul>
<li>Efficiency<br>缓存可以提高效率，并不需要到达 gateways 和 origin server；data control 意味着我们可以压缩数据来提高效率。</li>
<li>Scalability<br>缓存：理由同上。<br>无状态：如果服务器记录用户相关的状态，那么集群扩展时用户相关的状态就要及时地在集群中的各个服务器之间同步，对用户状态的同步将会是一个非常棘手的问题，当一个用户的相关状态在一个服务器上发生了更改，那么在什么时候，什么情况下对这些状态进行同步？如果该状态同步是同步进行的，那么同时刷新多个服务器上的用户状态将导致对用户请求的处理变得异常缓慢。如果该同步是异步的，那么用户在发送下一个请求时，其它服务器将可能由于用户状态不同步的原因无法正确地处理用户的请求。除此之外，如果集群进行了不停机的横向扩展，那么用户状态的同步需要如何完成？<br>不同的 gateways，增加 intermediaries 非常方便。</li>
<li>User Perceived Performance<br>通过 reduce media types，缓存等方式实现。</li>
<li>simplicity/evolvability/extensibility/customizability/configuration/reusability/visibility/portability/reliability</li>
</ul>
<h3 id="三种方案简单比较"><a href="#三种方案简单比较" class="headerlink" title="三种方案简单比较"></a>三种方案简单比较</h3><p>XML-RPC已慢慢的被SOAP所取代，现在很少采用了，但它还是有版权的。</p>
<ul>
<li>成熟度上：SOAP在成熟度上优于REST</li>
<li>效率和易用性上：REST更胜一筹</li>
<li>安全性上：SOAP安全性高于REST，因为 REST 更关注的是效率和性能问题</li>
</ul>
<p>总体上，因为 REST 模式的 Web 服务与复杂的 SOAP 和 XML-RPC 对比来讲明显的更加简洁，越来越多的 web 服务开始采用 REST 风格设计和实现。例如，Amazon.com 提供接近 REST 风格的 Web 服务进行图书查找；雅虎提供的 Web 服务也是REST风格的。REST 对于资源型服务接口来说很合适，同时特别适合对于效率要求很高，但是对于安全要求不高的场景。而 SOAP 的成熟性可以给需要提供给多开发语言的，对于安全性要求较高的接口设计带来便利。所以我觉得纯粹说什么设计模式将会占据主导地位没有什么意义，关键还是看应用场景，正是那句老话：适合的才是最好的</p>
<p>同时很重要一点就是不要扭曲了REST现在很多网站都跟风去开发 REST 风格的接口，其实都是在学其形，不知其心，最后弄得不伦不类，性能上不去，安全又保证不了，徒有一个看似象摸象样的皮囊。</p>
<p><a href="http://www.cnblogs.com/lanxuezaipiao/archive/2013/05/11/3072436.html" target="_blank" rel="external">三种主流的Web服务实现方案（REST+SOAP+XML-RPC）简述及比较</a></p>
<h1 id="API-styles"><a href="#API-styles" class="headerlink" title="API styles"></a>API styles</h1><h2 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h2><p>RPC 是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用 B 服务器上应用提供的函数/方法，需要通过网络来表达调用的语义和传达调用的数据。</p>
<p>这时候的 message 包括 procedure name 和 parameter list，service descriptor 通常是 WSDL 和 XSDL 或者 non-XML approach(JSON-RPC)，作用是生成一个在 client 上的 service connector(proxy)。</p>
<p>Framework: SOAP, JAX-WS(java), WCF(Microsoft)</p>
<p>RPC 模型是 tightly copuled system，如果 parameter list 改变了，那么 client 将会 break。如果 descriptor 改变了，那么必须重新生成 connector 来连接 client。</p>
<p>request/response 是 RPC 的默认模式，request/acknowlege 会 less coupled(seperation of concerns)，request 可以在队列中等待，之后再进行处理，这样可以提高 scalability。如果 client 不想再等待的时候 block，那么可以用 asynchronous response handler。</p>
<h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>通过 http 调用别的机器上的进程／方法<br>为什么 RPC 呢？就是无法在一个进程内，甚至一个计算机内通过本地调用的方式完成的需求，比如不同的系统间的通讯，甚至不同的组织间的通讯。由于计算能力需要横向扩展，需要在多台机器组成的集群上部署应用.</p>
<p>RPC 的协议有很多，比如最早的 CORBA，Java RMI，Web Service的 RPC 风格，Hessian，Thrift，甚至 Rest API。</p>
<p>通信细节<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-Service-Design-Patterns/Web-Service-Design-Patterns/RPC.jpg" class="ful-image" alt="Web-Service-Design-Patterns/RPC.jpg"></p>
<ol>
<li>服务消费方（client）调用以本地调用方式调用服务；</li>
<li>client stub 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；</li>
<li>server stub 收到消息后进行解码；</li>
<li>server stub 根据解码结果调用本地的服务；</li>
<li>本地服务执行并将结果返回给 server stub；</li>
<li>server stub 将返回结果打包成消息并发送至消费方；</li>
<li>client stub 接收到消息，并进行解码；</li>
<li>服务消费方得到最终结果。</li>
</ol>
<p>RPC的目标就是要2~8这些步骤都封装起来，让用户对这些细节透明。</p>
<h2 id="Message"><a href="#Message" class="headerlink" title="Message"></a>Message</h2><p>通过 http 给别的机器发送 command 命令、通知、其它信息，不用和别的进程 direct coupling，也不用知道别的机器的方法的签名。message 只包含一个参数。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-Service-Design-Patterns/Web-Service-Design-Patterns/message.jpg" class="ful-image" alt="Web-Service-Design-Patterns/message.jpg">
<p>这时候的 message 不包括 procedure name 和 parameter list，service descriptor 通常是 WSDL 和 XSDL，作用是生成一个 service connector(proxy)，service 的主要任务是分发，要求 service 更加的聪明，能够评估消息内容并决定去执行哪个进程调用哪个方法。</p>
<p>response 包括了相关 service 的地址（url）。<br>Framework: SOAP, WS-Policy, WS-Security</p>
<p>默认是 request/acknowledge 模式而不是 request/response。<br>追加一条新的 message type 很简单。</p>
<h2 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h2><p>操作另一台机器上的数据，不用和别的进程 direct coupling，最小化 the need for domain specific api’s。<br>消息内容是一个 http 方法，一个 uri，一个 media type。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-Service-Design-Patterns/Web-Service-Design-Patterns/resource.jpg" class="ful-image" alt="Web-Service-Design-Patterns/resource.jpg"></p>
<p>Resource API 可能是 Restful 的。<br>request/acknowledge 或者 request/response。<br>会产生 Security risk 因为 uri is hackable</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Web-Service-Design-Patterns/Web-Service-Design-Patterns/conclusion.jpg" class="ful-image" alt="Web-Service-Design-Patterns/conclusion.jpg">
<p>如果要建一个分布式系统，high performance(speed) 是最主要的要求，那么以下选项选哪个？</p>
<ul>
<li>REST sytle web service</li>
<li>JAVA RMI</li>
<li>SOAP based web service</li>
<li>Javascript using JSON</li>
<li>Plain old XML(POX) over HTTP</li>
</ul>
<p>选 JAVA RMI，因为 java RMI 把 message 都编码成 binary 的形式，减少了在各终端的 conversion time。</p>
<h1 id="Postel’s-Law"><a href="#Postel’s-Law" class="headerlink" title="Postel’s Law"></a>Postel’s Law</h1><p>最后加一条 Postel’s Law。简单来说就是 <strong>对自己严格，对他人宽容</strong>。“发送时保守”是告诫 web 开发人员的，HTML代码应该写的尽可能符合标准，能够方便别人（浏览器）去解析。“接收时开放”主要是说对一个不遵循固定标准（如不遵循HTML标准）的网页，或者说网站中出现的一个或多个错误，浏览器仍能够尽可能的解析并呈现。另外，浏览器必须向后兼容也是“接收时开放”的一个典型例子，不能因为大家都用 HTML5 编写网站浏览器就不再支持之前的 HTML 版本。</p>
<blockquote>
<p>参考链接：<br><a href="http://www.cnblogs.com/LBSer/p/4853234.html" target="_blank" rel="external">你应该知道的RPC原理</a><br><a href="http://www.cnblogs.com/loveis715/p/4669091.html" target="_blank" rel="external">REST简介</a><br><a href="http://stevenjohn.iteye.com/blog/1442776" target="_blank" rel="external">WebService的两种方式SOAP和REST比较 (转)</a><br><a href="http://www.cnblogs.com/lanxuezaipiao/archive/2013/05/11/3072436.html" target="_blank" rel="external">三种主流的Web服务实现方案（REST+SOAP+XML-RPC）简述及比较</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Distributed Systems </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Web Service </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Learning to Rank]]></title>
      <url>http://www.shuang0420.com/2016/10/25/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。我们已经学习了很多检索方法，如果把这些方法结合起来，效果会不会更好呢？<br><a id="more"></a></p>
<h1 id="传统的排序模型"><a href="#传统的排序模型" class="headerlink" title="传统的排序模型"></a>传统的排序模型</h1><h2 id="相关度排序模型-Relevance-Ranking-Model"><a href="#相关度排序模型-Relevance-Ranking-Model" class="headerlink" title="相关度排序模型(Relevance Ranking Model)"></a>相关度排序模型(Relevance Ranking Model)</h2><p>相关度排序模型根据查询和文档之间的相似度来对文档进行排序。常用的模型包括：<a href="http://www.shuang0420.com/2016/09/06/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Exact-match%20retrieval/">布尔模型(Boolean Model)</a>，<a href="http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/">向量空间模型(Vector Space Model)</a>，隐语义分析(Latent Semantic Analysis)，<a href="http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/">BM25</a>，LMIR 模型等等。</p>
<h2 id="重要性排序模型-Importance-Ranking-Model"><a href="#重要性排序模型-Importance-Ranking-Model" class="headerlink" title="重要性排序模型(Importance Ranking Model)"></a>重要性排序模型(Importance Ranking Model)</h2><p>重要性排序模型考虑的是 query-independent 的因素，根据网页(亦即文档)之间的图结构来判断文档的<a href="http://www.shuang0420.com/2016/11/04/Search-Engines%E7%AC%94%E8%AE%B0-Authority-Metrics/">权威程度(Authority Score)</a>，典型的权威网站包括 Google，Yahoo! 等。常用的模型包括 PageRank，HITS，HillTop，TrustRank 等等。</p>
<h1 id="Learning-to-Rank"><a href="#Learning-to-Rank" class="headerlink" title="Learning to Rank"></a>Learning to Rank</h1><h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>对于传统的排序模型，单个模型往往只能考虑某一个方面(相关度或者重要性)，所以只是用单个模型达不到要求。搜索引擎通常会组合多种排序模型来进行排序，但是，如何组合多个排序模型来形成一个新的排序模型，以及如何调节这些参数，都是一个很大的问题。使用机器学习的方法，我们可以把各个现有排序模型的输出作为特征，然后训练一个新的模型，并自动学得这个新的模型的参数，从而很方便的可以组合多个现有的排序模型来生成新的排序模型。</p>
<p>先来看看我们现在已经拥有的东西，这些都可以作为 feature</p>
<ul>
<li>Retrieval models: Vector space, BM25, language models, …</li>
<li>Representations: Title, body, url, inlink, …</li>
<li>Query templates: Sequential dependency models, …</li>
<li>Query-independent evidence: PageRank, url depth, …</li>
</ul>
<p>我们可以把 retrieval model 也当作 feature，然后用 machine learning 的算法将上面这些 evidence 综合起来。</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/l2rs.jpg" class="ful-image" alt="l2rs.jpg">
<h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/l2rf.jpg" class="ful-image" alt="l2rf.jpg">
<h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>从三个角度来讨论。</p>
<ul>
<li>Features</li>
<li>Training data</li>
<li>Algorithm</li>
</ul>
<h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><p>其实之前已经讲过，retrieval models, representations, query templates, query-independent evidence 都能作为特征。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/features.jpg" class="ful-image" alt="features.jpg"></p>
<h2 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h2><p>L2R 的训练数据可以有三种形式，这三种形式的训练数据之间可以相互转换，详见[1]。</p>
<ul>
<li>对于每个查询，各个文档的绝对相关值(非常相关，比较相关，不相关，等等)</li>
<li>对于每个查询，两两文档之间的相对相关值(文档1比文档2相关，文档4比文档3相关，等等)</li>
<li>对于每个查询，所有文档的按相关度排序的列表(文档1&gt;文档2&gt;文档3)</li>
</ul>
<p>训练数据的获取有两种主要方法：人工标注和从日志文件中挖掘。</p>
<p><strong>人工标注：</strong> 首先从搜索引擎的搜索记录中随机抽取一些查询，将这些查询提交给多个不同的搜索引擎，然后选取各个搜索引擎返回结果的前 K 个，最后由专业人员来对这些文档按照和查询的相关度进行标注。</p>
<p><strong>从日志中挖掘：</strong> 搜索引擎都有大量的日志记录用户的行为，我们可以从中提取出 L2R 的训练数据。Joachims 提出了一种很有意思的方法：给定一个查询，搜索引擎返回的结果列表为 L ，用户点击的文档的集合为 C，如果一个文档 di 被点击过，另外一个文档 dj 没有被点击过，并且 dj 在结果列表中排在 di 之前，则 di&gt;dj 就是一条训练记录。亦即训练数据为：${di&gt;dj，di \in C，dj \in L-C，p(dj)&lt;p(di)}$，其中 p(d) 表示文档 d 在查询结果列表中的位置，越小表示越靠前。</p>
<h2 id="Machine-Learning-Algorithm"><a href="#Machine-Learning-Algorithm" class="headerlink" title="Machine Learning Algorithm"></a>Machine Learning Algorithm</h2><p>L2R算法主要包括三种类别：PointWise，PairWise，ListWise。</p>
<p><strong>Pointwise</strong></p>
<ul>
<li>训练数据是一个文档类别或分数</li>
<li>Accurate score ≠ accurate ranking</li>
<li>忽略文档的位置信息</li>
</ul>
<p><strong>Pairwise</strong></p>
<ul>
<li>训练数据是文档对的一个偏好(一对文档选哪个)</li>
<li>Accurate preference ≠ accurate ranking</li>
<li>忽略文档的位置信息</li>
</ul>
<p><strong>Listwise</strong></p>
<ul>
<li>训练数据是文档的排名</li>
<li>难以直接优化 ranking metrics</li>
</ul>
<h3 id="PointWise-L2R"><a href="#PointWise-L2R" class="headerlink" title="PointWise L2R"></a>PointWise L2R</h3><p>只考虑给定查询下，单个文档的绝对相关度，而不考虑其他文档和给定查询的相关度。亦即给定查询 q 的一个真实文档序列，我们只需要考虑单个文档 di 和该查询的相关程度 ci。</p>
<p><strong>Approach:</strong> 用 individual documents 训练模型<br><strong>Training data:</strong> x -&gt; score<br><strong>Learned model:</strong> h(x) -&gt; score<br>  <strong>Regression</strong> (e.g., linear regression)<br>    – Scores are { -1, +1 } or { 4, 3, 2, 1, 0 }<br>  <strong>Classification</strong> (e.g., SVM)<br>    – Categories are { -1, +1 } or { 4, 3, 2, 1, 0 }</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/pointwise.jpg" class="ful-image" alt="pointwise.jpg">
<p><strong>局限：</strong></p>
<ol>
<li>要求 score 必须在一定范围内 E.g., { -1, +1 } or { 4, 3, 2, 1, 0 } 然而经过排序算法出来的分数往往不是这样的，它可能是 { 189, 57, 42, 16, 1}.</li>
<li>重要的是 order，而不是 score。</li>
<li>没有考虑到排序的一些特征，比如文档之间的排序结果针对的是给定查询下的文档集合，而 Pointwise 方法仅仅考虑单个文档的绝对相关度</li>
<li>在排序中，排在最前的几个文档对排序效果的影响非常重要，Pointwise 没有考虑这方面的影响</li>
</ol>
<p>Pointwise方法主要包括以下算法：Pranking (NIPS 2002), OAP-BPM (EMCL 2003), Ranking with Large Margin Principles (NIPS 2002), Constraint Ordinal Regression (ICML 2005)。</p>
<h3 id="Pairwise-L2R"><a href="#Pairwise-L2R" class="headerlink" title="Pairwise L2R"></a>Pairwise L2R</h3><p>Pairwise 方法考虑给定查询下，两个文档之间的相对相关度。亦即给定查询 q 的一个真实文档序列，我们只需要考虑任意两个相关度不同的文档之间的相对相关度：di&gt;dj，或者 di&lt;dj。</p>
<p><strong>Approach:</strong> 用文档对来训练模型<br>Training data: prefer (x1, x2)<br>Learned model: h (x1) &gt; h (x2)</p>
<p><strong>Pair value</strong><br>Binary assessments { &gt;, &lt; }</p>
<p><strong>Loss function</strong><br>如果文档对顺序正确，为 0，否则为 1</p>
<p><strong>Minimize the number of misclassified document pairs</strong><br>关注偏好，而不是 raw scores/labels</p>
<p>E.g.,<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/pairwise.jpg" class="ful-image" alt="pairwise.jpg"></p>
<p><strong>Ranking SVM</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/svm.jpg" class="ful-image" alt="svm.jpg"></p>
<p><strong>Properties of Ranking SVM</strong></p>
<ul>
<li>泛化能力强</li>
<li>Kernels 可以用来提高准确率<br>– linear kernels 往往效果不错</li>
<li>继承了 SVM 的优势<br>– 有许多开源工具<br>– 有很多关于优化的研究<br>– 训练速度快<br>– 有理论保证</li>
</ul>
<p><strong>Pairwise方法主要包括以下几种算法：</strong><br>Learning to Retrieve Information (SCC 1995),<br>Learning to Order Things (NIPS 1998),<br>Ranking SVM (ICANN 1999),<br>RankBoost (JMLR 2003), LDM (SIGIR 2005),<br>RankNet (ICML 2005), Frank (SIGIR 2007),<br>MHR(SIGIR 2007),<br>Round Robin Ranking (ECML 2003),<br>GBRank (SIGIR 2007),<br>QBRank (NIPS 2007),<br>MPRank (ICML 2007),<br>IRSVM (SIGIR 2006) 。</p>
<p><strong>Pairs</strong><br>在相关文档和不相关文档间能有很好的平衡的 queries 在训练数据中占主导地位</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Number of pairs = |R| × |NR|</div><div class="line">q1 = 5R × 5NR = 25 pairs (10 documents: 5R,5NR)</div><div class="line">q2 = 9R × 1NR = 9 pairs (10 documents: 9R,1NR)</div><div class="line">q3 = 2R × 8NR = 16 pairs (10 documents: 2R,8NR)</div></pre></td></tr></table></figure>
<p>相比于 Pointwise 方法，Pairwise 方法通过考虑两两文档之间的相对相关度来进行排序，有一定的进步。然而，因为一个 label 会产生很多的 training instances，所以 pairwise approach 容易受到 noisy labels 的影响。另外，Pairwise 使用的这种基于两两文档之间相对相关度的损失函数，和真正衡量排序效果的一些指标之间，可能存在很大的不同，有时甚至是负相关。</p>
<p>另外，有的Pairwise方法没有考虑到排序结果前几名对整个排序的重要性，也没有考虑不同查询对应的文档集合的大小对查询结果的影响(但是有的Pairwise方法对这些进行了改进，比如 IR SVM 就是对 Ranking SVM 针对以上缺点进行改进得到的算法)。</p>
<h3 id="Listwise-L2R"><a href="#Listwise-L2R" class="headerlink" title="Listwise L2R"></a>Listwise L2R</h3><p>与 Pointwise 和 Pairwise 方法不同，Listwise 方法直接考虑给定查询下的文档集合的整体序列，直接优化模型输出的文档序列，使得其尽可能接近真实文档序列。</p>
<p><strong>Approach:</strong> 用文档序列来训练模型<br>Training data: x1 &gt; x2 &gt; … &gt; xn<br>Learned model: h (x1) &gt; h (x2) &gt; …</p>
<p><strong>Loss function</strong><br>Some metric over the ranking<br>E.g., NDCG@n, with n=1, 3, 5, 10, … – E.g., MAP@n</p>
<p><strong>Goal: Maximize the value of the metric</strong><br>Directly align the model with the ranking target</p>
<p>E.g.,<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/likewise.jpg" class="ful-image" alt="likewise.jpg"></p>
<p>直接优化 metrics 很难，因为一些常用的 metrics (e.g., NDCG@n) 不连续或者不是凸函数。</p>
<p>Two common <strong>strategies</strong> in listwise approaches:</p>
<ol>
<li>找另一个直观且易于优化的指标<br>– E.g. likelihood of ‘best’ rankings in training data</li>
<li>用 approximation 直接优化 evaluation metrics, with</li>
</ol>
<p>Listwise 算法主要包括以下几种算法：LambdaRank (NIPS 2006), AdaRank (SIGIR 2007), SVM-MAP (SIGIR 2007), SoftRank (LR4IR 2007), GPRank (LR4IR 2007), CCA (SIGIR 2007), RankCosine (IP&amp;M 2007), ListNet (ICML 2007), ListMLE (ICML 2008) 。</p>
<p>相比于 Pointwise 和 Pairwise 方法，Listwise 方法直接优化给定查询下，整个文档集合的序列，所以比较好的克服了以上算法的缺陷。Listwise 方法中的 LambdaMART(是对 RankNet 和 LambdaRank 的改进)在 Yahoo Learning to Rank Challenge 表现出最好的性能。</p>
<h4 id="ListMLE-Listwise-Maximum-Likelihood-Estimation"><a href="#ListMLE-Listwise-Maximum-Likelihood-Estimation" class="headerlink" title="ListMLE(Listwise Maximum Likelihood Estimation)"></a>ListMLE(Listwise Maximum Likelihood Estimation)</h4><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/listMLE1.jpg" class="ful-image" alt="listMLE1.jpg">
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Learning%20to%20Rank/listMLE2.jpg" class="ful-image" alt="listMLE2.jpg">
<p>直接优化 metric of interest，然而很难做到，因为一些指标不连续或者不可微，如基于位置的 metrics</p>
<p><strong>Simpler possibilities:</strong></p>
<ul>
<li>优化 metric 的 approximation</li>
<li>约束目标函数</li>
<li>直接优化目标函数(不能保证结果)</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><strong>Pointwise</strong> 是三种方法里最弱的</p>
<p><strong>Pairwise</strong> 和 <strong>listwise</strong> 几乎同样有效</p>
<ul>
<li>Pairwise 有一个不完美的学习目标，但是容易实现<br>– 最小化 pairwise errors, 但我们想要的是最好的 ranking<br>－ 有理论保证的一个简单化的学习模型</li>
<li>Listwise 有一个完美的学习目标，但是更难实现<br>– 学习目标与我们想要的完全相同<br>－ 然而很难学习</li>
</ul>
<p><strong>Relative effectiveness:</strong><br> Listwise ≈ Pairwise &gt; Pointwise</p>
<p>许多 ML 算法使用 pointwise &amp; pairwise LeToR，因为易于开发，也比较有效。Listwise 算法可能最终更有效，然而成熟的解决方案比较少，现在仍然是一个开放的研究主题。</p>
<h2 id="效果评价"><a href="#效果评价" class="headerlink" title="效果评价"></a>效果评价</h2><p>L2R 是用机器学习的方法来进行排序，所以评价 L2R 效果的指标就是评价排序的指标，主要包括一下几种：</p>
<ol>
<li>WTA(Winners take all) 对于给定的查询 q，如果模型返回的结果列表中，第一个文档是相关的，则 WTA(q)=1，否则为0.</li>
<li>MRR(Mean Reciprocal Rank) 对于给定查询 q，如果第一个相关的文档的位置是 R(q)，则 MRR(q)=1/R(q)。</li>
<li>MAP(Mean Average Precision) 对于每个真实相关的文档 d，考虑其在模型排序结果中的位置 P(d)，统计该位置之前的文档集合的分类准确率，取所有这些准确率的平均值。</li>
<li>NDCG(Normalized Discounted Cumulative Gain) 是一种综合考虑模型排序结果和真实序列之间的关系的一种指标，也是最常用的衡量排序结果的指标，详见 Wikipedia。</li>
<li>RC(Rank Correlation) 使用相关度来衡量排序结果和真实序列之间的相似度，常用的指标是 Kendall’s Tau。</li>
</ol>
<blockquote>
<p>参考链接<br><a href="http://boston.lti.cs.cmu.edu/classes/11-642/Papers/clarke-11.7.pdf" target="_blank" rel="external">Learning to rank</a><br><a href="http://boston.lti.cs.cmu.edu/classes/11-642/Papers/li-ieice-tis-11.pdf" target="_blank" rel="external">A Short Introduction to Learning to Rank</a><br><a href="http://www.cnblogs.com/kemaswill/archive/2013/06/01/3109497.html" target="_blank" rel="external">Learning to Rank 简介</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[项目实战--搜索引擎]]></title>
      <url>http://www.shuang0420.com/2016/10/22/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的 project。<br><a id="more"></a></p>
<h1 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>数据：ClueWeb09 dataset，共 553,202 篇文档，用 Lucene 建立的索引。<br>部分框架是现成的，有 <a href="http://boston.lti.cs.cmu.edu/classes/11-642/HW/doc/" target="_blank" rel="external">api 文档</a><br>我们要做的是实现部分 operator 以及 ranking algorithm。</p>
<p>实现一个个性化的搜索引擎，具有以下能力：</p>
<ul>
<li>diversification</li>
<li>query expansion</li>
<li>learning to rank</li>
</ul>
<p><strong>支持的 operator:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#OR, #AND, #SYN, #NEAR/n, #WINDOW/n, #SUM #AND, #WAND, #WSUM, #WINDOW</div></pre></td></tr></table></figure></p>
<p><strong>支持的 fields:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&apos;url&apos;, &apos;keywords&apos; , &apos;title&apos;, &apos;body&apos;, &apos;inlink&apos;</div></pre></td></tr></table></figure></p>
<p><strong>支持的 ranking algorithm</strong><br>Unranked/Ranked boolean, Okapi BM25, Indri, Le2R(use SVM), diversification algorithm(xQuAD &amp; PM25), and etc.</p>
<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>程序输入: one parameter (name of parameter file)</p>
<p>parameter file 必须包括以下参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">- queryFilePath= The path to the query file.</div><div class="line">- indexPath= The path to the Lucene index directory. Typically this will be something like &quot;indexPath=index&quot;.</div><div class="line">- trecEvalOutputPath= The path to the file where your software will write its output for trec_eval.</div><div class="line">- retrievalAlgorithm= &quot;UnrankedBoolean&quot; ／ &quot;RankedBoolean&quot; ／ &quot;BM25&quot; / &quot;Indri&quot;</div></pre></td></tr></table></figure></p>
<p><strong>可选参数</strong><br>用于 “BM25” / “Indri” 模型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">- BM25:k_1= Values are real numbers &gt;= 0.0.</div><div class="line">- BM25:b= Values are real numbers between 0.0 and 1.0.</div><div class="line">- BM25:k_3= Values are real numbers &gt;= 0.0.</div><div class="line">- Indri:mu= Values are integers &gt;= 0.</div><div class="line">- Indri:lambda= Values are real numbers between 0.0 and 1.0</div></pre></td></tr></table></figure></p>
<p>用于 query expansion。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">- fb= Acceptable values are &quot;true&quot; and &quot;false&quot;. This value controls whether query expansion is performed (fb=true).</div><div class="line">- fbDocs= Acceptable values are integers &gt; 0. This value determines the number of documents to use for query expansion.</div><div class="line">- fbTerms= Acceptable values are integers &gt; 0. This value determines the number of terms that are added to the query.</div><div class="line">- fbMu= Acceptable values are integers &gt;= 0. This value determines the amount of smoothing used to calculate p(r|d).</div><div class="line">- fbOrigWeight= Acceptable values are between 0.0 and 1.0. This value determines the weight on the original query. The weight on the expanded query is (1-fbOrigWeight).</div><div class="line">- fbInitialRankingFile= The value is a string that contains the name of a file (in trec_eval input format) that contains an initial document ranking for the query.</div><div class="line">- fbExpansionQueryFile= The value is a string that contains the name of a file where your software must write its expansion query. The file format is described below.</div></pre></td></tr></table></figure></p>
<p>用于 Le2R:（解释待修正）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">letor:trainingQueryFile= HW4-train.qry.txt</div><div class="line">letor:trainingQrelsFile= HW4-train.qrel.txt</div><div class="line">letor:trainingFeatureVectorsFile= HW4-train.vec.txt</div><div class="line">letor:pageRankFile= HW4.pk.txt</div><div class="line">letor:svmRankLearnPath= svm_rank/svm_rank_learn</div><div class="line">letor:svmRankClassifyPath= svm_rank/svm_rank_classify</div><div class="line">letor:svmRankParamC= 0.001</div><div class="line">letor:svmRankModelFile= HW4-svm.model.txt</div><div class="line">letor:testingFeatureVectorsFile= HW4-test.vec.txt</div><div class="line">letor:testingDocumentScores= HW4-test.rank.txt</div></pre></td></tr></table></figure></p>
<p>用于 diversification:（解释待修正）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">diversity=true</div><div class="line">diversity:maxInputRankingsLength=100</div><div class="line">diversity:maxResultRankingLength=50</div><div class="line">diversity:algorithm=xquad</div><div class="line">diversity:intentsFile=q.intents.txt</div><div class="line">diversity:lambda=1</div></pre></td></tr></table></figure></p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>程序输出：<br>在 trecEvalOutputPath 指定的文件中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">QueryID	Q0	DocID	                    Rank	Score	RunID</div><div class="line">10	    Q0	clueweb09-enwp03-35-1378	1	    16	  run-1</div><div class="line">10	    Q0	clueweb09-enwp00-78-1360	2	    11	  run-1</div><div class="line">10	    Q0	clueweb09-enwp00-67-0958	3	    9	    run-1</div><div class="line">:	      :	  :	                        :	    :	    :</div><div class="line">11	    Q0	clueweb09-enwp00-63-1141	1	    18	  run-1</div></pre></td></tr></table></figure></p>
<p>如果有 query expansion，在 fbExpansionQueryFile 指定的文件中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">qid1: query1</div><div class="line">qid2: query2</div><div class="line"> :      :</div><div class="line"></div><div class="line">eg.</div><div class="line">1: #wand (0.73 obama 0.43 family 0.40 white 0.65 tree 0.33 politics ...)</div><div class="line">2: #wand (0.69 french 0.83 lick 0.76 indiana ...)</div></pre></td></tr></table></figure></p>
<h1 id="基本策略"><a href="#基本策略" class="headerlink" title="基本策略"></a>基本策略</h1><p><strong>要求</strong></p>
<ul>
<li>从 query file 中逐条读取 query</li>
<li>将 query parse 为 query tree，internal nodes 是 operators，leaves 是 index terms<ul>
<li>如果一个 query 没有 explicit operator，默认为 #OR</li>
<li>如果一个 query 没有 explicit field，默认为 body</li>
<li>对 query term 进行 stemming 和 stopwords 处理</li>
</ul>
</li>
<li>评估 query，用 <a href="http://www.shuang0420.com/2016/09/11/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Query%20Processing/">DAAT 策略</a>，对 leaf node 的 evaluation 就是如果这个 term 的 inverted list 存在，就获取它，注意有些 query term 是没有 inverted list 的。</li>
<li>对所有文档按文档分数降序排序，如果分数相同，按 external document id 升序排序。</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/query_tree.jpg" class="ful-image" alt="%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/query_tree.jpg">
<h1 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h1><p>不同模型支持的 operator 各有不同<br>系统需要支持的 Operator 有 #OR, #AND, #SYN, #NEAR/n, #WINDOW/n, 对 BM25 模型来说，还需要支持 #SUM，对 Indri 模型来说，还需要支持 #AND, #WAND, #WSUM, #WINDOW</p>
<h1 id="Fields"><a href="#Fields" class="headerlink" title="Fields"></a>Fields</h1><p>系统支持的 fields 有 ‘url’, ‘keywords’ (from the html ‘meta’ tag), ‘title’, ‘body’, 和 ‘inlink’ 5 种，query 形式为 apple.title。</p>
<h1 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#Operator( term_1.field term_2.field ... term_n.field )</div><div class="line">apples</div><div class="line">#AND (apple bananas)</div><div class="line">#OR (apple bananas)</div><div class="line">#NEAR/3 (apple pie)</div><div class="line">#NEAR/5 (pie apple)</div></pre></td></tr></table></figure>
<h1 id="排序模型"><a href="#排序模型" class="headerlink" title="排序模型"></a>排序模型</h1><h2 id="Exact-match"><a href="#Exact-match" class="headerlink" title="Exact-match"></a>Exact-match</h2><p>Boolean retrieval 需要支持的 Operator 有 #OR, #AND, #SYN, #NEAR/n, #WINDOW/n</p>
<ul>
<li>#OR 只要有一个 query term 在文档中出现，就算 match，在 ranked boolean retrieval 中分数为所有匹配的 query term 的 tf 的最大值。</li>
<li>#AND 只有在所有 query term 都在文档中出现时，才算 match，在 ranked boolean retrieval 中分数为所有 query term 的 tf 的最小值。</li>
<li>#NEAR/n 如果每对相邻两个 query term 之间的距离小于 n，才算 match，在 ranked boolean retrieval 中分数为 match 的次数。（For example, #NEAR/2(a b c) matches “a b c”, “a x b c”, “a b x c”, and “a x b x c”, but not “a x x b c”）。</li>
<li>#WINDOW/n 和 #NEAR/n 类似，但是不要求顺序。</li>
</ul>
<p><a href="http://www.shuang0420.com/2016/09/06/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Exact-match%20retrieval/">Search Engines笔记 - Exact-match retrieval</a></p>
<h3 id="Unranked-boolean-retrieval"><a href="#Unranked-boolean-retrieval" class="headerlink" title="Unranked boolean retrieval"></a>Unranked boolean retrieval</h3><p>对每个文档来说，如果 match，分数为 1，不 match 就为 0。</p>
<h3 id="Ranked-boolean-retrieval"><a href="#Ranked-boolean-retrieval" class="headerlink" title="Ranked boolean retrieval"></a>Ranked boolean retrieval</h3><p>每个文档的分数是 query term 在该文档中的 term frequency。</p>
<h2 id="Best-match"><a href="#Best-match" class="headerlink" title="Best-match"></a>Best-match</h2><p><a href="http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/">Search Engines笔记 - Best-Match</a></p>
<h3 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h3><p>需要支持的 Operator 有 #SYN, #NEAR/n, #SUM</p>
<h3 id="Indri"><a href="#Indri" class="headerlink" title="Indri"></a>Indri</h3><p>需要支持的 Operator 有 #AND(Indri #and), #WAND, #WSUM, #WINDOW。默认的 operator 是 #AND，注意这里的 #AND 和 boolean retrieval 中的算法不一样。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/indri_operator.jpg" class="ful-image" alt="%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/indri_operator.jpg"></p>
<h1 id="Query-expansion"><a href="#Query-expansion" class="headerlink" title="Query expansion"></a>Query expansion</h1><p>基本逻辑是把 initial query 当做 classifier，用它来 label 部分 data，得到 top-ranked documents，然后用 labeled data 来产生更优的 classifier。基本过程：</p>
<ul>
<li>用原始 query 检索文档</li>
<li>取结果的前 N 篇文档作为训练集，这些文档相关度可能不高，然而我们的目的是学习 vocabulary pattern。</li>
<li>应用 relevance feedback algorithm 选取 term 和 term weight</li>
<li>组成新的 query 来检索文档</li>
</ul>
<p>见 <a href="http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/">Search Engines笔记 - Pseudo Relevance Feedback</a></p>
<h1 id="Diversification"><a href="#Diversification" class="headerlink" title="Diversification"></a>Diversification</h1><p>具体见 <a href="http://www.shuang0420.com/2016/12/07/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Diversity/">Search Engines笔记 - Diversity
</a></p>
<p><a href="http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/">Search Engines笔记 - Pseudo Relevance Feedback</a></p>
]]></content>
      
        <categories>
            
            <category> Projects </category>
            
        </categories>
        
        
        <tags>
            
            <tag> mysql </tag>
            
            <tag> hbase </tag>
            
            <tag> webserver </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Cache]]></title>
      <url>http://www.shuang0420.com/2016/10/15/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Cache/</url>
      <content type="html"><![CDATA[<p>Web traffic is highly skewed，我们可以通过缓存提高 performance。缓存内容可以是 query, result page, inverted list。 <a id="more"></a></p>
<h1 id="Caching-of-Popular-Results"><a href="#Caching-of-Popular-Results" class="headerlink" title="Caching of Popular Results"></a>Caching of Popular Results</h1><h2 id="Query-distribution"><a href="#Query-distribution" class="headerlink" title="Query distribution"></a>Query distribution</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/query_cache.png" alt=""><br>query rank 和 frequency 符合长尾分布。Top 25 queries 占了 1% 的流量。在 distinct queries 里，</p>
<ul>
<li>64% occur once</li>
<li>16% occur twice</li>
<li>7% occur three times</li>
<li>14% occur &gt;=3 times</li>
<li>average query frequency: 4</li>
</ul>
<h2 id="RAM-amp-DISK"><a href="#RAM-amp-DISK" class="headerlink" title="RAM &amp; DISK"></a>RAM &amp; DISK</h2><ol>
<li>给 query cache 分配 RAM<br>储存标准 queries，按字母顺序排列 term<br>1.6GB cache 储存 40 million queries (40 bytes/query)</li>
<li>给 result page cache 分配磁盘<br>一页 30KB uncompressed, 10KB compressed<br>400GB cache 可以存 40 million result pages</li>
<li>Cache misses use RAM only(very fast)</li>
<li>Cache hits use RAM+disk<br>比正常 evaluate query 要快<br>只用一台机器</li>
</ol>
<h2 id="RAM-only"><a href="#RAM-only" class="headerlink" title="RAM only"></a>RAM only</h2><ol>
<li>给 query cache 分配 RAM<br>储存标准 queries，按字母顺序排列 term<br>9MB cache 储存 300,000 queries</li>
<li>给 result page cache 分配 RAM<br>一页 30KB uncompressed, 10KB compressed<br>2.1GB cache 存 210,000 compressed result pages</li>
<li>Cache misses and hits only use RAM (very fast)<br>因为缓存的比较少，所以更多的 query 会被 miss</li>
<li>Could partition caches across multiple machines<br>需要更复杂的 design</li>
</ol>
<h2 id="Cache-size"><a href="#Cache-size" class="headerlink" title="Cache size?"></a>Cache size?</h2><p>Markatos 提出，30% 的 queries 会与缓存里的 query 匹配，然而增加 cache size 只能非常小幅度的提高 hit rate。见下图：<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/cache%20size.png" alt=""><br>根据 UK2007 的 query log，44% 的 query 只出现了一次，56% 的 query 出现了超过一次，cache 这 56% 里的 query 有助于提高 performance，然而并不能帮助 first occurrence of a query。</p>
<h1 id="Caching-Inverted-List"><a href="#Caching-Inverted-List" class="headerlink" title="Caching Inverted List"></a>Caching Inverted List</h1><p>根据 UK2007 的 query log，4% 的 query term 只出现了一次，96% 的 query term 出现不止一次，对 96% 里对 query term 进行 inverted list 的 cache 才是有用的。在每个 partition 上分配一部分 RAM (a few GB)给 inverted list。这里的重点在于 <strong>which terms should be cached?</strong> 两个原则：</p>
<ul>
<li>Terms that are frequent in a query log (improve the hit rage)</li>
<li>Terms that don’t have massive inverted lists (consume limited cache space)</li>
</ul>
<p>对 term 的排序： $$Score(t)={qtf(t) \over df(t)}$$。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Index Construction]]></title>
      <url>http://www.shuang0420.com/2016/10/14/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Index/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。这篇讲了搜索引擎中创建索引的主要原则、方法以及优化方案。 <a id="more"></a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/index_cons.png" alt=""></p>
<p><strong>Inverted list:</strong><br>索引包含了各种数据结构，如 term dictionary，inverted list 等。inverted list 是一个重点。搜索引擎所拥有的文档中出现的每一个单词都拥有一个 inverted list，记录这个单词在多少文档中出现，分别是哪些文档，每个文档分部出现多少次，分别出现在什么位置等信息。比如 Apple 这个词出现在文档 1，7，19，34，102。其中文档 1 中出现了3次，分别在位置 20，105，700。这样当搜索 Apple 时，搜索引擎就不用遍历所有的文档，只需要查找每个单词对应的 inverted list 就可以知道这个词在哪里出现了。</p>
<p><strong>文档预处理:</strong><br>创建索引是个巨大工程。首先是对文档进行解析和处理。互联网上的文档格式各种各样，对每一种格式的文档都要有一个对应的解析器程序，这样才能忽略各种奇怪符号，提取出有用内容。每一个解析器的实现都是一个繁琐且困难的任务。对于解析后的干净文档，许多重要的自然语言处理算法就要派上用场。以英语为例，需要进行分词（tokenization，将一句话分割成一个个单词），词干提取（stemming， 将文本中出现的单词还原成它的原型），part-of-speech tagging（识别单词在一句话中的词性），创建 n-gram 模型等操作。此外还需要识别文档中的命名实体(named entity)，比如将“iphone 6”作为一个词，而不是 “iphone” 一个， “6” 一个。上述操作生成的信息都要存储下来。这样构造 inverted list 时就可以知道每个单词出现的位置，出现个数等信息。</p>
<p><strong>生成索引:</strong><br>索引生成程序的一个设计目标就是高效。它要求被尽可能地运行在多个机器上。对于每个机器来说，索引程序一边扫描输入文档，一边在内存中更新索引的数据结构。当内存中得数据大小超过一定阀值时，这些内容被作为一个块(block)一次性写入硬盘文件中。当所有文档扫描结束后这些块会再被合并成一个大的 inverted file。因为每一个块都是排好序的，合并操作是线性的复杂度。因为数据量太大，可以用 MapReduce 把一个大的任务分割成许多小任务，并下发给多个 Mapper 程序，Mapper计算好的中间结果会发给多个 Reducer 程序继续处理，得到最终结果。这个计算模型允许成千上万台机器同时运算，从而极大提高了运算效率。</p>
<p>inverted list 要和访问机制(access mechanism)一起可以工作。访问机制定义了如何通过一个单词找到它所对应的 inverted list。大概可以使用两种数据结构：b-tree 或 Hash table。</p>
<p><strong>Big facts:</strong></p>
<ul>
<li>索引中的单词和文档都用 integer 的 ID 表示而不是字符串，省空间省时间。</li>
<li>一般来说 corpus 比 RAM 要大，不能在内存中完成整个任务，所以一定会有部分写到磁盘中，访问磁盘数据比访问内存数据慢得多，所以可以做的是<ul>
<li>只在必要的时候写入磁盘</li>
<li>压缩数据减少 I/O。数据从磁盘传输到内存是由系统总线而不是处理器来实现的，所以磁盘 I/O 时处理器仍然可以处理数据。</li>
<li>顺序读取（比 random access 快）。因为磁盘读写时，磁头移到数据所在的磁道有一段时间，大概 5ms，称为寻道时间，这段时间并不进行数据的传输，所以连续读取的数据应该连续存放来节省时间。</li>
</ul>
</li>
</ul>
<p><strong>索引更新:</strong><br>互联网内容是不停变化的，这必然导致索引不停被更新。然而建立好的索引中，各个单词的反转列表是紧密的拼接在一起的，这使得更新变得非常困难。通常搜索引擎会积攒一批文件后才进行索引的更改，并且把索引分成静态和动态两个部分。程序把所有更改都写入动态部分，并且周期性地将动态部分合并进静态部分中。搜索时，动态和静态部分都会被访问。当从索引中删除一个文档时，这个文档中出现的词对应的反转列表都会被修改，开销极大。于是程序加入了“删除列表（delete lists）”来记录所有被删除的文档。搜索时会查询删除列表来把已经被删除的文档从搜索结果中移除。当删除列表足够大，垃圾回收机制会被触发，重新生成索引。</p>
<h1 id="Single-Processor-单机版"><a href="#Single-Processor-单机版" class="headerlink" title="Single Processor(单机版)"></a>Single Processor(单机版)</h1><h2 id="Block-sort-based-indexing-BSBI"><a href="#Block-sort-based-indexing-BSBI" class="headerlink" title="Block sort-based indexing(BSBI)"></a>Block sort-based indexing(BSBI)</h2><p>基于块的排序索引方法(Block sort-based indexing algorithm) 过程如下:</p>
<ol>
<li>将 corpus 分割成几个大小相等的部分</li>
<li>对每个部分的 (termId,docId)排序</li>
<li>一旦 in-memory buffer 满了，就把临时排序结果 flush 到磁盘中，然后重新初始化，重复2、3过程</li>
<li>将所有的中间文件合并成最终索引。(merge index blocks on disk)</li>
</ol>
<img class="ful-image" alt="block%20merge%20step">
<p>BSBI 的时间复杂度是 O(TlogT)</p>
<h2 id="Single-pass-in-memory-indexing-SPIMI"><a href="#Single-pass-in-memory-indexing-SPIMI" class="headerlink" title="Single-pass in-memory indexing(SPIMI)"></a>Single-pass in-memory indexing(SPIMI)</h2><p>BSBI 需要将 term 映射成 id，对大规模的 corpus 来说，这种数据结构会很大以致在内存中难以存放，SPIMI 使用 term 本身，将每个块的词典写入磁盘，对于下一个块则重新采用新的词典，这样带来的好处是，只要硬盘空间足够大，SPIMI 就能索引任何大小的 corpus。 算法如下，反复调用 SPIMI-INVERT 函数直到将全部 corpus 处理完。token_stream 就是 term-docid stream。 <img src="http://7xu83c.com1.z0.glb.clouddn.com/SPIMI.png" alt=""></p>
<p>BSBI 和 SPIMI 的一个区别是， SPIMI 直接在 inverted list 中增加一项，这个 inverted list 是动态增长对，大小会不断调整，而 BSBI 一开始就整理出所有的 termID-docID 并对它们进行排序。这样做的好处是：</p>
<ul>
<li>不需要进行排序，处理速度更快</li>
<li>保留 inverted list 对 term 的归属关系，能节省内存，也不用保存 term id，所以每次单独的 SPIMI-INVERT 调用能够处理的块可以非常大，整个的索引构建过程也会因此非常高效。</li>
</ul>
<p>SPIMI 的空间复杂度是 O(T)。</p>
<h1 id="Distributed-indexes"><a href="#Distributed-indexes" class="headerlink" title="Distributed indexes"></a>Distributed indexes</h1><h2 id="Size-of-web-search-engine-index"><a href="#Size-of-web-search-engine-index" class="headerlink" title="Size of web search engine index"></a>Size of web search engine index</h2><p>一些假设：</p>
<ul>
<li>网页数：500亿（2013年），假设 50％ 是 text 文本</li>
<li>平均网页大小：37K（2013年）</li>
<li>假设 non-text 网页的平均入链数：1K</li>
<li>索引大小约为原始文本大小的 20％</li>
</ul>
<p>Text: 25billion <em> 37K + 25billion </em> 1K = 925TB<br>Index: 20% * 950TB = 185TB (call it 200TB for convenience)</p>
<p>–&gt; 索引分布在 50 个 4TB 的磁盘驱动器上比较合理</p>
<h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><p>一个 computer cluster，又叫做 rack，有 40-80 台机器，每个 rack 有自己内部的网络，对大公司像 google 而言，机器的选择遵循的原则是：</p>
<ul>
<li>越便宜越好</li>
<li>每台计算机使用少量的普通磁盘</li>
<li>每台计算机使用比较大(not huge)的 RAM</li>
</ul>
<p>因为一台机子坏了得立刻换一台机子上去，自动部署，随时投入使用。而对于小的组织像 cmu，机子就会买好一点的，一台坏了会去修，而不是直接换一台。</p>
<h2 id="Partitioned-indexes"><a href="#Partitioned-indexes" class="headerlink" title="Partitioned indexes"></a>Partitioned indexes</h2><p>分布式索引用到了 sharding 和 replication 的原理。</p>
<h3 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h3><p>index 通常是被切片(sharding)的，每个分区包含了一堆不重复的文档集合，每个分区都被分到了一台机器，根据之前对索引大小的估计，就有 25 个分区(2 disks/node, 4TB/disk =&gt; 200TB) 。 那么 <strong>corpus 会怎样被分区呢？</strong> 可以随机分配(random assignment)，也可以按来源（source-based assignment），总的来说，随机分配用的比较多，因为随机可以平衡不同分区的 query traffic，让每台机得到充分使用。</p>
<h3 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h3><p>索引通常被存了好几份 copy(replication)，为了提高并行能力和容错能力。 所以一个 index server 标准的配置：</p>
<ul>
<li>40 machines in a rack</li>
<li>2*4TB disks/machine</li>
<li>320 TB of index/rack</li>
</ul>
<h3 id="Query-evalution"><a href="#Query-evalution" class="headerlink" title="Query evalution"></a>Query evalution</h3><p>分布式系统的 query 评估过程为</p>
<ul>
<li>从每个分区中找出一台机器。 (select a machine for each index partition)</li>
<li>把 query 分配到选出的机器中，然后每台机器返回一个 ranked list of matches。 (broadcast the query to each selected machine)</li>
<li>一个 aggregator 将这些 ranked list 合并(merge-sort)成最终的有序文档集合。(an aggregator assembles them into a final ranked list of doc ids)</li>
<li>其它机器对每个结果来寻找 title, urls, etc.。（other machines looks up titles, URLs, etc., for each result, a similar partitioning/pooling strategy is used for documents）</li>
</ul>
<h2 id="Tiered-indexes"><a href="#Tiered-indexes" class="headerlink" title="Tiered indexes"></a>Tiered indexes</h2><p>另一种分布式的 index 是将 web page 进行分层，10% 为 tier 1，是高价值的网页，其余 90% 是 tier 2，是低价值的网页。query 过来我们先从 tier 1 找，如果 good results 不够，再往 tier 2 找。 所以问题来了，<strong>怎么找 top tier(s)？</strong></p>
<ul>
<li>page rank 较高的网页，或来自 page rank 较高的网站的网页</li>
<li>对之前的一些常见 query 非常重要的网页（排名高，点击率高，停留时间长）</li>
<li>网址较短的网页（更可能是主页）</li>
<li>spam 分数较低的网页</li>
</ul>
<p>Tiered index 的优势如下：</p>
<ul>
<li>降低了大多数 query 的搜索成本，一个完整的搜索过程比 tier 1 的搜索要找 10x 的机器。</li>
<li>提高了大多数 query 的搜索质量，因为它更关注 “good” pages.</li>
</ul>
<p><strong>什么情况下会去找 tier 2?</strong></p>
<ul>
<li>匹配 query 的 Tier 1 的网页太少</li>
<li>query 非常少见</li>
</ul>
<h2 id="Index-Construction"><a href="#Index-Construction" class="headerlink" title="Index Construction"></a>Index Construction</h2><p>建立分布式索引用的框架是 MapReduce，基本过程是 Input reader –&gt; Map –&gt; Combine –&gt; Shuffle –&gt; Reduce。<br>从最基本的 binary inverted list 进行示范，format 是 (term,[docids]) 或者 (term,[docid,docid,…])，注意这里的 docid 是 internal document id（转化成了 integer） 而不是原来的 id。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>每个 Map task 相当于一个 document parser</p>
<ul>
<li>input: a stream of documents</li>
<li>output: a stream of (term,docid) tuples<br>eg. (men,1)(and,1)(women,1)…(once,2)(upon,2)</li>
</ul>
<h3 id="Shuffle／Sort"><a href="#Shuffle／Sort" class="headerlink" title="Shuffle／Sort"></a>Shuffle／Sort</h3><p>Shuffle 的过程相当于 route tuples 到 Reducers 里。在 Shuffle/Sort 中，都是 shuffle/sort by key，而不是 by value。</p>
<ul>
<li>input: (t5,docid1)(t1,docid3)(t1,docid1)…</li>
<li>output: (t1,docid3)(t1,docid1)(t5,docid1)…</li>
</ul>
<h3 id="Redcuer"><a href="#Redcuer" class="headerlink" title="Redcuer"></a>Redcuer</h3><p>Reducer 的作用就是将 stream of keys 转化成 streams of inverted lists。Reducer 会 sort values 也就是 docids，然后建立 inverted list，这里要保证的是最长的 inverted list 必须能够 fit in memory。</p>
<ul>
<li>input: (men,1)(men,127)(men,49)(men,23)…</li>
<li>ouput: (men,[df:492,docids:1,23,49,127,…])</li>
</ul>
<h3 id="Improvement"><a href="#Improvement" class="headerlink" title="Improvement"></a>Improvement</h3><p>这个流程下来的效率并不高，因为文档里所有 unique term 都会产生一个 tuple，像 WSJ’87-92 (533 MB of text) 就会产生 20 million 的 tuple，每个 tuple 都会 shuffle 到 reducers 里，进 reducer 前还要先 sort，这个过程特别耗时。所以我们会用 Combiner 来提高效率。Combiner 的作用和 reducer 差不多，不过它是在每个 mapper 里进行的，它把每个 mapper 里的 docid 先进行了合并。(t1,docid1)(t2,docid1)(t4,docid2)…-&gt;(t1,[docid1,docid18,…])。这样的好处是需要 shuffle 的 tuple 更少，需要 hash 的 key 更少，需要进行 movement operation 的数据也更少，另外，需要 reduce 的 tuple 更少，需要 sort 的 tuple 也更少。<br>改进后的框架如下：</p>
<ol>
<li><strong>Map:</strong><br>$(docid_1,content_1)$ -&gt; $(t_1,ilist_{1,1})(t_2,ilist_{2,1})(t_3,ilist_{3,1})$</li>
<li><strong>Combine:</strong><br>Sort by t &amp; combine $(t_1 [ilist_{1,2} ilist_{1,3} ilist_{1,1},…])$-&gt;$(t_1,ilist_{1,27})$<br>每个 output inverted list 包含了一系列文档</li>
<li><strong>Shuffle by t</strong></li>
<li><strong>Sort by t</strong><br>$(t_4 ilist_{4,1}) (t_1 ilist_{1,3})$-&gt;$(t_1,ilist_{1,2})(t_1,ilist_{1,4})(t_4,ilist_{4,1})$</li>
<li><strong>Reduce</strong><br>$(t_1 [ilist_{1,2} ilist_{1,1} ilist_{1,4},…])$-&gt;$(t_1,ilist_final)$</li>
</ol>
<p>$ilist_{i,j}$: the j’th inverted list fragment for term i</p>
<p>注意每个 reducer 里的 inverted list 都是完整的，每个 reducer 相当于存了个 result block，每个 block 包括不同的 term，每个 term 只在一个 block 里出现。</p>
<p>如果要创建 partitioned inverted list，只用在 key 里加上一个 partition id 即可。</p>
<ol>
<li><strong>Map:</strong><br>$(docid_1,content_1)$ -&gt; $([p,t_1],ilist_{1,1})([p,t_2],ilist_{2,1})([p,t_3],ilist_{3,1})$</li>
<li><strong>Combine:</strong><br>Sort by t &amp; combine $([p,t_1] [ilist_{1,2} ilist_{1,3} ilist_{1,1},…])$-&gt;$([p,t_1],ilist_{1,27})$<br>每个 output inverted list 包含了一系列文档</li>
<li><strong>Shuffle by p</strong></li>
<li><strong>Sort by [p,t]</strong><br>$([p,t_4] ilist_{4,1}) ([p,t_1] ilist_{1,3})$-&gt;$([p,t_1],ilist_{1,2})([p,t_1],ilist_{1,4})([p,t_4],ilist_{4,1})$</li>
<li><strong>Reduce</strong><br>$([p,t_1] [ilist_{1,2} ilist_{1,1} ilist_{1,4},…])$-&gt;$([p,t_1],ilist_final)$</li>
</ol>
<h1 id="Inverted-list-compression"><a href="#Inverted-list-compression" class="headerlink" title="Inverted list compression"></a>Inverted list compression</h1><p>概念上来讲 inverted list 看起来像一个 object</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># apple</div><div class="line">df: 4356</div><div class="line">docid: 42</div><div class="line">tf: 3</div><div class="line">locs: 14</div><div class="line">      83</div><div class="line">      157</div><div class="line">      94</div><div class="line">docid: 94</div><div class="line">...</div></pre></td></tr></table></figure>
<p>而实际上它在磁盘中只是一串数字</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">4356</div><div class="line">42</div><div class="line">3</div><div class="line">14</div><div class="line">83</div><div class="line">157</div><div class="line">94</div></pre></td></tr></table></figure>
<p>通常 intered list 会被压缩，目的不同，选择的压缩方法也就不同。要节省空间我们就用 aggressive compression algorithms，要节省时间我们就用 simple compression algorithms。现在我们最主要的目的是节省 query 时间，用的压缩算法主要有</p>
<ul>
<li>Gap encoding</li>
<li>Restricted variable-length(RVL) encoding</li>
</ul>
<h2 id="Delta-Gap"><a href="#Delta-Gap" class="headerlink" title="Delta Gap"></a>Delta Gap</h2><p>Delta Gap 的基本思想是保存数字的差值而不是数字本身，意义在于</p>
<ul>
<li>增加较小的数字的概率</li>
<li>更 skewed 的分布</li>
<li>降低信息熵</li>
</ul>
<h2 id="Variable-Byte-Encoding"><a href="#Variable-Byte-Encoding" class="headerlink" title="Variable Byte Encoding"></a>Variable Byte Encoding</h2><p>Variable Byte Encoding 存了一串 bytes，每个 byte 由开头 1 位 flag 和 7 位的 payload（the number）组成。flag 为 0，表示不是最后一个 byte，flag 为 1 表示这是最后一个 byte。通过连接 payload 来重建 number。</p>
<p>好处是编码和解码的效率都很高，可以找到第 n 位数字而不用 decode 之前的数字。</p>
<p>Example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[0..2^7-1]:      1 byte : 1xxxxxxx</div><div class="line">[2^7...2^14-1]:  2 bytes: 0xxxxxxx1xxxxxxx</div><div class="line">[2^14...2^21-1]: 3 bytes: 0xxxxxxx0xxxxxxx1xxxxxxx</div><div class="line">...</div><div class="line"></div><div class="line">Decimal: 5</div><div class="line">Binary: 00000000 00000000 00000000 00000101</div><div class="line"># 照抄最后7位，第一位补上1</div><div class="line">Compressed: 10000101</div><div class="line"></div><div class="line">Decimal: 127</div><div class="line">Binary: 00000000 00000000 00000000 01111111</div><div class="line"># 照抄最后7位，第一位补上1</div><div class="line">Compressed: 11111111</div><div class="line"></div><div class="line">Decimal: 128</div><div class="line">Binary: 00000000 00000000 00000000 10000000</div><div class="line"># 照抄最后7位，第一位补上0，再往前找7位，照抄，第一位补上1</div><div class="line">Compressed: 00000000 10000001</div><div class="line"></div><div class="line">Decimal: 131</div><div class="line">Binary: 00000000 00000000 00000000 10000011</div><div class="line"># 照抄最后7位，第一位补上0，再往前找7位，照抄，第一位补上1</div><div class="line">Compressed: 00000011 10000001</div></pre></td></tr></table></figure>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>最高效的压缩算法比 variable byte encoding 节省 15%－20% 的空间，但是比 restricted variable length encoding 要慢。注意我们这里要把握的原则是 <strong>“Disks are cheap, and speed is important”</strong>，所以 Restricted variable length compression 还是非常通用的。</p>
<p>压缩不包含地址信息的 inverted file，所用空间是 original text 的 10%，压缩包含地址信息的 inverted file，所用空间是 original text 的 15%-20%。</p>
<h1 id="Inverted-list-Optimization"><a href="#Inverted-list-Optimization" class="headerlink" title="Inverted list Optimization"></a>Inverted list Optimization</h1><h2 id="Skip-lists"><a href="#Skip-lists" class="headerlink" title="Skip lists"></a>Skip lists</h2><p>我们可以跳过一些文档来减少 I/O，减少计算。</p>
<h3 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h3><p>#NEAR,#WINDOW,#SYN,Boolean AND，skip lists 在这些 operator 中会非常有效。回顾 #NEAR 的算法，假设 query 是 #NEAR/3(a b)，包含 a 的第一个 docid 是 59356，包含 b 的第一个 docid 是 43，之前的做法是让 b 的 doc pointer 不断指向 next，直到 a,b 的pointer 指向同一篇文档，如果考虑 skip lists，就可以直接指向 a 的 docid，（调用 docIteratorAdvanceTo(doc_id_a)方法）。</p>
<h3 id="Score-calculation-Top-Docs"><a href="#Score-calculation-Top-Docs" class="headerlink" title="Score calculation (Top-Docs)"></a>Score calculation (Top-Docs)</h3><p>有些 inverted list 太长了，而大多 query 只需要返回 &lt;100 的文档，所以我们可以截取 inverted list 里 top docs 的部分，这样就能提高效率，代价是 更低的召回率。</p>
<p><strong>怎么找到 Top-Docs</strong></p>
<ul>
<li>tf</li>
<li>PageRank</li>
</ul>
<p><strong>怎么对 Top-Docs 排序</strong></p>
<ul>
<li>Order by doc id</li>
<li>Order by tf</li>
</ul>
<p><strong>How many terms are frequent enough to have a top-docs list?</strong><br>根据 <a href="http://www.shuang0420.com/2016/07/10/Tfidf总结笔记/">Zipf’s Law</a><br>$$Rank * Frequency = A * N$$</p>
<p>所以 ctf&gt;=800 的 term 大概占比 ${A*N/800 \over A * N}=1/800=0.125%$</p>
<p><strong>why 800?</strong><br>假设一个 inverted list 有5个 integer，没压缩就有 16 bytes，30%压缩比，压缩了有 5 bytes，linux filesystem page size是 4096 bytes, 所以有4096/5=819条 inverted list 能 fit in one page</p>
<p>假设 vocabulary 有 1,000,000 个 term，那么大概只有 1,250 个 top-docs lists，每个 list 大概 4-8KB，一共占 5-10 MB。</p>
<h2 id="Multiple-inverted-lists-per-term"><a href="#Multiple-inverted-lists-per-term" class="headerlink" title="Multiple inverted lists per term"></a>Multiple inverted lists per term</h2><p>有些 operator 并不需要 tf，像 unranked boolean operators， 有些 operator 并不需要 locations，像 #SUM,#WEIGHT,#AND,#OR,#ANDNOT,…，而 inverted lists with locations 会产生 I/O 浪费，对没有 location 的 inverted list，我们只用存 docid, tf 两个 integer，而对存了 location 的 inverted list,假定我们对每篇文档多用了 1.5 个 integer，那么我们其实浪费了 42% 的 I/O。 所以对于每个 term，我们可以存两份 inverted list，一份有 location，一份没有，对不需要 location 的 operator，我们就直接访问没有location 的 inverted list，这样就能避免不必要的 I/O，当然代价是额外的磁盘空间。</p>
<p><strong>我们要对每个 term 都存两个 inverted list 吗？</strong> 其实并不需要。因为大概只有 0.125% 对 term 有 topdocs/champion list，其它 term 对 inverted list 都很短。所以我们只用对 frequent terms 建两个 inverted list 就可以啦。</p>
<h1 id="Index-updates"><a href="#Index-updates" class="headerlink" title="Index updates"></a>Index updates</h1><p>corpus 并不是静态的，随着文档的增加，我们需要将新的 term 加入词典，对已有的 inverted list 进行更新，然而这个代价非常的大。最简单的索引更新方法是 <strong>周期性地对 corpus 进行索引重构</strong>，如果 corpus 更新次数不多，而且能接受新文档检索的一定延迟，也有足够资源支持建立新索引时让旧索引继续工作，那么周期性索引重构不失为一种好选择。 另外一种解决方法是保持两个索引：一个主索引，一个辅助索引，辅助索引用于存储新文档信息，保存在内存中，检索时可以同时遍历两个索引并将结果合并。如果有文档删除，可以把删除的 docid 记录在一个 delete list 里，在返回结果之前利用它过滤掉已经删除的文档。文档的更新通过先删除后重新插入实现。当辅助索引变得很大时，就将它合并到主索引中。</p>
<h1 id="Storing-document-structure"><a href="#Storing-document-structure" class="headerlink" title="Storing document structure"></a>Storing document structure</h1><h2 id="Treat-each-element-as-independent-of-other-elements"><a href="#Treat-each-element-as-independent-of-other-elements" class="headerlink" title="Treat each element as independent of other elements"></a>Treat each element as independent of other elements</h2><p>简单明了的结构，简单、高效，对 shallow structure 的 document 非常有效。 在这种结构下，我们分别保存每个 field 下的词汇，可以有一下两种形式</p>
<ul>
<li>FIELD::TERM</li>
<li>(FIELD,TERM)</li>
</ul>
<h2 id="Treat-elements-as-part-of-an-element-hierarchy"><a href="#Treat-elements-as-part-of-an-element-hierarchy" class="headerlink" title="Treat elements as part of an element hierarchy"></a>Treat elements as part of an element hierarchy</h2><p>$Document \supset Section \supset Subsection$ 非常灵活的结构，更好的符合用户需求，基本思想是 <strong>“Terms in “Subsection” should also appear in “Section””</strong> 对 complex structure 非常有效。</p>
<h3 id="Storing-fields-as-trees"><a href="#Storing-fields-as-trees" class="headerlink" title="Storing fields as trees"></a>Storing fields as trees</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/tree.png" alt=""></p>
<p>这种方式代价太高，I/O 和内存代价都很高。做个简单计算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">20 bytes/node * 100 nodes/doc * 1,000,000 docs=2GB</div></pre></td></tr></table></figure>
<h3 id="Storing-fields-as-inverted-lists"><a href="#Storing-fields-as-inverted-lists" class="headerlink" title="Storing fields as inverted lists"></a>Storing fields as inverted lists</h3><p>多存一份 field 的 inverted list，包含 field 起始和终止位置。然后通过这个位置区间找到 term inverted list 中符合条件的 location。 <img src="http://7xu83c.com1.z0.glb.clouddn.com/inverted_list.png" alt=""></p>
<h2 id="Indri-Index-Components"><a href="#Indri-Index-Components" class="headerlink" title="Indri Index Components"></a>Indri Index Components</h2><h3 id="Statistic-files"><a href="#Statistic-files" class="headerlink" title="Statistic files"></a>Statistic files</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_static_file.png" alt=""></p>
<h3 id="Term-dictionaries"><a href="#Term-dictionaries" class="headerlink" title="Term dictionaries"></a>Term dictionaries</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_term_diction.png" alt=""></p>
<h3 id="Inverted-files"><a href="#Inverted-files" class="headerlink" title="Inverted files"></a>Inverted files</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_inverted.png" alt=""></p>
<h3 id="Compressed-collection"><a href="#Compressed-collection" class="headerlink" title="Compressed collection"></a>Compressed collection</h3><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/indri_compress.png" alt=""></p>
<h2 id="Lucene-Index"><a href="#Lucene-Index" class="headerlink" title="Lucene Index"></a>Lucene Index</h2><p>略</p>
<blockquote>
<p>参考链接:<br><a href="http://boston.lti.cs.cmu.edu/classes/11-642/" target="_blank" rel="external">Search Engines: 11-442 / 11-642</a><br>本文图片来自书本 Introduction to Information Retrieval 和 Jamie Callen 的 slides。<br><a href="http://mp.weixin.qq.com/s?__biz=MzA5NTcyMjg1Nw==&amp;mid=401947034&amp;idx=1&amp;sn=572787750d0083b995c1e5dda00d0d70&amp;scene=1&amp;srcid=12312wNHo2HWD6o6Gs2czbPB&amp;from=groupmessage&amp;isappinstalled=0#wechat_redirect" target="_blank" rel="external">搜索引擎原理扫盲</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Pseudo Relevance Feedback]]></title>
      <url>http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。怎样产生更好的 query 来得到更多的相关文档？从用户角度看，用户一开始会用 short query 来进行检索，在看到结果文档后通过增加或减少 term 以及调整 term weight 的方式进一步优化 query。而对系统而言，能自动产生更好的 query 的方式莫过于机器学习算法。 <a id="more"></a></p>
<p>relevance feedback 其实是一个有监督的机器学习的问题，理想中我们要学习的是 f(document)–&gt;{relevant, not relevant}，然而一般我们学习的是 f(document)–&gt;score。训练集的大小一般来说 10-20 页是 good, 100-200 页就 great 了。</p>
<p>relevance feedback 并不经常被使用。一方面是因为用户不喜欢给评价(因训练数据会很少，准确度也不一定高)，另一方面是这种评价有风险，如果评估的文档很少，结果是 highly variable 的，stability 和 consistency 可能会受到影响。所以一般我们用的是 Pseudo-relevance feedback，一种无监督的机器学习方法。</p>
<h1 id="Pseudo-relevance-feedback"><a href="#Pseudo-relevance-feedback" class="headerlink" title="Pseudo-relevance feedback"></a>Pseudo-relevance feedback</h1><p>基本逻辑是把原始查询当做分类起，用它来给部分数据打标签，得到 top-ranked documents，然后用 labeled data 来产生更优的 classifier。基本过程：</p>
<ol>
<li>用原始 query 检索文档</li>
<li>取结果的前 N 篇文档作为训练集，这些文档相关度可能不高，然而我们的目的是学习 vocabulary pattern。</li>
<li>应用 relevance feedback algorithm 选取 term 和 term weight</li>
<li>组成新的 query 来检索文档</li>
</ol>
<h2 id="Okapi-BM25"><a href="#Okapi-BM25" class="headerlink" title="Okapi BM25"></a>Okapi BM25</h2><p>过程：</p>
<ol>
<li>用原始 query 检索文档</li>
<li>取前 N 篇文档的 term 作为 potential expansion terms</li>
<li>为每个 potential expansion term 计算分数</li>
<li>用前 m 个 term 创建新的 $query_{learned}$</li>
<li>用新的 query 检索文档</li>
</ol>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/bm25_expan.png" alt=""></p>
<h2 id="Inference-networks-Indri"><a href="#Inference-networks-Indri" class="headerlink" title="Inference networks (Indri)"></a>Inference networks (Indri)</h2><p>过程：</p>
<ol>
<li>用原始 query 检索文档</li>
<li>取前 N 篇文档的 term 作为 potential expansion terms</li>
<li>为每个 potential expansion term 计算分数</li>
<li>用前 m 个 term 创建新的 $Q_{learned}$</li>
<li>合并 $Q_{original}$ 和 $Q_{learned}$ 创建 $Q_{expanded}$</li>
<li>用新的 query 检索文档</li>
</ol>
<p>对每个 expansion term，计算 p(t|I)<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_expand_1.png" alt=""></p>
<p>并没有对文档集合里的常见词做出惩罚，所以加上一个类似 idf 对 weight<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_expand_2.png" alt=""></p>
<p>最后的 expanded query 是<br>$$Q_{expanded} ＝ \#wand(wQ_{original}, (1-w)Q_{learned})$$</p>
<p>需要的参数:</p>
<ul>
<li>fbdocs: number of judged documents</li>
<li>fbterms: number of terms to add to the query, indri’s default is 10</li>
<li>$\mu$: smoothing weight to use for new terms, indri’s default is 0</li>
<li>$w$: weight of the original query, indri’s default is 0.5</li>
</ul>
<p><strong>How many terms is enough</strong><br>标准答案来了: It depends! 因 query 而异。</p>
<p><strong>Corpus</strong><br>其实原始查询和最终的查询语句可以在不同的语料上跑，比如说原始查询在 wikipedia 上跑，产生高质量的 expansion term，然后用扩充的 query 在 web 上跑，这能够显著提高 MAP 和 P@10。</p>
<p>直接上代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/**</div><div class="line"> * @param score_list</div><div class="line"> * @return</div><div class="line"> * @throws IOException</div><div class="line"> */</div><div class="line">private static String expandQuery(ScoreList score_list) throws IOException &#123;</div><div class="line">	double fbMu = Double.parseDouble(parameters.get(&quot;fbMu&quot;));</div><div class="line">	int fbDocs = Integer.parseInt(parameters.get(&quot;fbDocs&quot;));</div><div class="line">	int fbTerms = Integer.parseInt(parameters.get(&quot;fbTerms&quot;));</div><div class="line">	int docNum = Math.min(fbDocs, score_list.size());</div><div class="line">	Map&lt;String, ArrayList&lt;Integer&gt;&gt; invertedList = new HashMap();</div><div class="line">	// map&lt;term, score&gt;</div><div class="line">	Map&lt;String, Double&gt; termScore = new HashMap();</div><div class="line">	// get expanded term</div><div class="line">	for (int i = 0; i &lt; docNum; i++) &#123;</div><div class="line">		int doc_id = score_list.getDocid(i);</div><div class="line">		TermVector vec = new TermVector(doc_id, &quot;body&quot;);</div><div class="line">		// termVecMap.put(doc_id, vec);</div><div class="line">		double docScore = score_list.getDocidScore(i);</div><div class="line">		double docLen = Idx.getFieldLength(&quot;body&quot;, doc_id);</div><div class="line">		// for each term</div><div class="line">		for (int j = 1; j &lt; vec.stemsLength(); j++) &#123;</div><div class="line"></div><div class="line">			String term = vec.stemString(j);</div><div class="line">			// ignore any candidate expansion term that contains a period</div><div class="line">			// (&apos;.&apos;) or a comma (&apos;,&apos;)</div><div class="line">			if (term.contains(&quot;.&quot;) || term.contains(&quot;,&quot;)) &#123;</div><div class="line">				continue;</div><div class="line">			&#125;</div><div class="line">			// update inverted list for current term</div><div class="line">			if (invertedList.containsKey(term)) &#123;</div><div class="line">				ArrayList&lt;Integer&gt; cur_inverted_list = invertedList.get(term);</div><div class="line">				cur_inverted_list.add(doc_id);</div><div class="line">				invertedList.put(term, cur_inverted_list);</div><div class="line">			&#125; else &#123;</div><div class="line">				ArrayList&lt;Integer&gt; cur_inverted_list = new ArrayList();</div><div class="line">				cur_inverted_list.add(doc_id);</div><div class="line">				invertedList.put(term, cur_inverted_list);</div><div class="line">			&#125;</div><div class="line">			// score potential expansion term for current doc</div><div class="line">			long tf = vec.stemFreq(j);</div><div class="line">			long ctf = vec.totalStemFreq(j);</div><div class="line">			double mle = ctf / (double) Idx.getSumOfFieldLengths(&quot;body&quot;);</div><div class="line">			double Ptd = (tf + fbMu * mle) / (docLen + fbMu);</div><div class="line">			double idf = Math.log(1 / mle);</div><div class="line">			double cur_doc_score = Ptd * docScore * idf;</div><div class="line">			if (termScore.containsKey(term)) &#123;</div><div class="line">				termScore.put(term, termScore.get(term) + cur_doc_score);</div><div class="line">			&#125; else &#123;</div><div class="line">				termScore.put(term, cur_doc_score);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// get top k terms</div><div class="line">	PriorityQueue&lt;Map.Entry&lt;String, Double&gt;&gt; termScorePq = new PriorityQueue&lt;Map.Entry&lt;String, Double&gt;&gt;(</div><div class="line">			termScore.size(), new Comparator&lt;Map.Entry&lt;String, Double&gt;&gt;() &#123;</div><div class="line">				@Override</div><div class="line">				public int compare(Map.Entry&lt;String, Double&gt; m1, Map.Entry&lt;String, Double&gt; m2) &#123;</div><div class="line">					return m2.getValue().compareTo(m1.getValue());</div><div class="line">				&#125;</div><div class="line">			&#125;);</div><div class="line">	termScorePq.addAll(termScore.entrySet());</div><div class="line"></div><div class="line">	// get new query</div><div class="line">	String learnedQuery = &quot;#wand ( &quot;;</div><div class="line">	for (int i = 0; i &lt; fbTerms; i++) &#123;</div><div class="line">		String score = String.format(&quot;%.4f&quot;, termScorePq.peek().getValue());</div><div class="line">		String term = termScorePq.peek().getKey();</div><div class="line">		learnedQuery = learnedQuery + &quot; &quot; + score + &quot; &quot; + term;</div><div class="line">		termScorePq.poll();</div><div class="line">	&#125;</div><div class="line">	learnedQuery += &quot; )&quot;;</div><div class="line">	System.out.println(&quot;learnedQuery &quot; + learnedQuery);</div><div class="line">	return learnedQuery;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>处理 query file。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Process the query file.</div><div class="line"> *</div><div class="line"> * @param queryFilePath</div><div class="line"> * @param model</div><div class="line"> * @throws Exception</div><div class="line"> */</div><div class="line">static void processQueryFile(String queryFilePath, String trecEvalOutputPath, RetrievalModel model)</div><div class="line">    throws Exception &#123;</div><div class="line"></div><div class="line">  BufferedReader input = null;</div><div class="line">  BufferedWriter output = null;</div><div class="line">  BufferedWriter bw = null;</div><div class="line"></div><div class="line">  try &#123;</div><div class="line">    String qLine = null;</div><div class="line"></div><div class="line">    input = new BufferedReader(new FileReader(queryFilePath));</div><div class="line">    output = new BufferedWriter(new FileWriter(trecEvalOutputPath));</div><div class="line">    bw = new BufferedWriter(new FileWriter(parameters.get(&quot;fbExpansionQueryFile&quot;)));</div><div class="line"></div><div class="line">    // Each pass of the loop processes one query.</div><div class="line"></div><div class="line">    while ((qLine = input.readLine()) != null) &#123;</div><div class="line">      int d = qLine.indexOf(&apos;:&apos;);</div><div class="line"></div><div class="line">      if (d &lt; 0) &#123;</div><div class="line">        throw new IllegalArgumentException(&quot;Syntax error:  Missing &apos;:&apos; in query line.&quot;);</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      printMemoryUsage(false);</div><div class="line"></div><div class="line">      String qid = qLine.substring(0, d);</div><div class="line">      String query = qLine.substring(d + 1);</div><div class="line"></div><div class="line">      System.out.println(&quot;Query &quot; + qLine);</div><div class="line"></div><div class="line">      ScoreList r = null;</div><div class="line"></div><div class="line">      String defaultOp = model.defaultQrySopName();</div><div class="line">      query = defaultOp + &quot;(&quot; + query + &quot;)&quot;;</div><div class="line">      // if not expand query</div><div class="line">      if (!(parameters.containsKey(&quot;fb&quot;) &amp;&amp; parameters.get(&quot;fb&quot;).equals(&quot;true&quot;))) &#123;</div><div class="line">        r = processQuery(query, model);</div><div class="line">      &#125; else &#123; // if expand query</div><div class="line">        // check parameters</div><div class="line">        if (!(parameters.containsKey(&quot;fbTerms&quot;) &amp;&amp; parameters.containsKey(&quot;fbMu&quot;)</div><div class="line">            &amp;&amp; parameters.containsKey(&quot;fbOrigWeight&quot;)</div><div class="line">            &amp;&amp; parameters.containsKey(&quot;fbExpansionQueryFile&quot;))) &#123;</div><div class="line">          throw new IllegalArgumentException(&quot;Required parameters were missing from the parameter file.&quot;);</div><div class="line">        &#125;</div><div class="line">        // check if there&apos;s ranking file</div><div class="line">        if (!parameters.containsKey(&quot;fbInitialRankingFile&quot;)) &#123;</div><div class="line">          r = processQuery(query, model);</div><div class="line">          r.sort();</div><div class="line">        &#125; else &#123;</div><div class="line">          Map&lt;Integer, ScoreList&gt; score_list_map = readRankingFile(</div><div class="line">              parameters.get(&quot;fbInitialRankingFile&quot;));</div><div class="line">          if (!score_list_map.containsKey(Integer.parseInt(qid))) &#123;</div><div class="line">            throw new Exception(&quot;No query &quot; + qid + &quot; in ranking file!&quot;);</div><div class="line">          &#125;</div><div class="line">          r = score_list_map.get(Integer.parseInt(qid));</div><div class="line">        &#125;</div><div class="line">//					r.sort();</div><div class="line">        String expandedQuery = expandQuery(r);</div><div class="line">        printExpandedQuery(bw, qid, expandedQuery);</div><div class="line">        double fbOrigWeight = Double.parseDouble(parameters.get(&quot;fbOrigWeight&quot;));</div><div class="line">        String newQuery = &quot;#wand (&quot; + String.valueOf(fbOrigWeight) + &quot; &quot; + query + &quot; &quot;</div><div class="line">            + String.valueOf(1 - fbOrigWeight) + &quot; &quot; + expandedQuery + &quot; )&quot;;</div><div class="line">        // System.out.println(&quot; new Query &quot; + newQuery);</div><div class="line">        r = processQuery(newQuery, model);</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      if (r != null) &#123;</div><div class="line">        printResults(qid, r, output);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; catch (IOException ex) &#123;</div><div class="line">    ex.printStackTrace();</div><div class="line">  &#125; finally &#123;</div><div class="line">    input.close();</div><div class="line">    output.close();</div><div class="line">    bw.close();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>如果有 initial ranking file。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> *</div><div class="line"> * @param fbInitialRankingFile</div><div class="line"> * @return</div><div class="line"> */</div><div class="line">private static Map&lt;Integer, ScoreList&gt; readRankingFile(String fbInitialRankingFile) &#123;</div><div class="line">  // System.out.println(&quot;filename &quot;+fbInitialRankingFile);</div><div class="line">  Map&lt;Integer, ScoreList&gt; scoreList_map = new HashMap&lt;&gt;();</div><div class="line">  try (BufferedReader br = new BufferedReader(new FileReader(fbInitialRankingFile))) &#123;</div><div class="line">    String str;</div><div class="line">    int last_qry = -1;</div><div class="line">    ScoreList score_list = new ScoreList();</div><div class="line">    while ((str = br.readLine()) != null) &#123;</div><div class="line">      String[] data = str.split(&quot; &quot;);</div><div class="line">      int cur_qry = Integer.parseInt(data[0].trim());</div><div class="line">      if (last_qry == -1) &#123;</div><div class="line">        last_qry = cur_qry;</div><div class="line">      &#125;</div><div class="line">      if (cur_qry != last_qry) &#123;</div><div class="line">        scoreList_map.put(last_qry, score_list);</div><div class="line">        last_qry = cur_qry;</div><div class="line">        score_list = new ScoreList();</div><div class="line">      &#125;</div><div class="line">      score_list.add(Idx.getInternalDocid(data[2].trim()), Double.parseDouble(data[4].trim()));</div><div class="line">    &#125;</div><div class="line">    // add the last query and scorelist</div><div class="line">    scoreList_map.put(last_qry, score_list);</div><div class="line">  &#125; catch (Exception e) &#123;</div><div class="line">    // TODO Auto-generated catch block</div><div class="line">    e.printStackTrace();</div><div class="line">  &#125;</div><div class="line">  return scoreList_map;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="Effectiveness"><a href="#Effectiveness" class="headerlink" title="Effectiveness"></a>Effectiveness</h2><ul>
<li>Query expansion 平均能使 MAP 提高 20%</li>
<li>但同时也有可能让 1/3 的用户感到 annoy</li>
</ul>
<p>所以通常来说，query expansion 会用在召回率很重要的场景，或者 average performance 很重要的场景，比如 legal retrieval, TREC, research paper 等。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hexo local search 错误解决]]></title>
      <url>http://www.shuang0420.com/2016/10/07/Hexo%20local%20search%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/</url>
      <content type="html"><![CDATA[<p>swiftype 适用期只有一个月，不要跟我说是高级用户一个月，就是一个月之后搜索就不能用了！！！<br>转而回到 local search，就出现了之前没有出现的问题，好久才整出了办法，必须记录一下。<br><a id="more"></a></p>
<h1 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h1><p>常用的是 local search。</p>
<p>安装 hexo-generator-search，在站点的根目录下执行以下命令：</p>
<pre>$ npm install hexo-generator-search --save</pre>

<p>编辑 站点配置文件，新增以下内容到任意位置：</p>
<pre>
search:
  path: search.xml
  field: post
</pre>

<h1 id="问题1-ERROR-Process-failed-layout-DS-Store"><a href="#问题1-ERROR-Process-failed-layout-DS-Store" class="headerlink" title="问题1: ERROR Process failed: layout/.DS_Store"></a>问题1: ERROR Process failed: layout/.DS_Store</h1><p>hexo g 运行出现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ERROR Process failed: layout/.DS_Store</div><div class="line">TypeError: Cannot read property &apos;compile&apos; of undefined</div></pre></td></tr></table></figure></p>
<p>可能不影响大局，但还是会很不爽，解决办法不是简单粗暴的把 .DS_Store 删掉(command+delete)，这样是没用的，要在命令行里 rm -rf 删。查看隐藏文件的命令是 ls -al。</p>
<h1 id="问题2-xmlParseEntityRef-no-name"><a href="#问题2-xmlParseEntityRef-no-name" class="headerlink" title="问题2: xmlParseEntityRef: no name"></a>问题2: xmlParseEntityRef: no name</h1><p>在网站后输入/search.xml查看页面，出现 “error on line 7 at column 81: xmlParseEntityRef: no name” 错误。<br>原因：标题中的 &amp; 会和 HTML tags 冲突！<br>解决：把 &amp; 换掉啊换掉！</p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Information Needs]]></title>
      <url>http://www.shuang0420.com/2016/10/02/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Information%20Needs/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。这一篇概括了用户信息需求的分类、查询语句的结构以及查询的前期处理过程(非结构化的查询语句-&gt;结构化的查询语句)。 <a id="more"></a></p>
<h1 id="Query-type"><a href="#Query-type" class="headerlink" title="Query type"></a>Query type</h1><ul>
<li>Informational(39%)<br>像 iphones 之类，用户想了解一个 topic。</li>
<li>Transactional(36%)<br>像购物、买机票之类，用户想找个网站进行交易，但是并没有特定的 destination.</li>
<li>Navigational(25%)<br>像 CMU 网站之类的，用户有一个特定的想要浏览的 location/destination</li>
</ul>
<h1 id="Query-language"><a href="#Query-language" class="headerlink" title="Query language"></a>Query language</h1><p>一条标准的 query 分为 3 部分。</p>
<ul>
<li>Source of information: fields, XML elements, metadata</li>
<li>Query operators: AND, OR, NEAR/n, …</li>
<li>Rules: 怎样使用这些 operators (顺序、权重等)</li>
</ul>
<p>每一条 query 都会被转化成一个结构化的查询语句。</p>
<h2 id="Query-operators"><a href="#Query-operators" class="headerlink" title="Query operators"></a>Query operators</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Boolean operators: AND, OR, AND-NOT</div><div class="line">Distance operators: NEAR/n, WINDOW/n, SENTENCE/n, PARAGRAPH/n</div><div class="line">Extent(field) restrictions: BODY, TITLE, INLINK, ABSTRACT, AUTHOR,...</div><div class="line">Comparison operators: &lt;, &gt;, BEFORE, AFTER, ...</div><div class="line">Score operators: WEIGHT, AVERAGE, MAX, MIN, ...</div><div class="line">Synonym</div><div class="line">Filter-And-Rank(q1,q2): q1 forms a set, use q2 ranks it</div></pre></td></tr></table></figure>
<h2 id="Query-Processing"><a href="#Query-Processing" class="headerlink" title="Query Processing"></a>Query Processing</h2><p>查询处理，这里最常用的是 #NEAR 和 #SYNONYM，对于一些词组(phrases)，搜索引擎会用 #NEAR 进行规范化，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">die-cast -&gt; #NEAR/1 (die cast)</div><div class="line">virginia beach -&gt; #NEAR/1 (virginia beach)</div><div class="line">barack obama -&gt; #NEAR/3 (barack obama)</div></pre></td></tr></table></figure></p>
<p>对于一些缩写，或者拼写错误，一般会用 #SYNONYM 进行调整，如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Abbreviations</div><div class="line">virginia -&gt; (virginia,va)</div><div class="line"></div><div class="line"># Spelliing correction:</div><div class="line">brittany -&gt; britney</div><div class="line">brittany -&gt; #SYNONYM (brittany,britney)</div></pre></td></tr></table></figure></p>
<h2 id="Query-Reformulation"><a href="#Query-Reformulation" class="headerlink" title="Query Reformulation"></a>Query Reformulation</h2><p>Sequential-Dependency Models(SDM) 会将非结构化的查询转化成结构化的查询语句，一个 SDM query 分为三部分:</p>
<ul>
<li>Bag of words matches<br>作用是保证能找到东西。eg. #AND(q1,q2…qn)</li>
<li>N-gram matches (ordered,phrase-like)<br>给匹配的 n-gram 提供了额外的权重。eg. #NEAR/1(q1,q2) #NEAR/1(q2,q3)…#NEAR/1(qn-1,qn)</li>
<li>Short window matches (unordered, sentence-like)<br>给匹配的窗口提供了额外的权重。eg. #WINDOW/8(q1,q2)…#WINDOW/8(qn-1,qn)</li>
</ul>
<p>Eg.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">User Query:</div><div class="line">	sherwood regional library</div><div class="line"></div><div class="line">A sequential dependency model query:</div><div class="line">	#wand(</div><div class="line">		0.5 #and( sherwood regional library )</div><div class="line">		0.25 #and( #near/1( regional library )  #near/1( sherwood regional ) )</div><div class="line">		0.25 #and( #window/8( regional library )  #window/8( sherwood regional ) ) )</div></pre></td></tr></table></figure></p>
<p>Perl 代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/perl</div><div class="line"></div><div class="line">#</div><div class="line"># Perl subroutine that generates Indri dependence model queries.</div><div class="line">#</div><div class="line"># Written by: Don Metzler (metzler@cs.umass.edu)</div><div class="line"># Last update: 06/27/2005</div><div class="line">#</div><div class="line"># Feel free to distribute, edit, modify, or mangle this code as you see fit. If you make any interesting</div><div class="line"># changes please email me a copy.</div><div class="line">#</div><div class="line"># For more technical details, see:</div><div class="line">#</div><div class="line">#    * Metzler, D. and Croft, W.B., &quot;A Markov Random Field Model for Term Dependencies,&quot; ACM SIGIR 2005.</div><div class="line">#</div><div class="line">#    * Metzler, D., Strohman T., Turtle H., and Croft, W.B., &quot;Indri at TREC 2004: Terabyte Track&quot;, TREC 2004.</div><div class="line">#</div><div class="line">#    * http://ciir.cs.umass.edu/~metzler/</div><div class="line">#</div><div class="line"># MODIFICATIONS</div><div class="line">#  - Updated by Jamie Callan:  02/11/2015</div><div class="line">#    Modified to support a less cryptic Indri-like query language.</div><div class="line">#    #combine --&gt; #and, #1 --&gt; #near/1, #weight --&gt; #wand, and #uw --&gt; #window/</div><div class="line">#</div><div class="line"># NOTES</div><div class="line">#</div><div class="line">#    * this script assumes that the query string has already been parsed and that all characters</div><div class="line">#      that are not compatible with Indri&apos;s query language have been removed.</div><div class="line">#</div><div class="line">#    * it is not advisable to do a &apos;full dependence&apos; variant on long strings because of the exponential</div><div class="line">#      number of terms that will result. it is suggested that the &apos;sequential dependence&apos; variant be</div><div class="line">#      used for long strings. either that, or split up long strings into smaller cohesive chunks and</div><div class="line">#      apply the &apos;full dependence&apos; variant to each of the chunks.</div><div class="line">#</div><div class="line">#    * the unordered features use a window size of 4 * number of terms within the phrase. this has been</div><div class="line">#      found to work well across a wide range of collections and topics. however, this may need to be</div><div class="line">#      modified on an individual basis.</div><div class="line">#</div><div class="line"></div><div class="line"># example usage</div><div class="line">print formulate_query( &quot;sherwood regional library&quot;, &quot;sd&quot;, 0.02, 0.49, 0.49 ) . &quot;\n\n&quot;;</div><div class="line">#print formulate_query( &quot;sherwood regional library&quot;, &quot;fd&quot;, 0.8, 0.1, 0.1 ) . &quot;\n\n&quot;;</div><div class="line"></div><div class="line">#</div><div class="line"># formulates a query based on query text and feature weights</div><div class="line">#</div><div class="line"># arguments:</div><div class="line">#    * query - string containing original query terms separated by spaces</div><div class="line">#    * type  - string. &quot;sd&quot; for sequential dependence or &quot;fd&quot; for full dependence variant. defaults to &quot;fd&quot;.</div><div class="line">#    * wt[0] - weight assigned to term features</div><div class="line">#    * wt[1] - weight assigned to ordered (#near) features</div><div class="line">#    * wt[2] - weight assigned to unordered (#window) features</div><div class="line">#</div><div class="line">sub formulate_query &#123;</div><div class="line">    my ( $q, $type, @wt ) = @_;</div><div class="line"></div><div class="line">    # trim whitespace from beginning and end of query string</div><div class="line">    $q =~ s/^\s+|\s+$//g;</div><div class="line"></div><div class="line">    my $queryT = &quot;#and( &quot;;</div><div class="line">    my $queryO = &quot;#and(&quot;;</div><div class="line">    my $queryU = &quot;#and(&quot;;</div><div class="line"></div><div class="line">    # generate term features (f_T)</div><div class="line">    my @terms = split(/\s+/ , $q);</div><div class="line">    my $term;</div><div class="line">    foreach $term ( @terms ) &#123;</div><div class="line">	$queryT .= &quot;$term &quot;;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    my $num_terms = @terms;</div><div class="line"></div><div class="line">    # skip the rest of the processing if we&apos;re just</div><div class="line">    # interested in term features or if we only have 1 term</div><div class="line">    if( ( $wt[1] == 0.0 &amp;&amp; $wt[2] == 0.0 ) || $num_terms == 1 ) &#123;</div><div class="line">	return $queryT . &quot;)&quot;;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    # generate the rest of the features</div><div class="line">    my $start = 1;</div><div class="line">    if( $type eq &quot;sd&quot; ) &#123; $start = 3; &#125;</div><div class="line">    for( my $i = $start ; $i &lt; 2 ** $num_terms ; $i++ ) &#123;</div><div class="line">	my $bin = unpack(&quot;B*&quot;, pack(&quot;N&quot;, $i)); # create binary representation of i</div><div class="line">	my $num_extracted = 0;</div><div class="line">	my $extracted_terms = &quot;&quot;;</div><div class="line"></div><div class="line">	# get query terms corresponding to &apos;on&apos; bits</div><div class="line">	for( my $j = 0 ; $j &lt; $num_terms ; $j++ ) &#123;</div><div class="line">	    my $bit = substr($bin, $j - $num_terms, 1);</div><div class="line">	    if( $bit eq &quot;1&quot; ) &#123;</div><div class="line">		$extracted_terms .= &quot;$terms[$j] &quot;;</div><div class="line">		$num_extracted++;</div><div class="line">	    &#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	if( $num_extracted == 1 ) &#123; next; &#125; # skip these, since we already took care of the term features...</div><div class="line">	if( $bin =~ /^0+11+[^1]*$/ ) &#123; # words in contiguous phrase, ordered features (f_O)</div><div class="line">	    $queryO .= &quot; #near/1( $extracted_terms) &quot;;</div><div class="line">	&#125;</div><div class="line">	$queryU .= &quot; #window/&quot; . 4*$num_extracted . &quot;( $extracted_terms) &quot;; # every subset of terms, unordered features (f_U)</div><div class="line">	if( $type eq &quot;sd&quot; ) &#123; $i *= 2; $i--; &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    my $query = &quot;#wand(&quot;;</div><div class="line">    if( $wt[0] != 0.0 &amp;&amp; $queryT ne &quot;#and( &quot; ) &#123; $query .= &quot; $wt[0] $queryT)&quot;; &#125;</div><div class="line">    if( $wt[1] != 0.0 &amp;&amp; $queryO ne &quot;#and(&quot; ) &#123; $query .= &quot; $wt[1] $queryO)&quot;; &#125;</div><div class="line">    if( $wt[2] != 0.0 &amp;&amp; $queryU ne &quot;#and(&quot; ) &#123; $query .= &quot; $wt[2] $queryU)&quot;; &#125;</div><div class="line"></div><div class="line">    if( $query eq &quot;#wand(&quot; ) &#123; return &quot;&quot;; &#125; # return &quot;&quot; if we couldn&apos;t formulate anything</div><div class="line"></div><div class="line">    return $query . &quot; )&quot;;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>另外常用的模型还有 <a href="http://www.shuang0420.com/2016/10/10/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Pseudo%20Relevance%20Feedback/">query expansion</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AES 和 RSA 笔记]]></title>
      <url>http://www.shuang0420.com/2016/10/02/AES%20%E5%92%8C%20RSA%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>简单回顾 AES 和 RSA 算法。<br><a id="more"></a></p>
<h1 id="Symmetric-key-对称加密"><a href="#Symmetric-key-对称加密" class="headerlink" title="Symmetric key 对称加密"></a>Symmetric key 对称加密</h1><p>加密和解密均采用同一把密钥，而且通信双方都必须获得这把密钥。一方通过密钥将信息加密后，把密文传给另一方，另一方通过这个相同的密钥将密文解密，转换成可以理解的明文。<br>常见的对称加密算法有DES、3DES、AES、Blowfish、IDEA、RC5、RC6、AES。</p>
<ul>
<li>DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合。</li>
<li>3DES（Triple DES）：是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。</li>
<li>AES（Advanced Encryption Standard）：高级加密标准，是下一代的加密算法标准，速度快，安全级别高；</li>
</ul>
<p>对称加密的最大优点是速度快，然而它也存在着诸多问题。</p>
<h2 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h2><ul>
<li>要求提供一条安全的渠道使通讯双方在首次通讯时协商一个共同的密钥。直接的面对面协商可能是不现实而且难于实施的，所以双方可能需要借助于邮件和电话等其它相对不够安全的手段来进行协商；</li>
<li>密钥的数目难于管理。因为对于每一个合作者都需要使用不同的密钥，很难适应开放社会中大量的信息交流；而如果大家都使用同一个密钥，只要其中一个人密钥被盗窃了，那么整体加密的信息将都被破解了。</li>
<li>对称加密算法一般不能提供信息完整性的鉴别。它无法验证发送者和接受者的身份。</li>
<li>对称密钥的管理和分发工作是一件具有潜在危险的和烦琐的过程。对称加密是基于共同保守秘密来实现的，采用对称加密技术的贸易双方必须保证采用的是相同的密钥，保证彼此密钥的交换是安全可靠的，同时还要设定防止密钥泄密和更改密钥的程序。</li>
</ul>
<h2 id="AES"><a href="#AES" class="headerlink" title="AES"></a>AES</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/AES.png" alt=""><br>AES加密过程涉及到4种操作：字节替代（SubBytes）、行移位（ShiftRows）、列混淆（MixColumns）和轮密钥加（AddRoundKey）。从上图可以看出：1）解密过程的每一步分别对应操作的逆操作，2）加解密所有操作的顺序正好是相反的。正是由于这两点保证了解密能够正确地恢复明文。加解密中每轮的密钥分别由初始密钥扩展得到。算法中16字节的明文、密文和轮密钥都以一个4x4的矩阵表示。</p>
<p><a href="http://www.cnblogs.com/luop/p/4334160.html" target="_blank" rel="external">算法详解</a></p>
<h1 id="Asymmetric-key-非对称加密"><a href="#Asymmetric-key-非对称加密" class="headerlink" title="Asymmetric key 非对称加密"></a>Asymmetric key 非对称加密</h1><p>使用非对称加密算法，首先要有一对key，一个是private key私钥，另一个是public key公钥，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。可以把你的public key分发给想给你传密文的用户，然后用户使用该public key加密过的密文，只有使用你的 private key 才能解密，也就是说，只要你自己保存好你的 private key，就能确保，别人想给你发的密文不被破解，所以你不用担心别人的密钥被盗。</p>
<p>过程：</p>
<ol>
<li>乙方生成两把密钥（公钥和私钥）。公钥是公开的，任何人都可以获得，私钥则是保密的。</li>
<li>甲方获取乙方的公钥，然后用它对信息加密。</li>
<li>乙方得到加密后的信息，用私钥解密。</li>
</ol>
<p>非对称加密算法对 symmetric key 进行了加密，保密性比较好，它消除了最终用户交换密钥的需要，而且能提供长期的 signatures。但加密和解密花费时间长、速度慢，在某些极端情况下，甚至能比非对称加密慢上1000倍。因此它不适合于对文件加密而只适用于对少量数据进行加密。</p>
<p>举个例子，如果企业中有n个用户，企业需要生成n对密钥，并分发n个公钥。由于公钥是可以公开的，用户只要保管好自己的私钥即可(企业分发后一般保存的是私钥,用户拿的是公钥)，因此加密密钥的分发将变得十分简单。同时，由于每个用户的私钥是唯一的，其他用户除了可以通过信息发送者的公钥来验证信息的来源是否真实，还可以确保发送者无法否认曾发送过该信息。</p>
<p>这种加密算法应用非常广泛，SSH, HTTPS, TLS，电子证书，电子签名，电子身份证等等。</p>
<h2 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h2><p>1977年，三位数学家Rivest、Shamir 和 Adleman 设计了一种算法，可以实现非对称加密。这种算法用他们三个人的名字命名，叫做RSA算法。从那时直到现在，RSA算法一直是最广为使用的”非对称加密算法”。毫不夸张地说，只要有计算机网络的地方，就有RSA算法。<br>这种算法非常可靠，密钥越长，它就越难破解。根据已经披露的文献，目前被破解的最长RSA密钥是768个二进制位。也就是说，长度超过768位的密钥，还无法破解（至少没人公开宣布）。因此可以认为，1024位的RSA密钥基本安全，2048位的密钥极其安全。<br>代码理解 RSA 算法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line">/* Demonstrate RSA in Java using BigIntegers */</div><div class="line"></div><div class="line">import java.math.BigInteger;</div><div class="line">import java.util.Random;</div><div class="line"></div><div class="line">/**</div><div class="line"> *  RSA Algorithm from CLR</div><div class="line"> *</div><div class="line"> * 1. Select at random two large prime numbers p and q.</div><div class="line"> * 2. Compute n by the equation n = p * q.</div><div class="line"> * 3. Compute phi(n)=  (p - 1) * ( q - 1)</div><div class="line"> * 4. Select a small odd integer e that is relatively prime to phi(n).</div><div class="line"> * 5. Compute d as the multiplicative inverse of e modulo phi(n). A theorem in</div><div class="line"> *    number theory asserts that d exists and is uniquely defined.</div><div class="line"> * 6. Publish the pair P = (e,n) as the RSA public key.</div><div class="line"> * 7. Keep secret the pair S = (d,n) as the RSA secret key.</div><div class="line"> * 8. To encrypt a message M compute C = M^e (mod n)</div><div class="line"> * 9. To decrypt a message C compute M = C^d (mod n)</div><div class="line"> */</div><div class="line"></div><div class="line">public class RSAExample &#123;</div><div class="line"></div><div class="line">  public static void main(String[] args) &#123;</div><div class="line">    // Each public and private key consists of an exponent and a modulus</div><div class="line">    BigInteger n; // n is the modulus for both the private and public keys</div><div class="line">    BigInteger e; // e is the exponent of the public key</div><div class="line">    BigInteger d; // d is the exponent of the private key</div><div class="line"></div><div class="line">    Random rnd = new Random();</div><div class="line"></div><div class="line">    // Step 1: Generate two large random primes.</div><div class="line">    // We use 400 bits here, but best practice for security is 2048 bits.</div><div class="line">    // Change 400 to 2048, recompile, and run the program again and you will</div><div class="line">    // notice it takes much longer to do the math with that many bits.</div><div class="line">    BigInteger p = new BigInteger(400,100,rnd);</div><div class="line">    BigInteger q = new BigInteger(400,100,rnd);</div><div class="line"></div><div class="line">    // Step 2: Compute n by the equation n = p * q.</div><div class="line">    n = p.multiply(q);</div><div class="line"></div><div class="line">    // Step 3: Compute phi(n) = (p-1) * (q-1)</div><div class="line">    BigInteger phi = (p.subtract(BigInteger.ONE)).multiply(q.subtract(BigInteger.ONE));</div><div class="line"></div><div class="line">    // Step 4: Select a small odd integer e that is relatively prime to phi(n).</div><div class="line">    // By convention the prime 65537 is used as the public exponent.</div><div class="line">    e = new BigInteger (&quot;65537&quot;);</div><div class="line"></div><div class="line">    // Step 5: Compute d as the multiplicative inverse of e modulo phi(n).</div><div class="line">    d = e.modInverse(phi);</div><div class="line"></div><div class="line">    System.out.println(&quot; e = &quot; + e);  // Step 6: (e,n) is the RSA public key</div><div class="line">    System.out.println(&quot; d = &quot; + d);  // Step 7: (d,n) is the RSA private key</div><div class="line">    System.out.println(&quot; n = &quot; + n);  // Modulus for both keys</div><div class="line"></div><div class="line">    // Encode a simple message. For example the letter &apos;A&apos; in UTF-8 is 65</div><div class="line">    BigInteger m = new BigInteger(&quot;65&quot;);</div><div class="line"></div><div class="line">    // Step 8: To encrypt a message M compute C = M^e (mod n)</div><div class="line">    BigInteger c = m.modPow(e, n);</div><div class="line"></div><div class="line">    // Step 9: To decrypt a message C compute M = C^d (mod n)</div><div class="line">    BigInteger clear = c.modPow(d, n);</div><div class="line">    System.out.println(&quot;Cypher text = &quot; + c);</div><div class="line">    System.out.println(&quot;Clear text = &quot; + clear); // Should be &quot;65&quot;</div><div class="line"></div><div class="line">    // Step 8 (reprise) Encrypt the string &apos;Hello&apos;</div><div class="line">    String s = &quot;RSA is way cool.&quot;;</div><div class="line">    m = new BigInteger(s.getBytes()); // m is the original clear text</div><div class="line">    c = m.modPow(e, n);     // Do the encryption, c is the cypher text</div><div class="line"></div><div class="line">    // Step 9 (reprise) Decrypt...</div><div class="line">    clear = c.modPow(d, n); // Decrypt, clear is the resulting clear text</div><div class="line">    String clearStr = new String(clear.toByteArray());  // Decode to a string</div><div class="line"></div><div class="line">    System.out.println(&quot;Cypher text = &quot; + c);</div><div class="line">    System.out.println(&quot;Clear text = &quot; + clearStr);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>数学原理参见<br><a href="http://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html" target="_blank" rel="external">RSA算法原理（一）</a><br><a href="http://www.ruanyifeng.com/blog/2013/07/rsa_algorithm_part_two.html" target="_blank" rel="external">RSA算法原理（二）</a></p>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 加密 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SHA-1和MD5 笔记]]></title>
      <url>http://www.shuang0420.com/2016/10/01/SHA-1%E5%92%8CMD5%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>简单回顾 SHA-1 和 MD5 算法。<br><a id="more"></a></p>
<h1 id="Hash-函数"><a href="#Hash-函数" class="headerlink" title="Hash 函数"></a>Hash 函数</h1><p>Hash 函数 H(M)， 作用于一任意长度的消息 M，返回一固定长度的散列值h:h=H(M)，作为初始消息的独一无二的“数字指纹”，从而能保证数据的完整性和惟一性。Hash算法是现代密码体系中的一个重要组成部分。由于非对称算法的运算速度较慢，所以在数字签名协议中，Hash 函数扮演了一个重要的角色。对 Hash 值，又称”数字摘要”进行数字签名，在统计上可以认为与对文件本身进行数字签名是等效的。hash函数并不完全可靠，不同文件产生相同 MD5 和 SHA1 的几率还是有的，只是不高。</p>
<p>通过 Hash 算法可实现数字签名实现，数字签名的原理是将要传送的明文通过一种函数运算（Hash）转换成报文摘要（不同的明文对应不同的报文摘要），报文摘要加密后与明文一起传送给接受方，接受方将接受的明文产生新的报文摘要与发送方的发来报文摘要解密比较，比较结果一致表示明文未被改动，如果不一致表示明文已被篡改。</p>
<p>Hash 函数的安全性在于其产生散列值的操作过程具有较强的单向性（不可逆性）。如果在输入序列中嵌入密码，那么任何人在不知道密码的情况下都不能产生正确的散列值，从而保证了其安全性。这符合了数字签名的特性，数字签名只能用非对称算法。</p>
<p>Hash 函数一般用于产生消息摘要，密钥加密等，常见的有：</p>
<ul>
<li>MD5（Message Digest Algorithm 5）：是RSA数据安全公司开发的一种单向散列算法。</li>
<li>SHA（Secure Hash Algorithm）：可以对任意长度的数据运算生成一个160位的数值；</li>
</ul>
<h1 id="MD5"><a href="#MD5" class="headerlink" title="MD5"></a>MD5</h1><p>MD5，一种不可逆的加密算法，目前是最牢靠的加密算法之一，尚没有能够逆运算的程序被开发出来，它对应任何字符串都可以加密成一段唯一的固定长度的代码。</p>
<p>MD5(RFC1321)是Rivest于1991年对MD4的改进版本。它对输入仍以 512 位分组，其输出是 4个32位字的级联，与 MD4 相同。MD5 比 MD4 来得复杂，并且速度较之要慢一点，但更安全，在抗分析和抗差分方面表现更好。</p>
<p>MD5 通常用于密码的加密存储，数字签名，文件完整性验证等。</p>
<h2 id="Java-代码"><a href="#Java-代码" class="headerlink" title="Java 代码"></a>Java 代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import java.security.NoSuchAlgorithmException;</div><div class="line"></div><div class="line">public static String computeHash(String input) &#123;</div><div class="line">    byte[] hashed_str = null;</div><div class="line">    String res = null;</div><div class="line">    try &#123;</div><div class="line">        java.security.MessageDigest alg = java.security.MessageDigest.getInstance(&quot;md5&quot;);</div><div class="line">        alg.update(input.getBytes());</div><div class="line">        hashed_str = alg.digest();</div><div class="line">        res = javax.xml.bind.DatatypeConverter.printHexBinary(hashed_str);</div><div class="line">    &#125; catch (NoSuchAlgorithmException ex) &#123;</div><div class="line">        System.out.println(&quot;Exception: &quot; + ex);</div><div class="line">    &#125; finally &#123;</div><div class="line">        return res;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="SHA-1"><a href="#SHA-1" class="headerlink" title="SHA-1"></a>SHA-1</h1><p>SHA-1，一种不可逆的、防冲突，并具有良好的雪崩效应的加密算法。该算法输入报文的最大长度不超过 2^64 位，产生的输出是一个 160 位的报文摘要。输入是按 512 位（64 字节）的分组进行处理的，并产生２０个字节的被称为信息认证代码或信息摘要的输出。</p>
<h2 id="Java-代码-1"><a href="#Java-代码-1" class="headerlink" title="Java 代码"></a>Java 代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import java.security.NoSuchAlgorithmException;</div><div class="line"></div><div class="line">public static String computeHash(String input) &#123;</div><div class="line">    byte[] hashed_str = null;</div><div class="line">    String res = null;</div><div class="line">    try &#123;</div><div class="line">        java.security.MessageDigest alg = java.security.MessageDigest.getInstance(&quot;SHA-1&quot;);</div><div class="line">        alg.update(input.getBytes());</div><div class="line">        hashed_str = alg.digest();</div><div class="line">        res = javax.xml.bind.DatatypeConverter.printHexBinary(hashed_str);</div><div class="line">    &#125; catch (NoSuchAlgorithmException ex) &#123;</div><div class="line">        System.out.println(&quot;Exception: &quot; + ex);</div><div class="line">    &#125; finally &#123;</div><div class="line">        return res;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="SHA-1与MD5的比较"><a href="#SHA-1与MD5的比较" class="headerlink" title="SHA-1与MD5的比较"></a>SHA-1与MD5的比较</h1><p>因为二者均由MD4导出，SHA-1和MD5彼此很相似。相应的，他们的强度和其他特性也是相似，但还有以下几点不同：</p>
<ul>
<li>强行攻击的安全性：SHA-1与MD5 的最大区别在于其摘要比MD5 摘要长 32 比特。对于强行攻击，产生任何一个报文使之摘要等于给定报文摘要的难度：MD5 是2128 数量级的操作，SHA-1 是2160 数量级的操作。因而,SHA-1 对强行攻击的强度更大。</li>
<li>速度：由于SHA-1 的循环步骤比MD5 多（80:64）且要处理的缓存大（160 比特:128 比特），SHA-1 的运行速度比MD5 慢。</li>
</ul>
<p>最后上张比较图。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/md5&amp;sha-1.jpg" alt=""></p>
<h1 id="应用－网站用户名密码保存"><a href="#应用－网站用户名密码保存" class="headerlink" title="应用－网站用户名密码保存"></a>应用－网站用户名密码保存</h1><p>网站用户名密码的保存方式：</p>
<ul>
<li>明文 hash 后保存，如 md5</li>
<li>MD5+Salt 方式,这个 salt 可以随机</li>
</ul>
<p>网站用户名密码保存通常会用到 MD5 + Salt。salt 就是服务端在接收了客户输入的原字符串后再加一段自定义的字符串，然后对新产生的字符串一起进行加密，提高安全性。<a href="https://github.com/Shuang0420/Distributed-System/tree/master/Project2/Project2Task2/src/project2task2" target="_blank" rel="external">示例代码</a></p>
<blockquote>
<p>参考链接<br><a href="http://stark-summer.iteye.com/blog/1313884" target="_blank" rel="external">数字签名算法MD5和SHA-1的比较</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 加密 </tag>
            
            <tag> Hash </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Best-Match]]></title>
      <url>http://www.shuang0420.com/2016/09/30/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Best-Match/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。Best match模型衡量的是一篇文档与 information need 的匹配程度，与 Exact match模型（匹配／不匹配）相比更注重用户体验，不管有没有匹配 Best match 都会返回文档结果。<br><a id="more"></a></p>
<p>这一篇考虑的是 query dependent 的分数，网页打分的依据是文档和查询的相关性分数，也就是信息检索得分(IR score)。有很多理论来计算 IR score，这里主要介绍以下 4 种理论：</p>
<ul>
<li>向量空间 Vector space retrieval model(VSM)</li>
<li>概率理论 Probabilistic retrieval model(BM25)</li>
<li>统计语言模型 Statistical language model(query likelihood)</li>
<li>推理网络 Inference networks(Indri)</li>
</ul>
<p>它们的公式其实都和 <a href="http://www.shuang0420.com/2016/07/10/Tfidf%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/">tf-idf</a> 的公式相似。每个单词－文档组合都有一个tf-idf值。tf 表示此文档中这个单词出现的次数；df 表示含有这个单词的文档的数量。通常如果一个单词在文档中出现次数越多说明这个文档与这个单词相关性越大。但是有的单词太常用了，比如英文里“the”，“a” 在任何一个文档中都会大量出现。idf 就表示一个文档含有此单词的概率的倒数，用来消除常用词干扰。如果某个词或短语在一篇文章中出现的频率 TF 高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
<p>Document Priors 指与 query 无关的用来评估文档价值的 estimates(query independent)，主要的 document priors 有 spam score, PageRank, length of url 等，与上面的算法结合可以综合评估网页的分数。</p>
<h1 id="VSM"><a href="#VSM" class="headerlink" title="VSM"></a>VSM</h1><p>假设文档 d 对应的向量用 $\overrightarrow {V}(d)$ 表示，每个维度对应一个 term，向量分量一般可以采用 tf-idf 权重计算方式。一组文档的集合看作向量空间的多个向量，每个 term 对应一个坐标轴，然后在向量空间下进行相似度计算。</p>
<h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>文档与查询都是高维空间中的一个向量</p>
<ul>
<li>文档是词语组成的向量</li>
<li>词语是文档组成的向量</li>
<li>查询是词语组成的向量</li>
</ul>
<h2 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h2><p>相似度的计算方法。</p>
<ul>
<li>Inner product</li>
<li>Dice coefficient</li>
<li>Jackard coefficient</li>
<li>Cosine correlation</li>
</ul>
<h3 id="余弦相似度-cosine-similarity"><a href="#余弦相似度-cosine-similarity" class="headerlink" title="余弦相似度 (cosine similarity)"></a>余弦相似度 (cosine similarity)</h3><p>直接向量差(overlap measures)衡量相似度会产生下面的问题，</p>
<ul>
<li>并没有对向量长度进行归一化</li>
<li>所有的 term 都被看作是同等重要的</li>
</ul>
<p>可能导致的结果是，两篇内容相似的文档向量的差向量可能很大，因为一篇文档可能比另一篇文档长很多。</p>
<p>最常用的 similarity metric 还是 cosine similarity.<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/cos.png" alt=""></p>
<p>关于 Vector Coefficient 我们需要考虑以下三点：</p>
<ul>
<li>Document term weight: 文档中每个 term 的重要性 ==&gt; tf -&gt; log(tf+1)</li>
<li>Collection term weight: 文档集合中每个 term 的重要性  ==&gt; idf -&gt; $log{N \over df}+1$ (avoid idf=0)</li>
<li>Length normalization: 对文档长度进行的补偿</li>
</ul>
<p><strong>关于文档长度：</strong></p>
<ul>
<li>长文档由于更可能包含匹配词语，因而更可能相关</li>
<li>然而，如果两篇文档具有同样的相似值，用户更倾向于短文档，短文档更聚焦在用户信息需求上</li>
<li>因此相似性计算中应该考虑文档长度(进行规范化)</li>
</ul>
<p>更进一步的 cosine-similarity，Inc.ltc<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/inc_ltc_sim.png" alt=""></p>
<p>这个公式可以用作 #SUM 的计算，仅计算包含了查询词的文档的分数。</p>
<h3 id="Length-Bias"><a href="#Length-Bias" class="headerlink" title="Length Bias"></a>Length Bias</h3><p>大多数的 similarity metrics 都会有一个 length bias，就是说短文档的分数被高估了，长文档的分数被低估了，<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/length_bias_sim.png" alt=""></p>
<p>所以我们需要 pivote document length normalization，可以采用 Lnu.Ltu metric.<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/lnu.png" alt=""></p>
<h2 id="Lucene-应用"><a href="#Lucene-应用" class="headerlink" title="Lucene 应用"></a>Lucene 应用</h2><p>Lucene 的检索过程：</p>
<ol>
<li>使用布尔查询检索到一个文档集合</li>
<li>用 VSM 算法来对这个集合对文档进行排序</li>
</ol>
<p>Simplified Lucene’s tf.idf Ranker<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/lucene-tfidf.png" alt=""></p>
<p>与 Inc.ltc 的不同</p>
<ul>
<li>tf weight 用了 sqrt(tf) 而不是 log(tf)+1，stronger reward for frequent terms in document</li>
<li>idf weight 用了 square 而不是 idf，stronger penalty for frequent terms across corpus</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><blockquote>
<p>Key idea: Measure similarity among weighted term vectors</p>
</blockquote>
<p>Vector Space Retrieval Model 没有告诉我们怎么 set term weights，没有告诉我们怎么确定 similarity，也没有告诉我们怎么支持 query-independent weights，它的优点是灵活，缺点也是灵活，所有的东西都要我们自己设置。</p>
<h1 id="Okapi-BM25"><a href="#Okapi-BM25" class="headerlink" title="Okapi BM25"></a>Okapi BM25</h1><p>BM25 十分重视 term frequency 和 document length，这里省略了公式推导过程，直接分析参数。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/bm25.png" alt=""></p>
<h2 id="k1"><a href="#k1" class="headerlink" title="k1"></a>k1</h2><p>如果 k1 取 0，则对应 BIM 模型，document term frequency 完全没有影响，rare word (idf)和 repeated query terms(query tf) dominate；如果 k1 取较大值，对应使用原始的 term frequency。</p>
<h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><p>b (0&lt;=b&lt;1) 决定文档的缩放长度：b=1 表示基于文档长度对 term frequency 进行完全的缩放，b=0 表示归一化时不考虑文档长度因素，长文档更有可能排在前面。</p>
<h2 id="k3"><a href="#k3" class="headerlink" title="k3"></a>k3</h2><p>如果查询很长，对于 query term 也可以采用类似的权重计算方法。对查询长度没有进行归一化（相当于b=0）。k3=0 表示 term frequency in query 并没有影响，(apple apple pie) 和 (apple pie) 完全一样。<br>这一项通常是由用户确定的，对应的 operator 是 $WSUM</p>
<h2 id="参数优化"><a href="#参数优化" class="headerlink" title="参数优化"></a>参数优化</h2><p>整个公式的参数可以通过在单独的开发测试集上搜索最优参数来最大化检索性能，如网格搜索方法（grid search）。现有的试验中，参数的合理取值范围是 k1,k3 取 1.2~2，b 取 0.75。</p>
<p>除了对用户查询提供 term frequency 计算方法外，在相关反馈中还可以考虑查询扩展，在已知的相关文档利用公式对 term 进行排序，并选取最靠前的多个 term 构成新的查询，再进行计算。</p>
<h2 id="RSJ-weight"><a href="#RSJ-weight" class="headerlink" title="RSJ weight"></a>RSJ weight</h2><p>RSJ weight 和 idf 相似，都 favor rare words in corpus，因为 rare words 能更好的区分相关文档与不相关文档。RSJ 也有不足的地方。<br>如果 df=N/2，那么 RSJ weight 就会变成 log(1)=0，匹配一个 term 对文档分数没有任何影响。<br>如果 df&gt;N/2，RSJ weight=log(fraction)&lt;0，匹配一个经常出现的 term 会降低文档分数。</p>
<p>通常的解决方案是，把 RSJ weight 设置成 $Max(0，log{N-df+0.5 \over df+0.5})$。如果我们用 idf 公式代替 RSJ weight，那么对 frequent words 的惩罚就会减小，尤其是对那些出现了 N/2 的词。</p>
<p>最近，Lucene 转变了 ranking 算法，变成了 BM25 Ranker<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/lucene_bm25.png" alt=""></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><strong>优点：</strong></p>
<ul>
<li>有很强的概率理论支持</li>
<li>在新的环境中参数可以被调整</li>
<li>在大量的 evalution 中都非常有效</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>经验调整参数</li>
</ul>
<h1 id="Query-Likelihood"><a href="#Query-Likelihood" class="headerlink" title="Query Likelihood"></a>Query Likelihood</h1><p>假定四个变量</p>
<ul>
<li>d: document</li>
<li>$\theta_d$: language model for document d</li>
<li>q: query</li>
<li>$\theta_q$: language model for query q</li>
</ul>
<p>q 和 $\theta_q$，d 和 $\theta_d$ 不是同一个东西，然而为了方便表示，我们就用 q 直接表示，p(d|q) 代替 p(d|$\theta_q$)</p>
<p>我们生成两个模型，一个是 document 的语言模型，一个是 query 的语言模型，有两种方案来对文档进行排序</p>
<ul>
<li>Rank d by $p(d|\theta_q)$     (query likelihood)</li>
<li>Rank d by similarity of $\theta_d$ and $\theta_q$  (KL divergence)</li>
</ul>
<h2 id="Rank-by-P-d-q"><a href="#Rank-by-P-d-q" class="headerlink" title="Rank by P(d|q)"></a>Rank by P(d|q)</h2><p>给定一个 query，出现文档 d 的概率，query 一般很短，$\theta_q$ 非常的稀疏，它包含了很少的 term frequency 信息，所以我们用 Bayes rule 来转换它。</p>
<p>$p(d|q)={p(q|d)p(d) \over p(q)}$<br>–&gt; 丢掉 document-independent term<br>$p(q|d)p(d)$<br>–&gt; 丢掉 constant term<br>$p(q|d)$<br>–&gt;<br>$\prod p(q_i|d)$</p>
<p>于是问题就变成了怎么估计 $p(q_i|d)$</p>
<h3 id="p-q-i-d"><a href="#p-q-i-d" class="headerlink" title="$p(q_i|d)$"></a>$p(q_i|d)$</h3><p>我们用最大似然 (Maximum likelihood estimation MLE)。<br>$$P_{MLE}(q_i|d)={tf_{q_i,d} \over length(d)}$$</p>
<p><strong>是一个好的估计吗？</strong><br>首先它基于一篇文档，所以结果可能没那么准确，如果 document 里没有出现 $q_i$，那么结果就是 0，这相当于一个 boolean AND，所以 $q_i$ 是对 document 的一个不错的描述，即使它不在 document 中。</p>
<p>所以我们要用 smoothing，来提高 MLE 的准确性，同时来预测没有出现过的词。</p>
<h3 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h3><h4 id="Jelinek-Mercer-“Mixture-Model”-Smoothing"><a href="#Jelinek-Mercer-“Mixture-Model”-Smoothing" class="headerlink" title="Jelinek-Mercer(“Mixture Model”) Smoothing"></a>Jelinek-Mercer(“Mixture Model”) Smoothing</h4><p>$$p(q_i|d)=(1-\lambda)p_{MLE}(q_i|d)+ \lambda p_{MLE}(q_i|C)$$</p>
<p>C 代表整个 collection，$\lambda$ 越小，smoothing 的作用越小，越适合短 query，$\lambda$ 越大，smoothing 的作用越大，越适合长 query。为什么？对短文档而言，通常每一个 query term 都要匹配，所以 idf weighting 并没有那么重要，越小的 smoothing 越好，而对于长 query 而言，大部分 query term 必须匹配，而另一部分可以不 match，所以 idf weighting 会更重要，就可以多 smoothing 一点。</p>
<p>Jelinek-Mercer smoothing 的作用与 idf 类似，它能够区分文档集合里常见的和不常见的 term 。</p>
<p>看一个具体例子，有两个 query term，一个 frequent 一个 rare。</p>
<pre>p(apple|C)=0.01, p(ipod|C)=0.001</pre>

<p>两篇文档</p>
<pre>doc1: doclen=50,$tf\_{apple}=2$,$tf\_{ipod}=3$
doc2: doclen=50,$tf\_{apple}=3$,$tf\_{ipod}=2$</pre>

<p>没有 smoothing 前，两篇文档的 p(q|d)都是 0.0024</p>
<pre>2/50 ＊ 3/50=0.0024
3/50 ＊ 2/50=0.0024</pre>

<p>JM Smooth 后，假设 $\lambda=0.4$</p>
<pre>doc1: p(q|d)=(0.6* 2/50 + 0.4*0.01) * (0.6 * 3/50 + 0.4*0.001)=0.001019
doc2: p(q|d)=(0.6* 3/50 + 0.4*0.01) * (0.6 * 2/50 + 0.4*0.001)=0.000976</pre>

<p>这就发现，smooth 能够区分文档集合里常见的和不常见的 term。<br>我们也可以计算 doclen=50,$tf_{apple}=2$,$tf_{ipod}=2$ 的情况，看多加入一个 apple 或 ipod 后 p(q|d) 发生了什么，同样的，unsmoothed effect 对常见的和不常见的 term 并没有差别，但是 smooth 带来了显著差异。</p>
<p>最后上张推导图<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/JM_smo.png" alt=""></p>
<h4 id="Bayesian-Smoothing-With-Dirichlet-Priors"><a href="#Bayesian-Smoothing-With-Dirichlet-Priors" class="headerlink" title="Bayesian Smoothing With Dirichlet Priors"></a>Bayesian Smoothing With Dirichlet Priors</h4><p>$$p(q_i|d)={tf_{q_i,d}+ \mu p_{MLE}(q_i|C) \over length(d)+ \mu }$$</p>
<p>$\mu$ 在 [1000-10000] 区间内比较好<br>Bayesian smoothing 是对文档长度的平滑，对短文档而言，$p(q_i|d)$ 的概率分布更不平滑，需要更大的 $\mu $，对长文档而言，概率分布更平滑，需要更小的 $\mu$.</p>
<h3 id="Two-Stage-Smoothing"><a href="#Two-Stage-Smoothing" class="headerlink" title="Two-Stage Smoothing"></a>Two-Stage Smoothing</h3><p>可以结合以上两种平滑方式，得到<br>$$p(q_i|d)=(1- \lambda ){tf_{q_i,d}+ \mu p_{MLE}(q_i|C) \over length(d)+ \mu }+ \lambda p_{MLE}(q_i|C)$$</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/query_likeli.png" alt=""></p>
<h2 id="Rank-by-similarity"><a href="#Rank-by-similarity" class="headerlink" title="Rank by similarity"></a>Rank by similarity</h2><p>KL Divergence<br>Kullback-Leibler 距离，也叫相对熵（Relative Entropy）。计算公式如下：<br>$$KL(p||q)=\sum p(x)log{p(x) \over q(x)}$$</p>
<p>KL 距离不是对称的, KL(p||q)!=KL(q||p)，我们要计算的是 KL(q||d)，query 和 document 的相对熵，推导公式如下。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/KL_sim.png" alt=""></p>
<h2 id="Comparsion"><a href="#Comparsion" class="headerlink" title="Comparsion"></a>Comparsion</h2><p>两种方式其实是一样的，仔细看公式！<br>Query likelihood ranks by<br>$$p(q|d)=\prod p(q_i|d)$$</p>
<p>KL diverge ranks by<br>$$\sum p(x)log{p(x) \over q(x)}$$</p>
<h1 id="Inference-networks-Indri"><a href="#Inference-networks-Indri" class="headerlink" title="Inference networks(Indri)"></a>Inference networks(Indri)</h1><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_framework.png" alt=""></p>
<p>document + smoothing parameter($\alpha$ $\beta$) -&gt; language model($\theta$) -&gt; language model vocabulary(r)</p>
<p>information needs(I)由 query(q) 表示，query 由 operator(c) 组成.</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_operator.png" alt=""></p>
<p>在 Indri 中，#AND 认为所有的 argument 都是独立概率， #WSUM 认为所有的 argument 都用来估计同一个概率。<br>在实现 Indri 的 ranking algorithm 时，要注意的是我们必须实现一个 getDefaultScore，来处理 tf=0 的情况，以保证用户总能得到搜索结果。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_default.png" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/Indri_default2.png" alt=""></p>
<h1 id="Document-Priors"><a href="#Document-Priors" class="headerlink" title="Document Priors"></a>Document Priors</h1><p>Document Priors 指与 query 无关的用来评估文档价值的 estimates (query-independent estimates of the value of each document)，一般是根据文档本身的性质来决定的，主要的 document priors 有 spam score, PageRank, length of url 等。<br>在 BM25，query likelihood 和 KL divergence 中的使用。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/priors.png" alt=""><br>Indri 中，prior 在 Query likelihood 中的表示为 #and(#prior(url) a b c)，在 KL divergence 中的表示为 #and(#prior(url) #and(a b c))</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Document Representations]]></title>
      <url>http://www.shuang0420.com/2016/09/25/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Document%20Representation/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。这一篇讲两种 document representation 方法，<strong>Controlled vocabulary index terms</strong> vs <strong>Free-text or full-text index terms</strong><br><a id="more"></a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/Doc_repres.png" alt=""></p>
<h1 id="Controlled-vocabulary-index-terms"><a href="#Controlled-vocabulary-index-terms" class="headerlink" title="Controlled vocabulary index terms"></a>Controlled vocabulary index terms</h1><p>从一个 well-defined classification scheme 中挑取 term，比较有名的开放分类目录有 dmoz。</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>broad vocabularies 来描述概括性的 topic; detailed vocabularies 来描述更加细节的 topic。一个 well-defined classification scheme 主要有以下构成：</p>
<ul>
<li>用于识别文档主题的一组规则</li>
<li>指定的词库</li>
<li>一组索引的术语(term)</li>
<li>用于分配索引项的一组规则</li>
</ul>
<h2 id="Advantages-and-Disadvantages"><a href="#Advantages-and-Disadvantages" class="headerlink" title="Advantages and Disadvantages"></a>Advantages and Disadvantages</h2><p>优点：</p>
<ul>
<li>高的召回率</li>
<li>支持浏览和搜索</li>
<li>在一些领域非常流行（如医学，法律，专利等）</li>
</ul>
<p>缺点：</p>
<ul>
<li>coverage vs detail tradeoff</li>
<li>人工创建和维护的成本很高</li>
<li>人们难以一致地分配文件</li>
<li>检索受限制</li>
</ul>
<h1 id="Free-text-or-full-text-index-terms"><a href="#Free-text-or-full-text-index-terms" class="headerlink" title="Free-text or full-text index terms"></a>Free-text or full-text index terms</h1><p>从原文档或者相关文档中挑取 term。Free-text or full-text indexing 用的是 uncontrolled vocabulary。Free-text 和 full-text indexing 的区别在于前者只用了部分的 term 作为 index，而后者用了几乎所有的 term 来作为 index。</p>
<p><strong>How to select terms?</strong></p>
<ul>
<li>selected terms 人工选择</li>
<li>all terms 就不用考虑选择的问题</li>
</ul>
<h2 id="Advantages-and-Disadvantages-1"><a href="#Advantages-and-Disadvantages-1" class="headerlink" title="Advantages and Disadvantages"></a>Advantages and Disadvantages</h2><p>优点：</p>
<ul>
<li>索引词汇保证与文档的内容有很好的匹配</li>
<li>无需学习（可能会很复杂的）受控词表</li>
<li>可能比控制词汇更容易自动化</li>
</ul>
<p>缺点：</p>
<ul>
<li>更可能会导致词汇比匹配<br>比如文档里有 automobile，query 说是 car，就不能 match</li>
</ul>
<h2 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h2><blockquote>
<p>Search engine uses shallow language analysis and heuristics to convert lexical tokens (usually words) into index terms (features)</p>
</blockquote>
<p>Heuristic methods: map tokens to indexing terms</p>
<h3 id="Stopwords"><a href="#Stopwords" class="headerlink" title="Stopwords"></a>Stopwords</h3><p>一些 stopwords 如 the, a 并没有实际意义，删除 stopwords 可以减小 index size，提高准确性和效率，然而也会带来一些问题，如无法处理一些 query(eg. To be or not to be, let it be)。解决方案是我们把 index 的 stopwords 存下来，在处理 query 的时候去掉 query 里的 stopwords，如果 stopwords 在 query terms 里占比很高，或者用户明确要求留下 stopwords (eg. +the last)，就把 stopwords 留下。</p>
<p>优点：</p>
<ul>
<li>丢掉不具有内容信息的词</li>
<li>大幅减少索引大小，减少检索时间</li>
<li>提高准确性</li>
</ul>
<p>缺点：</p>
<ul>
<li>难以满足某些特殊的 query (eg. To be or not to be, let it be)</li>
</ul>
<p><strong>创建 stopword list</strong><br>通过 frequency analysis 和 manual review 来完成。</p>
<ul>
<li>基于频率对字典进行排序</li>
<li>检查最常用的 term</li>
<li>检查查询日志，查看哪些频繁的 term 可能很重要</li>
</ul>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><p>通常我们需要对 token 进行规范化，比如大小写转换，以便下一步处理。</p>
<p>优点:</p>
<ul>
<li>提高召回率，匹配更多查询</li>
</ul>
<p>缺点：</p>
<ul>
<li>如 Apple 可以用作公司名称，而 apple 将被视为一种水果。</li>
</ul>
<h3 id="Morphological-analysis"><a href="#Morphological-analysis" class="headerlink" title="Morphological analysis"></a>Morphological analysis</h3><p>其实是一种映射。Map a token to another token (“stemming”,”conflation”) eg. images -&gt; image<br>常用的 stemming algorithms 有 Porter, KSTEM 等，一般来说，Porter 和 KSTEM 能产生的差不多准确的 search results。Porter 更加的 aggressive，可能会出现一些不是词的词，而 KSTEM 更加的保守，很少会产生 smaller conflation classes，更加像”词”。<br>对于 <strong>企业检索</strong> 而言，corpus 相对较小，recall 通常很重要，所以用户为了得到更多的相关文档，对 stemming mistakes 容忍度较高。而对于 <strong>网页检索</strong> 而言，corpus 很大，recall 并没有那么重要，precision 更重要，所以对 stemming/lemmatization mistakes 容忍度更低，所以并不使用。Google 之前是不做 stemming 的，现在似乎开始做了。</p>
<p>这些技术都是因语言而异的，不同的语言有不同的语法规则，不能一概而论。</p>
<p><strong>优点:</strong></p>
<ul>
<li>Conflating variations of a word<br>  更准确地表示文档<br>  匹配更广泛的查询</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>效果不一致，Stemming 的结果可能不是词语</li>
<li>term 可能被错误地分组，不相关的词可能具有相同的 stem（例如，Apple,apple）</li>
<li>复杂的 morphological analysis 可能非常缓慢</li>
</ul>
<h3 id="Phrases"><a href="#Phrases" class="headerlink" title="Phrases"></a>Phrases</h3><p>对 phrase 的处理，一般有两种方案。</p>
<ol>
<li>一种是 precoordinate(one inverted list)，把词组存为 index，比如 interest rate，inverted list 存成 interest_rate，在用户查询时 interest rate 时，替换成 interested_rate 进行 match。这种做法耗费了很多空间，怎样选择要存储的词组也是个问题，事实上可能会存很多永远不会被查询的词组。</li>
<li>另一种是 postcoordinate(more than one inverted lists)，对 query 进行 reformulation, 如 interest rate 变成 #NEAR/1(interest rate)，然后进行 match。这种方法查询时会有些慢，然而不必纠结于词组的选择。</li>
</ol>
<h3 id="De-compounding"><a href="#De-compounding" class="headerlink" title="De-compounding"></a>De-compounding</h3><p>computer-virus -&gt; computer,virus</p>
<p><strong>优点</strong></p>
<ul>
<li>更准确地表示文档</li>
<li>匹配更广泛的查询</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>N-grams 像 “roe v. wade” 会变得没有意义</li>
</ul>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>Basic lexical processing</p>
<ul>
<li>tokens</li>
<li>stopwords</li>
<li>morphologial processing (“stemming”)</li>
</ul>
<p>Other representations</p>
<ul>
<li>phrases, citations and inlink text, paths and urls</li>
</ul>
<p>Multiple representations</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Evaluating Search Effectiveness]]></title>
      <url>http://www.shuang0420.com/2016/09/20/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。怎样评估 search engine 的效果？<br><a id="more"></a></p>
<h1 id="Cranfield-Methodology"><a href="#Cranfield-Methodology" class="headerlink" title="Cranfield Methodology"></a>Cranfield Methodology</h1><ul>
<li>获得 <strong>文档(documents)</strong> 集合</li>
<li>获得 <strong>信息需求(information needs)</strong> 集合</li>
<li>获得 <strong>相关性判断(relevance judgments)</strong></li>
<li><strong>计算(Measure)</strong> 各种方法找到相关文档的效果</li>
<li><strong>比较(Compare)</strong> 各个方法的 effectiveness</li>
</ul>
<p>所以有五个部分：</p>
<ul>
<li>文档(documents)</li>
<li>信息需求(information needs)</li>
<li>相关性判断(relevance judgments)</li>
<li>指标(metrics)</li>
<li>对比(comparison of methods)</li>
</ul>
<p>我们逐一来讨论</p>
<h2 id="Test-collections"><a href="#Test-collections" class="headerlink" title="Test collections"></a>Test collections</h2><p>documents, information needs, relevance judgements 三部分合起来称为一个 test collection。常用的 test collections 有<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/Test_collections.png" alt=""></p>
<p>这些 test collections 都非常实用，然而都有各自的 bias。</p>
<h3 id="Information-Needs"><a href="#Information-Needs" class="headerlink" title="Information Needs"></a>Information Needs</h3><p>一般来说，一个 test collection 有 50-200 个 information needs。information need 通常由 query 来体现，当然，也可以通过 search engines 中获得的 user behavior, user history, population behavior 来体现。那么，<strong>怎样获得 information needs 呢？</strong> 通常有三种办法。</p>
<ul>
<li>Ask 向用户询问他们要找什么，这当然是最优的方法。</li>
<li>Observe 通过 search log，根据 query, clicks 等来观察用户需要什么，然后根据观察结果来还原 information needs。</li>
<li>Guess 根据文档来猜这些文档能满足什么样的 information needs，这是 weakest option，但往往也是唯一的选择。</li>
</ul>
<h3 id="Relevance-Assessment"><a href="#Relevance-Assessment" class="headerlink" title="Relevance Assessment"></a>Relevance Assessment</h3><p>人为判断，通常是主观的。<br>用不同的 techniques 检索出文档，然后人为判断每一种 technique 下的 top n 的文档，relevant set 就是这些判断为相关的文档的集合。</p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><h3 id="Unranked-Boolean-Retrieval-Model"><a href="#Unranked-Boolean-Retrieval-Model" class="headerlink" title="Unranked Boolean Retrieval Model"></a>Unranked Boolean Retrieval Model</h3><p>P,R,P@n,F1 都是 set-based measures。适合 unranked boolean retrieval model，适合文本分类，然而对 ranked retrieval model 没那么适用。</p>
<h4 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h4><p>$$Precision = {|Relevant \cap Retrieved| \over |Retrieved|} $$<br>$$Recall = {|Relevant \cap Retrieved| \over |Relevant|}$$</p>
<p>Precision-recall curve 呈现明显的锯齿形状，因为如果返回的第 k+1 篇文档不相关，那么在 k+1 篇文档位置上的 recall 和前 k 篇文档位置上的 recall 一样，但是 precision 显然下降。反之，如果返回的第 k+1 篇文档相关，那么 recall 和 precision 都会增大，这时候曲线会呈锯齿形上升。将这些细微的变化去掉通常采用差值</p>
<p>理论上来讲，整个文档集都会被 rank，然后对整个文档集来计算 P&amp;R，然而这是没有必要的，所以引入了 P@n 和 MAP(Mean average precision)。</p>
<h4 id="P-n"><a href="#P-n" class="headerlink" title="P@n"></a>P@n</h4><p>非常好理解，排名前 n 的文档的 precision。如 P@5,P@10。带来的问题是并没有对 query 的难度进行 normalize。简单的 query 可能会有更多的相关文档，难的 query 能得到的相关文档更少。所以 P@n 的 stability 不如 MAP.</p>
<h4 id="F-Measure"><a href="#F-Measure" class="headerlink" title="F-Measure"></a>F-Measure</h4><p>对 precision 和 recall 进行 weight<br>$$F = {1 \over \alpha {1 \over P} + (1- \alpha) {1 \over R}}$$<br>如果 precision 和 recall 的权重相同，那就是 $F={2PR \over P+R}$</p>
<h4 id="Average-Results"><a href="#Average-Results" class="headerlink" title="Average Results"></a>Average Results</h4><h5 id="Micro-average-across-documents"><a href="#Micro-average-across-documents" class="headerlink" title="Micro average across documents"></a>Micro average across documents</h5><p>每篇文档的重要性相同，具有许多相关文档的查询占主导地位，machine learning 常用， IR 不常用，因为 class distribution 更加的 skewed。</p>
<h5 id="Macro-average-across-queries"><a href="#Macro-average-across-queries" class="headerlink" title="Macro average across queries"></a>Macro average across queries</h5><p>每个 query 的重要性相同，ad-hoc retrieval 最常用的 averaging method。</p>
<h3 id="Ranked-Retrieval"><a href="#Ranked-Retrieval" class="headerlink" title="Ranked Retrieval"></a>Ranked Retrieval</h3><ul>
<li>Average Precision (AP)</li>
<li>Mean Average Precision (MAP)</li>
<li>Interpolated Average Precision</li>
</ul>
<h4 id="AP-and-MAP"><a href="#AP-and-MAP" class="headerlink" title="AP and MAP"></a>AP and MAP</h4><p>MAP 是 single-value。AP 对单个需求，求返回结果中每篇相关文档位置上的 precision 的平均值。相当于某个 query 下对应的多条 precision-recall curve 下面积的平均值。对所有需求平均就能得到 MAP。MAP 可以在每个 recall 水平上提供单指标结果，具有非常好的 discrimination 和 stability。MAP 不需要选择固定的 recall 水平，也不需要插值，即使有些 query 的相关文档数很多而有些很少，最终的 MAP 显示每个 query 的作用却是相等的。单个系统在不同 information needs 的 MAP值相差较大（0.1-0.7），不同系统在同一 information need 上的 MAP 差异反而相对要小一些。</p>
<p>一道题解决。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/AP&amp;MAP.png" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">AP1=(1+1+0.75+0.67)/4=0.855</div><div class="line">AP2=(1+0.84+0.5)/5=0.468</div><div class="line">MAP=(0.468+0.855)/2=0.6615</div></pre></td></tr></table></figure></p>
<p>看一下 AP 和 MAP 的分布。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/AP_MAP_dis.png" alt=""></p>
<p>MAP 用的非常多，一方面是因为它能很好的体现系统的优劣，一般来说，如果 MAP(A)&gt;MAP(B)，那么 A 系统更有可能比 B 系统好，像 P@n 等其它 metrics 就不能如此肯定。另外，MAP 计算很快，和 NDCG 相比。</p>
<h4 id="MRR-Mean-Reciprocal-Rank"><a href="#MRR-Mean-Reciprocal-Rank" class="headerlink" title="MRR (Mean Reciprocal Rank)"></a>MRR (Mean Reciprocal Rank)</h4><p>有时候我们更关心第一篇相关文档。而排名较低的文档通常不会被浏览到。<br>Reciprocal rank 指的是 1/rank of first relevant document。所以 MRR 就是对所有需求的 RR 值求平均。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/metrics_reconsider.png" alt=""></p>
<p>适用场景举例：某个学生想找 cmu 11642 的课程主页。</p>
<h4 id="NDCG-Normalized-Discounted-Cumulative-Gain"><a href="#NDCG-Normalized-Discounted-Cumulative-Gain" class="headerlink" title="NDCG (Normalized Discounted Cumulative Gain)"></a>NDCG (Normalized Discounted Cumulative Gain)</h4><p>Web search engines 中常用的方法。multi-valued relevance assessment，评估 ranking 的质量。<br>$$NDCG@k = Z_k \sum_{i=1}^k{2^{R_i}-1 \over log(1+i)}$$</p>
<ul>
<li>$R_i$ 指排名在 i 的相关文档的 relevance 分数</li>
<li>$Z_k$ normalize，所以 NDCG@k=1 时是一个 perfect ranking。<br>$Z_k$ = 1/DCG@k for the “ideal” ranking</li>
</ul>
<p>适用场景：可能有多个相关文档，且用户浏览文档的概率取决于页面的排名。eg. 顾客想要在网上买一台电脑，需要比较不同的型号、外观、价钱、排名。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/NCDG.png" alt=""></p>
<h4 id="RBP-Rank-Biased-Precision"><a href="#RBP-Rank-Biased-Precision" class="headerlink" title="RBP (Rank-Biased Precision)"></a>RBP (Rank-Biased Precision)</h4><p>非常简单的 model，对用户行为进行建模。multi-valued relevance assessments，用 user’s persistence 来评估 rank 质量。<br>$$RBP = (1-p) \sum_{i=1}^n R_ip^{i-1}$$</p>
<ul>
<li>p: user’s persistence</li>
<li>n: 文档数量</li>
<li>Ri: 第 i 篇文档的排名</li>
</ul>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/RBP%20.png" alt=""></p>
<h2 id="trec-eval"><a href="#trec-eval" class="headerlink" title="trec-eval"></a>trec-eval</h2><p>ad-hoc retrieval 的标准的评估工具。<br><a href="http://boston.lti.cs.cmu.edu/classes/11-642/HW/HTS/trec_eval.pdf" target="_blank" rel="external">格式</a></p>
<h1 id="Create-test-collections"><a href="#Create-test-collections" class="headerlink" title="Create test collections"></a>Create test collections</h1><ol>
<li>收集大量的代表性文档(representative documents)</li>
<li>收集代表性信息需求(representative information needs)，至少 25 条， 最好 50-100 条</li>
<li>把信息需求转换成 query 集合，一个信息需求至少要有两三个 query</li>
<li>在每个搜索引擎上运行 query，保存 top N 的文档，至少每个查询 50 篇文档</li>
<li>合并同一个信息需求的所有 query 在不同搜索引擎上的结果并随机排序</li>
<li>雇佣人员来判断文档相关性，保证一条信息需求下的所有文档必须由同一个人来判断。</li>
</ol>
<h1 id="Evaluation-in-a-Dynamic-Environment"><a href="#Evaluation-in-a-Dynamic-Environment" class="headerlink" title="Evaluation in a Dynamic Environment"></a>Evaluation in a Dynamic Environment</h1><h2 id="Interleaved-testing"><a href="#Interleaved-testing" class="headerlink" title="Interleaved testing"></a>Interleaved testing</h2><ul>
<li>Input: 两个 rankings，分别由不同方法产生</li>
<li>Output: 一个 ranking，由不同方法的所有 document 产生，一个好的 output 不会偏好任何一个方法。</li>
</ul>
<p><strong>Requirements:</strong></p>
<ul>
<li>用户不会注意到这一过程</li>
<li>对用户偏见不敏感</li>
<li>需要有反映用户偏好的用户行为</li>
<li>不应该改变用户的搜索体验</li>
</ul>
<p><strong>Procedure:</strong><br>One trial</p>
<ul>
<li>用户提交 query</li>
<li>搜索引擎选择两种排序方法(“A” and “B”)，每种方法产生一个 document ranking</li>
<li>交替选择两种 document rankings</li>
<li>追踪 interleaved document ranking 的 click 情况</li>
<li>当用户停止点击的时候，根据被点击的文档给 “A” and “B” 两种方法分配 credit，确定在这一次 trial 中获胜的方法。</li>
</ul>
<p>Repeat until enough trials are collected</p>
<h3 id="Balanced-interleaving"><a href="#Balanced-interleaving" class="headerlink" title="Balanced interleaving"></a>Balanced interleaving</h3><p>Assume that people read from top to bottom<br><strong>假设:</strong></p>
<ul>
<li>用户从上往下浏览文档</li>
<li>用户会点击看起来合适的文档</li>
<li>当用户觉得已经满意了或者是失望了时，他们会停止浏览</li>
<li>每种方法(“A” and “B”)呈现文档的概率是相同的</li>
<li>用户(random clicker)点击的文档来自 “A” 或者 “B”的概率都是 50%</li>
</ul>
<p><strong>算法：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/balI.jpg" class="ful-image" alt="balI.jpg"></p>
<p>算法非常简单。首先决定从哪个方法开始，然后交替把文档加入 interleaved ranking，如果遇到了已经评估过的文档，就直接 counter++，但是不把文档加进 interleaved ranking 里。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/balI.pic.jpg" class="ful-image" alt="balI.pic.jpg"></p>
<p>这样我们就得到了一个 ranking，然后我们还有一组数据是用户 click 的顺序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">I   C</div><div class="line">i1  </div><div class="line">i2  c1</div><div class="line">i3</div><div class="line">i4</div><div class="line">i5  c2</div><div class="line">i6</div><div class="line">i7</div><div class="line">i8  c_max</div></pre></td></tr></table></figure></p>
<p>$a_1,…,a_k$ 的集合与 $b_1,…b_k$ 的集合的并集包含了所有在 $i_1,…,i_{c_{max}}$ 中的文档，然后我们可以计算在 a’s top k 的点击数和 b’s top k 的点击数，得到最多 clicks 的方法获胜。<br>$$\Delta(A,B) = {wins(A)+0.5*ties(A,B) \over wins(A)+wins(B)+ties(A,B)}$$</p>
<p><strong>E.g.,</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/balI.eg.jpg" class="ful-image" alt="balI.eg.jpg"></p>
<p>然而 Balanced-Interleaving 也可能带来意想不到的结果，如下图，假设用户随机点击了一个 result，那么有 3/4 的结果都是对 B 有利的，为什么？因为 3/4 的文档在 B 里的 ranking 都比 A 高！<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/balI.eg2.jpg" class="ful-image" alt="balI.eg2.jpg"></p>
<h3 id="Team-draft-interleaving"><a href="#Team-draft-interleaving" class="headerlink" title="Team-draft interleaving"></a>Team-draft interleaving</h3><p><strong>算法：</strong><br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/TDI.jpg" class="ful-image" alt="TDI.jpg"></p>
<p>每一轮都随机产生先取哪种方法，如果有重复，跳过，取下一个。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/TDI.pic.jpg" class="ful-image" alt="TDI.pic.jpg"></p>
<p>之后的步骤与 Balanced-Interleaving 相同。Team-draft 也可能产生难以预料的结果，如下图。<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/TD.jpg" class="ful-image" alt="TD.jpg"></p>
<h2 id="Metrics-1"><a href="#Metrics-1" class="headerlink" title="Metrics"></a>Metrics</h2><ul>
<li>Abandonment rate: % of queries that receive no clicks</li>
<li>Reformulation rate: % of queries that are reformulated</li>
<li>Queries per session: Session == Information need</li>
<li>Clicks per query, Clicks@1</li>
<li>pSAT-clicks: % of documents with dwell time &gt; 30 seconds</li>
<li>pSkip: % of documents that are skipped</li>
<li>Max Reciprocal Rank, Mean Reciprocal Rank</li>
<li>Time to First Click, Time to Last Click</li>
</ul>
<h1 id="Cranfield-vs-Interleaving"><a href="#Cranfield-vs-Interleaving" class="headerlink" title="Cranfield vs. Interleaving"></a>Cranfield vs. Interleaving</h1><p>一般来说，我们更多的会使用 Cranfield，因为</p>
<ul>
<li>Cranfield 更成熟，已经使用了很多年而且易于理解</li>
<li>Cranfield 支持大量的 metrics，能提供更多关于 ranking behavior 的信息</li>
<li>Cranfield 几乎在所有场景下都使用，而 Interleaving 需要有 query traffic</li>
</ul>
<p>尽管如此，interleaving 仍然是一个很有用的工具，在下面的条件下可以使用。</p>
<ul>
<li>Inexpensive, adaptive, sensitive to small differences</li>
</ul>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Evaluating%20Search%20Effectiveness/compare.jpg" class="ful-image" alt="compare.jpg">
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 搜索]]></title>
      <url>http://www.shuang0420.com/2016/09/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%90%9C%E7%B4%A2/</url>
      <content type="html"><![CDATA[<p>两种搜索方式，对 unordered array 用 linear search，对 ordered array 用 binary search。<br><a id="more"></a></p>
<h1 id="二分查找-Binary-search"><a href="#二分查找-Binary-search" class="headerlink" title="二分查找 (Binary search)"></a>二分查找 (Binary search)</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>对于已排序的有序线性容器而言(比如数组，vector)，二分查找(Binary search)几乎总是最优的搜索方案。二分查找将容器等分为两部分，再根据中间节点与待搜索数据的相对大小关系，进一步搜索其中某一部分。二分查找的算法复杂度为O(logn)。<br>对于局部有序的数据，也可以根据其局部有序的特性，尽可能地利用逼近、剪枝，使用二分查找的变种进行搜索。</p>
<p>二分寻找要注意的问题是：</p>
<ul>
<li>Which way should middle pointer go next</li>
<li>Avoid infinite loop in the code</li>
</ul>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><blockquote>
<p>Compare the number in the middle of the array with x. If it is equal, we are done. If the number is greater, we know to look in the second half of the array. If it is smaller, we know to look in the first half. We can repeat the search on the appropriate half of the array by comparing the middle element of that array with x, once again narrowing our search by a factor of 2. We repeat this process until we find x. This algorithm takes O(log n) time.</p>
</blockquote>
<h2 id="Complexity"><a href="#Complexity" class="headerlink" title="Complexity"></a>Complexity</h2><p>Time complexity: O(logN)<br>Worst case: O(logN+1) -&gt; O(logN)</p>
<h2 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h2><p><strong>Recursive</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def binarySearch(data, start, end, key):</div><div class="line">    if start &gt; end:</div><div class="line">        return -1</div><div class="line">    mid = start + (end - start) / 2</div><div class="line">    if data[mid] == key:</div><div class="line">        return mid</div><div class="line">    if data[mid] &lt; key:</div><div class="line">        return binarySearch(data, mid + 1, end, key)</div><div class="line">    else:</div><div class="line">        return binarySearch(data, start, mid - 1, key)</div></pre></td></tr></table></figure></p>
<p><strong>Non-recursive</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def binarySearch(data, key):</div><div class="line">    start = 0</div><div class="line">    end = len(data) - 1</div><div class="line">    while True:</div><div class="line">        if start &gt; end:</div><div class="line">            return -1</div><div class="line">        mid = start + (end - start) / 2</div><div class="line">        if data[mid] == key:</div><div class="line">            return mid</div><div class="line">        if data[mid] &lt; key:</div><div class="line">            start = mid + 1</div><div class="line">        else:</div><div class="line">            end = mid - 1</div></pre></td></tr></table></figure></p>
<p><strong>Find closest value</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def binarySearch(house, heaters):</div><div class="line">    left, right = 0, len(heaters)</div><div class="line">    while left &lt; right:</div><div class="line">        mid = left + (right - left) / 2</div><div class="line">        if heaters[mid] &lt; house:</div><div class="line">            left = mid + 1</div><div class="line">        else:</div><div class="line">            right = mid</div><div class="line">    return left</div></pre></td></tr></table></figure></p>
<h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><h3 id="69-Sqrt-x"><a href="#69-Sqrt-x" class="headerlink" title="69. Sqrt(x)"></a>69. Sqrt(x)</h3><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>Implement int sqrt(int x).<br>Compute and return the square root of x.</p>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Search n from 0 to x, every time increment 1, maintain a global varible pre to record the last n whose square is smaller than x, for each n, check if n*n&gt;x, if so, return pre. Takes O(n) time.</div><div class="line"></div><div class="line">Followup:</div><div class="line">    Use binary search to find correct n. Takes O(logn) time.</div><div class="line"></div><div class="line">About overflow:</div><div class="line">    use mid=start+(end-start)/2</div><div class="line">    mid * mid will overflow when mid &gt; sqrt(INT_MAX)</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    O(n)</div><div class="line">    def mySqrt(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if x&lt;2: return x</div><div class="line">        pre=0</div><div class="line">        for n in range(2,x):</div><div class="line">            if n*n==x: return n</div><div class="line">            if n*n&lt;x: pre=n</div><div class="line">            else: return pre</div><div class="line">            &apos;&apos;&apos;</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    binary search O(logn)</div><div class="line">    &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def mySqrt(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not x or x &lt; 2:</div><div class="line">            return x</div><div class="line">        start = 0</div><div class="line">        end = x</div><div class="line">        while start &lt;= end:</div><div class="line">            mid = start + (end - start) / 2</div><div class="line">            if mid &lt;= x / mid and x / (mid + 1) &lt; mid + 1:</div><div class="line">                return mid</div><div class="line">            elif mid * mid &lt; x:</div><div class="line">                start = mid + 1</div><div class="line">            else:</div><div class="line">                end = mid - 1</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    Integer Newton</div><div class="line"></div><div class="line">    def mySqrt(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if x&lt;2: return x</div><div class="line">        r = x</div><div class="line">        while r &gt; x/r:</div><div class="line">            r = (r + x/r) / 2</div><div class="line">        return r</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h3 id="Find-first-bad-version"><a href="#Find-first-bad-version" class="headerlink" title="Find first bad version"></a>Find first bad version</h3><blockquote>
<p>You are a product manager and currently leading a team to develop a new product. Unfortunately, the latest version of your product fails the quality check. Since each version is developed based on the previous version, all the versions after a bad version are also bad.<br>Suppose you have n versions [1, 2, …, n] and you want to find out the first bad one, which causes all the following ones to be bad.<br>You are given an API bool isBadVersion(version) which will return whether version is bad. Implement a function to find the first bad version. You should minimize the number of calls to the API.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># The isBadVersion API is already defined for you.</div><div class="line"># @param version, an integer</div><div class="line"># @return a bool</div><div class="line"># def isBadVersion(version):</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def firstBadVersion(self, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type n: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if n==0:</div><div class="line">            return 0</div><div class="line">        start = 0</div><div class="line">        end = n</div><div class="line">        while start &lt; end-1:</div><div class="line">            mid = start + (start - end)/2</div><div class="line">            if isBadVersion(mid):</div><div class="line">                end = mid</div><div class="line">            else:</div><div class="line">                start = mid</div><div class="line">        return start if isBadVersion(start) else end</div></pre></td></tr></table></figure>
<h3 id="Search-a-2D-Matrix"><a href="#Search-a-2D-Matrix" class="headerlink" title="Search a 2D Matrix"></a>Search a 2D Matrix</h3><blockquote>
<p>Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties:<br>Integers in each row are sorted from left to right.<br>The first integer of each row is greater than the last integer of the previous row.<br>For example,<br>Consider the following matrix:<br>[<br>  [1,   3,  5,  7],<br>  [10, 11, 16, 20],<br>  [23, 30, 34, 50]<br>]<br>Given target = 3, return true.</p>
</blockquote>
<p>把这个 2D 数组拉平成 1D 就是一个单调递增的(sorted)的数组，这种情况下找一个数用 binary search 就好，时间复杂度是 O(log(mn))=O(logN)，要注意的就是 index 之间怎么转换，观察发现：<br>2D -&gt; 1D  (i,j) -&gt; i*n+j<br>1D -&gt; 2D  index -&gt; (index/n,index%n)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def searchMatrix(self, matrix, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type matrix: List[List[int]]</div><div class="line">        :type target: int</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not matrix:</div><div class="line">            return False</div><div class="line"></div><div class="line">        m = len(matrix) # row</div><div class="line">        n = len(matrix[0]) # column</div><div class="line"></div><div class="line">        start = 0</div><div class="line">        end = m*n-1</div><div class="line">        while start &lt;= end:</div><div class="line">            mid = start + (start - end) / 2</div><div class="line">            if matrix[mid/n][mid%n] == target:</div><div class="line">                return True</div><div class="line">            if matrix[mid/n][mid%n] &gt; target:</div><div class="line">                end = mid-1</div><div class="line">            else:</div><div class="line">                start = mid+1</div><div class="line">        return False</div></pre></td></tr></table></figure></p>
<h3 id="Search-a-2D-Matrix-II"><a href="#Search-a-2D-Matrix-II" class="headerlink" title="Search a 2D Matrix II"></a>Search a 2D Matrix II</h3><p>问题再变难一点。</p>
<blockquote>
<p>Search a 2D Matrix II  QuestionEditorial Solution  My Submissions<br>Total Accepted: 50080<br>Total Submissions: 136871<br>Difficulty: Medium<br>Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties:<br>Integers in each row are sorted in ascending from left to right.<br>Integers in each column are sorted in ascending from top to bottom.<br>For example,<br>Consider the following matrix:<br>[<br>  [1,   4,  7, 11, 15],<br>  [2,   5,  8, 12, 19],<br>  [3,   6,  9, 16, 22],<br>  [10, 13, 14, 17, 24],<br>  [18, 21, 23, 26, 30]<br>]<br>Given target = 5, return true.<br>Given target = 20, return false.</p>
</blockquote>
<p>当然还是可以用 binary search 做，需要非常仔细。时间复杂度 max(O(mlogn,nlogm)),由 T(n)=2T(n/2)+cn 推导而来<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/2D-matrix.jpg" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def searchMatrix(self, matrix, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type matrix: List[List[int]]</div><div class="line">        :type target: int</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not matrix:</div><div class="line">            return False</div><div class="line">        return self.helper(matrix,target,0,0,len(matrix)-1,len(matrix[0])-1)</div><div class="line"></div><div class="line"></div><div class="line">    def helper(self,matrix,target,startX,startY,endX,endY):</div><div class="line">        if startX&gt;endX or startY&gt;endY:</div><div class="line">            return False</div><div class="line">        midX=startX+(startX-endX)/2</div><div class="line">        midY=startY+(startX-endY)/2</div><div class="line">        mid = matrix[midX][midY]</div><div class="line">        if mid == target:</div><div class="line">            return True</div><div class="line">        if mid &gt; target:</div><div class="line">            return self.helper(matrix,target,startX,midY,midX-1,endY) or self.helper(matrix,target,startX,startY,endX,midY-1)</div><div class="line">        else:</div><div class="line">            return self.helper(matrix,target,midX+1,startY,endX,midY) or self.helper(matrix,target,startX,midY+1,endX,endY)</div><div class="line">        return False</div></pre></td></tr></table></figure></p>
<p>或者，不用 binary search，用比较通用的方法，很好理解，从 top rightmost 开始，比较与 target 的大小，if curr&gt;target，往左，if curr<target，往下。要注意的是确定 ending="" case,="" j<0="" or="" i="">len(matrix)-1。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def searchMatrix(self, matrix, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type matrix: List[List[int]]</div><div class="line">        :type target: int</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not matrix:</div><div class="line">            return False</div><div class="line">        i,j = 0,len(matrix[0])-1</div><div class="line">        while True:</div><div class="line">            if j&lt;0 or i&gt;len(matrix)-1:</div><div class="line">                return False</div><div class="line">            curr = matrix[i][j]</div><div class="line">            if curr == target:</div><div class="line">                return True</div><div class="line">            if curr &gt; target:</div><div class="line">                j -= 1</div><div class="line">            else:</div><div class="line">                i += 1</div></pre></td></tr></table></figure></target，往下。要注意的是确定></p>
<h2 id="475-Heaters"><a href="#475-Heaters" class="headerlink" title="475. Heaters"></a>475. Heaters</h2><h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Winter is coming! Your first job during the contest is to design a standard heater with fixed warm radius to warm all the houses.</p>
<p>Now, you are given positions of houses and heaters on a horizontal line, find out minimum radius of heaters so that all houses could be covered by those heaters.</p>
<p>So, your input will be the positions of houses and heaters seperately, and your expected output will be the minimum radius standard of heaters.</p>
<p>Note:<br>Numbers of houses and heaters you are given are non-negative and will not exceed 25000.<br>Positions of houses and heaters you are given are non-negative and will not exceed 10^9.<br>As long as a house is in the heaters’ warm radius range, it can be warmed.<br>All the heaters follow your radius standard and the warm radius will the same.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [1,2,3],[2]</div><div class="line">Output: 1</div><div class="line">Explanation: The only heater was placed in the position 2, and if we use the radius 1 standard, then all the houses can be warmed.</div></pre></td></tr></table></figure></p>
<p>Example 2:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [1,2,3,4],[1,4]</div><div class="line">Output: 1</div><div class="line">Explanation: The two heater was placed in the position 1 and 4. We need to use radius 1 standard, then all the houses can be warmed.</div></pre></td></tr></table></figure></p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Sort the heater</div><div class="line">    For each house, find out the positions of two heaters where the house is between, calculate the minimum value, update global radius</div><div class="line">    Be careful about corner case</div><div class="line"></div><div class="line">Followup:</div><div class="line">    use binary search</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def findRadius(self, houses, heaters):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type houses: List[int]</div><div class="line">        :type heaters: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        houses.sort()</div><div class="line">        heaters.sort()</div><div class="line"></div><div class="line">        def binarySearch(house, heaters):</div><div class="line">            left, right = 0, len(heaters)</div><div class="line">            while left &lt; right:</div><div class="line">                mid = left + (right - left) / 2</div><div class="line">                if heaters[mid] &lt; house:</div><div class="line">                    left = mid + 1</div><div class="line">                else:</div><div class="line">                    right = mid</div><div class="line">            return left</div><div class="line"></div><div class="line">        r = 0</div><div class="line">        for house in houses:</div><div class="line">            i = binarySearch(house, heaters)</div><div class="line">            if i == 0:</div><div class="line">                cur = abs(house - heaters[0])</div><div class="line">            elif i == len(heaters):</div><div class="line">                cur = abs(heaters[-1] - house)</div><div class="line">            else:</div><div class="line">                cur = min(abs(house - heaters[i]), abs(heaters[i - 1] - house))</div><div class="line">            r = max(r, cur)</div><div class="line">        return r</div><div class="line"></div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def findRadius(self, houses, heaters):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type houses: List[int]</div><div class="line">        :type heaters: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        houses.sort()</div><div class="line">        heaters.sort()</div><div class="line">        heaters=[float(&apos;-inf&apos;)]+heaters+[float(&apos;inf&apos;)] # add 2 fake heaters</div><div class="line">        r,i = 0,0</div><div class="line">        for house in houses:</div><div class="line">            while house &gt; heaters[i+1]:  # search to put house between heaters</div><div class="line">                i +=1</div><div class="line">            cur = min (house - heaters[i], heaters[i+1]- house)</div><div class="line">            r = max(r,cur)</div><div class="line">        return r</div><div class="line">&apos;&apos;&apos;</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 搜索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 数组]]></title>
      <url>http://www.shuang0420.com/2016/09/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%95%B0%E7%BB%84/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="策略-amp-注意点"><a href="#策略-amp-注意点" class="headerlink" title="策略 &amp; 注意点"></a>策略 &amp; 注意点</h1><p>array 在内存里是连续存储的，意味着 immutable length 和 no holes allowed。带来的优点是支持随机访问，缺点是当 resize array 的时候需要 copy 原有的所有元素，当删除一个不在末尾的元素时，又要 shift 很多元素。</p>
<p>数组最需要注意的：</p>
<ol>
<li>length 问题，最后一个数 array[len(array)-1]</li>
<li>指针问题，deepcopy or shallowcopy，对原数组进行多次变形并需纪录每次结果时，要注意 deepcopy 而不是存指针。res.append(list(nums))，如 permutation 这种题。</li>
<li>可以用虚拟边界</li>
<li>利用 array 的 index 可以做很多事情（利用 nums[i] 和 nums[nums[i]]）。如 Find the Duplicate Number，把 array 变成 linkedlist，或者</li>
</ol>
<h2 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h2><p>Insertion at back: O(1)<br>Insertion at front: O(n)<br>Insertion in middle: O(n)<br>Searching (using linear search): O(n)<br>Deletion: O(n)<br>Access to an element with its index: O(1)</p>
<h2 id="python-lists"><a href="#python-lists" class="headerlink" title="python lists"></a>python lists</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">The list data type has some more methods. Here are all of the methods of list objects:</div><div class="line"></div><div class="line">list.append(x)</div><div class="line">Add an item to the end of the list; equivalent to a[len(a):] = [x].</div><div class="line"></div><div class="line">list.extend(L)</div><div class="line">Extend the list by appending all the items in the given list; equivalent to a[len(a):] = L.</div><div class="line"></div><div class="line">list.insert(i, x)</div><div class="line">Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).</div><div class="line"></div><div class="line">list.remove(x)</div><div class="line">Remove the first item from the list whose value is x. It is an error if there is no such item.</div><div class="line"></div><div class="line">list.pop([i])</div><div class="line">Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)</div><div class="line"></div><div class="line">list.index(x)</div><div class="line">Return the index in the list of the first item whose value is x. It is an error if there is no such item.</div><div class="line"></div><div class="line">list.count(x)</div><div class="line">Return the number of times x appears in the list.</div><div class="line"></div><div class="line">list.sort(cmp=None, key=None, reverse=False)</div><div class="line">Sort the items of the list in place (the arguments can be used for sort customization, see sorted() for their explanation).</div><div class="line"></div><div class="line">list.reverse()</div></pre></td></tr></table></figure>
<h2 id="Java-Arrays"><a href="#Java-Arrays" class="headerlink" title="Java Arrays"></a>Java Arrays</h2><p>在 java 里，array 有一个方法，clone()，属于 shallow copy。有一个 immutable field，length。<br>Java Arrays 好用的方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">int[] a = &#123; 7, 1, 2, 3, 4, 5 &#125;;</div><div class="line">int[] b = &#123; 7, 1, 2, 3, 4, 5 &#125;;</div><div class="line"></div><div class="line">System.out.println(&quot;a equals b: &quot; + Arrays.equals(a, b));</div><div class="line">System.out.println(&quot;a: &quot; + Arrays.toString(a));</div><div class="line"></div><div class="line">Arrays.sort(a);</div><div class="line">int[] c = Arrays.copyOf(b, b.length);</div><div class="line">// public static void arraycopy(Object source, int srcIndex, Object</div><div class="line">// destination, int destIndex, int length)</div><div class="line">int[] d = new int[b.length];</div><div class="line">System.arraycopy(b, 0, d, 0, 3);</div><div class="line">int[] e = b.clone(); // shallow copy, only reference</div><div class="line"></div><div class="line">System.out.println(&quot;Sorted a: &quot; + Arrays.toString(a));</div><div class="line">System.out.println(&quot;c (copied from b): &quot; + Arrays.toString(c));</div><div class="line">System.out.println(&quot;d (copied from b): &quot; + Arrays.toString(d));</div><div class="line">System.out.println(&quot;e (copied from b): &quot; + Arrays.toString(e));</div></pre></td></tr></table></figure></p>
<p>outputs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">a equals b: true</div><div class="line">a: [7, 1, 2, 3, 4, 5]</div><div class="line">Sorted a: [1, 2, 3, 4, 5, 7]</div><div class="line">c (copied from b): [7, 1, 2, 3, 4, 5]</div><div class="line">d (copied from b): [7, 1, 2, 0, 0, 0]</div><div class="line">e (copied from b): [7, 1, 2, 3, 4, 5]</div></pre></td></tr></table></figure></p>
<h2 id="Java-List"><a href="#Java-List" class="headerlink" title="Java List"></a>Java List</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">add(object) : adds a new element to the end</div><div class="line">add(index, object) : inserts a new element at the specified</div><div class="line">index</div><div class="line">set(index, object) : replaces an existing element at the</div><div class="line">specified index with the new element.</div><div class="line">get(index) : returns the element at the specified index.</div><div class="line">remove(index) : deletes the element at the specified index.</div><div class="line">size() : returns the number of elements.</div></pre></td></tr></table></figure>
<p>Java 7，ArrayList 用的是 doubling-up policy。也就是说，假定 ArrayList 的初始 capacity 为 4，那么加入第 n(n&lt;=4) 个元素的 running time 是 1，而加入第 5 个 item 时，running time 是 5 而不是 1，因为要重新创建一个 double-size list 再把原来的元素 copy 进去。同样的，加入第 9 个元素时的 running time 是 9 而不是 1。用 amortized 的方法可以发现 add(E e) 的操作的 running time 是一个常数，也就是 O(1)。</p>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="66-Plus-One"><a href="#66-Plus-One" class="headerlink" title="66. Plus One"></a>66. Plus One</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Given a non-negative integer represented as a non-empty array of digits, plus one to the integer.</p>
<p>You may assume the integer do not contain any leading zero, except the number 0 itself.</p>
<p>The digits are stored such that the most significant digit is at the head of the list.</p>
<p><strong>Similar problems</strong> (M) Multiply Strings (E) Add Binary (M) Add Two Numbers (M) Plus One Linked List<br><a href="http://www.shuang0420.com/2016/09/04/数据结构和算法%20--%20链表/">2.445. Add Two Numbers 369. Plus One Linked List</a><br><a href="http://www.shuang0420.com/2016/10/12/数据结构和算法%20--%20String/">43. Multiply Strings</a></p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def plusOne(self, digits):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type digits: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if digits[-1] != 9:</div><div class="line">            digits[-1] += 1</div><div class="line">            return digits</div><div class="line">        i = len(digits) - 1</div><div class="line">        while i &gt;= 0 and digits[i] == 9:</div><div class="line">            digits[i] = 0</div><div class="line">            i -= 1</div><div class="line">        if i == -1:</div><div class="line">            cur = [1]</div><div class="line">            cur.extend(digits)</div><div class="line">            return cur</div><div class="line">        digits[i] += 1</div><div class="line">        return digits</div></pre></td></tr></table></figure>
<h2 id="67-Add-Binary"><a href="#67-Add-Binary" class="headerlink" title="67. Add Binary"></a>67. Add Binary</h2><h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Given two binary strings, return their sum (also a binary string).</p>
<p>For example,<br>a = “11”<br>b = “1”<br>Return “100”.</p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    1. do it as we do the math</div><div class="line">        make sure a is longer than b</div><div class="line">        while pb&gt;=0, start from right to left, calculate sum value at each digit and update result and carry.</div><div class="line">            cur=int(a[pa])+int(b[pb])+carry</div><div class="line">            res=str(cur%2)+res</div><div class="line">            carry=cur/2</div><div class="line">        while pa&gt;=0, do similar task</div><div class="line">            cur=int(a[pa])+carry</div><div class="line">            res=str(cur%2)+res</div><div class="line">            carry=cur/2</div><div class="line">        deal with carry if carry==1</div><div class="line">            res=&apos;1&apos;+res</div><div class="line">    2. do it recursively</div><div class="line">        when a[-1]==&apos;1&apos; and b[-1]==&apos;1&apos;, recursive case would be</div><div class="line">            addBinary(self.addBinary(a[:-1],b[:-1]),&apos;1&apos;)+&apos;0&apos;</div><div class="line">                current layer: &apos;0&apos;</div><div class="line">                previous digit: a[:-1]+b[:-1]+&apos;1&apos;</div><div class="line">                    add previous digit as usual: addBinary(a[:-1],b[:-1])</div><div class="line">                    add one: addBinary(prev,&apos;1&apos;)</div><div class="line">                =&gt; addBinary(addBinary(a[:-1],b[:-1]),&apos;1&apos;)+&apos;0&apos;</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def addBinary(self, a, b):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type a: str</div><div class="line">        :type b: str</div><div class="line">        :rtype: str</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not a: return b</div><div class="line">        if not b: return a</div><div class="line">        if a[-1]==&apos;0&apos; and b[-1]==&apos;0&apos;:</div><div class="line">            return self.addBinary(a[:-1],b[:-1])+&apos;0&apos;</div><div class="line">        if a[-1]==&apos;1&apos; and b[-1]==&apos;1&apos;:</div><div class="line">            return self.addBinary(self.addBinary(a[:-1],b[:-1]),&apos;1&apos;)+&apos;0&apos;</div><div class="line">        else:</div><div class="line">            return self.addBinary(a[:-1],b[:-1])+&apos;1&apos;</div><div class="line">            &apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def addBinary(self, a, b):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type a: str</div><div class="line">        :type b: str</div><div class="line">        :rtype: str</div><div class="line">        &quot;&quot;&quot;</div><div class="line"></div><div class="line">        # make sure a is longer than b</div><div class="line">        if len(a) &lt; len(b):</div><div class="line">            a, b = b, a</div><div class="line"></div><div class="line">        pa = len(a) - 1</div><div class="line">        pb = len(b) - 1</div><div class="line">        res, carry = &apos;&apos;, 0</div><div class="line">        while pb &gt;= 0:</div><div class="line">            cur = int(a[pa]) + int(b[pb]) + carry</div><div class="line">            carry = cur / 2</div><div class="line">            res = str(cur % 2) + res</div><div class="line">            pb -= 1</div><div class="line">            pa -= 1</div><div class="line"></div><div class="line">        while pa &gt;= 0:</div><div class="line">            cur = int(a[pa]) + carry</div><div class="line">            carry = cur / 2</div><div class="line">            res = str(cur % 2) + res</div><div class="line">            pa -= 1</div><div class="line"></div><div class="line">        if carry == 1:</div><div class="line">            res = &apos;1&apos; + res</div><div class="line"></div><div class="line">        return res</div></pre></td></tr></table></figure>
<h2 id="228-Summary-Ranges"><a href="#228-Summary-Ranges" class="headerlink" title="228. Summary Ranges"></a>228. Summary Ranges</h2><h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h3><p>Given a sorted integer array without duplicates, return the summary of its ranges.</p>
<p>For example, given [0,1,2,4,5,7], return [“0-&gt;2”,”4-&gt;5”,”7”].</p>
<h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def summaryRanges(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[str]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def joinStr(start,end):</div><div class="line">            if start==end:</div><div class="line">                return str(nums[start])</div><div class="line">            else:</div><div class="line">                return str(nums[start])+&apos;-&gt;&apos;+str(nums[end])</div><div class="line">        if not nums:</div><div class="line">            return []</div><div class="line">        res=[]</div><div class="line">        start,end=0,0</div><div class="line">        while end+1&lt;len(nums):</div><div class="line">            if nums[end]+1==nums[end+1]:</div><div class="line">                end+=1</div><div class="line">            else:</div><div class="line">                res.append(joinStr(start,end))</div><div class="line">                start=end+1</div><div class="line">                end=start</div><div class="line">        res.append(joinStr(start,end))</div><div class="line">        return res</div></pre></td></tr></table></figure>
<h2 id="360-Sort-Transformed-Array"><a href="#360-Sort-Transformed-Array" class="headerlink" title="360. Sort Transformed Array"></a>360. Sort Transformed Array</h2><h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem"></a>Problem</h3><p>Given a sorted array of integers nums and integer values a, b and c. Apply a function of the form f(x) = ax2 + bx + c to each element x in the array.</p>
<p>The returned array must be in sorted order.</p>
<p>Expected time complexity: O(n)</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">nums = [-4, -2, 2, 4], a = 1, b = 3, c = 5,</div><div class="line"></div><div class="line">Result: [3, 9, 15, 33]</div><div class="line"></div><div class="line">nums = [-4, -2, 2, 4], a = -1, b = 3, c = 5</div><div class="line"></div><div class="line">Result: [-23, -5, 1, 7]</div></pre></td></tr></table></figure></p>
<p>Credits:<br>Special thanks to @elmirap for adding this problem and creating all test cases.</p>
<h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Notes:</div><div class="line">    Because the input is sorted and the function is a second degree polynomial, simply applying the function will result in at most two increasing/decreasing runs. Which Python&apos;s sort function will recognize and simply reverse/merge in O(n).</div><div class="line">    Refer links:</div><div class="line">        https://en.wikipedia.org/wiki/Timsort</div><div class="line">        http://blog.csdn.net/yangzhongblog/article/details/8184707</div><div class="line"></div><div class="line">Tips:</div><div class="line">    heapq.merge(*iterables): Merge multiple sorted inputs into a single sorted output (for example, merge timestamped entries from multiple log files). Returns an iterator over the sorted values.</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"># Method 1</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def sortTransformedArray(self, nums, a, b, c):</div><div class="line">        return sorted(a*x*x + b*x + c for x in nums)</div><div class="line">    &apos;&apos;&apos;   </div><div class="line"></div><div class="line"></div><div class="line"># Method 2: two parts solution</div><div class="line">&apos;&apos;&apos;</div><div class="line">from collections import deque</div><div class="line">import heapq</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def sortTransformedArray(self, nums, a, b, c):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type a: int</div><div class="line">        :type b: int</div><div class="line">        :type c: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line"></div><div class="line">        &quot;&quot;&quot;</div><div class="line">        # can simply use heapq.merge()</div><div class="line">        def mergeLists(left,right):</div><div class="line">            lp,rp=0,0</div><div class="line">            res=[]</div><div class="line">            while lp&lt;len(left) and rp&lt;len(right):</div><div class="line">                if left[lp]&lt;=right[rp]:</div><div class="line">                    res.append(left[lp])</div><div class="line">                    lp+=1</div><div class="line">                else:</div><div class="line">                    res.append(right[rp])</div><div class="line">                    rp+=1</div><div class="line">            while lp&lt;len(left):</div><div class="line">                res.append(left[lp])</div><div class="line">                lp+=1</div><div class="line">            while rp&lt;len(right):</div><div class="line">                res.append(right[rp])</div><div class="line">                rp+=1</div><div class="line">            return res</div><div class="line">            &quot;&quot;&quot;</div><div class="line"></div><div class="line">        if not nums: return None</div><div class="line">        d=deque()</div><div class="line">        if a==0:</div><div class="line">            if b&gt;0:</div><div class="line">                for n in nums:</div><div class="line">                    d.append(n*b+c)</div><div class="line">            else:</div><div class="line">                for n in nums:</div><div class="line">                    d.appendleft(n*b+c)</div><div class="line">        else:</div><div class="line">            left=deque()</div><div class="line">            right=deque()</div><div class="line">            if a&gt;0:</div><div class="line">                line=float(-1*b)/(2*a)</div><div class="line">                for n in nums:</div><div class="line">                    if n&lt;line:</div><div class="line">                        left.appendleft(a*n*n+b*n+c)</div><div class="line">                    else:</div><div class="line">                        right.append(a*n*n+b*n+c)</div><div class="line">            else:</div><div class="line">                line=float(-1*b)/(2*a)</div><div class="line">                for n in nums:</div><div class="line">                    if n&lt;line:</div><div class="line">                        left.append(a*n*n+b*n+c)</div><div class="line">                    else:</div><div class="line">                        right.appendleft(a*n*n+b*n+c)</div><div class="line">            #d=mergeLists(left,right)</div><div class="line">            d=heapq.merge(left,right)</div><div class="line">        return list(d)</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line"># Method 3: two pointers solution</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def sortTransformedArray(self, nums, a, b, c):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type a: int</div><div class="line">        :type b: int</div><div class="line">        :type c: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def f(x):</div><div class="line">            return a*x*x+b*x+c</div><div class="line"></div><div class="line"></div><div class="line">        start,end=0,len(nums)-1</div><div class="line">        res=[]</div><div class="line">        while start&lt;=end:</div><div class="line">            resS,resE=f(nums[start]),f(nums[end])</div><div class="line">            if a&gt;0:</div><div class="line">                if resS&lt;resE:</div><div class="line">                    res.append(resE)</div><div class="line">                    end-=1</div><div class="line">                else:</div><div class="line">                    res.append(resS)</div><div class="line">                    start+=1</div><div class="line">            else:</div><div class="line">                if resS&gt;resE:</div><div class="line">                    res.append(resE)</div><div class="line">                    end-=1</div><div class="line">                else:</div><div class="line">                    res.append(resS)</div><div class="line">                    start+=1</div><div class="line">        if a&gt;0:</div><div class="line">            return res[::-1]</div><div class="line">        return res</div></pre></td></tr></table></figure>
<h2 id="346-Moving-Average-from-Data-Stream"><a href="#346-Moving-Average-from-Data-Stream" class="headerlink" title="346. Moving Average from Data Stream"></a>346. Moving Average from Data Stream</h2><h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem"></a>Problem</h3><p>Given a stream of integers and a window size, calculate the moving average of all integers in the sliding window.<br>For example,<br>MovingAverage m = new MovingAverage(3);<br>m.next(1) = 1<br>m.next(10) = (1 + 10) / 2<br>m.next(3) = (1 + 10 + 3) / 3<br>m.next(5) = (10 + 3 + 5) / 3</p>
<h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- maintain a deque of at most &apos;size&apos; length, for each next call, enque the number, calculate average, check if the length of deque is 3, if it is, popleft, and finally return the average. Time complexity O(n); Space complexity O(size)</div><div class="line"></div><div class="line">Follow-up:</div><div class="line">- make it O(1), save the sum each time, that is for each next call, enque the number, add to global sum, calculate average, check if the length of deque is 3, if it is, pop left, minus popped number from sum, and finally return the average.</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">from collections import deque</div><div class="line">class MovingAverage(object):</div><div class="line"></div><div class="line">    def __init__(self, size):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        Initialize your data structure here.</div><div class="line">        :type size: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.q=deque()</div><div class="line">        self.size=size</div><div class="line">        self.sum=0</div><div class="line"></div><div class="line">    def next(self, val):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type val: int</div><div class="line">        :rtype: float</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.q.append(val)</div><div class="line">        self.sum+=val</div><div class="line">        avg=self.sum/float(len(self.q))</div><div class="line">        if len(self.q)==self.size:</div><div class="line">            self.sum-=self.q.popleft()</div><div class="line">        return avg</div><div class="line"></div><div class="line"># Your MovingAverage object will be instantiated and called as such:</div><div class="line"># obj = MovingAverage(size)</div><div class="line"># param_1 = obj.next(val)</div></pre></td></tr></table></figure>
<h2 id="75-Sort-Colors"><a href="#75-Sort-Colors" class="headerlink" title="75. Sort Colors"></a>75. Sort Colors</h2><p>subarray 问题</p>
<h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem"></a>Problem</h3><p>Given an array with n objects colored red, white or blue, sort them so that objects of the same color are adjacent, with the colors in the order red, white and blue.<br>Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively.<br>Note:<br>You are not suppose to use the library’s sort function for this problem.</p>
<h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- quicksort?  Time complexity: O(nlogn)</div><div class="line">- 3 numbers, so count and reset, Time complexity: O(n), Space complexity: O(3)-&gt;O(1), two-pass</div><div class="line">- subarray with different states, Time complexity: O(n)</div><div class="line"></div><div class="line">Attention:</div><div class="line">- for solution 3, boundary is tricky, remember pointer is not included</div><div class="line"></div><div class="line">Test case:</div><div class="line">- [0]</div><div class="line">- [0,0,0]</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def sortColors(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: void Do not return anything, modify nums in-place instead.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;=1:</div><div class="line">            return</div><div class="line">        left,right,cur=0,len(nums)-1,0</div><div class="line">        while cur&lt;=right:</div><div class="line">            if nums[cur]==0:</div><div class="line">                nums[cur],nums[left]=nums[left],nums[cur]</div><div class="line">                left+=1</div><div class="line">                cur+=1</div><div class="line">            elif nums[cur]==2:</div><div class="line">                nums[cur],nums[right]=nums[right],nums[cur]</div><div class="line">                right-=1</div><div class="line">            else:</div><div class="line">                cur+=1</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;            </div><div class="line">    # two-pass solution: count and reset            </div><div class="line">    def sortColors(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: void Do not return anything, modify nums in-place instead.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;1:</div><div class="line">            return</div><div class="line">        a0,a1,a2=0,0,0</div><div class="line">        for n in nums:</div><div class="line">            if n==0: a0+=1</div><div class="line">            elif n==1: a1+=1</div><div class="line">            elif n==2: a2+=1</div><div class="line">        for i in range(len(nums)):</div><div class="line">            if a0&gt;0:</div><div class="line">                nums[i]=0</div><div class="line">                a0-=1</div><div class="line">            elif a1&gt;0:</div><div class="line">                nums[i]=1</div><div class="line">                a1-=1</div><div class="line">            elif a2&gt;0:</div><div class="line">                nums[i]=2</div><div class="line">                a2-=1</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="53-Maximum-Subarray"><a href="#53-Maximum-Subarray" class="headerlink" title="53. Maximum Subarray"></a>53. Maximum Subarray</h2><h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem"></a>Problem</h3><p>Find the contiguous subarray within an array (containing at least one number) which has the largest sum.<br>For example, given the array [-2,1,-3,4,-1,2,1,-5,4],<br>the contiguous subarray [4,-1,2,1] has the largest sum = 6.<br>More practice:<br>If you have figured out the O(n) solution, try coding another solution using the divide and conquer approach, which is more subtle.</p>
<h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- subarray: 2 pointers: head,tail</div><div class="line">- any repeated work? no   any meaningless work? yes    check the sum-array and we can find that the non-max-sum either subtract one more number or miss one more addition --&gt; cur_sum=max(cur_sum,nums[start]+cur_sum)</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def maxSubArray(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        cur_sum,max_sum=nums[0],nums[0]</div><div class="line">        for start in range(1,len(nums)):</div><div class="line">            cur_sum=max(nums[start],nums[start]+cur_sum)</div><div class="line">            max_sum=max(cur_sum,max_sum)</div><div class="line">        return max_sum</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # brute-force, time limit exceeded</div><div class="line">    def maxSubArray(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)==0:</div><div class="line">            return 0</div><div class="line">        global_sum=nums[0]</div><div class="line">        for i in range(0,len(nums)-1):</div><div class="line">            part_sum=nums[i]</div><div class="line">            if part_sum&gt;global_sum:</div><div class="line">                global_sum=part_sum</div><div class="line">            for j in range(i+1,len(nums)):</div><div class="line">                part_sum+=nums[j]</div><div class="line">                if part_sum&gt;global_sum:</div><div class="line">                    global_sum=part_sum</div><div class="line">        part_sum=nums[-1]</div><div class="line">        if part_sum&gt;global_sum:</div><div class="line">            global_sum=part_sum</div><div class="line">        return global_sum</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="Snapchat-面经"><a href="#Snapchat-面经" class="headerlink" title="Snapchat 面经"></a>Snapchat 面经</h2><h3 id="Problem-7"><a href="#Problem-7" class="headerlink" title="Problem"></a>Problem</h3><p>Returns unsorted part of an array.<br>For example, input -&gt; [1,2,5,7,6,4,9], output -&gt; [5,7,6,4]</p>
<h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Easiest way is to sort the array and compare it with the original array, get the first and last digit of different number, and return the list.</div><div class="line">    It takes O(nlogn) time.</div><div class="line"></div><div class="line">Followup -&gt; O(n) time</div><div class="line">    Use two pointers, lastDigit and firstDigit, and two global variables, curMax and curMin</div><div class="line">    For a sorted array, if we start for left to right curMax should be current number, and if we start from right to left, curMin should be current number.</div><div class="line">    So starts from left to right, keep track of curMax, first update curMax, and then check if current number is current max, if not, update lastDigit, repeat till we finish the loop.</div><div class="line">    Then starts from right to left, keep track of curMin, first update curMin, and then check if current number is current min, if not, update firstDigit, repeat till we finish the loop.</div><div class="line">    Return array[firstDigit,lastDigit+1]</div><div class="line">    Time complexity: O(n)</div><div class="line">&apos;&apos;&apos;</div><div class="line">def solution(array):</div><div class="line">    if not array:</div><div class="line">        return []</div><div class="line">    # keep track of current maximum and minimum</div><div class="line">    lastDigit, firstDigit = 0, len(array) - 1</div><div class="line">    curMax, curMin = array[0], array[len(array) - 1]</div><div class="line">    for i in xrange(len(array)):</div><div class="line">        curMax = max(array[i], curMax)</div><div class="line">        if array[i] &lt; curMax:</div><div class="line">            lastDigit = i</div><div class="line">    for i in xrange(len(array) - 1, -1, -1):</div><div class="line">        curMin = min(array[i], curMin)</div><div class="line">        if array[i] &gt; curMin:</div><div class="line">            firstDigit = i</div><div class="line">    return array[firstDigit:lastDigit + 1]</div><div class="line"></div><div class="line">print solution([])</div><div class="line">print solution([1, 2, 5, 7, 6, 4, 9])</div><div class="line">print solution([1, 2, 5, 5, 1])</div></pre></td></tr></table></figure>
<h2 id="Finding-sum-of-Absolute-Difference-of-Every-pair-of-integer-from-an-array"><a href="#Finding-sum-of-Absolute-Difference-of-Every-pair-of-integer-from-an-array" class="headerlink" title="Finding sum of Absolute Difference of Every pair of integer from an array"></a>Finding sum of Absolute Difference of Every pair of integer from an array</h2><h3 id="Problem-8"><a href="#Problem-8" class="headerlink" title="Problem"></a>Problem</h3><p>Given an array, find the sum of the absolute difference of every pair of integers.<br>For example: Given a[]= {2, 3, 5, 7 };<br>output would be (3-2) + (5-2) + (7-2) + (5-3) + (7-3) + (7-5) = 17.<br>It must be done better than O(n^2).<br>The original array isn’t necessarily sorted.</p>
<h3 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">First, sort the array. Let&apos;s say the sorted array looks like [-5, 2, 3, 5].  To consider only distinct pairs, for each element, pair it with only the elements that came before it. That means that the contribution of 3 to the total result is abs(3 - 2) + abs(3 - (-5)). Because the array is sorted, all the elements that came before are smaller or equal, so this can safely be rewritten as 3 * 2 - (2 + (-5)). In general terms, the contribution of arr[i] to the sum is arr[i] * i - sum (arr[0...i-1]). Fortunately, we can maintain the sum term as we go, avoiding expensive recomputation. We&apos;ll get the answer when we sum the contributions of each arr[i].</div><div class="line"></div><div class="line">This code runs in O(n)O(n) time after sorting, so the overall algorithm is O(nlogn)O(nlog⁡n), unless the data you have makes it easy to sort faster than that.</div><div class="line"></div><div class="line">Depending on the meaning of the term &quot;distinct pairs&quot;, you may need to dedupe the array before running the linear pass over the data. That is, if on an input like [1, 2, 1, 2, 3], you wouldn&apos;t want to count (2, 1) several times (even though there&apos;s several ways the pair can form), then you should just dedupe the input. Convert [1, 2, 1, 2, 3] to [1, 2, 3].</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">def solution(arr):</div><div class="line">    if not arr:</div><div class="line">        return 0</div><div class="line">    arr.sort()</div><div class="line">    total = 0</div><div class="line">    arraySum = 0</div><div class="line">    for i in range(0, len(arr)):</div><div class="line">        total += (arr[i] * i - arraySum)</div><div class="line">        arraySum += arr[i]</div><div class="line">    return total</div></pre></td></tr></table></figure>
<h2 id="11-Container-With-Most-Water"><a href="#11-Container-With-Most-Water" class="headerlink" title="11. Container With Most Water"></a>11. Container With Most Water</h2><h3 id="Problem-9"><a href="#Problem-9" class="headerlink" title="Problem"></a>Problem</h3><p>Given n non-negative integers a1, a2, …, an, where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water.</p>
<p>Note: You may not slant the container and n is at least 2.</p>
<h3 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    check every possible container (each combination of (i,j) where i,j &lt; len(height))</div><div class="line">    v=(j-i)*min(height[i],height[j])</div><div class="line">    cur_max=max(v,cur_max)</div><div class="line">    takes O(n^2) time</div><div class="line"></div><div class="line">Followup: make it O(n)</div><div class="line">    if we choose i=0 and an any other number in array, we can ensure v&lt;height[0]*l where l=len(height), same reason, if we choose j=len(height)-1 and an any other number in array, we can ensure v&lt;height[-1]*l</div><div class="line">    thus, we start from i=0 and j=len(height)-1 and get a container with volume v1, then in order to get a larger container, we wanna a heigher boundary(height), so we discard the smaller number between height[i] and height[j], and get a new volume v2, repeat this process till i==j</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def maxArea(self, height):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type height: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not height or len(height)==1:</div><div class="line">            return 0</div><div class="line">        start,end=0,len(height)-1</div><div class="line">        cur_max=float(&apos;-inf&apos;)</div><div class="line">        while start&lt;end:</div><div class="line">            cur=(end-start)*min(height[start],height[end])</div><div class="line">            cur_max=max(cur,cur_max)</div><div class="line">            if height[start]&gt;height[end]:</div><div class="line">                end-=1</div><div class="line">            else:</div><div class="line">                start+=1</div><div class="line">        return cur_max</div></pre></td></tr></table></figure>
<h2 id="84-Largest-Rectangle-in-Histogram"><a href="#84-Largest-Rectangle-in-Histogram" class="headerlink" title="84. Largest Rectangle in Histogram"></a>84. Largest Rectangle in Histogram</h2><h3 id="Problem-10"><a href="#Problem-10" class="headerlink" title="Problem"></a>Problem</h3><p>Given n non-negative integers representing the histogram’s bar height where the width of each bar is 1, find the area of largest rectangle in the histogram.</p>
<p>Above is a histogram where width of each bar is 1, given height = [2,1,5,6,2,3].</p>
<p>The largest rectangle is shown in the shaded area, which has area = 10 unit.</p>
<p>For example,<br>Given heights = [2,1,5,6,2,3],<br>return 10.</p>
<h3 id="Solution-10"><a href="#Solution-10" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">A simple solution is to one by one consider all bars as starting points and calculate area of all rectangles starting with every bar. Finally return maximum of all possible areas. Time complexity of this solution would be O(n^2).</div><div class="line"></div><div class="line">We can use Divide and Conquer to solve this in O(nLogn) time. The idea is to find the minimum value in the given array. Once we have index of the minimum value, the max area is maximum of following three values.</div><div class="line">a) Maximum area in left side of minimum value (Not including the min value)</div><div class="line">b) Maximum area in right side of minimum value (Not including the min value)</div><div class="line">c) Number of bars multiplied by minimum value.</div><div class="line">The areas in left and right of minimum value bar can be calculated recursively. If we use linear search to find the minimum value, then the worst case time complexity of this algorithm becomes O(n^2). In worst case, we always have (n-1) elements in one side and 0 elements in other side and if the finding minimum takes O(n) time, we get the recurrence similar to worst case of Quick Sort.</div><div class="line"></div><div class="line">Followup:</div><div class="line">- O(n)?</div><div class="line">    - avoid repeated work</div><div class="line">    - identify a rectangle: identify 2 boundaries</div><div class="line">    - if cur&gt;stack.peek() --&gt; offer, else --&gt; continously poll</div><div class="line"></div><div class="line"></div><div class="line">    For every bar ‘x’, we calculate the area with ‘x’ as the smallest bar in the rectangle. If we calculate such area for every bar ‘x’ and find the maximum of all areas, our task is done.</div><div class="line">    How to calculate area with ‘x’ as smallest bar? We need to know index of the first smaller (smaller than ‘x’) bar on left of ‘x’ and index of first smaller bar on right of ‘x’. Let us call these indexes as ‘left index’ and ‘right index’ respectively.</div><div class="line">    We traverse all bars from left to right, maintain a stack of bars. Every bar is pushed to stack once. A bar is popped from stack when a bar of smaller height is seen. When a bar is popped, we calculate the area with the popped bar as smallest bar. How do we get left and right indexes of the popped bar – the current index tells us the ‘right index’ and index of previous item in stack is the ‘left index’. Following is the complete algorithm.</div><div class="line"></div><div class="line">    1) Create an empty stack.</div><div class="line">    2) Start from first bar, and do following for every bar ‘hist[i]’ where ‘i’ varies from 0 to n-1.</div><div class="line">    ……a) If stack is empty or hist[i] is higher than the bar at top of stack, then push ‘i’ to stack.</div><div class="line">    ……b) If this bar is smaller than the top of stack, then keep removing the top of stack while top of the stack is greater. Let the removed bar be hist[tp]. Calculate area of rectangle with hist[tp] as smallest bar. For hist[tp], the ‘left index’ is previous (previous to tp) item in stack and ‘right index’ is ‘i’ (current index).</div><div class="line"></div><div class="line">    3) If the stack is not empty, then one by one remove all bars from stack and do step 2.b for every removed bar.</div><div class="line"></div><div class="line">Corner case: [0]</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def largestRectangleArea(self, heights):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type heights: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not heights: return 0</div><div class="line">        stack=[]</div><div class="line">        max_area=0</div><div class="line">        for i in range(len(heights)+1):</div><div class="line">            while stack and (i==len(heights) or heights[i]&lt;heights[stack[-1]]):</div><div class="line">                height=heights[stack.pop()]</div><div class="line">                leftBound=0 if not stack else stack[-1]+1</div><div class="line">                rightBound=i</div><div class="line">                cur_area=(rightBound-leftBound)*height</div><div class="line">                max_area=max(cur_area,max_area)</div><div class="line">            stack.append(i)</div><div class="line">        return max_area</div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # primitive, 2 loops</div><div class="line">    def largestRectangleArea(self, heights):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type heights: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not heights: return 0</div><div class="line">        max_area=0</div><div class="line">        for i in range(len(heights)):</div><div class="line">            max_area=max(heights[i],max_area)</div><div class="line">            min_height=heights[i]</div><div class="line">            for j in range(i,len(heights)):</div><div class="line">                min_height=min(min_height,heights[j])</div><div class="line">                max_area=max(min_height*(j-i+1),max_area)</div><div class="line">        return max_area</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # DP: avg: O(nlogn)   worst: O(n^2)</div><div class="line">    def largestRectangleArea(self, heights):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type heights: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        # DP Solution</div><div class="line">        def maxArea(left,right):</div><div class="line">            if right-left==1:</div><div class="line">                return heights[left]</div><div class="line">            if right==left:</div><div class="line">                return 0</div><div class="line">            minHeight=min(heights[left:right])</div><div class="line">            minHeightIndex=heights[left:right].index(minHeight)+left</div><div class="line">            maxLeft=maxArea(left,minHeightIndex)</div><div class="line">            maxRight=maxArea(minHeightIndex+1,right)</div><div class="line">            cur=minHeight*(right-left)</div><div class="line">            return max(maxLeft,maxRight,cur)</div><div class="line"></div><div class="line">        if not heights: return 0</div><div class="line">        return maxArea(0,len(heights))</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="463-Island-Perimeter"><a href="#463-Island-Perimeter" class="headerlink" title="463. Island Perimeter"></a>463. Island Perimeter</h2><h3 id="Problem-11"><a href="#Problem-11" class="headerlink" title="Problem"></a>Problem</h3><p>You are given a map in form of a two-dimensional integer grid where 1 represents land and 0 represents water. Grid cells are connected horizontally/vertically (not diagonally). The grid is completely surrounded by water, and there is exactly one island (i.e., one or more connected land cells). The island doesn’t have “lakes” (water inside that isn’t connected to the water around the island). One cell is a square with side length 1. The grid is rectangular, width and height don’t exceed 100. Determine the perimeter of the island.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[[0,1,0,0],</div><div class="line"> [1,1,1,0],</div><div class="line"> [0,1,0,0],</div><div class="line"> [1,1,0,0]]</div><div class="line"></div><div class="line">Answer: 16</div><div class="line">Explanation: The perimeter is the 16 yellow stripes in the image below:</div></pre></td></tr></table></figure></p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%95%B0%E7%BB%84/island.jpg" class="ful-image" alt="island.jpg">
<h3 id="Solution-11"><a href="#Solution-11" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def islandPerimeter(self, grid):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type grid: List[List[int]]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        newGrid=[[0]*(len(grid[0])+2) for i in range(len(grid)+2)]</div><div class="line">        for i in range(len(grid)):</div><div class="line">            for j in range(len(grid[0])):</div><div class="line">                newGrid[i+1][j+1]=grid[i][j]</div><div class="line">        res=0</div><div class="line">        for i in range(1,len(newGrid)-1):</div><div class="line">            for j in range(1,len(newGrid[0])-1):</div><div class="line">                if newGrid[i][j]==1:</div><div class="line">                    # check around</div><div class="line">                    if newGrid[i][j-1]==0:</div><div class="line">                        res+=1</div><div class="line">                    if newGrid[i][j+1]==0:</div><div class="line">                        res+=1</div><div class="line">                    if newGrid[i-1][j]==0:</div><div class="line">                        res+=1</div><div class="line">                    if newGrid[i+1][j]==0:</div><div class="line">                        res+=1</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    Simplify version</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def islandPerimeter(self, grid):</div><div class="line">        def water_around(y, x):</div><div class="line">            return ((x == 0              or grid[y][x-1] == 0) +</div><div class="line">                    (x == len(grid[0])-1 or grid[y][x+1] == 0) +</div><div class="line">                    (y == 0              or grid[y-1][x] == 0) +</div><div class="line">                    (y == len(grid)-1    or grid[y+1][x] == 0) )</div><div class="line">        return sum(water_around(y, x) for y in xrange(len(grid)) for x in xrange(len(grid[0])) if grid[y][x])</div></pre></td></tr></table></figure>
<h2 id="448-Find-All-Numbers-Disappeared-in-an-Array"><a href="#448-Find-All-Numbers-Disappeared-in-an-Array" class="headerlink" title="448. Find All Numbers Disappeared in an Array"></a>448. Find All Numbers Disappeared in an Array</h2><h3 id="Problem-12"><a href="#Problem-12" class="headerlink" title="Problem"></a>Problem</h3><p>Given an array of integers where 1 ≤ a[i] ≤ n (n = size of array), some elements appear twice and others appear once.</p>
<p>Find all the elements of [1, n] inclusive that do not appear in this array.</p>
<p>Could you do it without extra space and in O(n) runtime? You may assume the returned list does not count as extra space.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">[4,3,2,7,8,2,3,1]</div><div class="line"></div><div class="line">Output:</div><div class="line">[5,6]</div></pre></td></tr></table></figure></p>
<p><strong>Tags:</strong> Array<br><strong>Similar Problems:</strong> (H) First Missing Positive (M) Find All Duplicates in an Array</p>
<h3 id="Solution-12"><a href="#Solution-12" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Soltuion:</div><div class="line">    Bucket sort.</div><div class="line">    The idea is simple, we&apos;re gonna make all numbers into the corrent position. Each element w should be put into the w th position of the array.If nums[i] != i + 1 and nums[i] != nums[nums[i] - 1], then we swap nums[i] with nums[nums[i] - 1], for example, nums[0] = 4 and nums[3] = 7, then we swap nums[0] with nums[3]. So In the end the array will be sorted and if nums[i] != i + 1, then i + 1 is missing.</div><div class="line">    The example run as follows</div><div class="line"></div><div class="line">    [4,3,2,7,8,2,3,1]</div><div class="line">    [7,3,2,4,8,2,3,1]</div><div class="line">    [3,3,2,4,8,2,7,1]</div><div class="line">    [2,3,3,4,8,2,7,1]</div><div class="line">    [3,2,3,4,8,2,7,1]</div><div class="line">    [3,2,3,4,1,2,7,8]</div><div class="line">    [1,2,3,4,3,2,7,8]</div><div class="line">    Since every swap we put at least one number to its correct position, the time is O(n)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">Special case: [2,2]</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    brute-force: O(n^2)</div><div class="line">    def findDisappearedNumbers(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums: return []</div><div class="line">        res=[]</div><div class="line">        for i in range(1,len(nums)+1):</div><div class="line">            if i not in nums:</div><div class="line">                res.append(i)</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    Sort: O(nlogn)</div><div class="line"></div><div class="line">    def findDisappearedNumbers(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums: return []</div><div class="line">        nums.sort()</div><div class="line">        i=0</div><div class="line">        n=len(nums)</div><div class="line">        res=[]</div><div class="line">        for cur in nums:</div><div class="line">            if cur==i:</div><div class="line">                continue</div><div class="line">            if cur==i+1:</div><div class="line">                i+=1</div><div class="line">                continue</div><div class="line">            while i &lt; cur-1:</div><div class="line">                i+=1</div><div class="line">                res.append(i)</div><div class="line">            i=cur</div><div class="line">        while i&lt;n:</div><div class="line">            i+=1</div><div class="line">            res.append(i)</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">    Sort: O(n)</div><div class="line">    def findDisappearedNumbers(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums:</div><div class="line">            return []</div><div class="line">        for i in range(len(nums)):</div><div class="line">            while nums[i] != nums[nums[i] - 1]:</div><div class="line">                nums[nums[i] - 1], nums[i] = nums[i], nums[nums[i] - 1]</div><div class="line"></div><div class="line">        res = []</div><div class="line">        print nums</div><div class="line">        for i, num in enumerate(nums):</div><div class="line">            if i + 1 != num:</div><div class="line">                res.append(i + 1)</div><div class="line">        return res</div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def findDisappearedNumbers(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        # Use nums as hashmap</div><div class="line">        # For each number i in nums, mark the number that i points as negative.</div><div class="line">        # Then filter the list, get all the indexes that points to a positive number</div><div class="line">        for i in range(len(nums)):</div><div class="line">            index = abs(nums[i]) - 1</div><div class="line">            nums[index] = - abs(nums[index])</div><div class="line"></div><div class="line">        return [i + 1 for i in range(len(nums)) if nums[i] &gt; 0]</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="442-Find-All-Duplicates-in-an-Array"><a href="#442-Find-All-Duplicates-in-an-Array" class="headerlink" title="442. Find All Duplicates in an Array"></a>442. Find All Duplicates in an Array</h2><h3 id="Problem-13"><a href="#Problem-13" class="headerlink" title="Problem"></a>Problem</h3><p>Given an array of integers, 1 ≤ a[i] ≤ n (n = size of array), some elements appear twice and others appear once.</p>
<p>Find all the elements that appear twice in this array.</p>
<p>Could you do it without extra space and in O(n) runtime?</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">[4,3,2,7,8,2,3,1]</div><div class="line"></div><div class="line">Output:</div><div class="line">[2,3]</div></pre></td></tr></table></figure></p>
<p><strong>Tags:</strong> Array<br><strong>Similar Problems:</strong> (E) Find All Numbers Disappeared in an Array</p>
<h3 id="Solution-13"><a href="#Solution-13" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Bucket sort. Each element w should be put into the w th position of the array.</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def findDuplicates(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        hashset=set()</div><div class="line">        res=[]</div><div class="line">        for n in nums:</div><div class="line">            if n in hashset:</div><div class="line">                res.append(n)</div><div class="line">            else:</div><div class="line">                hashset.add(n)</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def findDuplicates(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums: return []</div><div class="line">        res=[]</div><div class="line">        for i in range(len(nums)):</div><div class="line">            while nums[i] != nums[nums[i]-1]:</div><div class="line">                nums[nums[i]-1],nums[i]=nums[i],nums[nums[i]-1]</div><div class="line">        for i,n in enumerate(nums):</div><div class="line">            if i+1 != n:</div><div class="line">                res.append(n)</div><div class="line">        return res</div></pre></td></tr></table></figure>
<h2 id="41-First-Missing-Positive"><a href="#41-First-Missing-Positive" class="headerlink" title="41. First Missing Positive"></a>41. First Missing Positive</h2><h3 id="Problem-14"><a href="#Problem-14" class="headerlink" title="Problem"></a>Problem</h3><p>Given an unsorted integer array, find the first missing positive integer.</p>
<p>For example,<br>Given [1,2,0] return 3,<br>and [3,4,-1,1] return 2.</p>
<p>Your algorithm should run in O(n) time and uses constant space.</p>
<p><strong>Tags:</strong> Array<br><strong>Similar Problems:</strong> (M) Missing Number (H) Find the Duplicate Number (E) Find All Numbers Disappeared in an Array</p>
<h3 id="Solution-14"><a href="#Solution-14" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Bucket sort is the only way. As only the elements between 1...n are useful, each element w should be put into the w th position of the array. As it is possible there is some other element v in the w th position, take the v out before overwriting and then iteratively use the same logic on v and go on.</div><div class="line"></div><div class="line">    Time complexity: each element is looped 2 times and swapped 1 time, so the whole time compexity is O(n)</div><div class="line">    Space: O(1) apparently</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def firstMissingPositive(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        n = len(nums)</div><div class="line">        for i in xrange(len(nums)):</div><div class="line">            while nums[i] &gt; 0 and nums[i] &lt;= n and nums[i] != nums[nums[i] - 1]:</div><div class="line">                nums[nums[i] - 1], nums[i] = nums[i], nums[nums[i] - 1]</div><div class="line">        for i, num in enumerate(nums):</div><div class="line">            if i + 1 != num:</div><div class="line">                return i + 1</div><div class="line">        return n + 1</div></pre></td></tr></table></figure>
<h2 id="287-Find-the-Duplicate-Number"><a href="#287-Find-the-Duplicate-Number" class="headerlink" title="287. Find the Duplicate Number"></a>287. Find the Duplicate Number</h2><h3 id="Problem-15"><a href="#Problem-15" class="headerlink" title="Problem"></a>Problem</h3><p>Given an array nums containing n + 1 integers where each integer is between 1 and n (inclusive), prove that at least one duplicate number must exist. Assume that there is only one duplicate number, find the duplicate one.</p>
<p>Note:<br>You must not modify the array (assume the array is read only).<br>You must use only constant, O(1) extra space.<br>Your runtime complexity should be less than O(n2).<br>There is only one duplicate number in the array, but it could be repeated more than once.</p>
<p><strong>Tags:</strong> Binary Search Array Two Pointers<br><strong>Similar Problems:</strong> (H) First Missing Positive (E) Single Number (M) Linked List Cycle II (M) Missing Number</p>
<h3 id="Solution-15"><a href="#Solution-15" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    The main idea is the same with problem Linked List Cycle II. Use two pointers the fast and the slow. The fast one goes forward two steps each time, while the slow one goes only step each time. They must meet the same item when slow==fast. In fact, they meet in a circle, the duplicate number must be the entry point of the circle when visiting the array from nums[0]. Next we just need to find the entry point. We use a point(we can use the fast one before) to visit form begining with one step each time, do the same job to slow. When fast==slow, they meet at the entry point of the circle.</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def findDuplicate(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums:</div><div class="line">            return -1</div><div class="line">        fast = nums[nums[0]]</div><div class="line">        slow = nums[0]</div><div class="line">        while slow != fast:</div><div class="line">            slow = nums[slow]</div><div class="line">            fast = nums[nums[fast]]</div><div class="line"></div><div class="line">        head = 0</div><div class="line">        while head != slow:</div><div class="line">            head = nums[head]</div><div class="line">            slow = nums[slow]</div><div class="line">        return slow</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数组 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- TWO-SUM 问题和python dict]]></title>
      <url>http://www.shuang0420.com/2016/09/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20TWO-SUM%20%E9%97%AE%E9%A2%98%E5%92%8C%20python%20dict/</url>
      <content type="html"><![CDATA[<p>打尽 two-sum 问题。<br><a id="more"></a></p>
<h1 id="策略-amp-注意点"><a href="#策略-amp-注意点" class="headerlink" title="策略 &amp; 注意点"></a>策略 &amp; 注意点</h1><h2 id="Assumption"><a href="#Assumption" class="headerlink" title="Assumption"></a>Assumption</h2><ul>
<li>array is sorted?</li>
<li>each input would have exactly one solution?</li>
<li>duplicates in array?</li>
<li>return index is sorted?</li>
</ul>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><ol>
<li><p>头尾指针，经典模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">while start&lt;end:</div><div class="line">    sum=numbers[start]+numbers[end]</div><div class="line">    if sum==target:</div><div class="line">        return [start,end]</div><div class="line">    if sum&gt;target:</div><div class="line">        end-=1</div><div class="line">    else:</div><div class="line">        start+=1</div></pre></td></tr></table></figure>
</li>
<li><p>加上去重的模板：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">while start&lt;end:</div><div class="line">    cur_sum=nums[start]+nums[end]</div><div class="line">    if cur_sum&lt;target:</div><div class="line">        start+=1</div><div class="line">    elif cur_sum&gt;target:</div><div class="line">        end-=1</div><div class="line">    else:</div><div class="line">        cur_res.append([nums[0],nums[start],nums[end]])</div><div class="line">        start+=1</div><div class="line">        end-=1</div><div class="line">        while start&lt;end and nums[start]==nums[start-1]:</div><div class="line">            start+=1</div><div class="line">        while start&lt;end and nums[end]==nums[end+1]:</div><div class="line">            end-=1</div></pre></td></tr></table></figure>
</li>
<li><p>Hashmap 来 search target-nums[i]，1－pass 先 check 在不在 map 中，不在就放进去。</p>
</li>
</ol>
<h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><ul>
<li>涉及 index 一般就不先 sort 了，因为会 disrupt the order</li>
<li>如果上来就把整个 list 转成 hashmap<val,index>，之后在 search，那么就要注意 val==target-val 的情况了，也要判断 val 出现几次（hashmap 必须考虑 key 是否会重复）</val,index></li>
<li>要去重的问题用两个 pointer 可以顺便去重，但要注意保证大条件 start&lt;end</li>
<li>注意数字可能是 negative，初始化变量不要想当然的为0，计算 difference 的时候用 abs(n) 绝对值。</li>
<li>python dict 的用法，哪些 O(1) 哪些 O(n)</li>
</ul>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="1-Two-Sum"><a href="#1-Two-Sum" class="headerlink" title="1. Two Sum"></a>1. Two Sum</h2><blockquote>
<p>Given an array of integers, return indices of the two numbers such that they add up to a specific target.<br>You may assume that each input would have exactly one solution.<br>Example:<br>Given nums = [2, 7, 11, 15], target = 9,<br>Because nums[0] + nums[1] = 2 + 7 = 9,<br>return [0, 1].</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Check assumption:</div><div class="line">- array is not sorted</div><div class="line">- each input would have exactly one solution</div><div class="line">- duplicates in array</div><div class="line">- return index is not sorted</div><div class="line"></div><div class="line">Corner case:</div><div class="line">- len(nums)&lt;2 or nums==None</div><div class="line"></div><div class="line">Solution:</div><div class="line">- Loop array, search target-nums[i] for each nums[j] on the right. Time complexity: O(n^2)</div><div class="line"></div><div class="line">Attention:</div><div class="line">- we cannot sort array, compare target and sum and move pointers to get the answer as it would disrupt the order</div><div class="line">- array.index(value) returns first match, but this may not be what you expect</div><div class="line"></div><div class="line">Optimization:</div><div class="line">- While loop, use hashmap&lt;target-nums[i],i&gt; to store remaining index and value, so that the second loop will have O(1) time complexity, and the total complexity would be O(n). The cost is space complexity. This is two-pass solution.</div><div class="line">- Two-pass --&gt; One pass. While loop, for each i, check if it is in hashmap, if not, add it to the hashmap, if exists, return index.</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    # with hashmap</div><div class="line">    def twoSum(self, nums, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums or len(nums)&lt;2:</div><div class="line">            return None</div><div class="line">        hashmap=dict()</div><div class="line">        for index,value in enumerate(nums):</div><div class="line">            if target-value in hashmap:</div><div class="line">                return[index,hashmap[target-value]]</div><div class="line">            hashmap[value]=index</div><div class="line">        return None</div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # two loops</div><div class="line">    def twoSum(self, nums, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not nums or len(nums)&lt;2:</div><div class="line">            return None</div><div class="line">        for index1,value in enumerate(nums):</div><div class="line">            for index2 in range(index1+1,len(nums)):</div><div class="line">                if nums[index2]==target-value:</div><div class="line">                    return [index1,index2]</div><div class="line">        return None</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="167-Two-Sum-II-Input-array-is-sorted"><a href="#167-Two-Sum-II-Input-array-is-sorted" class="headerlink" title="167. Two Sum II - Input array is sorted"></a>167. Two Sum II - Input array is sorted</h2><blockquote>
<p>Given an array of integers that is already sorted in ascending order, find two numbers such that they add up to a specific target number.<br>The function twoSum should return indices of the two numbers such that they add up to the target, where index1 must be less than index2. Please note that your returned answers (both index1 and index2) are not zero-based.<br>You may assume that each input would have exactly one solution.<br>Input: numbers={2, 7, 11, 15}, target=9<br>Output: index1=1, index2=2</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def twoSum(self, numbers, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type numbers: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not numbers or len(numbers)&lt;2:</div><div class="line">            return None</div><div class="line">        start=0</div><div class="line">        end=len(numbers)-1</div><div class="line">        while start&lt;end:</div><div class="line">            sum=numbers[start]+numbers[end]</div><div class="line">            if sum==target:</div><div class="line">                return [start+1,end+1]</div><div class="line">            if sum&gt;target:</div><div class="line">                end-=1</div><div class="line">            else:</div><div class="line">                start+=1</div><div class="line">        return None</div></pre></td></tr></table></figure>
<h2 id="170-Two-Sum-III-Data-structure-design"><a href="#170-Two-Sum-III-Data-structure-design" class="headerlink" title="170. Two Sum III - Data structure design"></a>170. Two Sum III - Data structure design</h2><blockquote>
<p>Design and implement a TwoSum class. It should support the following operations: add and find.<br>add - Add the number to an internal data structure.<br>find - Find if there exists any pair of numbers which sum is equal to the value.<br>For example,<br>add(1); add(3); add(5);<br>find(4) -&gt; true<br>find(7) -&gt; false</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Data structure:</div><div class="line">- hashmap&lt;number,frequency&gt;.   Use hashmap because duplicates matter!</div><div class="line"></div><div class="line">Corner case:</div><div class="line">- hashmap is None</div><div class="line"></div><div class="line">Attention:</div><div class="line">- avoid case val==target-val</div><div class="line"></div><div class="line">Time complexity:</div><div class="line">- add() O(1)</div><div class="line">- find() O(n)</div><div class="line"></div><div class="line">About python dictionary:</div><div class="line">- Do not use dict.keys!</div><div class="line">In Python 2 dict.keys() creates the whole list of keys first that&apos;s why it is an O(N) operation, while key in dict is an O(1) operation.</div><div class="line">&gt;&gt;&gt; dic = dict.fromkeys(range(10**5))</div><div class="line">&gt;&gt;&gt; %timeit 10000 in dic</div><div class="line">1000000 loops, best of 3: 170 ns per loop</div><div class="line">&gt;&gt;&gt; %timeit 10000 in dic.keys()</div><div class="line">100 loops, best of 3: 4.98 ms per loop</div><div class="line">&gt;&gt;&gt; %timeit 10000 in dic.iterkeys()</div><div class="line">1000 loops, best of 3: 402 us per loop</div><div class="line">&gt;&gt;&gt; %timeit 10000 in dic.viewkeys()</div><div class="line">1000000 loops, best of 3: 457 ns per loop</div><div class="line"></div><div class="line">- Use dict.get(key,default=None)!</div><div class="line">self.hashmap[number]=self.hashmap.get(number,0)+1</div><div class="line">&apos;&apos;&apos;</div><div class="line">class TwoSum(object):</div><div class="line"></div><div class="line">    def __init__(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.hashmap=dict()</div><div class="line"></div><div class="line">    def add(self, number):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        Add the number to an internal data structure.</div><div class="line">        :rtype: nothing</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        if self.hashmap.has_key(number):</div><div class="line">            self.hashmap[number]+=1</div><div class="line">        else:</div><div class="line">            self.hashmap[number]=1</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        self.hashmap[number]=self.hashmap.get(number,0)+1</div><div class="line"></div><div class="line">    def find(self, value):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        Find if there exists any pair of numbers which sum is equal to the value.</div><div class="line">        :type value: int</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.hashmap:</div><div class="line">            return False</div><div class="line">        for v in self.hashmap:</div><div class="line">            if value-v in self.hashmap:</div><div class="line">                if self.hashmap[v]&gt;1 or v!=value-v:</div><div class="line">                    return True</div><div class="line">        return False</div><div class="line"></div><div class="line"></div><div class="line"># Your TwoSum object will be instantiated and called as such:</div><div class="line"># twoSum = TwoSum()</div><div class="line"># twoSum.add(number)</div><div class="line"># twoSum.find(value)</div></pre></td></tr></table></figure>
<h2 id="15-3Sum"><a href="#15-3Sum" class="headerlink" title="15. 3Sum"></a>15. 3Sum</h2><blockquote>
<p>Given an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.<br>Note: The solution set must not contain duplicate triplets.<br>For example, given array S = [-1, 0, 1, 2, -1, -4],<br>A solution set is:<br>[<br>  [-1, 0, 1],<br>  [-1, -1, 2]<br>]</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- convert to 2-sum problem, avoid duplicate triplets: sort the array, move pointers to skip duplicates</div><div class="line"></div><div class="line">Attention:</div><div class="line">- when avoiding duplicates, keep in mind start&lt;end, consider corner case [0,0,0] with 0</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def threeSum(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :rtype: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;3:</div><div class="line">            return []</div><div class="line">        nums=sorted(nums)</div><div class="line">        result=[]</div><div class="line">        for i in range(len(nums)-2):</div><div class="line">            if i&gt;0 and nums[i]==nums[i-1]:</div><div class="line">                continue</div><div class="line">            result+=self.twoSum(nums[i:],0-nums[i])</div><div class="line">        return result</div><div class="line"></div><div class="line"></div><div class="line">    def twoSum(self,nums,target):</div><div class="line">        if len(nums)&lt;3:</div><div class="line">            return []</div><div class="line">        start,end=1,len(nums)-1</div><div class="line">        cur_res=[]</div><div class="line">        while start&lt;end:</div><div class="line">            cur_sum=nums[start]+nums[end]</div><div class="line">            if cur_sum&lt;target:</div><div class="line">                start+=1</div><div class="line">            elif cur_sum&gt;target:</div><div class="line">                end-=1</div><div class="line">            else:</div><div class="line">                cur_res.append([nums[0],nums[start],nums[end]])</div><div class="line">                start+=1</div><div class="line">                end-=1</div><div class="line">                while start&lt;end and nums[start]==nums[start-1]:</div><div class="line">                    start+=1</div><div class="line">                while start&lt;end and nums[end]==nums[end+1]:</div><div class="line">                    end-=1</div><div class="line">        return cur_res</div></pre></td></tr></table></figure>
<h2 id="259-3Sum-Smaller"><a href="#259-3Sum-Smaller" class="headerlink" title="259. 3Sum Smaller"></a>259. 3Sum Smaller</h2><blockquote>
<p>Given an array of n integers nums and a target, find the number of index triplets i, j, k with 0 &lt;= i &lt; j &lt; k &lt; n that satisfy the condition nums[i] + nums[j] + nums[k] &lt; target.<br>For example, given nums = [-2, 0, 1, 3], and target = 2.<br>Return 2. Because there are two triplets which sums are less than 2:<br>[-2, 0, 1]<br>[-2, 0, 3]<br>Follow up:<br>Could you solve it in O(n2) runtime?</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Same with normal 3sum problem, just consider all possibilities.</div><div class="line"></div><div class="line">Solution:</div><div class="line">- sort nums, for nums[i] in nums, search if nums[start]+nums[end]&lt;target-nums[i] for nums[i+1:], if it is, count+=end-start and keep going</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def threeSumSmaller(self, nums, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;3:</div><div class="line">            return 0</div><div class="line">        count=0</div><div class="line">        nums=sorted(nums)</div><div class="line">        for i in range(len(nums)):</div><div class="line">            start=i+1</div><div class="line">            end=len(nums)-1</div><div class="line">            while start&lt;end:</div><div class="line">                cur_sum=nums[start]+nums[end]</div><div class="line">                new_target=target-nums[i]</div><div class="line">                if cur_sum&lt;new_target:</div><div class="line">                    count+=end-start</div><div class="line">                    start+=1</div><div class="line">                else:</div><div class="line">                    end-=1</div><div class="line">        return count</div></pre></td></tr></table></figure>
<h2 id="16-3Sum-Closest"><a href="#16-3Sum-Closest" class="headerlink" title="16. 3Sum Closest"></a>16. 3Sum Closest</h2><blockquote>
<p>Given an array S of n integers, find three integers in S such that the sum is closest to a given number, target. Return the sum of the three integers. You may assume that each input would have exactly one solution.<br>    For example, given array S = {-1 2 1 -4}, and target = 1.<br>    The sum that is closest to the target is 2. (-1 + 2 + 1 = 2).</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Simliar to 3sum problem, but have a global_diff to record current minimum difference (remember it should be absolute value) and global_sum to record current closet result</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def threeSumClosest(self, nums, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;3:</div><div class="line">            return None</div><div class="line">        nums=sorted(nums)</div><div class="line">        global_diff=abs(target-nums[0])</div><div class="line">        global_sum=sum(nums[0:3])</div><div class="line">        for i in range(len(nums)-2):</div><div class="line">            start=i+1</div><div class="line">            end=len(nums)-1</div><div class="line">            cur_target=target-nums[i]</div><div class="line">            while start&lt;end:</div><div class="line">                sum2=nums[start]+nums[end]</div><div class="line">                if sum2&lt;cur_target:</div><div class="line">                    if abs(cur_target-sum2)&lt;global_diff:</div><div class="line">                        global_sum=sum2+nums[i]</div><div class="line">                        global_diff=abs(cur_target-sum2)</div><div class="line">                    start+=1</div><div class="line">                elif sum2&gt;cur_target:</div><div class="line">                    if abs(sum2-cur_target)&lt;global_diff:</div><div class="line">                        global_diff=abs(sum2-cur_target)</div><div class="line">                        global_sum=sum2+nums[i]</div><div class="line">                    end-=1</div><div class="line">                else:</div><div class="line">                    return target</div><div class="line">        return global_sum</div></pre></td></tr></table></figure>
<h2 id="18-4Sum"><a href="#18-4Sum" class="headerlink" title="18. 4Sum"></a>18. 4Sum</h2><blockquote>
<p>Given an array S of n integers, are there elements a, b, c, and d in S such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target.<br>Note: The solution set must not contain duplicate quadruplets.<br>For example, given array S = [1, 0, -1, 0, -2, 2], and target = 0.<br>A solution set is:<br>[<br>  [-1,  0, 0, 1],<br>  [-2, -1, 1, 2],<br>  [-2,  0, 0, 2]<br>]</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def fourSum(self, nums, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type nums: List[int]</div><div class="line">        :type target: int</div><div class="line">        :rtype: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if len(nums)&lt;4:</div><div class="line">            return []</div><div class="line">        res=[]</div><div class="line">        nums=sorted(nums)</div><div class="line">        for i in range(len(nums)-3):</div><div class="line">            if i&gt;0 and nums[i]==nums[i-1]:</div><div class="line">                continue</div><div class="line">            res+=self.sum_3(nums[i:],target-nums[i])</div><div class="line">        return res</div><div class="line"></div><div class="line">    def sum_3(self,nums,target):</div><div class="line">        if len(nums)&lt;4:</div><div class="line">            return []</div><div class="line">        res=[]</div><div class="line">        cur=nums[0]</div><div class="line">        nums=nums[1:]</div><div class="line">        for i in range(len(nums)-2):</div><div class="line">            if i&gt;0 and nums[i]==nums[i-1]:</div><div class="line">                continue</div><div class="line">            new_target=target-nums[i]</div><div class="line">            start=i+1</div><div class="line">            end=len(nums)-1</div><div class="line">            while start&lt;end:</div><div class="line">                cur_sum=nums[start]+nums[end]</div><div class="line">                if cur_sum&gt;new_target:</div><div class="line">                    end-=1</div><div class="line">                elif cur_sum&lt;new_target:</div><div class="line">                    start+=1</div><div class="line">                else:</div><div class="line">                    res.append([cur,nums[i],nums[start],nums[end]])</div><div class="line">                    start+=1</div><div class="line">                    end-=1</div><div class="line">                    while start&lt;end and nums[start]==nums[start-1]:</div><div class="line">                        start+=1</div><div class="line">                    while start&lt;end and nums[end]==nums[end+1]:</div><div class="line">                        end-=1</div><div class="line">        return res</div></pre></td></tr></table></figure>
<h1 id="dict-详解"><a href="#dict-详解" class="headerlink" title="dict 详解"></a>dict 详解</h1><h2 id="内置函数和方法"><a href="#内置函数和方法" class="headerlink" title="内置函数和方法"></a>内置函数和方法</h2><table><tr><th style="width:5%">序号</th><th style="width:95%">函数及描述</th></tr><tr><td>1</td><td><a href="att-dictionary-cmp.html">cmp(dict1, dict2)</a><br>比较两个字典元素。</td></tr><tr><td>2</td><td><a href="att-dictionary-len.html">len(dict)</a><br>计算字典元素个数，即键的总数。</td></tr><tr><td>3</td><td><a href="att-dictionary-str.html">str(dict)</a><br>输出字典可打印的字符串表示。</td></tr><tr><td>4</td><td><a href="att-dictionary-type.html">type(variable)</a><br>返回输入的变量类型，如果变量是字典就返回字典类型。</td></tr></table>

<p>Python字典包含了以下内置方法：</p>
<table><tr><th style="width:5%">序号</th><th style="width:95%">函数及描述</th></tr><tr><td>1</td><td><a href="att-dictionary-clear.html">radiansdict.clear()</a><br>删除字典内所有元素 </td></tr><tr><td>2</td><td><a href="att-dictionary-copy.html">radiansdict.copy()</a><br>返回一个字典的浅复制</td></tr><tr><td>3</td><td><a href="att-dictionary-fromkeys.html">radiansdict.fromkeys()</a><br> 创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值</td></tr><tr><td>4</td><td><a href="att-dictionary-get.html">radiansdict.get(key, default=None)</a><br>返回指定键的值，如果值不在字典中返回default值</td></tr><tr><td>5</td><td><a href="att-dictionary-has_key.html">radiansdict.has_key(key)</a><br>如果键在字典dict里返回true，否则返回false</td></tr><tr><td>6</td><td><a href="att-dictionary-items.html">radiansdict.items()</a><br>以列表返回可遍历的(键, 值) 元组数组</td></tr><tr><td>7</td><td><a href="att-dictionary-keys.html">radiansdict.keys()</a><br>以列表返回一个字典所有的键</td></tr><tr><td>8</td><td><a href="att-dictionary-setdefault.html">radiansdict.setdefault(key, default=None)</a><br><br>和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default</td></tr><tr><td>9</td><td><a href="att-dictionary-update.html">radiansdict.update(dict2)</a><br>把字典dict2的键/值对更新到dict里</td></tr><tr><td>10</td><td><a href="att-dictionary-values.html">radiansdict.values()</a><br>以列表返回字典中的所有值</td></tr></table>

<h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>下表 python 3 中 dictinoary (包括 dict 和 defaultdict) 的时间复杂度，要注意的 d.keys() 在 python 2 中的复杂度是 O(n)，因为它返回的是一个 list</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Example</th>
<th>Class</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Index</td>
<td>d[k]</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Store</td>
<td>d[k] = v</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Length</td>
<td>len(d)</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Delete</td>
<td>del d[k]</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>get/setdefault</td>
<td>d.method</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Pop</td>
<td>d.pop(k)</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Pop item</td>
<td>d.popitem()</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Clear</td>
<td>d.clear()</td>
<td>O(1)</td>
<td>similar to s = {} or = dict()</td>
</tr>
<tr>
<td>Views</td>
<td>d.keys()</td>
<td>O(1)</td>
<td>——————————</td>
</tr>
<tr>
<td>Construction</td>
<td>dict(…)</td>
<td>O(len(…))</td>
<td>depends # (key,value) 2-tuples</td>
</tr>
<tr>
<td>Iteration</td>
<td>for k in d:</td>
<td>O(N)</td>
<td>all forms: keys, values, items</td>
</tr>
</tbody>
</table>
<p>So, most dict operations are O(1).</p>
<p>defaultdicts support all operations that dicts support, with the same complexity classes (because it inherits all the operations); this assumes that calling the constructor when a values isn’t found in the defaultdict is O(1) - which is true for int(), list(), set(), … (the things commonly used)</p>
<blockquote>
<p>参考链接<br><a href="https://wiki.python.org/moin/TimeComplexity" target="_blank" rel="external">Python TimeComplexity</a><br><a href="https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt" target="_blank" rel="external">Complexity of Python Operations</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> array </tag>
            
            <tag> two-sum </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 树]]></title>
      <url>http://www.shuang0420.com/2016/09/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%A0%91/</url>
      <content type="html"><![CDATA[<h2 id="最大最小值"><a href="#最大最小值" class="headerlink" title="最大最小值"></a>最大最小值</h2><p>python 里找 float 的最小值，float(‘-inf’)，最大值，float(‘inf’)<br>找 int 的最大最小值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">max = sys.maxint</div><div class="line">min = -sys.maxint-1</div></pre></td></tr></table></figure></p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>class 里创建 helper 方法第一个参数传 self, 调用 self.helper()<br>python 的三元运算符，python 的 max 方法。</p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul>
<li>Root: The node at the top of the tree</li>
<li>Parent: When any node (except the root) has exactly one edge running upward to another node. The node above is called parent of the node.</li>
<li>Child: Any node may have one or more lines running downward to other nodes. These nodes below the given node called its children.</li>
<li>Edge: connection between one node to another.</li>
<li>Leaf: A node that has no children is called a leaf. There can be only one root in a tree but there can be many leaves.</li>
<li>Level: The level of a node is defined by 1 + the number of connections between the node and the root.</li>
<li>Path: – a sequence of nodes and edges connecting a node with a descendant.</li>
<li>Height of node – The height of a node is the number of edges on the longest downward path between that node and a leaf.</li>
<li>Height of tree –The height of a tree is the number of edges on the longest downward path between the root and a leaf.</li>
<li>Depth –The depth of a node is the number of edges from the node to the tree’s root node.</li>
</ul>
<h2 id="complexity"><a href="#complexity" class="headerlink" title="complexity"></a>complexity</h2><p>insertion, 平均 O(logN)，左树找到 SPOT,右树找到 SPOT,一次砍一半，就是 O(logN)<br>deletion,平均情况 O(logN)多一些，先搜索到元素，O(logN)，没找到就 end，找到，分 4 种，记录元素是 left 还是 right</p>
<ul>
<li>leaf parent 对应指针指到 null</li>
<li>仅有左边 child，parent 对应指针指到左 child,元素指针全部删除</li>
<li>仅有右边 child，parent 对应指针指到右 child,元素指针全部删除</li>
<li>有左右两个 child，先找 successor，就是右子树的最小严肃，从要删除的元素往下一路向左，找到最左元素，定为 successor，然后如果 successor 有右树，将其连到 parent 的左树上，successor 新的右树，连到被删除元素的右树上。</li>
</ul>
<h2 id="Binary-search-tree-二叉搜索树"><a href="#Binary-search-tree-二叉搜索树" class="headerlink" title="Binary search tree 二叉搜索树"></a>Binary search tree 二叉搜索树</h2><p>二叉搜索树每个节点比其左子树元素大，比其右子树元素小。</p>
<blockquote>
<p>The left subtree of a node contains only nodes with keys less than node’s key.<br>The right subtree of a node contains only nodes with keys greater than node’s key.</p>
</blockquote>
<p>二叉搜索树的作用：保持元素顺序，相当于是一个排序好的 list，插入删除操作，比排序的 list 快，维护元素顺序或对元素排序时，非常适用。</p>
<h2 id="Balanced-binary-tree-平衡树"><a href="#Balanced-binary-tree-平衡树" class="headerlink" title="Balanced binary tree 平衡树"></a>Balanced binary tree 平衡树</h2><p>树结构越接近一个链条，各操作就越像线性结构，就越失去了树结构独特的优势，所以引入了平衡树</p>
<h2 id="遍历方法"><a href="#遍历方法" class="headerlink" title="遍历方法"></a>遍历方法</h2><ul>
<li>深度优先（DFS）<br>先根(preorder)，中根(inorder)，后根(postorder)</li>
<li>广度优先（BFS）<br>优先遍历完同层，是一个 queue 结构，先 dequeue 根节点 q，按照层序 enqueue 其 children，visit(q)然后 dequeue 根部新根节点。继续 enqueue，重复刀 dequeue 空为止。所有 node 都被 enqueue 和 dequeue 一遍，复杂度是 O(2n)，即 O(n)</li>
</ul>
<h3 id="stack-amp-DFS"><a href="#stack-amp-DFS" class="headerlink" title="stack &amp; DFS"></a>stack &amp; DFS</h3><h4 id="先根-144-Binary-Tree-Preorder-Traversal"><a href="#先根-144-Binary-Tree-Preorder-Traversal" class="headerlink" title="先根(144.Binary Tree Preorder Traversal)"></a>先根(144.Binary Tree Preorder Traversal)</h4><p>注意 conner case root==None 的时候返回的是[],不是 root(None)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def preorderTraversal(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if root is None:</div><div class="line">            return []</div><div class="line">        else:</div><div class="line">            return [root.val]+self.preorderTraversal(root.left)+self.preorderTraversal(root.right)</div></pre></td></tr></table></figure></p>
<h4 id="中根-94-Binary-Tree-Inorder-Traversal"><a href="#中根-94-Binary-Tree-Inorder-Traversal" class="headerlink" title="中根(94.Binary Tree Inorder Traversal)"></a>中根(94.Binary Tree Inorder Traversal)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def inorderTraversal(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if root is None:</div><div class="line">            return []</div><div class="line">        return self.inorderTraversal(root.left)+[root.val]+self.inorderTraversal(root.right)</div></pre></td></tr></table></figure>
<h4 id="后根-145-Binary-Tree-Postorder-Traversal"><a href="#后根-145-Binary-Tree-Postorder-Traversal" class="headerlink" title="后根(145.Binary Tree Postorder Traversal)"></a>后根(145.Binary Tree Postorder Traversal)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def postorderTraversal(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if root is None:</div><div class="line">            return []</div><div class="line">        return self.postorderTraversal(root.left)+self.postorderTraversal(root.right)+[root.val]</div></pre></td></tr></table></figure>
<h3 id="queue-amp-BFS-102-Binary-Tree-Level-Order-Traversal"><a href="#queue-amp-BFS-102-Binary-Tree-Level-Order-Traversal" class="headerlink" title="queue &amp; BFS(102. Binary Tree Level Order Traversal)"></a>queue &amp; BFS(102. Binary Tree Level Order Traversal)</h3><h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>Given a binary tree, return the level order traversal of its nodes’ values. (ie, from left to right, level by level).<br>For example:<br>Given binary tree [3,9,20,null,null,15,7],<br>   3<br>  / \<br> 9   20<br>/  \<br>15 7<br>return its level order traversal as:<br>[<br>[3],<br>[9,20],<br>[15,7]<br>]</p>
<p>遍历当前的 queue, 把每个 node value 存到 list，将每个 node 的 left 和 right node 存到 queue，遍历完后将当前 list 加进 result 里。<br>问题是怎么遍历当前 queue，通过纪录每个 layer（也就是 queue）的长度来实现。<br>corner case: root == None, return []</p>
<p>Time complexity: O(n)，每个 node enqueue 一次，dequeue 一次<br>Space complexity: O(n)，worst space,最后一次，满树，大概 O(n/2)</p>
<h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">def levelOrder(self, root):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    :type root: TreeNode</div><div class="line">    :rtype: List[List[int]]</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    if not root:</div><div class="line">        return []</div><div class="line">    q = deque([root])</div><div class="line">    result = []</div><div class="line">    while q:</div><div class="line">        layer = []</div><div class="line">        size = len(q)</div><div class="line">        for i in range(size):</div><div class="line">            node = q.popleft()</div><div class="line">            layer.append(node.val)</div><div class="line">            if node.left:</div><div class="line">                q.append(node.left)</div><div class="line">            if node.right:</div><div class="line">                q.append(node.right)</div><div class="line">        result.append(layer)</div><div class="line">    return result</div></pre></td></tr></table></figure>
<h1 id="解题策略"><a href="#解题策略" class="headerlink" title="解题策略"></a>解题策略</h1><h2 id="Divide-and-conquer"><a href="#Divide-and-conquer" class="headerlink" title="Divide and conquer"></a>Divide and conquer</h2><p>树和图的很多问题，可以分解成子问题递归求解（divide and conquer），一般思路是综合节点本身，左子树，右子树三方的局部解得到全局解。</p>
<p>要注意的是，传节点的时候</p>
<ul>
<li>设置出口，ending case，一般是 node==None；</li>
<li>Recursive down</li>
<li>Return up</li>
<li>Current layer</li>
</ul>
<p>构造递归的时候，可以 suppose all subtrees are handled.</p>
<h2 id="特定路径的问题"><a href="#特定路径的问题" class="headerlink" title="特定路径的问题"></a>特定路径的问题</h2><p>关于寻找特定路径的问题，通常需要回溯思想，我们往往需要设计一个 helper function，传入当前节点和其它需要记录的参数。</p>
<h2 id="树和其他数据结构的相互转换"><a href="#树和其他数据结构的相互转换" class="headerlink" title="树和其他数据结构的相互转换"></a>树和其他数据结构的相互转换</h2><p>树 –&gt; 其它数据结构：树的遍历，合并局部解来得到全局解<br>其它数据结构 –&gt; 树：递归将数据结构的两部分分别转换成子树，再合并。</p>
<h2 id="寻找特定节点"><a href="#寻找特定节点" class="headerlink" title="寻找特定节点"></a>寻找特定节点</h2><p>此类题目通常会传入一个当前节点，要求找到与此节点具有一定关系的特定节点：例如前驱、后继、左／右兄弟等。</p>
<p>对于这类题目，首先可以了解一下常见特定节点的定义及性质。在存在指向父节点指针的情况下，通常可以由当前节点出发，向上倒推解决。如果节点没有父节点指针，一般需要从根节点出发向下搜索，搜索的过程就是DFS。</p>
<h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><p>Error control:</p>
<ul>
<li>确定 node.left, node.right 是否为 None，尤其是 leverl order traversal 中。</li>
<li>connection between nodes 在原来的 tree 上更改箭头，这样更清楚要不要抛弃某些箭头。</li>
</ul>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="101-Symmetric-Tree"><a href="#101-Symmetric-Tree" class="headerlink" title="101. Symmetric Tree"></a>101. Symmetric Tree</h2><h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center).</p>
<p>For example, this binary tree [1,2,2,3,4,4,3] is symmetric:</p>
<pre><code>1
</code></pre><p>   / \<br>  2   2<br> / \ / \<br>3  4 4  3<br>But the following [1,2,2,null,3,null,3] is not:<br>    1<br>   / \<br>  2   2<br>   \   \<br>   3    3<br>Note:<br>Bonus points if you could solve it both recursively and iteratively.</p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- primitive idea: level order traversal, use deque, check popleft()==pop(), but should consider how to handle [1,2,2,null,3,null,3], the easiest way is to treat it as a complete tree, keep None there</div><div class="line">    this is two-pass solution, with O(n) space complexity and O(n) time complexity</div><div class="line">- make it one-pass</div><div class="line"></div><div class="line">Followup:</div><div class="line">- without stack: use recursive</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">from collections import deque</div><div class="line">class Solution(object):</div><div class="line">    def isSymmetric(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root:</div><div class="line">            return True</div><div class="line">        return self.helper(root.left, root.right)</div><div class="line"></div><div class="line">    def helper(self, left, right):</div><div class="line">        if not left or not right:</div><div class="line">            if left == right:</div><div class="line">                return True</div><div class="line">            else:</div><div class="line">                return False</div><div class="line">        if left.val == right.val:</div><div class="line">            return self.helper(left.left, right.right) and self.helper(left.right, right.left)</div><div class="line">        return False</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # two-pass solution with deque</div><div class="line">    def isSymmetric(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root:</div><div class="line">            return True</div><div class="line">        # level order traversal</div><div class="line">        result=deque()</div><div class="line">        queue=deque([root])</div><div class="line">        while queue:</div><div class="line">            size=len(queue)</div><div class="line">            layer=deque()</div><div class="line">            for i in range(size):</div><div class="line">                node=queue.popleft()</div><div class="line">                if not node:</div><div class="line">                    layer.append(None)</div><div class="line">                    continue</div><div class="line">                layer.append(node.val)</div><div class="line">                queue.append(node.left)</div><div class="line">                queue.append(node.right)</div><div class="line">            result.append(layer)</div><div class="line">        # pop root and last level(all None)</div><div class="line">        result.popleft()</div><div class="line">        result.pop()</div><div class="line">        while result:</div><div class="line">            layer=result.pop()</div><div class="line">            while layer:</div><div class="line">                if layer.pop()!=layer.popleft():</div><div class="line">                    return False</div><div class="line">        return True</div><div class="line">        &apos;&apos;&apos;     </div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # one-pass solution with deque</div><div class="line">    def isSymmetric(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root:</div><div class="line">            return True  </div><div class="line">        queue=deque([root.left,root.right])</div><div class="line">        while queue:</div><div class="line">            left=queue.popleft()</div><div class="line">            right=queue.pop()</div><div class="line">            if not left and not right: continue</div><div class="line">            if not left or not right or left.val!=right.val: return False</div><div class="line">            queue.appendleft(left.left)</div><div class="line">            queue.appendleft(left.right)</div><div class="line">            queue.append(right.right)</div><div class="line">            queue.append(right.left)</div><div class="line">        return True</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="156-Binary-tree-upside-down"><a href="#156-Binary-tree-upside-down" class="headerlink" title="156. Binary tree upside down"></a>156. Binary tree upside down</h2><h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree where all the right nodes are either leaf nodes with a sibling (a left node that shares the same parent node) or empty, flip it upside down and turn it into a tree where the original right nodes turned into left leaf nodes. Return the new root.<br>For example:<br>Given a binary tree {1,2,3,4,5},<br>    1<br>  /   \<br> 2    3<br>/ \<br>4  5<br>return the root of the binary tree [4,5,2,#,#,3,1].<br>     4<br>    / \<br>   5   2<br>      / \<br>      3  1</p>
<p>Assumption: input tree is valid<br>Conner Case: Null root –&gt; Null new root</p>
<h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><h4 id="Stack-解法"><a href="#Stack-解法" class="headerlink" title="Stack 解法"></a>Stack 解法</h4><p>Time compexity: O(n)<br>Space complexity: O(n/2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- with stack: store all nodes along the left path in a stack, and flit it.</div><div class="line">    root=&gt;root.left,</div><div class="line">    root.left=&gt;root.right,</div><div class="line">    root.right=&gt;root</div><div class="line"></div><div class="line">- recursive: assume lower layers are handled (we can pass root.left as parameter and call the function till leftmost node), handle current layer only</div><div class="line"></div><div class="line">Attention:</div><div class="line">- draw connection between nodes at original tree, so that you won&apos;t forget to clear pointers after each flip, and transfer root control.</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">def upsideDownBinaryTree(self, root):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    :type root: TreeNode</div><div class="line">    :rtype: TreeNode</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    if not root:</div><div class="line">        return root</div><div class="line"></div><div class="line">    stack = []</div><div class="line">    # store all nodes along the path in stack</div><div class="line">    while root:</div><div class="line">        stack.append(root)</div><div class="line">        root = root.left</div><div class="line"></div><div class="line">    # start from leftmost leaf</div><div class="line">    newRoot = stack.pop()</div><div class="line">    head = newRoot</div><div class="line">    while stack:</div><div class="line">        parent = stack.pop()</div><div class="line">        head.left = parent.right # parent</div><div class="line">        head.right = parent</div><div class="line"></div><div class="line">        head = parent</div><div class="line">        parent.left = None</div><div class="line">        parent.right = None</div><div class="line"></div><div class="line">    return newRoot</div></pre></td></tr></table></figure></p>
<h4 id="Recursive-解法"><a href="#Recursive-解法" class="headerlink" title="Recursive 解法"></a>Recursive 解法</h4><p>Identical subproblem, lower level first<br>通过 root.left 过渡到下一个子问题，从下往上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def upsideDownBinaryTree(self, root):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    :type root: TreeNode</div><div class="line">    :rtype: TreeNode</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    # stop case</div><div class="line">    if not root or not root.left:</div><div class="line">        return root</div><div class="line">    # assume all lower levels are handled</div><div class="line">    newRoot = self.upsideDownBinaryTree(root.left)</div><div class="line"></div><div class="line">    # handle current level</div><div class="line">    root.left.left = root.right</div><div class="line">    root.left.right = root</div><div class="line"></div><div class="line">    root.left = None</div><div class="line">    root.right = None</div><div class="line"></div><div class="line">    return newRoot</div></pre></td></tr></table></figure></p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/binary_tree_upside_down.jpg" alt=""></p>
<h2 id="98-Valid-binary-search-tree"><a href="#98-Valid-binary-search-tree" class="headerlink" title="98. Valid binary search tree"></a>98. Valid binary search tree</h2><h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree, determine if it is a valid binary search tree (BST).<br>Assume a BST is defined as follows:<br>The left subtree of a node contains only nodes with keys less than the node’s key.<br>The right subtree of a node contains only nodes with keys greater than the node’s key.<br>Both the left and right subtrees must also be binary search trees.<br>Example 1:<br>    2<br>   / \<br>  1   3<br>Binary tree [2,1,3], return true.<br>Example 2:<br>    1<br>   / \<br>  2   3</p>
<h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><h4 id="Inorder-traversal"><a href="#Inorder-traversal" class="headerlink" title="Inorder traversal"></a>Inorder traversal</h4><p>根据二叉搜索树的性质，我们知道二叉搜索树中序遍历之后是一个 sorted list，所以最直观的方法就是中序遍历存到一个 list，然后看它是不是 sorted。这样的空间复杂度是 O(N)，时间复杂度由排序算法决定。</p>
<h4 id="Global-max"><a href="#Global-max" class="headerlink" title="Global max"></a>Global max</h4><p>用 global max 能让空间复杂度变成 O(1)？这个还不知道。。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def isValidBST(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root:</div><div class="line">            return True</div><div class="line">        list = self.getList(root)</div><div class="line">        max = float(&apos;inf&apos;)</div><div class="line">        while list:</div><div class="line">            node = list.pop()</div><div class="line">            if node &gt;= max:</div><div class="line">                return False</div><div class="line">            max = node</div><div class="line">        return True</div><div class="line"></div><div class="line">    def getList(self, root):</div><div class="line">        if not root:</div><div class="line">            return []</div><div class="line">        return self.getList(root.left)+[root.val]+self.getList(root.right)</div></pre></td></tr></table></figure></p>
<h4 id="Range"><a href="#Range" class="headerlink" title="Range"></a>Range</h4><p>分解子问题，每一个 node 都大于它的 left child 并且小于它的 right child。所以可以写一个 helper function，传入 (node, min, max) 来判断 node 在不在正确的区间里。主要问题如下：</p>
<ul>
<li>how to compare cross-layer</li>
<li>how to get a valid range for each node<br>implementation: pass parameter top-to-bottom, helper(current_node,min,max)</li>
</ul>
<p>注意小知识点，python 里找 float 的最小值，float(‘-inf’)，最大值，float(‘inf’)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def isValidBST(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def isBST(root, max=float(&apos;inf&apos;), min=float(&apos;-inf&apos;)):</div><div class="line">            if not root:</div><div class="line">                return True</div><div class="line">            if root.val &gt;= max or root.val &lt;= min:</div><div class="line">                return False</div><div class="line">            return isBST(root.left, root.val, min) and isBST(root.right, max, root.val)</div><div class="line">        return isBST(root)</div></pre></td></tr></table></figure>
<h2 id="333-Largest-BST-Subtree"><a href="#333-Largest-BST-Subtree" class="headerlink" title="333. Largest BST Subtree"></a>333. Largest BST Subtree</h2><h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree, find the largest subtree which is a Binary Search Tree (BST), where largest means subtree with largest number of nodes in it.</p>
<p>Note:<br>A subtree must include all of its descendants.<br>Here’s an example:<br>    10<br>    / \<br>   5  15<br>  / \   \<br> 1   8   7<br>The Largest BST Subtree in this case is the highlighted one.<br>The return value is the subtree’s size, which is 3.</p>
<h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    brute-force: O(n^2)</div><div class="line"></div><div class="line">Followup: make it O(n)</div><div class="line">    Current layer: if both left subtree and right subtree are BST and left.max&lt;=root.val&lt;=right.min,then current subtree is BST and size=left.size+right.size+1,else,current subtree is not BST and size=max(left.size,right.size)</div><div class="line">    Recursive down:</div><div class="line">        base case: if not root</div><div class="line">        recursive case:</div><div class="line">            left=recursive(root.left), right=recursive(root.right)</div><div class="line">    Return up:</div><div class="line">        return res which includes:</div><div class="line">            - isBST: if current subtree is BST</div><div class="line">            - max: max value of left subtree</div><div class="line">            - min: min value of right subtree</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    brute-force: O(n^2)</div><div class="line"></div><div class="line">    def largestBSTSubtree(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def isBST(root, max=float(&apos;inf&apos;), min=float(&apos;-inf&apos;)):</div><div class="line">            if not root:</div><div class="line">                return True</div><div class="line">            if root.val &gt;= max or root.val &lt;= min:</div><div class="line">                return False</div><div class="line">            return isBST(root.left, root.val, min) and isBST(root.right, max, root.val)</div><div class="line"></div><div class="line">        def size(root):</div><div class="line">            if not root:</div><div class="line">                return 0</div><div class="line">            return size(root.left) + size(root.right) + 1</div><div class="line"></div><div class="line">        if isBST(root):</div><div class="line">            return size(root)</div><div class="line">        return max(self.largestBSTSubtree(root.left), self.largestBSTSubtree(root.right))</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def largestBSTSubtree(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        class Result(object):</div><div class="line">            def __init__(self,myMax=float(&apos;-inf&apos;),myMin=float(&apos;inf&apos;)):</div><div class="line">                self.isBST=True</div><div class="line">                self.max=myMax</div><div class="line">                self.min=myMin</div><div class="line">                self.size=0</div><div class="line"></div><div class="line">        def recursive(root):</div><div class="line">            res=Result()</div><div class="line">            # base case</div><div class="line">            if not root:</div><div class="line">                return res</div><div class="line">            # recursive case</div><div class="line">            left=recursive(root.left)</div><div class="line">            right=recursive(root.right)</div><div class="line">            res.max=max(root.val,right.max)</div><div class="line">            res.min=min(root.val,left.min)</div><div class="line">            # current layer</div><div class="line">            if left.isBST and right.isBST and left.max&lt;=root.val and right.min&gt;=root.val:</div><div class="line">                res.size=left.size+right.size+1</div><div class="line">                res.isBST=True</div><div class="line">            else:</div><div class="line">                res.isBST=False</div><div class="line">                res.size=max(left.size,right.size)</div><div class="line">            return res</div><div class="line"></div><div class="line">        res= recursive(root)  </div><div class="line">        return res.size</div></pre></td></tr></table></figure>
<h2 id="298-Binary-Tree-Longest-Consecutive-Sequence"><a href="#298-Binary-Tree-Longest-Consecutive-Sequence" class="headerlink" title="298. Binary Tree Longest Consecutive Sequence"></a>298. Binary Tree Longest Consecutive Sequence</h2><h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree, find the length of the longest consecutive sequence path.<br>The path refers to any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The longest consecutive path need to be from parent to child (cannot be the reverse).<br>For example,<br>   1<br>    \<br>     3<br>    / \<br>   2   4<br>        \<br>         5<br>Longest consecutive sequence path is 3-4-5, so return 3.<br>   2<br>    \<br>     3<br>    /<br>   2<br>  /<br> 1<br>Longest consecutive sequence path is 2-3,not3-2-1, so return 2.</p>
<p>直接想到的是，preorder 遍历，得到 list，用 count 更新 current max。然而这是有问题的！以 example 为例，遍历后的 list 是 [1,3,2,4,5],它把 2,4 连起来了，但是 2,4 属于不同的分支，所以不能这么做。</p>
<p>换一种思路，用 recursion 的方法，左子树的 max length, 右子树的 max length，和本身目前的 max length，求最大值。</p>
<p>第一个传进去的是什么？float(‘inf’)</p>
<h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">recurse down</div><div class="line">helper(node.left, global_max, node.val)</div><div class="line">helper(node.right, global_max, node.val)</div><div class="line"></div><div class="line">return up</div><div class="line">max (curr_lengh, left_max_length, right_max_length)</div><div class="line"></div><div class="line">current level</div><div class="line">global_max = node.val == lastVal + 1 : length + 1 : 1</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def longestConsecutive(self, root):</div><div class="line">        if not root:</div><div class="line">            return 0</div><div class="line"></div><div class="line">        return self.helper(root, 1, float(&apos;inf&apos;))</div><div class="line"></div><div class="line">    def helper(self, node, global_max, lastVal):</div><div class="line">        if not node:</div><div class="line">            return global_max</div><div class="line">        global_max = global_max + 1 if node.val == lastVal + 1 else 1</div><div class="line">        return max(global_max, self.helper(node.left, global_max, node.val), self.helper(node.right, global_max, node.val))</div></pre></td></tr></table></figure>
<h1 id="待补充"><a href="#待补充" class="headerlink" title="待补充"></a>待补充</h1><p>trie or prefix tree，full binary tree，complete binary tree，heap</p>
<h2 id="222-Count-Complete-Tree-Nodes"><a href="#222-Count-Complete-Tree-Nodes" class="headerlink" title="222. Count Complete Tree Nodes"></a>222. Count Complete Tree Nodes</h2><h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem"></a>Problem</h3><p>Given a complete binary tree, count the number of nodes.</p>
<p>Definition of a complete binary tree from Wikipedia:<br>In a complete binary tree every level, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible. It can have between 1 and 2h nodes inclusive at the last level h.</p>
<h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><p>Time complexity: O(h^2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Primitive idea:</div><div class="line">    traversal, BFS, DFS(preorder, inorder, postorder), takes O(n) time.</div><div class="line">Faster:</div><div class="line">    List all posibilities of a complete trees and we can conlude there&apos;re two cases one is the height of left subtree equals that of right subtree, in this case, the right subtree is always full, another case is that the height of left subtree is smaller than that of right subtree, then the left subtree is full. It&apos;s easy to count the nodes of full tree.</div><div class="line">    We can use recursive method to solve this problem. That is,</div><div class="line">    if height(left)==height(right): count(root)=2**leftH-1+1+count(root.right)</div><div class="line">    else: count(root)==2**rightH-1+1+count(root.left)</div><div class="line">    Time complexity: O(logn^2)</div><div class="line">Tips:</div><div class="line">    1&lt;&lt;h is faster than 2**h but remember to use () due to precedence</div><div class="line">    be careful about getHeight method if not root should return -1 instead of 0</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def countNodes(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def getHeight(root):</div><div class="line">            if not root:</div><div class="line">                return-1</div><div class="line">            return getHeight(root.left) + 1</div><div class="line"></div><div class="line">        # base case</div><div class="line">        if not root:</div><div class="line">            return 0</div><div class="line">        # recursive case</div><div class="line">        leftH = getHeight(root)</div><div class="line">        rightH = getHeight(root.right) + 1</div><div class="line">        # left subtree is full</div><div class="line">        if leftH == rightH:</div><div class="line">            return (1 &lt;&lt; leftH) + self.countNodes(root.right)</div><div class="line">        # right subtree is full</div><div class="line">        else:</div><div class="line">            return (1 &lt;&lt; rightH) + self.countNodes(root.left)</div></pre></td></tr></table></figure></p>
<h2 id="270-Closest-Binary-Search-Tree-Value"><a href="#270-Closest-Binary-Search-Tree-Value" class="headerlink" title="270. Closest Binary Search Tree Value"></a>270. Closest Binary Search Tree Value</h2><h3 id="Problem-7"><a href="#Problem-7" class="headerlink" title="Problem"></a>Problem</h3><p>Given a non-empty binary search tree and a target value, find the value in the BST that is closest to the target.</p>
<p>Note:<br>Given target value is a floating point.<br>You are guaranteed to have only one unique value in the BST that is closest to the target.</p>
<h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">- Solution: keeps a global closest variable and update it when doing the search. Time complexity: O(logN). Space complexity: O(1)</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def closestValue(self, root, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :type target: float</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root: return None</div><div class="line">        closest=root.val</div><div class="line">        diff=abs(root.val-target)</div><div class="line">        while root:</div><div class="line">            if abs(root.val-target)&lt;diff:</div><div class="line">                diff=abs(root.val-target)</div><div class="line">                closest=root.val</div><div class="line">            root=root.right if root.val&lt;target else root.left</div><div class="line">        return closest</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def closestValue(self, root, target):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :type target: float</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root: return None</div><div class="line">        if not root.left and not root.right: return root.val</div><div class="line">        def inorderTraversal(root):</div><div class="line">            if not root: return []</div><div class="line">            return inorderTraversal(root.left)+[root.val]+inorderTraversal(root.right)</div><div class="line">        vals=inorderTraversal(root)</div><div class="line">        for i in range(len(vals)):</div><div class="line">            if target&lt;vals[i]:</div><div class="line">                if abs(target-vals[i])&lt;abs(target-vals[i-1]):</div><div class="line">                    return vals[i]</div><div class="line">                else:</div><div class="line">                    return vals[i-1]</div><div class="line">        return vals[-1]</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="272-Closest-Binary-Search-Tree-Value-II"><a href="#272-Closest-Binary-Search-Tree-Value-II" class="headerlink" title="272. Closest Binary Search Tree Value II"></a>272. Closest Binary Search Tree Value II</h2><h3 id="Problem-8"><a href="#Problem-8" class="headerlink" title="Problem"></a>Problem</h3><p>Given a non-empty binary search tree and a target value, find k values in the BST that are closest to the target.</p>
<p>Note:<br>Given target value is a floating point.<br>You may assume k is always valid, that is: k ≤ total nodes.<br>You are guaranteed to have only one unique set of k values in the BST that are closest to the target.<br>Follow up:<br>Assume that the BST is balanced, could you solve it in less than O(n) runtime (where n = total nodes)?</p>
<h3 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">assume k is smaller than the # of nodes in BST</div><div class="line"></div><div class="line">Wrong solution:</div><div class="line">    Recall Closest Binary Search Tree Value I, maybe we can maintain a k-size of diff priorityqueue and k-size of closest priorityqueue and do the pop and push the same way as problem I?</div><div class="line">    -&gt; We can do that but only when H&gt;=k where H is the height of BST, but when H&lt;k, we should add more values into closest, it&apos;s hard to decide or to track these nodes because we may need to access the predecessors.</div><div class="line">    -&gt; So we may think of two helper function getSuccessor, getPredecessor,</div><div class="line"></div><div class="line">- Solution 1:</div><div class="line">    - have an in-order traversal and get a sorted array, then find the closest value to the target in the sorted array, and look forward and backwards to get k closest values.</div><div class="line">    - or, same idea, have two stacks one stores the values that are smaller than the target, and the other stores the values that are larger than the target, and finally merge it</div><div class="line">    - O(n) time complexity</div><div class="line"></div><div class="line">- Solution 2:</div><div class="line">    - first find the closest value cur to target, while k&lt;0, find the closest smaller value to the cur and the closest larger value to the cur, compare them to find the the closest value to the target and add it to the result list till there&apos;re k values in the result</div><div class="line">    - O(klogn) time complexity</div><div class="line"></div><div class="line"></div><div class="line">Corner case:</div><div class="line">    - Solution 1 must consider case when target&lt;0</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line">from collections import deque</div><div class="line">class Solution(object):</div><div class="line">    # O(klogn)</div><div class="line">    def closestKValues(self, root, target, k):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :type target: float</div><div class="line">        :type k: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root: return None</div><div class="line">        closest=self.getClosestVal(root,target)</div><div class="line">        res=[closest.val]</div><div class="line">        k-=1</div><div class="line">        # merge k values</div><div class="line">        s=self.getSmaller(root,closest.val)</div><div class="line">        l=self.getLarger(root,closest.val)</div><div class="line">        while k&gt;0:</div><div class="line">            if not s:</div><div class="line">                res.append(l.val)</div><div class="line">                l=self.getLarger(root,l.val)</div><div class="line">            elif not l:</div><div class="line">                res.append(s.val)</div><div class="line">                s=self.getSmaller(root,s.val)</div><div class="line">            elif abs(s.val-target)&lt;abs(l.val-target):</div><div class="line">                res.append(s.val)</div><div class="line">                s=self.getSmaller(root,s.val)</div><div class="line">            else:</div><div class="line">                res.append(l.val)</div><div class="line">                l=self.getLarger(root,l.val)</div><div class="line">            k-=1;</div><div class="line">        return res</div><div class="line"></div><div class="line"></div><div class="line">    def getSmaller(self,root,t):</div><div class="line">        s=None</div><div class="line">        while root:</div><div class="line">            if root.val&lt;t:</div><div class="line">                s=root</div><div class="line">                root=root.right</div><div class="line">            else:</div><div class="line">                root=root.left</div><div class="line">        return s</div><div class="line"></div><div class="line">    def getLarger(self,root,t):</div><div class="line">        l=None</div><div class="line">        while root:</div><div class="line">            if root.val&gt;t:</div><div class="line">                l=root</div><div class="line">                root=root.left</div><div class="line">            else:</div><div class="line">                root=root.right  </div><div class="line">        return l</div><div class="line"></div><div class="line">    def getClosestVal(self,root,target):</div><div class="line">        closest=root</div><div class="line">        diff=abs(root.val-target)</div><div class="line">        while root:</div><div class="line">            if abs(root.val-target)&lt;diff:</div><div class="line">                diff=abs(root.val-target)</div><div class="line">                closest=root</div><div class="line">            root=root.right if root.val&lt;target else root.left</div><div class="line">        return closest</div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # O(n)</div><div class="line">    def closestKValues(self, root, target, k):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :type target: float</div><div class="line">        :type k: int</div><div class="line">        :rtype: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root: return None</div><div class="line">        def dfs(root):</div><div class="line">            if not root: return []</div><div class="line">            return dfs(root.left)+[root.val]+dfs(root.right)</div><div class="line"></div><div class="line">        values=dfs(root)</div><div class="line">        # get smaller values and larger values</div><div class="line">        res,smaller,larger=[],[],[]</div><div class="line">        for v in values:</div><div class="line">            if v&lt;target:</div><div class="line">                smaller.append(v)</div><div class="line">            elif v&gt;target:</div><div class="line">                larger.append(v)</div><div class="line">            else:</div><div class="line">                res.append(v)</div><div class="line">                k-=1</div><div class="line">        # merge k values</div><div class="line">        if target&lt;0:</div><div class="line">            smaller=smaller[::-1]</div><div class="line">        larger=larger[::-1]</div><div class="line">        while k&gt;0:</div><div class="line">            if not smaller:</div><div class="line">                res.append(larger.pop())</div><div class="line">            elif not larger:</div><div class="line">                res.append(smaller.pop())</div><div class="line">            elif abs(smaller[-1]-target)&lt;abs(larger[-1]-target):</div><div class="line">                res.append(smaller.pop())</div><div class="line">            else:</div><div class="line">                res.append(larger.pop())</div><div class="line">            k-=1;</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="366-Find-Leaves-of-Binary-Tree"><a href="#366-Find-Leaves-of-Binary-Tree" class="headerlink" title="366. Find Leaves of Binary Tree"></a>366. Find Leaves of Binary Tree</h2><h3 id="Problem-9"><a href="#Problem-9" class="headerlink" title="Problem"></a>Problem</h3><p>Given a binary tree, collect a tree’s nodes as if you were doing this: Collect and remove all leaves, repeat until the tree is empty.</p>
<p>Example:<br>Given binary tree<br>          1<br>         / \<br>        2   3<br>       / \<br>      4   5<br>Returns [4, 5, 3], [2], [1].</p>
<p>Explanation:<br>1.Removing the leaves [4, 5, 3] would result in this tree:</p>
<pre><code>  1
 /
2          
</code></pre><p>2.Now removing the leaf [2] would result in this tree:</p>
<pre><code>1          
</code></pre><p>3.Now removing the leaf [1] would result in the empty tree:</p>
<pre><code>[]         
</code></pre><p>Returns [4, 5, 3], [2], [1].</p>
<h3 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    brute-force</div><div class="line">    string encoding(preorder): 124##5##3##</div><div class="line"></div><div class="line">Followup: make it O(n)</div><div class="line">    nodes with the same height should be leaves for each turn</div><div class="line">&apos;&apos;&apos;</div><div class="line"># Definition for a binary tree node.</div><div class="line"># class TreeNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.left = None</div><div class="line">#         self.right = None</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def findLeaves(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root: return []</div><div class="line">        def getHeight(root):</div><div class="line">            if not root: return 0</div><div class="line">            return max(getHeight(root.left),getHeight(root.right))+1</div><div class="line"></div><div class="line">        def traverse(root):</div><div class="line">            global res</div><div class="line">            if not root:</div><div class="line">                return 0</div><div class="line">            lev=max(traverse(root.left),traverse(root.right))+1</div><div class="line">            res[lev-1].append(root.val)</div><div class="line">            return lev</div><div class="line"></div><div class="line">        global res</div><div class="line">        h=getHeight(root)</div><div class="line">        res=[list() for i in range(h)]</div><div class="line">        traverse(root)</div><div class="line">        return res</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def findLeaves(self, root):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: List[List[int]]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not root:</div><div class="line">            return []</div><div class="line"></div><div class="line">        def traverse(root, res):</div><div class="line">            if not root:</div><div class="line">                return 0</div><div class="line">            lev = max(traverse(root.left, res), traverse(root.right, res)) + 1</div><div class="line">            res[lev].append(root.val)</div><div class="line">            return lev</div><div class="line"></div><div class="line">        res = collections.defaultdict(list)</div><div class="line">        traverse(root, res)</div><div class="line">        return res.values()</div></pre></td></tr></table></figure>
<h2 id="307-Range-Sum-Query-Mutable-Binary-indexed-tree"><a href="#307-Range-Sum-Query-Mutable-Binary-indexed-tree" class="headerlink" title="307. Range Sum Query - Mutable (Binary indexed tree)"></a>307. Range Sum Query - Mutable (Binary indexed tree)</h2><h3 id="Problem-10"><a href="#Problem-10" class="headerlink" title="Problem"></a>Problem</h3><p>Given an integer array nums, find the sum of the elements between indices i and j (i ≤ j), inclusive.</p>
<p>The update(i, val) function modifies nums by updating the element at index i to val.<br>Example:<br>Given nums = [1, 3, 5]</p>
<p>sumRange(0, 2) -&gt; 9<br>update(1, 2)<br>sumRange(0, 2) -&gt; 8<br>Note:<br>The array is only modifiable by the update function.<br>You may assume the number of calls to update and sumRange function is distributed evenly.</p>
<h3 id="Solution-10"><a href="#Solution-10" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">update: O(1)</div><div class="line">sumRange: O(n)</div><div class="line"></div><div class="line">class NumArray(object):</div><div class="line">    def __init__(self, nums):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        :type nums: List[int]</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.nums=nums</div><div class="line"></div><div class="line">    def update(self, i, val):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type i: int</div><div class="line">        :type val: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.nums[i]=val</div><div class="line"></div><div class="line"></div><div class="line">    def sumRange(self, i, j):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        sum of elements nums[i..j], inclusive.</div><div class="line">        :type i: int</div><div class="line">        :type j: int</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return sum(self.nums[i:j+1])</div><div class="line">        &apos;&apos;&apos;</div><div class="line">&apos;&apos;&apos;</div><div class="line">Use self.bit to represent Binary Indexed Tree. Section sums are stored in self.c[1..len(nums)]. x &amp; -x is lowbit function, which will return x&apos;s rightmost bit 1, e.g. lowbit(7) = 1, lowbit(20) = 4.</div><div class="line"></div><div class="line">Both update and sumRange takes O(logn) time.</div><div class="line">See http://www.cnblogs.com/grandyang/p/4985506.html</div><div class="line">&apos;&apos;&apos;</div><div class="line">class NumArray(object):</div><div class="line">    def __init__(self, nums):</div><div class="line">        self.n = len(nums)</div><div class="line">        self.nums, self.bit = [0] * (self.n + 1), [0] * (self.n + 1)</div><div class="line">        for i,n in enumerate(nums):</div><div class="line">            self.update(i,n)</div><div class="line"></div><div class="line">    def update(self, i, val):</div><div class="line">        diff, self.nums[i] = val - self.nums[i], val</div><div class="line">        i += 1</div><div class="line">        while i &lt;= self.n:</div><div class="line">            self.bit[i] += diff</div><div class="line">            i += (i &amp; -i)</div><div class="line"></div><div class="line">    def sumRange(self, i, j):</div><div class="line">        res, j = 0, j + 1</div><div class="line">        while j:</div><div class="line">            res += self.bit[j]</div><div class="line">            j -= (j &amp; -j)</div><div class="line">        while i:</div><div class="line">            res -= self.bit[i]</div><div class="line">            i -= (i &amp; -i)</div><div class="line">        return res</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 树 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Query Processing]]></title>
      <url>http://www.shuang0420.com/2016/09/11/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Query%20Processing/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。搜索引擎是怎么处理 query 的？三种方法，Term-at-a-time(TAAT)，Document-at-a-time(DAAT)，TAAT/DAAT hybrids。<br><a id="more"></a></p>
<h1 id="TAAT"><a href="#TAAT" class="headerlink" title="TAAT"></a>TAAT</h1><p>主要思路：</p>
<ul>
<li>处理完一个 inverted list 再处理下一个。</li>
<li>每处理完一个 inverted list，部分更新 document score。</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>易于理解</li>
<li>高效</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>难以控制内存<br>每个 operator 都会同时在内存里存 3 个 list(arg1,arg2,result)<br>每个深度为 d 的 query 都会同时在内存里存 d+2 个 list。</li>
<li>可能会 run out of memory<br>包含有 frequent term 的 query (很长的 inverted list)<br>复杂的 query (更多的 inverted list)<br>同时处理多个 query 的系统</li>
</ul>
<p>所以 TAAT 很少用在 large-scale systems。</p>
<p>Eg.#AND(a b #OR (c #NEAR/3(d e)) f)<br>转化成 query tree<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">          AND</div><div class="line">(a   b     OR            f)</div><div class="line">         (c  NEAR/3</div><div class="line">              (d  e))</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Retrieve a</div><div class="line">Retrieve b</div><div class="line">a AND b -&gt; Result(AND_1)</div><div class="line">Retrieve c</div><div class="line">Retrieve d</div><div class="line">Retrieve e</div><div class="line">d NEAR/3 e -&gt; Result(NEAR)</div><div class="line">c OR Result(NEAR) -&gt; Result(OR)</div><div class="line">Result(AND_1) AND Result(OR) -&gt; Result(AND_2)</div><div class="line">Retrieve f</div><div class="line">Result(AND_2) AND f -&gt; Result(Q)</div></pre></td></tr></table></figure>
<p><strong>Memory usage</strong><br>内存中同时存在 5 个 list<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">size(a AND b) +</div><div class="line">size(c) +</div><div class="line">size(d) + size(e) + size(d NEAR/3 e) bytes</div></pre></td></tr></table></figure></p>
<h1 id="DAAT"><a href="#DAAT" class="headerlink" title="DAAT"></a>DAAT</h1><p>主要思路：</p>
<ul>
<li>处理完一篇文档后，再处理下一篇文档。</li>
<li>每处理一篇文档，就算出 complete score</li>
</ul>
<p>找到所有 term 的 inverted list，每个 inverted list 分配一个 iterator，分配一个空的 result list。之后找到每个 inverted list 当前的 doc id，取最小的 doc id，算出当前分数，保存到 result list 中，然后把这个 iterator 往下移一个 doc id，重复这个过程。</p>
<p>简化一下，主要就重复两件事：</p>
<ul>
<li>update the score</li>
<li>advance the pointer</li>
</ul>
<p>代码描述<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">q.initialize()</div><div class="line">while (q.hasNext())</div><div class="line">  q.evalNext() returns next [docid,score] tuple</div></pre></td></tr></table></figure></p>
<p>优点：</p>
<ul>
<li>易于进行内存管理<br>需要同时 access 所有 args 的 inverted list (seems bad)，然而，这些 inverted list 可以以 block 的形式分批从 disk 读进 RAM。等当前 block 处理完了再读下一个 block，这样处理一个 query 所需的内存就取决于 block 的大小。</li>
<li>可以进行很多 query evaluation 的优化<br>低分文档只进行 partial evaluation</li>
</ul>
<p>所以 TAAT 经常用在 large-scale systems。</p>
<h1 id="TAAT-DAAT-hybrids"><a href="#TAAT-DAAT-hybrids" class="headerlink" title="TAAT/DAAT hybrids"></a>TAAT/DAAT hybrids</h1><p>平衡 Efficiency 和 memory control。Eg. block-based TAAT(compute TAAT over blocks of document ids)</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 栈和队列]]></title>
      <url>http://www.shuang0420.com/2016/09/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/</url>
      <content type="html"><![CDATA[<h1 id="Stack-implementation"><a href="#Stack-implementation" class="headerlink" title="Stack implementation"></a>Stack implementation</h1><p>实现一个 stack 可以用两种数据结构，array(dynamic or fixed) 或者是 linked list。</p>
<p><strong>dynamic array</strong> 的优势是支持 random access，因为可以通过 index 获取数据，然而 stack 主要作用是 pop，所以 dynamic array 的这个优势 gains you little。dynamic array 另一个优势是 resize，这个非常的 time-consuming 因为需要 copy array to a new one。</p>
<p><strong>linked list</strong> 会为每一个元素分配内存，这比 dynamic array 的 resize 更费时，因此基于 dynamic array 的 stack 通常要比基于 linked list 的 stack 快一些。然而，基于 linked list 的 stack 更容易实现。</p>
<p><strong>Proper functionality</strong><br>基本方法：</p>
<ul>
<li>push<br>allocate new element, checks for failure, sets the data of the new element, places it at the top of the stack, adjust the stack pointer</li>
<li>pop<br>check the stack isn’t empty, fetches data from top element, adjusts the stack pointer, free the element that is no longer on the stack</li>
</ul>
<p>完整方法：</p>
<ul>
<li>createStack<br>push a null pointer</li>
<li>deleteStack<br>call pop repeatedly</li>
</ul>
<p><strong>Error handling</strong></p>
<ul>
<li>pop<br>如果 stack 为空，返回 null? 问题是需要保证 stack 里没有存 null pointer；返回 special value(or negative value)？问题是需要 assume stack 里没有这些元素。感觉 raise error 比较简单。</li>
<li>push<br>如果传进去的值为 null，raise error</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"># implement a stack using linkedlist</div><div class="line">class Stack(object):</div><div class="line"></div><div class="line">    class Node(object):</div><div class="line"></div><div class="line">        def __init__(self, val=None, next=None):</div><div class="line">            self.val = val</div><div class="line">            self.next = next</div><div class="line"></div><div class="line">    def __init__(self):</div><div class="line">        self.head = None</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    check the stack isn&apos;t empty, fetches data from top element, adjusts the stack pointer, free the element that is no longer on the stack</div><div class="line">    &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        if not self.head: raise ValueError(&quot;Empty stack!&quot;)</div><div class="line">        val = self.head.val</div><div class="line">        self.head = self.head.next</div><div class="line">        return val</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    allocate new element, checks for failure, sets the data of the new element, places it at the top of the stack, adjust the stack pointer</div><div class="line">    &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def push(self, val):</div><div class="line">        if not val:</div><div class="line">            raise ValueError(&quot;Invalid value!&quot;)</div><div class="line">        node = self.Node(val, self.head)</div><div class="line">        self.head = node</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">push a null pointer</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">def createStack(stack):</div><div class="line">    stack.head = Node()</div><div class="line">    return True</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">call pop repeatedly</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">def deleteStack(stack):</div><div class="line">    while stack.head:</div><div class="line">        stack.pop()</div><div class="line">    return True</div><div class="line"></div><div class="line">stack=Stack()</div><div class="line">stack.push(1)</div><div class="line">stack.push(2)</div><div class="line">stack.push(3)</div><div class="line">print stack.pop()</div><div class="line">print stack.pop()</div><div class="line">print stack.pop()</div><div class="line">print stack.pop()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"># implement a stack using fixed-size array</div><div class="line">class Stack(object):</div><div class="line"></div><div class="line">    def __init__(self, capacity=None):</div><div class="line">        if capacity:</div><div class="line">            self.elements = [None] * capacity</div><div class="line">        else:</div><div class="line">            self.elements = [None] * 10</div><div class="line">        # set the top to be -1, indicating the stack is empty</div><div class="line">        self.top = -1</div><div class="line"></div><div class="line">    def isEmpty(self):</div><div class="line">        return self.top == -1</div><div class="line"></div><div class="line">    def push(self, item):</div><div class="line">        if self.top == len(self.elements):</div><div class="line">            raise ValueError(&quot;Full stack!&quot;)</div><div class="line">        self.top += 1</div><div class="line">        self.elements[self.top] = item</div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        if self.isEmpty():</div><div class="line">            raise ValueError(&quot;Empty stack!&quot;)</div><div class="line">        # get the element on the top</div><div class="line">        item = self.elements[self.top]</div><div class="line">        self.elements[self.top] = None</div><div class="line">        # reduce the top variable</div><div class="line">        self.top -= 1</div><div class="line">        return item</div><div class="line"></div><div class="line">    def peek(self):</div><div class="line">        if self.isEmpty():</div><div class="line">            raise ValueError(&quot;Empty stack!&quot;)</div><div class="line">        return self.elements[self.top]</div></pre></td></tr></table></figure>
<h1 id="Queue-implementation"><a href="#Queue-implementation" class="headerlink" title="Queue implementation"></a>Queue implementation</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"># implement a queue using fixed-size array</div><div class="line">class Queue(object):</div><div class="line"></div><div class="line">    def __init__(self, capacity=None):</div><div class="line">        if capacity:</div><div class="line">            self.elements = [None] * capacity</div><div class="line">        else:</div><div class="line">            self.elements = [None] * 5</div><div class="line">        self.front = 0</div><div class="line">        self.back = -1</div><div class="line">        self.nItems = 0</div><div class="line"></div><div class="line">    def enqueue(self, item):</div><div class="line">        if self.isFull():</div><div class="line">            raise ValueError(&quot;Queue is full&quot;)</div><div class="line">        self.back += 1</div><div class="line">        index = self.back % len(self.elements)</div><div class="line">        self.elements[index] = item</div><div class="line">        self.nItems += 1</div><div class="line"></div><div class="line">    def dequeue(self):</div><div class="line">        if self.isEmpty():</div><div class="line">            raise ValueError(&quot;Queue is empty&quot;)</div><div class="line">        index = self.front % len(self.elements)</div><div class="line">        result = self.elements[index]</div><div class="line">        self.elements[index] = None</div><div class="line">        self.front = index + 1</div><div class="line">        self.nItems -= 1</div><div class="line">        return result</div><div class="line"></div><div class="line">    def peekFront(self):</div><div class="line">        if self.isEmpty():</div><div class="line">            raise ValueError(&quot;Queue is empty&quot;)</div><div class="line">        return self.elements[self.front % len(self.elements)]</div><div class="line"></div><div class="line">    def isEmpty(self):</div><div class="line">        return self.nItems == 0</div><div class="line"></div><div class="line">    def isFull(self):</div><div class="line">        return self.nItems == len(self.elements)</div><div class="line"></div><div class="line"></div><div class="line">q=Queue()</div><div class="line">q.enqueue(4)</div><div class="line">q.enqueue(5)</div><div class="line">q.enqueue(6)</div><div class="line">print q.elements</div><div class="line">q.dequeue()</div><div class="line">print q.elements</div><div class="line">q.dequeue()</div><div class="line">print q.elements</div><div class="line">q.enqueue(10)</div><div class="line">q.enqueue(11)</div><div class="line">q.enqueue(12)</div><div class="line">print q.elements</div></pre></td></tr></table></figure>
<h1 id="Leetcode-实例"><a href="#Leetcode-实例" class="headerlink" title="Leetcode 实例"></a>Leetcode 实例</h1><h2 id="232-Implement-Queue-using-Stacks"><a href="#232-Implement-Queue-using-Stacks" class="headerlink" title="232.Implement Queue using Stacks"></a>232.Implement Queue using Stacks</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Implement the following operations of a queue using stacks.<br>push(x) – Push element x to the back of queue.<br>pop() – Removes the element from in front of queue.<br>peek() – Get the front element.<br>empty() – Return whether the queue is empty.<br>Notes:<br>You must use only standard operations of a stack – which means only push to top, peek/pop from top, size, and is empty operations are valid.<br>Depending on your language, stack may not be supported natively. You may simulate a stack by using a list or deque (double-ended queue), as long as you use only standard operations of a stack.<br>You may assume that all operations are valid (for example, no pop or peek operations will be called on an empty queue).</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">class Queue(object):</div><div class="line">    def __init__(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.stack1=list()</div><div class="line">        self.stack2=list()</div><div class="line"></div><div class="line">    def push(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: nothing</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.stack1.append(x)</div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: nothing</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.helper()</div><div class="line">        return self.stack2.pop()</div><div class="line"></div><div class="line">    def peek(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.helper()</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        element=self.stack2.pop()</div><div class="line">        self.stack2.append(element)</div><div class="line">        return element</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        return self.stack2[-1]</div><div class="line"></div><div class="line">    def empty(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if self.stack1 or self.stack2:</div><div class="line">            return False</div><div class="line">        return True</div><div class="line"></div><div class="line">    def helper(self):</div><div class="line">        if not self.stack2:</div><div class="line">            while self.stack1:</div><div class="line">                self.stack2.append(self.stack1.pop())</div></pre></td></tr></table></figure>
<h2 id="225-Implement-Stack-using-Queues"><a href="#225-Implement-Stack-using-Queues" class="headerlink" title="225.Implement Stack using Queues"></a>225.Implement Stack using Queues</h2><h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Implement the following operations of a stack using queues.<br>push(x) – Push element x onto stack.<br>pop() – Removes the element on top of the stack.<br>top() – Get the top element.<br>empty() – Return whether the stack is empty.<br>Notes:<br>You must use only standard operations of a queue – which means only push to back, peek/pop from front, size, and is empty operations are valid.<br>Depending on your language, queue may not be supported natively. You may simulate a queue by using a list or deque (double-ended queue), as long as you use only standard operations of a queue.<br>You may assume that all operations are valid (for example, no pop or top operations will be called on an empty stack).<br>Update (2015-06-11):<br>The class name of the Java function had been updated to MyStack instead of Stack.</p>
<p>用两个队列，push: O(n)，pop: O(1)，top: O(1)</p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">from collections import deque</div><div class="line">class Stack(object):</div><div class="line">    def __init__(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.queue1=deque()</div><div class="line">        self.queue2=deque()</div><div class="line"></div><div class="line">    def push(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: nothing</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.queue2:</div><div class="line">            self.queue2.append(x)</div><div class="line">            while self.queue1:</div><div class="line">                self.queue2.append(self.queue1.popleft())</div><div class="line">            self.queue1,self.queue2=self.queue2,self.queue1</div><div class="line"></div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: nothing</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.queue1.popleft()</div><div class="line"></div><div class="line">    def top(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return self.queue1[0]</div><div class="line"></div><div class="line"></div><div class="line">    def empty(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.queue1:</div><div class="line">            return True</div><div class="line">        return False</div></pre></td></tr></table></figure>
<h2 id="155-Min-Stack"><a href="#155-Min-Stack" class="headerlink" title="155. Min Stack"></a>155. Min Stack</h2><h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h3><p>Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.</p>
<p>push(x) – Push element x onto stack.<br>pop() – Removes the element on top of the stack.<br>top() – Get the top element.<br>getMin() – Retrieve the minimum element in the stack.<br>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">MinStack minStack = new MinStack();</div><div class="line">minStack.push(-2);</div><div class="line">minStack.push(0);</div><div class="line">minStack.push(-3);</div><div class="line">minStack.getMin();   --&gt; Returns -3.</div><div class="line">minStack.pop();</div><div class="line">minStack.top();      --&gt; Returns 0.</div><div class="line">minStack.getMin();   --&gt; Returns -2.</div></pre></td></tr></table></figure></p>
<h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">    Primitive idea would be to use a global varible to record current minimum value when push, but when pop value, the minimum value may change. One solution is when pop we empty the list and redo the push action for n-1 values but thus it takes O(n) time.</div><div class="line"></div><div class="line">Followup:</div><div class="line">    It would be perfect if we can keep track of the minmum value at each &apos;timestamp&apos;. We can use tuple to record current minimum value for each value when push a new element.</div><div class="line">&apos;&apos;&apos;</div><div class="line">&apos;&apos;&apos;</div><div class="line">class MinStack(object):</div><div class="line"></div><div class="line">    def __init__(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.stack=list()</div><div class="line">        self.min=float(&apos;inf&apos;)</div><div class="line"></div><div class="line">    def push(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: void</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.stack.append(x)</div><div class="line">        self.min=min(self.min,x)</div><div class="line"></div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: void</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.stack:</div><div class="line">            return None</div><div class="line">        if self.top()==self.min:</div><div class="line">            cur=self.stack[::]</div><div class="line">            self.stack=[]</div><div class="line">            self.min=float(&apos;inf&apos;)</div><div class="line">            for i in range(len(cur)-1):</div><div class="line">                self.push(cur[i])</div><div class="line">        else:</div><div class="line">            del self.stack[-1]</div><div class="line"></div><div class="line">    def top(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.stack: return None</div><div class="line">        return self.stack[-1]</div><div class="line"></div><div class="line"></div><div class="line">    def getMin(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return self.min</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">class MinStack(object):</div><div class="line"></div><div class="line">    def __init__(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        initialize your data structure here.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        self.stack=list()</div><div class="line"></div><div class="line">    def push(self, x):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type x: int</div><div class="line">        :rtype: void</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.stack:</div><div class="line">            self.stack.append((x,x))</div><div class="line">            return</div><div class="line">        self.stack.append((x,min(x,self.getMin())))</div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: void</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.stack:</div><div class="line">            return None</div><div class="line">        return self.stack.pop()[0]</div><div class="line"></div><div class="line">    def top(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not self.stack: return None</div><div class="line">        return self.stack[-1][0]</div><div class="line"></div><div class="line"></div><div class="line">    def getMin(self):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return self.stack[-1][-1]</div><div class="line"></div><div class="line"># Your MinStack object will be instantiated and called as such:</div><div class="line"># obj = MinStack()</div><div class="line"># obj.push(x)</div><div class="line"># obj.pop()</div><div class="line"># param_3 = obj.top()</div><div class="line"># param_4 = obj.getMin()</div></pre></td></tr></table></figure>
<h2 id="20-Valid-Parentheses"><a href="#20-Valid-Parentheses" class="headerlink" title="20.Valid Parentheses"></a>20.Valid Parentheses</h2><h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem"></a>Problem</h3><p>Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid.</p>
<p>The brackets must close in the correct order, “()” and “()[]{}” are all valid but “(]” and “([)]” are not.</p>
<h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- check when right meet; just need the last unpaired left =&gt; first in,last out =&gt; stack</div><div class="line">- create a dictionary for parenthese pairs, for each element in s, if it exists in dictionary.keys(), then append it into the stack, else, pop from the stack and check if the popped value and current element is a pair. Time complexity: O(n)</div><div class="line">- remember to check if stack is empty in every check and also in final check (look back for previous left parentheses)</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def isValid(self, s):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type s: str</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not s:</div><div class="line">            return None</div><div class="line">        dictionary=&#123;&apos;(&apos;:&apos;)&apos;,&apos;[&apos;:&apos;]&apos;,&apos;&#123;&apos;:&apos;&#125;&apos;&#125;</div><div class="line">        s=list(s)</div><div class="line">        stack=[]</div><div class="line">        for i in s:</div><div class="line">            if i in dictionary:</div><div class="line">                stack.append(i)</div><div class="line">            else:</div><div class="line">                if not stack or dictionary[stack.pop()]!=i:</div><div class="line">                    return False</div><div class="line">        return not stack</div></pre></td></tr></table></figure>
<h2 id="150-Evaluate-Reverse-Polish-Notation"><a href="#150-Evaluate-Reverse-Polish-Notation" class="headerlink" title="150. Evaluate Reverse Polish Notation"></a>150. Evaluate Reverse Polish Notation</h2><h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem"></a>Problem</h3><p>Evaluate the value of an arithmetic expression in Reverse Polish Notation.</p>
<p>Valid operators are +, -, *, /. Each operand may be an integer or another expression.</p>
<p>Some examples:<br>  [“2”, “1”, “+”, “3”, “*“] -&gt; ((2 + 1) * 3) -&gt; 9<br>  [“4”, “13”, “5”, “/“, “+”] -&gt; (4 + (13 / 5)) -&gt; 6</p>
<h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- each operation requires two operands and one operator, operator always appear after operands, so we search element from left to right, store numbers in the stack till we meet up with an operator, and with the operator, we pop two elements from the stack and caculate the results and push it back to stack, and continue, till the end of tokens, finally return the final value of the stack.</div><div class="line"></div><div class="line">Attention(negative integer division):</div><div class="line">- division in python, pls consider when one of the operand is negative, you would get surprising result. eg. -7/2=-4.</div><div class="line">    in order to avoid that, use int(float(a)/b) whenever there&apos;s a division operation</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def evalRPN(self, tokens):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type tokens: List[str]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        stack=[]</div><div class="line">        for t in tokens:</div><div class="line">            if t==&apos;+&apos;:</div><div class="line">                stack.append(stack.pop()+stack.pop())</div><div class="line">            elif t==&apos;-&apos;:</div><div class="line">                a=stack.pop()</div><div class="line">                b=stack.pop()</div><div class="line">                stack.append(b-a)</div><div class="line">            elif t==&apos;*&apos;:</div><div class="line">                stack.append(stack.pop()*stack.pop())</div><div class="line">            elif t==&apos;/&apos;:</div><div class="line">                a=stack.pop()</div><div class="line">                b=stack.pop()</div><div class="line">                stack.append(int(float(b)/a))</div><div class="line">            else:</div><div class="line">                stack.append(int(t))</div><div class="line">        return stack.pop()</div></pre></td></tr></table></figure>
<p>Followup<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Followup:</div><div class="line">- infix notation</div><div class="line">    ( =&gt; operator stack</div><div class="line">    number =&gt; number stack</div><div class="line">    ) =&gt; pop and calculate till a &apos;(&apos; is met</div><div class="line">    +,- =&gt; higher precedence met-&gt; push into the operator stack, lower precedence met-&gt; calculate higher operator in stack first and then push</div><div class="line">- test case:</div><div class="line">  input: [&quot;(&quot;,&quot;(&quot;,&quot;3&quot;,&quot;+&quot;,&quot;4&quot;,&quot;)&quot;,&quot;*&quot;,&quot;(&quot;,&quot;4&quot;,&quot;+&quot;,&quot;1&quot;,&quot;)&quot;,&quot;-&quot;,&quot;4&quot;,&quot;*&quot;,&quot;2&quot;,&quot;)&quot;,&quot;+&quot;,&quot;1&quot;]</div><div class="line">  output: 28</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def evalRPN(self,tokens):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type tokens: List[str]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        op_stack=[]</div><div class="line">        num_stack=[]</div><div class="line">        for i in tokens:</div><div class="line">            # case &apos;(&apos;</div><div class="line">            if i==&apos;(&apos;:</div><div class="line">                op_stack.append(i)</div><div class="line">            # case &apos;)&apos;</div><div class="line">            elif i==&apos;)&apos;:</div><div class="line">                while op_stack[-1]!=&apos;(&apos;:</div><div class="line">                    num_stack.append(self.cal(op_stack.pop(),num_stack.pop(),num_stack.pop()))</div><div class="line">                op_stack.pop()</div><div class="line">            # case &apos;+&apos;,&apos;-&apos;,&apos;*&apos;,&apos;/&apos;</div><div class="line">            elif i==&apos;+&apos; or i==&apos;-&apos; or i==&apos;*&apos; or i==&apos;/&apos;:</div><div class="line">                while op_stack and self.isLowerThan(i,op_stack[-1]):</div><div class="line">                    num_stack.append(self.cal(op_stack.pop(),num_stack.pop(),num_stack.pop()))</div><div class="line">                op_stack.append(i)</div><div class="line">            # case number</div><div class="line">            else:</div><div class="line">                num_stack.append(int(i))</div><div class="line">        while op_stack:</div><div class="line">            num_stack.append(self.cal(op_stack.pop(),num_stack.pop(),num_stack.pop()))</div><div class="line">        return num_stack.pop()</div><div class="line"></div><div class="line"></div><div class="line">    def cal(self,op,num1,num2):</div><div class="line">        if op==&apos;+&apos;:</div><div class="line">            return num1+num2</div><div class="line">        if op==&apos;-&apos;:</div><div class="line">            return num2-num1</div><div class="line">        if op==&apos;*&apos;:</div><div class="line">            return num1*num2</div><div class="line">        if op==&apos;/&apos;:</div><div class="line">            return int(float(num2)/num1)</div><div class="line">        raise ValueError(&quot;Not valid operator&quot;)</div><div class="line"></div><div class="line">    def isLowerThan(self,op1,op2):</div><div class="line">        if (op1==&apos;+&apos; or op1==&apos;-&apos;) and (op2==&apos;*&apos; or op2==&apos;/&apos;):</div><div class="line">            return True</div><div class="line">        return False</div></pre></td></tr></table></figure></p>
<h2 id="71-Simplify-Path"><a href="#71-Simplify-Path" class="headerlink" title="71. Simplify Path"></a>71. Simplify Path</h2><h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem"></a>Problem</h3><p>Given an absolute path for a file (Unix-style), simplify it.</p>
<p>For example,<br>path = “/home/“, =&gt; “/home”<br>path = “/a/./b/../../c/“, =&gt; “/c”<br>click to show corner cases.</p>
<p>Corner Cases:<br>Did you consider the case where path = “/../“?<br>In this case, you should return “/“.<br>Another corner case is the path might contain multiple slashes ‘/‘ together, such as “/home//foo/“.<br>In this case, you should ignore redundant slashes and return “/home/foo”.</p>
<h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- ignore &apos;.&apos;, when met &apos;..&apos;, pop from stack if stack is not empty, and finally join the stack</div><div class="line"></div><div class="line">Attention:</div><div class="line">- corner case: /../a =&gt; /a</div><div class="line">- always remember if you wanna pop from a stack check if it is empty first</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def simplifyPath(self, path):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type path: str</div><div class="line">        :rtype: str</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        vals=path.split(&apos;/&apos;)</div><div class="line">        stack=[]</div><div class="line">        for v in vals:</div><div class="line">            if v==&apos;&apos; or v==&apos;.&apos;: continue</div><div class="line">            if v==&apos;..&apos;:</div><div class="line">                if stack: stack.pop()</div><div class="line">            else: stack.append(v)</div><div class="line">        return &apos;/&apos;+&apos;/&apos;.join(stack)</div></pre></td></tr></table></figure>
<h2 id="84-Largest-Rectangle-in-Histogram"><a href="#84-Largest-Rectangle-in-Histogram" class="headerlink" title="84. Largest Rectangle in Histogram"></a>84. Largest Rectangle in Histogram</h2><h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem"></a>Problem</h3><p>Given n non-negative integers representing the histogram’s bar height where the width of each bar is 1, find the area of largest rectangle in the histogram.</p>
<p>Above is a histogram where width of each bar is 1, given height = [2,1,5,6,2,3].</p>
<p>The largest rectangle is shown in the shaded area, which has area = 10 unit.</p>
<p>For example,<br>Given heights = [2,1,5,6,2,3],<br>return 10.</p>
<h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- primitive idea: find all rectangles and get max one, time complexity O(n^2)</div><div class="line"></div><div class="line">Followup:</div><div class="line">- O(n)?</div><div class="line">    - avoid repeated work</div><div class="line">    - identify a rectangle: identify 2 boundaries</div><div class="line">    - if cur&gt;stack.peek() --&gt; offer, else --&gt; continously poll</div><div class="line"></div><div class="line">Corner case: [0]</div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def largestRectangleArea(self, heights):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type heights: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not heights: return 0</div><div class="line">        stack=[]</div><div class="line">        max_area=0</div><div class="line">        for i in range(len(heights)+1):</div><div class="line">            cur=0 if i==len(heights) else heights[i]</div><div class="line">            while stack and cur&lt;=heights[stack[-1]]:</div><div class="line">                height=heights[stack.pop()]</div><div class="line">                leftBound=0 if not stack else stack[-1]+1</div><div class="line">                rightBound=i</div><div class="line">                cur_area=(rightBound-leftBound)*height</div><div class="line">                max_area=max(cur_area,max_area)</div><div class="line">            stack.append(i)</div><div class="line">        return max_area</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # primitive, 2 loops</div><div class="line">    def largestRectangleArea(self, heights):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type heights: List[int]</div><div class="line">        :rtype: int</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not heights: return 0</div><div class="line">        max_area=0</div><div class="line">        for i in range(len(heights)):</div><div class="line">            max_area=max(heights[i],max_area)</div><div class="line">            min_height=heights[i]</div><div class="line">            for j in range(i,len(heights)):</div><div class="line">                min_height=min(min_height,heights[j])</div><div class="line">                max_area=max(min_height*(j-i+1),max_area)</div><div class="line">        return max_area</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h1 id="Python-stack-amp-deque"><a href="#Python-stack-amp-deque" class="headerlink" title="Python stack &amp; deque"></a>Python stack &amp; deque</h1><p>这篇用到的 python 的知识点/需要注意的地方。</p>
<h2 id="use-lists-as-stacks"><a href="#use-lists-as-stacks" class="headerlink" title="use lists as stacks"></a>use lists as stacks</h2><p>LIFO<br>add, use append()<br>retrieve, use pop()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; stack = [3, 4, 5]</div><div class="line">&gt;&gt;&gt; stack.append(6)</div><div class="line">&gt;&gt;&gt; stack.append(7)</div><div class="line">&gt;&gt;&gt; stack</div><div class="line">[3, 4, 5, 6, 7]</div><div class="line">&gt;&gt;&gt; stack.pop()</div><div class="line">7</div><div class="line">&gt;&gt;&gt; stack</div><div class="line">[3, 4, 5, 6]</div><div class="line">&gt;&gt;&gt; stack.pop()</div><div class="line">6</div><div class="line">&gt;&gt;&gt; stack.pop()</div><div class="line">5</div><div class="line">&gt;&gt;&gt; stack</div><div class="line">[3, 4]</div></pre></td></tr></table></figure></p>
<h2 id="use-lists-as-queues"><a href="#use-lists-as-queues" class="headerlink" title="use lists as queues"></a>use lists as queues</h2><p>FIFO，python list 作 queue 并不 efficent，用 collections.deque<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from collections import deque</div><div class="line">&gt;&gt;&gt; queue = deque([&quot;Eric&quot;, &quot;John&quot;, &quot;Michael&quot;])</div><div class="line">&gt;&gt;&gt; queue.append(&quot;Terry&quot;)           # Terry arrives</div><div class="line">&gt;&gt;&gt; queue.append(&quot;Graham&quot;)          # Graham arrives</div><div class="line">&gt;&gt;&gt; queue.popleft()                 # The first to arrive now leaves</div><div class="line">&apos;Eric&apos;</div><div class="line">&gt;&gt;&gt; queue.pop()                 # The second to arrive now leaves</div><div class="line">&apos;Graham&apos;</div><div class="line">&gt;&gt;&gt; queue                           # Remaining queue in order of arrival</div><div class="line">deque([&apos;John&apos;, &apos;Michael&apos;, &apos;Terry&apos;])</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 栈 </tag>
            
            <tag> 队列 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 排序算法]]></title>
      <url>http://www.shuang0420.com/2016/09/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>往往排序是作为其他算法的预处理算法，其重要性却不容小觑。本篇讲冒泡排序／选择排序／插入排序／希尔排序／归并排序／快速排序／桶排序／计数排序。<br><a id="more"></a></p>
<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><table>
<thead>
<tr>
<th>algorithm</th>
<th>in-place</th>
<th>worst</th>
<th>average</th>
<th>best</th>
<th>space complexity</th>
<th>remark</th>
</tr>
</thead>
<tbody>
<tr>
<td>bubble</td>
<td>yes</td>
<td>$N^2$</td>
<td>$N^2$</td>
<td>N</td>
<td>$O(1)$</td>
<td>——</td>
</tr>
<tr>
<td>selection</td>
<td>yes</td>
<td>$N^2$</td>
<td>$N^2$</td>
<td>$N^2$</td>
<td>$O(1)$</td>
<td>——</td>
</tr>
<tr>
<td>insertion</td>
<td>yes</td>
<td>$N^2$</td>
<td>$N^2$</td>
<td>N</td>
<td>$O(1)$</td>
<td>——</td>
</tr>
<tr>
<td>shell</td>
<td>yes</td>
<td>—–</td>
<td>——-</td>
<td>N</td>
<td>$O(1)$</td>
<td>——</td>
</tr>
<tr>
<td>merge</td>
<td></td>
<td>$NlogN$</td>
<td>$NlogN$</td>
<td>$NlogN$</td>
<td>$O(N)$</td>
<td>——</td>
</tr>
<tr>
<td>quick</td>
<td>yes</td>
<td>$N^2$</td>
<td>$NlogN$</td>
<td>$NlogN$</td>
<td>$O(logN)$</td>
<td>——</td>
</tr>
<tr>
<td>heap</td>
<td>yes</td>
<td>$NlogN$</td>
<td>$NlogN$</td>
<td>$NlogN$</td>
<td>$O(1)$</td>
<td>——</td>
</tr>
</tbody>
</table>
<h1 id="Bubble-sort-冒泡排序"><a href="#Bubble-sort-冒泡排序" class="headerlink" title="Bubble sort 冒泡排序"></a>Bubble sort 冒泡排序</h1><p>冒泡排序的原理非常简单，重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。<br>内部稳定排序，时间复杂度 $O(N^2)$，空间复杂度 $O(1)$</p>
<p>步骤：</p>
<ol>
<li>比较相邻的元素。如果前一个比后一个大，就交换他们两个。</li>
<li>对第 0 个到第 n-1 个数据做同样的工作。这时，最大的数就“浮”到了数组最后的位置上。</li>
<li>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</li>
</ol>
<p>每轮操作都将一个最大的数“浮”到最后的位置，也就是每轮都有最后 N-i 个数已经排序，所以内循环是从 1 到 N-i。</p>
<h2 id="基本款"><a href="#基本款" class="headerlink" title="基本款"></a>基本款</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line"># Time Complexity of Solution:</div><div class="line">#  Best O(n^2); Average O(n^2); Worst O(n^2).</div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#   Bubblesort is an elementarray sorting algorithm. The idea is to</div><div class="line">#   imagine bubbling the smallest elements of a (vertical) array to the</div><div class="line">#   top; then bubble the next smallest; then so on until the entire</div><div class="line">#   array is sorted. Bubble sort is worse than both insertion sort and</div><div class="line">#   selection sort. It moves elements as many times as insertion sort</div><div class="line">#   (bad) and it takes as long as selection sort (bad). On the positive</div><div class="line">#   side, bubble sort is easy to understand. Also there are highly</div><div class="line">#   improved variants of bubble sort.</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def bubble_sort(array):</div><div class="line">    n = len(array)</div><div class="line">    for i in range(n):</div><div class="line">        for j in range(1, n - i):</div><div class="line">            if array[j - 1] &gt; array[j]:</div><div class="line">                array[j - 1], array[j] = array[j], array[j - 1]</div><div class="line">    return array</div></pre></td></tr></table></figure>
<h2 id="优化一"><a href="#优化一" class="headerlink" title="优化一"></a>优化一</h2><p>优化1：某一趟遍历如果没有数据交换，则说明已经排好序了，因此不用再进行迭代了。用一个标记记录这个状态即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def bubble_sort2(array):</div><div class="line">    n = len(array)</div><div class="line">    for i in range(n):</div><div class="line">        sorted = True</div><div class="line">        for j in range(1, n - i):</div><div class="line">            if array[j - 1] &gt; array[j]:</div><div class="line">                flag = False</div><div class="line">                array[j - 1], array[j] = array[j], array[j - 1]</div><div class="line">        if sorted:</div><div class="line">            return array  # or break</div><div class="line">    return array</div></pre></td></tr></table></figure></p>
<h2 id="优化二"><a href="#优化二" class="headerlink" title="优化二"></a>优化二</h2><p>优化2：记录某次遍历时最后发生数据交换的位置，这个位置之后的数据显然已经有序，不用再排序了。因此通过记录最后发生数据交换的位置就可以确定下次循环的范围了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def bubble_sort3(array):</div><div class="line">    n = len(array)</div><div class="line">    k = n</div><div class="line">    for i in range(n):</div><div class="line">        flag = False  # 有没有交换</div><div class="line">        for j in range(1, k):  # 只遍历到最后交换的位置</div><div class="line">            if array[j - 1] &gt; array[j]:</div><div class="line">                flag = True</div><div class="line">                k = j  # 记录最后的交换位置</div><div class="line">                array[j - 1], array[j] = array[j], array[j - 1]</div><div class="line">        if not flag:</div><div class="line">            return array  # or break</div><div class="line">    return array</div></pre></td></tr></table></figure></p>
<h1 id="Selection-sort-选择排序"><a href="#Selection-sort-选择排序" class="headerlink" title="Selection sort 选择排序"></a>Selection sort 选择排序</h1><p>选择排序无疑是最简单直观的排序。<br>内部不稳定排序，时间复杂度 $O(N^2)$，空间复杂度 $O(1)$</p>
<p>步骤：</p>
<ol>
<li>在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。</li>
<li>再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。<br>以此类推，直到所有元素均排序完毕。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best O(n^2); Average O(n^2); Worst O(n^2).</div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#  Selection sort is a step up from insertion sort from a memory</div><div class="line">#  viewpoint. It only swaps elements that need to be swapped. In terms</div><div class="line">#  of time complexity, however, insertion sort is better.</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def select_sort(array):</div><div class="line">    n = len(array)</div><div class="line">    for i in range(n):</div><div class="line">        min = i</div><div class="line">        for j in range(i + 1, n):</div><div class="line">            if array[j] &lt; array[min]:</div><div class="line">                min = j</div><div class="line">        array[i], array[min] = array[min], array[i]</div><div class="line">    return array</div></pre></td></tr></table></figure>
<h1 id="Insertion-sort-插入排序"><a href="#Insertion-sort-插入排序" class="headerlink" title="Insertion sort 插入排序"></a>Insertion sort 插入排序</h1><p>每轮在已经排好的序列中插入一个新的数字。插入排序的工作原理是，对于每个未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。<br>内部稳定排序，时间复杂度 $O(N^2)$，空间复杂度 $O(1)$<br>步骤：</p>
<ol>
<li>从第一个元素开始，该元素可以认为已经被排序</li>
<li>取出下一个元素，在已经排序的元素序列中从后向前扫描</li>
<li>如果被扫描的元素（已排序）大于新元素，将该元素后移一位</li>
<li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置</li>
<li>将新元素插入到该位置后</li>
<li>重复步骤2~5</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best O(n); Average O(n^2); Worst O(n^2).</div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#  Insertion sort is good for collections that are very small</div><div class="line">#  or nearly sorted. Otherwise it&apos;s not a good sorting algorithm:</div><div class="line">#  it moves data around too much. Each time an insertion is made,</div><div class="line">#  all elements in a greater position are shifted.</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def insertion_sort(array):</div><div class="line">    n = len(array)</div><div class="line">    for i in range(1, n):</div><div class="line">        val = array[i]</div><div class="line">        position = i</div><div class="line">        while position &gt; 0 and array[position - 1] &gt; val:</div><div class="line">            array[position] = array[position - 1]</div><div class="line">            position -= 1</div><div class="line">        array[position] = val</div><div class="line">    return array</div></pre></td></tr></table></figure>
<h1 id="Shell-Sort-希尔排序"><a href="#Shell-Sort-希尔排序" class="headerlink" title="Shell Sort 希尔排序"></a>Shell Sort 希尔排序</h1><p>希尔排序，也称递减增量排序算法，实质是分组插入排序。<br>内部非稳定排序算法。时间复杂度不定，空间复杂度$O(1)$</p>
<p>希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。</p>
<p>例如，假设有这样一组数[ 13 14 94 33 82 25 59 94 65 23 45 27 73 25 39 10 ]，如果我们以步长为5开始进行排序，我们可以通过将这列表放在有5列的表中来更好地描述算法，这样他们就应该看起来是这样：</p>
<pre>
13 14 94 33 82
25 59 94 65 23
45 27 73 25 39
10
</pre>

<p>然后我们对每列进行排序：</p>
<pre>
10 14 73 25 23
13 27 94 33 39
25 59 94 65 82
45
</pre>

<p>将上述四行数字，依序接在一起时我们得到：[ 10 14 73 25 23 13 27 94 33 39 25 59 94 65 82 45 ]。这时10已经移至正确位置了，然后再以3为步长进行排序：</p>
<pre>
10 14 73
25 23 13
27 94 33
39 25 59
94 65 82
45
</pre>

<p>排序之后变为：</p>
<pre>
10 14 13
25 23 33
27 25 59
39 65 73
45 94 82
94
</pre>

<p>最后以1步长进行排序（此时就是简单的插入排序了）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def shell_sort(array):</div><div class="line">    n = len(array)</div><div class="line">    gap = n / 2 # 初始步长</div><div class="line">    while gap &gt; 0:</div><div class="line">        for i in range(gap, n):# 每一列进行插入排序 , 从gap 到 n-1</div><div class="line">            position = i</div><div class="line">            val = array[i]</div><div class="line">            while position &gt; 0 and array[position - 1] &gt; val:</div><div class="line">                array[position] = array[position - 1]</div><div class="line">                position -= 1</div><div class="line">            array[position] = val</div><div class="line">        gap = gap / 2 # 重新设置步长</div><div class="line">    return array</div></pre></td></tr></table></figure>
<p>上面源码的步长的选择是从n/2开始，每次再减半，直至为0。步长的选择直接决定了希尔排序的复杂度</p>
<h1 id="Merge-Sort-归并排序"><a href="#Merge-Sort-归并排序" class="headerlink" title="Merge Sort 归并排序"></a>Merge Sort 归并排序</h1><p>归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，解决子集的排序问题，再合并两个有序数组。由于基本的逻辑思维结构是二叉树，也叫二路归并排序。</p>
<p>先考虑合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。</p>
<p>再考虑递归分解，基本思路是将数组分解成 left 和 right，如果这两个数组内部数据是有序的，那么就可以用上面合并数组的方法将这两个数组合并排序。如何让这两个数组内部是有序的？可以再二分，直至分解出的小组只含有一个元素时为止，此时认为该小组内部已有序。然后合并排序相邻二个小组即可。</p>
<p>外部稳定排序。时间复杂度 $O(nlog(n))$，空间复杂度 $O(n)$。堆排序和快速排序的复杂度也都是 $O(nlog(n))$，但它们是不稳定的。在排序算法中，时间复杂度是 $O(nlog(n))$ 的排序算法只有归并排序。</p>
<h2 id="基本款-1"><a href="#基本款-1" class="headerlink" title="基本款"></a>基本款</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best = Average = Worst = O(nlog(n)).</div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#   Merge sort is a divide and conquer algorithm. In the divide and</div><div class="line">#   conquer paradigm, a problem is broken into pieces where each piece</div><div class="line">#   still retains all the properties of the larger problem -- except</div><div class="line">#   its size. To solve the original problem, each piece is solved</div><div class="line">#   individually; then the pieces are merged back together.</div><div class="line">#</div><div class="line">#   For illustration, imagine needing to sort an array of 200 elements</div><div class="line">#   using selection sort. Since selection sort takes O(n^2), it would</div><div class="line">#   take about 40,000 time units to sort the array. Now imagine</div><div class="line">#   splitting the array into ten equal pieces and sorting each piece</div><div class="line">#   individually still using selection sort. Now it would take 400</div><div class="line">#   time units to sort each piece; for a grand total of 4000.</div><div class="line">#   Once each piece is sorted, merging them back together would take</div><div class="line">#   about 200 time units; for a grand total of 200+4000 = 4,200.</div><div class="line">#   Clearly 4,200 is an impressive improvement over 40,000. Now</div><div class="line">#   imagine greater. Imagine splitting the original array into</div><div class="line">#   groups of two and then sorting them. In the end, it would take about</div><div class="line">#   1,000 time units to sort the array. That&apos;s how merge sort works.</div><div class="line">#</div><div class="line">#  NOTE to the Python experts:</div><div class="line">#     While it might seem more &quot;Pythonic&quot; to take such approach as</div><div class="line">#</div><div class="line">#         mid = len(aList) / 2</div><div class="line">#         left = mergesort(aList[:mid])</div><div class="line">#         right = mergesort(aList[mid:])</div><div class="line">#</div><div class="line">#     That approach take too much memory for creating sublists.</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def merge_sort(array):</div><div class="line">    if len(array) &lt;= 1:</div><div class="line">        return array</div><div class="line">    mid = len(array) / 2</div><div class="line">    left = merge_sort(array[:mid])</div><div class="line">    right = merge_sort(array[mid:])</div><div class="line">    return merge(left, right)</div><div class="line"></div><div class="line"></div><div class="line">def merge(left, right):</div><div class="line">    l, r = 0, 0</div><div class="line">    result = []</div><div class="line">    while l &lt; len(left) and r &lt; len(right):</div><div class="line">        if left[l] &lt; right[r]:</div><div class="line">            result.append(left[l])</div><div class="line">            l += 1</div><div class="line">        else:</div><div class="line">            result.append(right[r])</div><div class="line">            r += 1</div><div class="line">    # 如果有遗留没有比较的</div><div class="line">    result += left[l:]</div><div class="line">    result += right[r:]</div><div class="line">    return result</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="结合其他排序"><a href="#结合其他排序" class="headerlink" title="结合其他排序"></a>结合其他排序</h3><p>在数组长度比较短的情况下，不进行递归，而是采用其他排序方案，如 high - low &lt; 50 时，可以用快速／插入排序，适合整体时间最优</p>
<h3 id="建立索引"><a href="#建立索引" class="headerlink" title="建立索引"></a>建立索引</h3><p>实际过程中，可能排序的不是 int, 而是一个结构体，在此情况下，可以通过记录数组下标(index)来代替申请新内存空间，从而避免 A 和辅助数组间的频繁数据移动。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>归并排序非常适合做外排序(external sorting)。</p>
<h3 id="例1-9-路归并排序"><a href="#例1-9-路归并排序" class="headerlink" title="例1: 9 路归并排序"></a>例1: 9 路归并排序</h3><p>用 100M 内存对 900M 数据进行排序</p>
<ul>
<li>读入 100M 数据至内存，用常规方式（堆排序）排序</li>
<li>将排序后的数据写入磁盘</li>
<li>重复前两个步骤，得到 9 个 100M 的文件块。</li>
<li>将 100M 内存划分为 10 块，前 9 份为输入缓冲区，最后一个为输出缓冲区。如将 9 个 100M 的文件块每个分前 10M 放到输入缓冲区，然后同时指向第一个元素，把最小的那个放到输出缓冲区，然后指针后移一位。</li>
<li>执行 9 路归并算法，将结果输出到输出缓冲区。<ul>
<li>输出缓冲区满，写入目标文件，清空缓冲区</li>
<li>输入缓冲区空，读入相应文件的下一份数据。</li>
</ul>
</li>
</ul>
<h3 id="例2-逆序数问题"><a href="#例2-逆序数问题" class="headerlink" title="例2: 逆序数问题"></a>例2: 逆序数问题</h3><p>给定一个数组 A[0..N-1]，如果对于两个元素 a[i],a[j]，有 i<j 且="" a[j]="">a[i]，那么称 a[i],a[j] 为逆序对，一个数组中包含的逆序对的数目为逆序数。如 3,10,2,6 的逆序数为 3。如何求数组的逆序数？</j></p>
<p>当然可以选择暴力求解，两个循环，对每个数都要扫描它前面的所有数，时间复杂度为 $O(N^2)$<br>i -&gt; [0,N-1]<br>j -&gt; [i+1,N-1]</p>
<p>这里也可以用归并排序的思想。比如观察归并排序——合并数列(1，3，5)与(2，4)：</p>
<ol>
<li>先取出前面数列中的1。</li>
<li>然后取出后面数列中的2，明显这个2和前面的3，5都可以组成逆序数对即3和2，5和2都是逆序数对。</li>
<li>然后取出前面数列中的3。</li>
<li>然后取出后面数列中的4，同理，可知这个4和前面数列中的5可以组成一个逆序数对。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">def merge_sort(array):</div><div class="line">    if len(array) &lt;= 1:</div><div class="line">        return array</div><div class="line">    mid = len(array) / 2</div><div class="line">    left = merge_sort(array[:mid])</div><div class="line">    right = merge_sort(array[mid:])</div><div class="line">    count = 0</div><div class="line">    return merge(left, right)</div><div class="line"></div><div class="line">count = 0</div><div class="line"></div><div class="line"></div><div class="line">def merge(left, right):</div><div class="line">    global count</div><div class="line">    l, r = 0, 0</div><div class="line">    result = []</div><div class="line">    while l &lt; len(left) and r &lt; len(right):</div><div class="line">        if left[l] &lt;= right[r]:</div><div class="line">            result.append(left[l])</div><div class="line">            l += 1</div><div class="line">        else:</div><div class="line">            result.append(right[r])</div><div class="line">            r += 1</div><div class="line">            count += len(left) - l</div><div class="line">    result += left[l:]</div><div class="line">    result += right[r:]</div><div class="line">    return result</div><div class="line"></div><div class="line">print count # count 即为逆序数</div></pre></td></tr></table></figure>
<h2 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h2><p>思考：原地排序？让空间复杂度为 $O(1)$</p>
<h1 id="Quick-sort-快速排序"><a href="#Quick-sort-快速排序" class="headerlink" title="Quick sort 快速排序"></a>Quick sort 快速排序</h1><p>快速排序通常明显比同为Ο(nlogn)的其他算法更快，因此常被采用，而且快排采用了分治法的思想，所以在很多笔试面试中能经常看到快排的影子。可见掌握快排的重要性。<br>内部不稳定排序，最差时间复杂度 $O(N^2)$，平均时间复杂度 $O(nlogn)$<br>步骤：</p>
<ol>
<li>从数列中挑出一个元素作为基准数。</li>
<li>分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。</li>
<li>再对左右区间递归执行第二步，直至各区间只有一个数。</li>
</ol>
<h2 id="基本款-2"><a href="#基本款-2" class="headerlink" title="基本款"></a>基本款</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best = Average = O(nlog(n)); Worst = O(n^2).</div><div class="line"></div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#  Quicksort is admirably known as the algorithm that sorts an array</div><div class="line">#  while preparing to sort it. For contrast, recall that merge sort</div><div class="line">#  start partitions an array into smaller pieces, then sorts each piece,</div><div class="line">#  then merge the pieces back. Quicksort actually sorts the array</div><div class="line">#  during the partition phase.</div><div class="line">#</div><div class="line">#  Quicksort works by selecting an element called a pivot and splitting</div><div class="line">#  the array around that pivot such that all the elements in, say, the</div><div class="line">#  left sub-array are less than pivot and all the elements in the right</div><div class="line">#  sub-array are greater than pivot. The splitting continues until the</div><div class="line">#  array can no longer be broken into pieces. That&apos;s it. Quicksort is</div><div class="line">#  done.</div><div class="line">#</div><div class="line">#  All this fussing about quicksort sorting while preparing to sort</div><div class="line">#  may give the impression that it is better than mergesort, but its</div><div class="line">#  not. In practice their time complexity is about the same -- with</div><div class="line">#  one funny exception. Because quicksort picks its pivot randomly,</div><div class="line">#  there is a practically impossible possibility that the algorithm</div><div class="line">#    may take O(n^2) to compute.</div><div class="line">#</div><div class="line">#  The aforementioned notwithstanding, quicksort is better than</div><div class="line">#    mergesort if you consider memory usage. Quicksort is an in-place</div><div class="line">#    algorithm, requiring no additional storage to work.</div><div class="line">#=======================================================================</div><div class="line">def quick_sort(array):</div><div class="line">    if len(array) &lt; 2: return array</div><div class="line">    lesser = quick_sort([x for x in array[1:] if x &lt;= array[0]])</div><div class="line">    bigger = quick_sort([x for x in array[1:] if x &gt;  array[0]])</div><div class="line">    return sum([lesser, [array[0]], bigger], [])</div></pre></td></tr></table></figure>
<p>上面的代码选择第一个数作为基准数。</p>
<h2 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h2><p>正整数数字序列，求最大 K 个数。<br>输入项：一个无序的数字序列，和一个数字 K<br>输出项：K 个数字，代表最大的 K 个数字是什么<br>逻辑：将无序数列插入到二叉排序数中，采用中序遍历的方式输出前 K 个数字。</p>
<h1 id="Heap-sort-堆排序"><a href="#Heap-sort-堆排序" class="headerlink" title="Heap sort 堆排序"></a>Heap sort 堆排序</h1><p>堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。二叉堆是一个近似完全二叉树 。<br>内部不稳定排序，时间复杂度 $O(nlogn)$，空间复杂度 $O(1)$</p>
<p>步骤：</p>
<ol>
<li>构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是最大堆，则从下标n/2开始的元素均为最大堆。于是只要从n/2-1开始，向前依次构造最大堆，这样就能保证，构造到某个节点时，它的左右子树都已经是最大堆。</li>
<li>堆排序（HeapSort）：由于堆是用数组模拟的。得到一个最大堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。</li>
<li>最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best O(nlog(n)); Average O(nlog(n)); Worst O(nlog(n)).</div><div class="line">#</div><div class="line">#  Approach:</div><div class="line">#  Heap sort happens in two phases. In the start phase, the array</div><div class="line">#  is transformed into a heap. A heap is a binary tree where</div><div class="line">#  1) each node is greater than each of its children</div><div class="line">#  2) the tree is perfectly balanced</div><div class="line">#  3) all leaves are in the leftmost position available.</div><div class="line">#  In phase two the heap is continuously reduced to a sorted array:</div><div class="line">#  1) while the heap is not empty</div><div class="line">#  - remove the top of the head into an array</div><div class="line">#  - fix the heap.</div><div class="line">#  Heap sort was invented by John Williams not by B. R. Heap.</div><div class="line">#</div><div class="line">#  MoveDown:</div><div class="line">#  The movedown method checks and verifies that the structure is a heap.</div><div class="line">#</div><div class="line">#  Technical Details:</div><div class="line">#  A heap is based on an array just as a hashmap is based on an</div><div class="line">#  array. For a heap, the children of an element n are at index</div><div class="line">#  2n+1 for the left child and 2n+2 for the right child.</div><div class="line">#</div><div class="line">#  The movedown function checks that an element is greater than its</div><div class="line">#  children. If not the values of element and child are swapped. The</div><div class="line">#  function continues to check and swap until the element is at a</div><div class="line">#  position where it is greater than its children.</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def heap_sort(array):</div><div class="line">    # convert aList to heap 构造最大堆</div><div class="line">    n = len(array)</div><div class="line">    leastParent = n / 2 - 1  # n的父节点下标</div><div class="line">    for i in range(leastParent, -1, -1):</div><div class="line">        max_heapify(array, i, n - 1)  # 小堆转化为最大堆</div><div class="line"></div><div class="line">    # flatten heap into sorted array 将最大堆转化为有序数组</div><div class="line">    for i in range(n - 1, 0, -1):</div><div class="line">        if array[0] &gt; array[i]:</div><div class="line">            array[i], array[0] = array[0], array[i]</div><div class="line">            max_heapify(array, 0, i - 1)  # 调整最大堆</div><div class="line">    return array</div><div class="line"></div><div class="line"></div><div class="line"># 最大堆调整：将堆的末端子节点作调整，使得子节点永远小于父节点</div><div class="line"># start 为当前需要调整最大堆的位置，end为调整边界</div><div class="line"></div><div class="line"></div><div class="line">def max_heapify(array, start, end):</div><div class="line">    largest = 2 * start + 1  # consider left child is larger than right</div><div class="line">    while largest &lt;= end:</div><div class="line">        # right child exists and is larger than left child</div><div class="line">        if largest &lt; end and array[largest] &lt; array[largest + 1]:</div><div class="line">            largest += 1</div><div class="line"></div><div class="line">        # right child is larger than parent</div><div class="line">        if array[largest] &gt; array[start]:</div><div class="line">            array[largest], array[start] = array[start], array[largest]</div><div class="line">            # move down to largest child</div><div class="line">            start = largest</div><div class="line">            largest = 2 * start + 1</div><div class="line">        else:</div><div class="line">            return</div></pre></td></tr></table></figure>
<h1 id="Bucket-Sort-桶排序"><a href="#Bucket-Sort-桶排序" class="headerlink" title="Bucket Sort 桶排序"></a>Bucket Sort 桶排序</h1><p>桶排序和归并排序非常类似，也使用了归并的思想。大致步骤如下：<br>外部排序，稳定性取决于桶内排序算法。时间复杂度与分桶数量 K 有关<br>步骤：</p>
<ol>
<li>设置一个定量的数组当作空桶。桶排序的特点就是数据要有范围（桶不能无限多）。</li>
<li>Divide - 从待排序数组中取出元素，将元素按照一定的规则塞进对应的桶子去。</li>
<li>对每个非空桶进行排序，通常可在塞元素入桶时进行插入排序。</li>
<li>Conquer - 从非空桶把元素再放回原来的数组中。”</li>
</ol>
<p>假设输入数据服从均匀分布，然后将输入数据均匀地分配到有限数量的桶中，然后对每个桶再分别排序，对每个桶再使用插入排序算法，最后将每个桶中的数据有序的组合起来。前面了解到基数排序假设输入数据属于一个小区间内的整数，而桶排序则是假设输入是由一个随机过程生成，该过程将元素均匀的分布在一个区间[a,b]上。由于桶排序和计数排序一样均对输入的数据进行了某些假设限制，因此比一般的基于比较的排序算法复杂度低。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line">#=======================================================================</div><div class="line">#  Time Complexity of Solution:</div><div class="line">#  Best Case O(n); Average Case O(n); Worst Case O(n).</div><div class="line">#</div><div class="line"># Approach:</div><div class="line"># If it sounds too good to be true, then most likely it&apos;s not true.</div><div class="line"># Bucketsort is not an exception to this adage. For bucketsort to</div><div class="line">#   work at its blazing efficiency, there are multiple prerequisites.</div><div class="line">#   First the hash function that is used to partition the elements need</div><div class="line">#   to be very good and must produce ordered hash: if i &lt; k then</div><div class="line">#   hash(i) &lt; hash(k). Second, the elements to be sorted must be</div><div class="line">#   uniformly distributed.</div><div class="line">#</div><div class="line"># The aforementioned aside, bucket sort is actually very good</div><div class="line">#   considering that counting sort is reasonably speaking its upper</div><div class="line">#   bound. And counting sort is very fast. The particular distinction</div><div class="line">#   for bucket sort is that it uses a hash function to partition the</div><div class="line">#   keys of the input array, so that multiple keys may hash to the same</div><div class="line">#   bucket. Hence each bucket must effectively be a growable list;</div><div class="line">#   similar to radix sort.</div><div class="line">#</div><div class="line"># Numerous Internet sites, including university pages, have</div><div class="line">#   erroneously written counting sort code and call them bucket sort.</div><div class="line">#   Bucket sort uses a hash function to distribute keys; counting sort</div><div class="line">#   creates a bucket for each key. Indeed there are perhaps greater</div><div class="line">#   similarities between radix sort and bucket sort, than there are</div><div class="line">#   between counting sort and bucket sort.</div><div class="line">#</div><div class="line"># In the presented program insertionsort is used to sort</div><div class="line">#   each bucket. This is to inculcate that the bucket sort algorithm</div><div class="line">#   does not specify which sorting technique to use on the buckets.</div><div class="line">#   A programmer may choose to continuously use bucket sort on each</div><div class="line">#   bucket until the collection is sorted (in the manner of the radix</div><div class="line">#   sort program below). Whichever sorting method is used on the</div><div class="line">#   buckets, bucket sort still tends toward O(n).</div><div class="line">#=======================================================================</div><div class="line"></div><div class="line"></div><div class="line">def bucket_sort(array):</div><div class="line">    # get hash codes</div><div class="line">    code = hashing(array)</div><div class="line">    # number of buckets: math.sqrt(len(array))</div><div class="line">    buckets = [list() for _ in range(code[1])]</div><div class="line">    # distribute data into buckets: O(n)</div><div class="line">    for i in array:</div><div class="line">        x = re_hashing(i, code)</div><div class="line">        buck = buckets[x]</div><div class="line">        buck.append(i)</div><div class="line"></div><div class="line">  # Sort each bucket: O(n).</div><div class="line">  # I mentioned above that the worst case for bucket sort is counting</div><div class="line">  # sort. That&apos;s because in the worst case, bucket sort may end up</div><div class="line">  # with one bucket per key. In such case, sorting each bucket would</div><div class="line">  # take 1^2 = O(1). Even after allowing for some probabilistic</div><div class="line">  # variance, to sort each bucket would still take 2-1/n, which is</div><div class="line">  # still a constant. Hence, sorting all the buckets takes O(n).</div><div class="line">    for bucket in buckets:</div><div class="line">        insertion_sort(bucket)</div><div class="line">    ndx = 0</div><div class="line">    # merge the buckets: O(n)</div><div class="line">    for i in range(len(buckets)):</div><div class="line">        print buckets[i]</div><div class="line">        for v in buckets[i]:</div><div class="line">            array[ndx] = v</div><div class="line">            ndx += 1</div><div class="line">    return array</div><div class="line"></div><div class="line"></div><div class="line">import math</div><div class="line"></div><div class="line"></div><div class="line">def hashing(array):</div><div class="line">    m = array[0]</div><div class="line">    for i in range(1, len(array)):</div><div class="line">        if(m &lt; array[i]):</div><div class="line">            m = array[i]</div><div class="line">    result = [m, int(math.sqrt(len(array)))]</div><div class="line">    print result</div><div class="line">    return result</div><div class="line"></div><div class="line"></div><div class="line">def re_hashing(i, code):</div><div class="line">    # 桶是从小到大排的</div><div class="line">    return int(i / code[0] * (code[1] - 1))</div></pre></td></tr></table></figure>
<h1 id="Counting-Sort-计数排序"><a href="#Counting-Sort-计数排序" class="headerlink" title="Counting Sort 计数排序"></a>Counting Sort 计数排序</h1><p>桶的个数＝待排序个数，就是计数排序，是桶排序的特例。<br>计数排序，顾名思义，就是对待排序数组按元素进行计数。使用前提是需要先知道待排序数组的元素范围，将这些一定范围的元素置于新数组中，新数组的大小为待排序数组中最大元素与最小元素的差值。</p>
<p>本质是空间换时间，空间复杂度 O(max-min)。本质是哈希过程<br>前提：数据是 int 值</p>
<p>步骤：</p>
<ol>
<li>定新数组大小——找出待排序的数组中最大和最小的元素</li>
<li>统计次数——统计数组中每个值为i的元素出现的次数，存入新数组C的第i项</li>
<li>对统计次数逐个累加——对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）</li>
<li>反向填充目标数组——将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1</li>
<li>其中反向填充主要是为了避免重复元素落入新数组的同一索引处。</li>
</ol>
<blockquote>
<p>参考链接：<br><a href="https://www.bittiger.io/blog/post/4Q4iNNbRYXkWkrAM3#quicksort" target="_blank" rel="external">https://www.bittiger.io/blog/post/4Q4iNNbRYXkWkrAM3#quicksort</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 排序 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Search Engines笔记 - Exact-match retrieval]]></title>
      <url>http://www.shuang0420.com/2016/09/06/Search%20Engines%E7%AC%94%E8%AE%B0%20-%20Exact-match%20retrieval/</url>
      <content type="html"><![CDATA[<p>CMU 11642 的课程笔记。Exact match retrieval models 对专家来说很适用，它假定人能将需求描述为一个 boolean query，文档要么完全匹配要么完全不匹配，不匹配的文档分数就为 0。<br><a id="more"></a></p>
<h1 id="Unranked-Boolean-Model"><a href="#Unranked-Boolean-Model" class="headerlink" title="Unranked Boolean Model"></a>Unranked Boolean Model</h1><p>document score 为 1 或者 0，也就是匹配或者不匹配，没有匹配程度的分数，返回结果通常简单的按时间顺序排列。<br>很多系统在用，像 WestLaw, PubMed 等，因为它速度非常快，对一些问答系统来说，unranked boolean model 足够用了。</p>
<h1 id="Ranked-Boolean-Model"><a href="#Ranked-Boolean-Model" class="headerlink" title="Ranked Boolean Model"></a>Ranked Boolean Model</h1><p>为文档计算特定分数，文档 j 对 query $Q_{AND}(q_1…q_n)$ 的分数，一般取最小值。<br>$score(Q_{AND}(q_1…q_n),d_j) = MIN(score(q_1,d_j),score(q_n,d_j))$</p>
<p>文档 j 对 query $Q_{OR}(q_1…q_n)$ 的分数，一般计算 MEAN 或者 MAX，实践中 MEAN 比 MAX 更有效。<br>$score(Q_{OR}(q_1…q_n),d_j) = MAX(score(q_1,d_j),score(q_n,d_j))$<br>$score(Q_{OR}(q_1…q_n),d_j) = MEAN(score(q_1,d_j),score(q_n,d_j))$</p>
<p><strong>优点：</strong></p>
<ul>
<li>效率高</li>
<li>可预测，可解释，结构化的查询语句</li>
<li>当用户非常清楚自己需要的是怎样的文档时非常有用</li>
<li>也可以用其它的 term weighting 方法</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>仍然是完全匹配模型</li>
<li>很难在 Precision 和 Recall 间得到平衡</li>
<li>检索结果是按照文档有多么冗余的（redundantly）匹配 query 来排序的</li>
</ul>
<h1 id="Inverted-list"><a href="#Inverted-list" class="headerlink" title="Inverted list"></a>Inverted list</h1><h2 id="Binary-inverted-lists"><a href="#Binary-inverted-lists" class="headerlink" title="Binary inverted lists"></a>Binary inverted lists</h2><p>用于 unranked retrieval<br>Operators: AND, OR, AND-NOT, FIELD</p>
<h2 id="Frequency-inverted-lists"><a href="#Frequency-inverted-lists" class="headerlink" title="Frequency inverted lists"></a>Frequency inverted lists</h2><p>用于 ranked retrieval<br>Operators: AND, OR, AND-NOT, FIELD, SUM, SYNONYM</p>
<h2 id="Positional-inverted-lists"><a href="#Positional-inverted-lists" class="headerlink" title="Positional inverted lists"></a>Positional inverted lists</h2><p>用于 ranked retrieval<br>Operators: AND, OR, AND-NOT, NEAR/n, SENTENCE/n, PASSAGE/n, WINDOW/n</p>
<h2 id="Fixed-length-inverted-list"><a href="#Fixed-length-inverted-list" class="headerlink" title="Fixed-length inverted list"></a>Fixed-length inverted list</h2><p>早期的搜索引擎会用，它的优点是</p>
<ul>
<li>易于管理</li>
<li>bit-vector operations 速度快，并行化容易</li>
</ul>
<p>然而…效率不高。<br>假定 inverted list 长度为 |C| bits (C 是 corpus 里的文档总数)，那么在某个 inverted list 里为 1 的 bits 的个数就是 df(多少篇文档出现了这个 term)，我们看 term with median tf 的 df，记作 $df_{median}$，观察一些语料可以发现，$|C|&gt;&gt;|df_{median}|$，(Wall Street Journal，|C|=174K, $df_{median}=2$)</p>
<h2 id="Data-structure"><a href="#Data-structure" class="headerlink" title="Data structure"></a>Data structure</h2><h3 id="B-tree-B-tree-B-tree-etc"><a href="#B-tree-B-tree-B-tree-etc" class="headerlink" title="B tree(B+ tree, B* tree, etc)"></a>B tree(B+ tree, B* tree, etc)</h3><ul>
<li>$O(log n)$</li>
<li>易于扩展</li>
<li>可以用于完全匹配(exact-match lookup)，范围寻找(range lookup)，前缀寻找（prefix lookup）</li>
</ul>
<h3 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h3><ul>
<li>$O(1)$</li>
<li>不易于扩展</li>
<li>用于完全匹配(exact-match lookup)</li>
</ul>
<h1 id="Term-dictionary"><a href="#Term-dictionary" class="headerlink" title="Term dictionary"></a>Term dictionary</h1><p>string –&gt; integer，速度更快。</p>
<p><strong>问题：多少存在内存，多少存在硬盘</strong></p>
<ul>
<li>frequent terms in RAM (eg.,ctf&gt;=1,000)</li>
<li>less frequent terms to dis (eg.,ctf&lt;1,000)</li>
</ul>
<p>ctf &lt; 1000，根据 Zipf’s law 算出 99.9% 的词可以存在硬盘。<br>${(A*N/1)-(A*N/1000) \over (A*N)}={999 \over 1000}=99.9%$</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Search Engines </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Search Engines </tag>
            
            <tag> 信息检索 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据结构和算法 -- 链表]]></title>
      <url>http://www.shuang0420.com/2016/09/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%20--%20%E9%93%BE%E8%A1%A8/</url>
      <content type="html"><![CDATA[<p>链表实现／移除节点／链表相加／链表部分翻转／链表改序/链表去重／链表划分／链表的环/链表公共节点问题/链表复制。<br><a id="more"></a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>线性表是最基本、最简单、也是最常用的一种数据结构。线性表中数据元素之间的关系是一对一的关系，即除了第一个和最后一个数据元素之外，其它数据元素都是首尾相接的。数组 immutable length 和 no holes allowed 的特性带来的结果就是当 resize array 的时候需要 copy 原有的所有元素，当删除一个不在末尾的元素时，又要 shift 很多元素。而 Linked list 解决了这个问题，它的代价是需要额外的 4 bytes(32bit machine) 来储存指向下一个 node 的 reference，另外不允许随机访问元素。</p>
<p><strong>线性表的两种存储方式</strong></p>
<ul>
<li>顺序存储结构：随机读取，访问时是 O(1)</li>
<li>链式存储结构：插入和删除 O(1)，访问时最坏是 O(n)</li>
</ul>
<p><strong>线性表的分类（根据指针域）</strong></p>
<ul>
<li>单向链表(Singly linked list)</li>
<li>双向链表(Doubly linked list)</li>
<li>循环链表(Circular linked list)</li>
</ul>
<p>这一篇主要讲的是链表（linked list）。链表是一种常见的线性数据结构。<strong>单向链表(singly linked list)</strong>，每个节点有一个 next 指针指向后一个节点，还有一个成员变量用以存储数值；<strong>双向链表(doubly Linked List)</strong>，多了一个 prev 指针指向前一个节点。与数组类似，搜索链表需要O(n)的时间复杂度，但是链表不能通过常数时间 O(1) 读取第 k 个数据。链表的优势在于能够以较高的效率在任意位置插入或删除一个节点。</p>
<h2 id="Complexity"><a href="#Complexity" class="headerlink" title="Complexity"></a>Complexity</h2><h3 id="Linked-list"><a href="#Linked-list" class="headerlink" title="Linked list"></a>Linked list</h3><p>addFirst: O(1)<br>insertBefore or insertAfter: O(n)<br>delete: O(n)<br>search: O(n)</p>
<h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>locate: O(1)<br>insert/delete: O(N)<br>search: not sorted -&gt; linear search =&gt; O(N), sorted -&gt; binary search =&gt; O(logN)<br>iteration: O(N)</p>
<h1 id="基本策略"><a href="#基本策略" class="headerlink" title="基本策略"></a>基本策略</h1><h2 id="涉及头节点"><a href="#涉及头节点" class="headerlink" title="涉及头节点"></a>涉及头节点</h2><p>当涉及对头节点的操作，考虑创建哑节点</p>
<h2 id="修改单向链表的操作"><a href="#修改单向链表的操作" class="headerlink" title="修改单向链表的操作"></a>修改单向链表的操作</h2><p>考虑哪个节点的next指针会受到影响，则需要修正该指针；</p>
<h2 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h2><p>要把反转后的最后一个节点（即第一个节点）指向 null</p>
<h2 id="删除某个节点"><a href="#删除某个节点" class="headerlink" title="删除某个节点"></a>删除某个节点</h2><ul>
<li>由于需要知道前继节点的信息，而前继节点可能会导致表头产生变化，所以需要一些技巧 Dummy Node</li>
<li>全部操作结束后，判断是否有环；若有，则置其中一端为 null</li>
</ul>
<h2 id="快慢指针"><a href="#快慢指针" class="headerlink" title="快慢指针"></a>快慢指针</h2><p>快速找出未知长度单链表的中间节点／涉及在链表中寻找特定位置</p>
<ul>
<li>设置两个指针 *fast 和 *slow 都指向头节点</li>
<li>*fast 移动速度是 *slow 的两倍</li>
<li>*fast 指向末尾节点时，*slow 正好就在中间</li>
</ul>
<h2 id="判断单链表是否有环"><a href="#判断单链表是否有环" class="headerlink" title="判断单链表是否有环"></a>判断单链表是否有环</h2><ul>
<li>设置两个指针 *fast 和 *slow 都指向头节点</li>
<li>*fast 移动速度是 *slow 的两倍</li>
<li>如果 *fast == null 说明该单链表不是循环链表</li>
<li>如果 *fast == *slow 说明该链表是循环链表</li>
</ul>
<h2 id="找倒数第-N-个节点"><a href="#找倒数第-N-个节点" class="headerlink" title="找倒数第 N 个节点"></a>找倒数第 N 个节点</h2><ul>
<li>设置两个指针 fast 和 slow 都指向头节点</li>
<li>*fast 先移动 N 步，然后两个指针一起前进</li>
<li>*fast 到达末尾时，*slow 即为倒数第 N 个节点</li>
</ul>
<h2 id="检验有效性"><a href="#检验有效性" class="headerlink" title="检验有效性"></a>检验有效性</h2><p>访问某个节点 cur.next 时，要检验 cur 是否为 null。（同理，访问 cur.next.next，检验 cur.next）</p>
<h1 id="链表实现"><a href="#链表实现" class="headerlink" title="链表实现"></a>链表实现</h1><h2 id="Singly-linked-list-Implementation"><a href="#Singly-linked-list-Implementation" class="headerlink" title="Singly-linked list Implementation"></a>Singly-linked list Implementation</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div></pre></td><td class="code"><pre><div class="line"># implement a singly-linked list</div><div class="line">class Node(object):</div><div class="line"></div><div class="line">    def __init__(self, val=None, next=None):</div><div class="line">        self.val = val</div><div class="line">        self.next = next</div><div class="line"></div><div class="line"></div><div class="line">class LinkedList(object):</div><div class="line"></div><div class="line">    def __init__(self, head=None):</div><div class="line">        self.head = head</div><div class="line"></div><div class="line">    def addFirst(self, val):</div><div class="line">        self.head = Node(val, self.head)</div><div class="line">        return True</div><div class="line"></div><div class="line">    def addLast(self, item):</div><div class="line">        # if the list empty</div><div class="line">        if not self.head:</div><div class="line">            addFirst(val)</div><div class="line">            return False</div><div class="line">        # traverse to find the last element</div><div class="line">        tmp = self.head</div><div class="line">        while tmp.next:</div><div class="line">            tmp = tmp.next</div><div class="line">        # finally, add the new element into the list</div><div class="line">        tmp.next = Node(item, None)</div><div class="line">        return True</div><div class="line"></div><div class="line">    def insertAfter(self, key, item):</div><div class="line">        # find the location first with the given key</div><div class="line">        tmp = self.head</div><div class="line">        while tmp and tmp.val != key:</div><div class="line">            tmp = tmp.next</div><div class="line">        # as long as the key is in the list</div><div class="line">        if tmp:</div><div class="line">            toBeInserted = Node(item, tmp.next)</div><div class="line">            tmp.next = toBeInserted</div><div class="line">            return True  </div><div class="line">        return False</div><div class="line"></div><div class="line">    def insertBefore(self, key, item):</div><div class="line">        # if the list is empty</div><div class="line">        if not self.head:</div><div class="line">            return False</div><div class="line">        # if head has the key</div><div class="line">        if self.head.val == key:</div><div class="line">            addFirst(key)</div><div class="line">            return True</div><div class="line">        # key is not in the head</div><div class="line">        prev = None</div><div class="line">        cur = self.head</div><div class="line">        while cur and cur.val != key:</div><div class="line">            prev, cur = cur, cur.next</div><div class="line">        # found it, then add new node into next of the previous</div><div class="line">        if cur:</div><div class="line">            prev.next = Node(item, cur)</div><div class="line">            return True  </div><div class="line">        return False</div><div class="line"></div><div class="line">    def size(self):</div><div class="line">        length = 0</div><div class="line">        cur = self.head</div><div class="line">        while cur:</div><div class="line">            cur = cur.next</div><div class="line">            length += 1</div><div class="line">        return length</div><div class="line"></div><div class="line">    def remove(self, key):</div><div class="line">        if not self.head or not key:</div><div class="line">            return False</div><div class="line">        # if the key is found from the head element</div><div class="line">        if self.head.val == key:</div><div class="line">            head = head.next</div><div class="line">            return True</div><div class="line"></div><div class="line">        cur = self.head</div><div class="line">        prev = None</div><div class="line">        while cur and cur.val != key:</div><div class="line">            prev = cur</div><div class="line">            cur = cur.next</div><div class="line">        # as long as key is found</div><div class="line">        if cur:</div><div class="line">            prev.next = cur.next</div><div class="line">            return True</div><div class="line">        return False</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        while cur.next:</div><div class="line">            if cur.next == data:</div><div class="line">                cur.next = cur.next.next</div><div class="line">                return True</div><div class="line">            cur = cur.next</div><div class="line">        return False</div><div class="line">        #raise ValueError(&quot;Data not in list&quot;)</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">    def deleteList(self):</div><div class="line">        self.head = None</div><div class="line">        return True</div><div class="line"></div><div class="line">    def search(self, data):</div><div class="line">        cur = self.head</div><div class="line">        while cur and cur.val != data:</div><div class="line">            cur = cur.next</div><div class="line">        return cur</div><div class="line"></div><div class="line">    def printAll(self):</div><div class="line">        cur = self.head</div><div class="line">        while cur:</div><div class="line">            print cur.val,</div><div class="line">            cur = cur.next</div><div class="line">        print</div></pre></td></tr></table></figure>
<h1 id="Leetcode-实例"><a href="#Leetcode-实例" class="headerlink" title="Leetcode 实例"></a>Leetcode 实例</h1><h2 id="移除节点-19-Remove-Nth-Node-From-End-of-List"><a href="#移除节点-19-Remove-Nth-Node-From-End-of-List" class="headerlink" title="移除节点(19.Remove Nth Node From End of List)"></a>移除节点(19.Remove Nth Node From End of List)</h2><h3 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h3><p>Given a linked list, remove the nth node from the end of list and return its head.</p>
<p>For example,</p>
<p>   Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.</p>
<p>   After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5.<br>Note:<br>Given n will always be valid.<br>Try to do this in one pass.</p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">A singly list is a particularly poor choice for a data structure when you frequently need to find the mth-to-last element!</div><div class="line"></div><div class="line">Assumption:</div><div class="line">- m is less than the length of linked list?</div><div class="line">  if not, check first!</div><div class="line"></div><div class="line">Solution:</div><div class="line">You cannot traverse backward through a singly linked list, so may be we can store elements into another data structure so that we can look back, or for this problem, we can traverse from beginning of list.</div><div class="line"></div><div class="line">Two-pass solution:</div><div class="line">- first get the length of linked list, and then find the node before (length-n)th node, and node.next=node.next.next</div><div class="line">- use dummy node</div><div class="line"></div><div class="line">Follow-up:</div><div class="line">one-pass solution:</div><div class="line">- how to access nth node from the end?</div><div class="line">    use two pointers, fast and slow, keep the distance n between fast and slow node</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def removeNthFromEnd(self, head, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :type n: int</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head or not head.next:</div><div class="line">            return None</div><div class="line">        dummy=ListNode(0)</div><div class="line">        dummy.next=head</div><div class="line">        slow,fast=dummy,dummy</div><div class="line">        for i in range(n):</div><div class="line">            fast=fast.next</div><div class="line">        while fast.next:</div><div class="line">            fast,slow=fast.next,slow.next</div><div class="line">        slow.next=slow.next.next</div><div class="line">        return dummy.next</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # Two pass solution</div><div class="line"></div><div class="line">    def removeNthFromEnd(self, head, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :type n: int</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head or not head.next:</div><div class="line">            return None</div><div class="line">        length=0</div><div class="line">        dummy=ListNode(0)</div><div class="line">        dummy.next=head</div><div class="line">        pointer1,pointer2=dummy,dummy</div><div class="line">        while pointer1.next:</div><div class="line">            length+=1</div><div class="line">            pointer1=pointer1.next</div><div class="line">        # find nth node</div><div class="line">        for i in range(length-n):</div><div class="line">            pointer2=pointer2.next</div><div class="line">        pointer2.next=pointer2.next.next</div><div class="line">        return dummy.next</div><div class="line">        &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="链表相加-2-445-Add-Two-Numbers-369-Plus-One-Linked-List"><a href="#链表相加-2-445-Add-Two-Numbers-369-Plus-One-Linked-List" class="headerlink" title="链表相加(2.445. Add Two Numbers; 369. Plus One Linked List)"></a>链表相加(2.445. Add Two Numbers; 369. Plus One Linked List)</h2><h3 id="Problem-I"><a href="#Problem-I" class="headerlink" title="Problem(I)"></a>Problem(I)</h3><p>You are given two linked lists representing two non-negative numbers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.<br>Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)<br>Output: 7 -&gt; 0 -&gt; 8</p>
<p><strong>Similar problems</strong> (M) Multiply Strings (E) Add Binary (M) Add Two Numbers (M) Plus One Linked List<br><a href="http://www.shuang0420.com/2016/09/19/数据结构和算法%20--%20数组/">66.Plus One 67.Add Binary</a><br><a href="http://www.shuang0420.com/2016/10/12/数据结构和算法%20--%20String/">43. Multiply Strings</a></p>
<h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"># Straight-forward</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def addTwoNumbers(self, l1, l2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type l1: ListNode</div><div class="line">        :type l2: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        # make sure a l1 is longer than l2</div><div class="line">        p1, p2 = l1, l2</div><div class="line">        while p1 and p2:</div><div class="line">            p1 = p1.next</div><div class="line">            p2 = p2.next</div><div class="line">        if p1:</div><div class="line">            p1, p2 = l1, l2</div><div class="line">        else:</div><div class="line">            p1, p2 = l2, l1</div><div class="line"></div><div class="line">        # cal</div><div class="line">        carry = 0</div><div class="line">        res = ListNode(0)</div><div class="line">        cur = res</div><div class="line">        while p2:</div><div class="line">            sum = p1.val + p2.val + carry</div><div class="line">            carry = sum / 10</div><div class="line">            sum %= 10</div><div class="line">            cur.next = ListNode(sum)</div><div class="line">            cur = cur.next</div><div class="line">            p1, p2 = p1.next, p2.next</div><div class="line">        while p1:</div><div class="line">            sum = p1.val + carry</div><div class="line">            carry = sum / 10</div><div class="line">            sum %= 10</div><div class="line">            cur.next = ListNode(sum)</div><div class="line">            cur = cur.next</div><div class="line">            p1 = p1.next</div><div class="line">        if carry == 1:</div><div class="line">            cur.next = ListNode(1)</div><div class="line">        return res.next</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line"></div><div class="line"># improved, doesn&apos;t need to know which list is longer</div><div class="line">class Solution(object):</div><div class="line">    def addTwoNumbers(self, l1, l2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type l1: ListNode</div><div class="line">        :type l2: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        head = ListNode(0)</div><div class="line">        output = head</div><div class="line">        carry = 0</div><div class="line">        while True:</div><div class="line">            if l1:</div><div class="line">                carry += l1.val</div><div class="line">                l1 = l1.next</div><div class="line">            if l2:</div><div class="line">                carry += l2.val</div><div class="line">                l2 = l2.next</div><div class="line">            output.val = carry % 10</div><div class="line">            carry = carry / 10</div><div class="line">            if l1 or l2 or carry:</div><div class="line">                output.next = ListNode(0)</div><div class="line">                output = output.next</div><div class="line">            else:</div><div class="line">                break</div><div class="line">        return head</div></pre></td></tr></table></figure>
<p>注意考虑两个数位数不同的情况。<br>因为两位数相加进位最多影响后一位，不会影响 i+2 位，所以发现一个链表为空后，直接结束循环，最后只用进位和较长链表的当前节点相加，之后较长链表的 i+2 位直接照搬。<br>用这个结构可以实现大整数的计算。</p>
<h3 id="Problem-II"><a href="#Problem-II" class="headerlink" title="Problem(II)"></a>Problem(II)</h3><p>You are given two non-empty linked lists representing two non-negative integers. The most significant digit comes first and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.</p>
<p>You may assume the two numbers do not contain any leading zero, except the number 0 itself.</p>
<p>Follow up:<br>What if you cannot modify the input lists? In other words, reversing the lists is not allowed.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: (7 -&gt; 2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)</div><div class="line">Output: 7 -&gt; 8 -&gt; 0 -&gt; 7</div></pre></td></tr></table></figure></p>
<h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line"></div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def addTwoNumbers(self, l1, l2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type l1: ListNode</div><div class="line">        :type l2: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        p1, p2 = l1, l2</div><div class="line">        s1, s2 = [], []</div><div class="line">        while p1:</div><div class="line">            s1.append(p1.val)</div><div class="line">            p1 = p1.next</div><div class="line">        while p2:</div><div class="line">            s2.append(p2.val)</div><div class="line">            p2 = p2.next</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        # method 1: use a stack</div><div class="line">        carry=0</div><div class="line">        res=[]</div><div class="line">        while True:</div><div class="line">            if not s1 and not s2 and carry==0:</div><div class="line">                break</div><div class="line">            if s1:</div><div class="line">                carry+=s1.pop()</div><div class="line">            if s2:</div><div class="line">                carry+=s2.pop()</div><div class="line">            res.append(carry%10)</div><div class="line">            carry/=10</div><div class="line">        cur=ListNode(0)</div><div class="line">        tmp=cur</div><div class="line">        while res:</div><div class="line">            tmp.next=ListNode(res.pop())</div><div class="line">            tmp=tmp.next</div><div class="line">        return cur.next</div><div class="line">        &apos;&apos;&apos;</div><div class="line">        # method 2: Linkedlist addFirst</div><div class="line">        carry = 0</div><div class="line">        cur = ListNode(None)</div><div class="line">        while True:</div><div class="line">            if not s1 and not s2 and carry == 0:</div><div class="line">                break</div><div class="line">            if s1:</div><div class="line">                carry += s1.pop()</div><div class="line">            if s2:</div><div class="line">                carry += s2.pop()</div><div class="line">            head = ListNode(carry % 10)</div><div class="line">            head.next = cur if cur.val != None else None</div><div class="line">            cur = head</div><div class="line">            carry /= 10</div><div class="line">        return cur</div></pre></td></tr></table></figure>
<h3 id="Problem-III"><a href="#Problem-III" class="headerlink" title="Problem(III)"></a>Problem(III)</h3><p>Given a non-negative integer represented as non-empty a singly linked list of digits, plus one to the integer.</p>
<p>You may assume the integer do not contain any leading zero, except the number 0 itself.</p>
<p>The digits are stored such that the most significant digit is at the head of the list.</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">1-&gt;2-&gt;3</div><div class="line"></div><div class="line">Output:</div><div class="line">1-&gt;2-&gt;4</div></pre></td></tr></table></figure></p>
<h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"># use stack, O(n) space and O(n) time</div><div class="line">class Solution(object):</div><div class="line">    def plusOne(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        stack=[]</div><div class="line">        res=ListNode(0)</div><div class="line">        tmp=res</div><div class="line">        while head:</div><div class="line">            stack.append(head.val)</div><div class="line">            head=head.next</div><div class="line">        if stack[-1]&lt;9:</div><div class="line">            stack[-1]+=1</div><div class="line">        else:</div><div class="line">            i=len(stack)-1</div><div class="line">            while i&gt;=0 and stack[i]==9:</div><div class="line">                stack[i]=0</div><div class="line">                i-=1</div><div class="line">            if i&lt;0:</div><div class="line">                tmp.next=ListNode(1)</div><div class="line">                tmp=tmp.next</div><div class="line">            else:</div><div class="line">                stack[i]+=1</div><div class="line">        for i in stack:</div><div class="line">            tmp.next=ListNode(i)</div><div class="line">            tmp=tmp.next</div><div class="line">        return res.next</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"># Do it recursively</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def plusOne(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        def add(head):</div><div class="line">            if not head:</div><div class="line">                return 1</div><div class="line">            carry = head.val + add(head.next)</div><div class="line">            head.val = carry % 10</div><div class="line">            return carry / 10</div><div class="line"></div><div class="line">        carry = add(head)</div><div class="line">        if carry == 1:</div><div class="line">            tmp = ListNode(1)</div><div class="line">            tmp.next = head</div><div class="line">            head = tmp</div><div class="line">        return head</div><div class="line">        &apos;&apos;&apos;</div><div class="line"></div><div class="line"># find the first node that is not 9: O(n) time and O(1) space</div><div class="line">class Solution(object):</div><div class="line"></div><div class="line">    def plusOne(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        cur = head</div><div class="line">        notNine = None</div><div class="line">        # find the first node that is not 9</div><div class="line">        while cur:</div><div class="line">            if cur.val != 9:</div><div class="line">                notNine = cur</div><div class="line">            cur = cur.next</div><div class="line">        # plus 1</div><div class="line">        if not notNine:</div><div class="line">            notNine = ListNode(1)</div><div class="line">            notNine.next = head</div><div class="line">            head = notNine</div><div class="line">        else:</div><div class="line">            notNine.val += 1</div><div class="line">        # update digits</div><div class="line">        cur = notNine.next</div><div class="line">        while cur:</div><div class="line">            cur.val = 0</div><div class="line">            cur = cur.next</div><div class="line">        return head</div></pre></td></tr></table></figure>
<h2 id="链表的翻转-206-92-Reverse-Linked-List"><a href="#链表的翻转-206-92-Reverse-Linked-List" class="headerlink" title="链表的翻转(206.92.Reverse Linked List)"></a>链表的翻转(206.92.Reverse Linked List)</h2><h3 id="Problem-I-1"><a href="#Problem-I-1" class="headerlink" title="Problem(I)"></a>Problem(I)</h3><p>Reverse a singly linked list.<br>Hint:<br>A linked list can be reversed either iteratively or recursively. Could you implement both?</p>
<h3 id="初步思考"><a href="#初步思考" class="headerlink" title="初步思考"></a>初步思考</h3><p>每次走到最后一个数，把它放到最前面<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">=&gt;</div><div class="line">5 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4</div><div class="line">4 -&gt; 5 -&gt; 1 -&gt; 2 -&gt; 3</div><div class="line">...</div><div class="line">5 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 1</div></pre></td></tr></table></figure></p>
<p>时间复杂度 $O(n^2)$</p>
<h3 id="头插法"><a href="#头插法" class="headerlink" title="头插法"></a>头插法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">=&gt;</div><div class="line">2 -&gt; 1 -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">3 -&gt; 2 -&gt; 1 -&gt; 4 -&gt; 5</div><div class="line">...</div><div class="line">5 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 1</div></pre></td></tr></table></figure>
<p>时间复杂度 $O(n)$，空间复杂度 $O(1)$</p>
<p>要注意的是，把翻转后的最后一个节点（即原来的第一个节点）指向 Null(None)。<br>访问某个节点 cur.next，要检验 cur 是否为 None.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">=&gt;</div><div class="line">1 -&gt; None -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">2 -&gt; 1 -&gt; None -&gt; 3 -&gt; 4 -&gt; 5</div><div class="line">3 -&gt; 2 -&gt; 1 -&gt; None -&gt; 4 -&gt; 5</div><div class="line">...</div><div class="line">5 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 1 -&gt; None</div></pre></td></tr></table></figure>
<h4 id="迭代法"><a href="#迭代法" class="headerlink" title="迭代法"></a>迭代法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def reverseList(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        dummy = None</div><div class="line">        while head:</div><div class="line">            curr = head</div><div class="line">            head = head.next</div><div class="line">            curr.next= dummy</div><div class="line">            dummy = curr</div><div class="line">        return dummy</div></pre></td></tr></table></figure>
<p>更简单。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def reverseList(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        dummy = None</div><div class="line">        while head:</div><div class="line">            head.next,dummy,head=dummy,head,head.next # head.next=dummy 必须在 head=head.next之前</div><div class="line">        return dummy</div></pre></td></tr></table></figure></p>
<h4 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def reverseList(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        return self._reverse(head)</div><div class="line"></div><div class="line">    def _reverse(self, node, prev=None):</div><div class="line">        if not node:</div><div class="line">            return prev</div><div class="line">        n = node.next</div><div class="line">        node.next = prev</div><div class="line">        return self._reverse(n, node)</div></pre></td></tr></table></figure>
<h3 id="Problem-II-1"><a href="#Problem-II-1" class="headerlink" title="Problem(II)"></a>Problem(II)</h3><p>Reverse a linked list from position m to n. Do it in-place and in one-pass.<br>For example:<br>Given 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL, m = 2 and n = 4,<br>return 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;NULL.<br>Note:<br>Given m, n satisfy the following condition:<br>1 ≤ m ≤ n ≤ length of list.</p>
<p>oup, 最后要返回的是 oup.next 指针<br>dummy 指针，在原来的 m-1 位置<br>cur 指针，在原来的 n 位置<br>reverse，在原来的 m 位置</p>
<p>dummy 指针，空转 m-1 次，找到第 m-1 个节点，即开始翻转的第一个结点的前一个；<br>利用 cur, reverse 按完全翻转的方法翻转[m,n]部分<br>最后修改两个指针，dummy.next 指向 reverse，dummy.next.next 指向第 n+1 个节点。</p>
<h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def reverseBetween(self, head, m, n):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :type m: int</div><div class="line">        :type n: int</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if m==n:</div><div class="line">            return head</div><div class="line"></div><div class="line">        # [1,m-1] nodes</div><div class="line">        oup = ListNode(0)</div><div class="line">        oup.next = head</div><div class="line">        dummy = oup</div><div class="line"></div><div class="line">        for i in range(m-1):</div><div class="line">            dummy = dummy.next</div><div class="line"></div><div class="line">        # reverse [m,n] nodes</div><div class="line">        reverse = None</div><div class="line">        cur = dummy.next</div><div class="line">        for i in range(m,n+1):</div><div class="line">            cur.next,reverse,cur = reverse,cur,cur.next</div><div class="line"></div><div class="line">        # [n,end] nodes</div><div class="line">        dummy.next.next = cur</div><div class="line">        dummy.next = reverse</div><div class="line"></div><div class="line">        return oup.next</div></pre></td></tr></table></figure>
<h2 id="链表改序（143-Reorder-List）"><a href="#链表改序（143-Reorder-List）" class="headerlink" title="链表改序（143. Reorder List）"></a>链表改序（143. Reorder List）</h2><h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem"></a>Problem</h3><p>Given a singly linked list L: L0→L1→…→Ln-1→Ln,<br>reorder it to: L0→Ln→L1→Ln-1→L2→Ln-2→…</p>
<p>You must do this in-place without altering the nodes’ values.</p>
<p>For example,<br>Given {1,2,3,4}, reorder it to {1,4,2,3}.</p>
<h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- Two-pass</div><div class="line">    store all nodes in a stack, create a dummy node, every time while i &lt; len(linked list), linked one node from linked list and one node from stack, and finally deal with odd or even number of length</div><div class="line"></div><div class="line">Followup:</div><div class="line">- One-pass and O(1)</div><div class="line">    - find middle: use fast,slow pointers</div><div class="line">    - reverse second half list: 1-&gt;2-&gt;3-&gt;4-&gt;null =&gt; null&lt;-1&lt;-2&lt;-3&lt;-4, for each node, point the next node of current to previous one, update the previous and next node, do the same thing</div><div class="line">    - merge two lists</div><div class="line">&apos;&apos;&apos;</div><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line">class Solution(object):</div><div class="line">    def reorderList(self,head):</div><div class="line">        if not head:</div><div class="line">            return None</div><div class="line">        # find middle</div><div class="line">        fast,slow=head,head</div><div class="line">        while fast and fast.next:</div><div class="line">            fast=fast.next.next</div><div class="line">            slow=slow.next</div><div class="line">        # reverse list</div><div class="line">        pre=None</div><div class="line">        cur=slow</div><div class="line">        while cur:</div><div class="line">            # one line solution</div><div class="line">            pre, cur.next, cur = node, pre, cur.next</div><div class="line">            next=cur.next</div><div class="line">            cur.next=pre</div><div class="line">            pre=cur</div><div class="line">            cur=next</div><div class="line"></div><div class="line">        # merge list</div><div class="line">        first,second=head,pre</div><div class="line">        while second.next:</div><div class="line">            first.next,first=second,first.next</div><div class="line">            second.next,second=first,second.next</div><div class="line">        return</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def reorderList(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: void Do not return anything, modify head in-place instead.</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head or not head.next: return</div><div class="line">        stack=[]</div><div class="line">        dummy=ListNode(0)</div><div class="line">        dummy.next=head</div><div class="line">        while dummy.next:</div><div class="line">            stack.append(dummy.next)</div><div class="line">            dummy=dummy.next</div><div class="line">        dummy=ListNode(0)</div><div class="line">        dummy.next=head</div><div class="line">        length=len(stack)</div><div class="line">        half_len=len(stack)/2</div><div class="line">        for i in range(half_len):</div><div class="line">            node,dummy=dummy.next.next,dummy.next</div><div class="line">            # dummy.next,dummy=stack.pop(),dummy.next</div><div class="line">            # this is wrong. suppose a=0,a,b=3,a =&gt; a=3,b=0</div><div class="line">            dummy.next=stack.pop()</div><div class="line">            dummy,dummy.next=dummy.next,node</div><div class="line">        if length%2==1:</div><div class="line">            dummy.next.next=None</div><div class="line">        else:</div><div class="line">            dummy.next=None</div><div class="line">            &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h2 id="排序链表去重-82-83-Remove-Duplicates-from-Sorted-List-I-amp-II"><a href="#排序链表去重-82-83-Remove-Duplicates-from-Sorted-List-I-amp-II" class="headerlink" title="排序链表去重(82.83. Remove Duplicates from Sorted List I&amp;II)"></a>排序链表去重(82.83. Remove Duplicates from Sorted List I&amp;II)</h2><h3 id="Problem-I-2"><a href="#Problem-I-2" class="headerlink" title="Problem(I)"></a>Problem(I)</h3><p>Given a sorted linked list, delete all duplicates such that each element appear only once.<br>For example,<br>Given 1-&gt;1-&gt;2, return 1-&gt;2.<br>Given 1-&gt;1-&gt;2-&gt;3-&gt;3, return 1-&gt;2-&gt;3.</p>
<h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def deleteDuplicates(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        dummy = head</div><div class="line">        while head and head.next:</div><div class="line">            if head.val == head.next.val:</div><div class="line">                head.next = head.next.next</div><div class="line">            else:</div><div class="line">                head = head.next</div><div class="line">        return dummy</div></pre></td></tr></table></figure>
<p>注意用到 head.next 一定要判断前一个节点 head 是否为空，同理，head.next.next 判断 head.next 是否为空。</p>
<h3 id="Problem-II-2"><a href="#Problem-II-2" class="headerlink" title="Problem(II)"></a>Problem(II)</h3><p>Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list.<br>For example,<br>Given 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5, return 1-&gt;2-&gt;5.<br>Given 1-&gt;1-&gt;1-&gt;2-&gt;3, return 2-&gt;3.</p>
<h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def deleteDuplicates(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        dummy = ListNode(0)</div><div class="line">        cur = dummy</div><div class="line">        while head:</div><div class="line">            while head.next and head.val == head.next.val:</div><div class="line">                head = head.next</div><div class="line">                if not head.next or head.val != head.next.val:</div><div class="line">                    break</div><div class="line">            else:</div><div class="line">                cur.next = head</div><div class="line">                cur = head</div><div class="line">            head = head.next</div><div class="line">        cur.next = None</div><div class="line">        return dummy.next</div></pre></td></tr></table></figure>
<p>考虑的 bad case: [1,1],[1,1,1]，所以最后的 cur.next = None 不能少，否则还会返回 [1]</p>
<h2 id="链表的合并-21-Merge-Two-Sorted-Lists"><a href="#链表的合并-21-Merge-Two-Sorted-Lists" class="headerlink" title="链表的合并(21. Merge Two Sorted Lists)"></a>链表的合并(21. Merge Two Sorted Lists)</h2><p>快速排序对链表结构适用，然而不是所有排序都适合使用链表存储，如堆排序，不断寻找数组的 n/2 和 n 位置，用链表不大方便。</p>
<h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem"></a>Problem</h3><p>Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.</p>
<h3 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h3><p><strong>Recursive</strong><br>时间复杂度 O(N),空间复杂度 O(1)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">class Solution(object):</div><div class="line">    def mergeTwoLists(self, l1, l2):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type l1: ListNode</div><div class="line">        :type l2: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if l2 is None:</div><div class="line">            return l1</div><div class="line">        if l1 is None:</div><div class="line">            return l2</div><div class="line">        if l1.val &lt; l2.val:</div><div class="line">            head = l1</div><div class="line">            head.next = self.mergeTwoLists(l1.next,l2)</div><div class="line">        else:</div><div class="line">            head = l2</div><div class="line">            head.next = self.mergeTwoLists(l1,l2.next)</div><div class="line">        return head</div></pre></td></tr></table></figure></p>
<h2 id="链表的划分（89-Partition-List）"><a href="#链表的划分（89-Partition-List）" class="headerlink" title="链表的划分（89.Partition List）"></a>链表的划分（89.Partition List）</h2><h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem"></a>Problem</h3><p>Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.<br>You should preserve the original relative order of the nodes in each of the two partitions.<br>For example,<br>Given 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2 and x = 3,<br>return 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5.</p>
<h3 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h3><p>用两个指针 left,right，小于 x 的用 left，大于 x 的用 right，最后连接 left,right<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- create two lists less and greater, for each node in linkedlist, if it is less than x, add to less, else, add to greater, finally merge two lists. Time complexity: O(n), space complexity: O(1)</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def partition(self,head,x):</div><div class="line">        if not head:</div><div class="line">            return None</div><div class="line">        less,greater=ListNode(0),ListNode(0)</div><div class="line">        less_cur,greater_cur=less,greater</div><div class="line">        while head:</div><div class="line">            if head.val&lt;x:</div><div class="line">                less_cur.next=head</div><div class="line">                less_cur=less_cur.next</div><div class="line">            else:</div><div class="line">                greater_cur.next=head</div><div class="line">                greater_cur=greater_cur.next</div><div class="line">            head=head.next</div><div class="line">        less_cur.next=greater.next</div><div class="line">        greater_cur.next=None</div><div class="line">        return less.next</div></pre></td></tr></table></figure></p>
<p>这里要注意的是最后 right 要指向空，不然考虑 case [2,1]，会陷入[1,2,1,2…]的死循环中，因为 right_cur.next 指向了 head，形成了环。</p>
<h2 id="链表的环-141-142-Linked-List-Cycle-I-amp-II"><a href="#链表的环-141-142-Linked-List-Cycle-I-amp-II" class="headerlink" title="链表的环 (141.142.Linked List Cycle I &amp; II)"></a>链表的环 (141.142.Linked List Cycle I &amp; II)</h2><h3 id="Problem-I-3"><a href="#Problem-I-3" class="headerlink" title="Problem(I)"></a>Problem(I)</h3><p>Given a linked list, determine if it has a cycle in it.</p>
<p>Follow up:<br>Can you solve it without using extra space?</p>
<h3 id="Solution-10"><a href="#Solution-10" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- For every node in linkedlist, check if it is in hashset, if not, add it, else, return True. Time complexity: O(n)</div><div class="line"></div><div class="line">Followup:</div><div class="line">- no extra space</div><div class="line">    slow and fast pointers</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">class Solution(object):</div><div class="line">    def hasCycle(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head:</div><div class="line">            return False</div><div class="line">        fast,slow=head,head</div><div class="line">        while fast and fast.next:</div><div class="line">            fast=fast.next.next</div><div class="line">            slow=slow.next</div><div class="line">            if fast==slow:</div><div class="line">                return True</div><div class="line">        return False</div><div class="line"></div><div class="line">    &apos;&apos;&apos;</div><div class="line">    def hasCycle(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: bool</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head:</div><div class="line">            return False</div><div class="line">        nodelist=set()</div><div class="line">        while head.next:</div><div class="line">            if head in nodelist:</div><div class="line">                return True</div><div class="line">            nodelist.add(head)</div><div class="line">            head=head.next</div><div class="line">        return False</div><div class="line">    &apos;&apos;&apos;</div></pre></td></tr></table></figure>
<h3 id="Problem-II-3"><a href="#Problem-II-3" class="headerlink" title="Problem(II)"></a>Problem(II)</h3><p>Given a linked list, return the node where the cycle begins. If there is no cycle, return null.</p>
<p>Note: Do not modify the linked list.</p>
<p>Follow up:<br>Can you solve it without using extra space?</p>
<h3 id="Solution-11"><a href="#Solution-11" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- check if there&apos;s cycle</div><div class="line">- Let&apos;s say, the cycle starts at node u, and the length of the cycle is L, Moreover, after x steps, fast catches slow, and the length between current node and u is p.</div><div class="line">    then we can get for slow pointer, x=u+aL+p,</div><div class="line">    for fast pointer, 2x=u+bL+p,</div><div class="line">    =&gt; 2x-x=(b-a)L =&gt; x=nL.</div><div class="line">    Now, think about that, at step x, if we travels u more steps, where are we?</div><div class="line">    =&gt; u+x=u+nL.</div><div class="line">    =&gt; We are at the start of the cycle, because we have covered the first u nodes once and the entire cycle n times.</div><div class="line"></div><div class="line">Followup:</div><div class="line">- find the length of cycle</div><div class="line">    let slow move foward till meet with fast again</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def detectCycle(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head: return None</div><div class="line">        fast,slow=head,head</div><div class="line">        while True:</div><div class="line">            if not fast or not fast.next:return None</div><div class="line">            fast=fast.next.next</div><div class="line">            slow=slow.next</div><div class="line">            if fast==slow: break</div><div class="line">        while head != fast:</div><div class="line">            head,fast=head.next,fast.next</div><div class="line">        return head</div></pre></td></tr></table></figure>
<h2 id="单链公共节点问题"><a href="#单链公共节点问题" class="headerlink" title="单链公共节点问题"></a>单链公共节点问题</h2><h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem"></a>Problem</h3><p>Write a program to find the node at which the intersection of two singly linked lists begins.</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-09-04%20%E4%B8%8B%E5%8D%885.30.48.png" alt=""><br>假设两个链表长度为 m，n，认为 m &gt; n，两链表的第一个公共节点到尾节点一定是重合的。于是，可以分别遍历两个链表得到链表长度 m,n, 长链表空转 m-n 次，然后两链表齐头并进，同步遍历，直到找到公共节点。时间复杂度为 $O(m+n)$</p>
<p>如果链表存在环，则需要用快慢指针的方式计算公共节点。两个指针，每次分别移动 1 个／2 个节点。</p>
<h3 id="Solution-12"><a href="#Solution-12" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"># Definition for singly-linked list.</div><div class="line"># class ListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.val = x</div><div class="line">#         self.next = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    def getIntersectionNode(self, headA, headB):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head1, head1: ListNode</div><div class="line">        :rtype: ListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        curA,curB = headA,headB</div><div class="line">        lenA,lenB=0,0</div><div class="line">        while curA:</div><div class="line">            lenA += 1</div><div class="line">            curA = curA.next</div><div class="line">        while curB:</div><div class="line">            lenB += 1</div><div class="line">            curB = curB.next</div><div class="line">        curA,curB = headA,headB</div><div class="line">        if lenA &gt; lenB:</div><div class="line">            for i in range(lenA-lenB):</div><div class="line">                curA = curA.next</div><div class="line">        else:</div><div class="line">            for i in range(lenB-lenA):</div><div class="line">                curB = curB.next</div><div class="line">        while curA != curB:</div><div class="line">            curA = curA.next</div><div class="line">            curB = curB.next</div><div class="line">        return curA</div></pre></td></tr></table></figure>
<p>一个从代码层面来讲的简洁版本。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">class Solution:</div><div class="line">    # @param two ListNodes</div><div class="line">    # @return the intersected ListNode</div><div class="line">    def getIntersectionNode(self, headA, headB):</div><div class="line">        if headA is None or headB is None:</div><div class="line">            return None</div><div class="line"></div><div class="line">        pa = headA # 2 pointers</div><div class="line">        pb = headB</div><div class="line"></div><div class="line">        while pa is not pb:</div><div class="line">            # if either pointer hits the end, switch head and continue the second traversal,</div><div class="line">            # if not hit the end, just move on to next</div><div class="line">            pa = headB if pa is None else pa.next</div><div class="line">            pb = headA if pb is None else pb.next</div><div class="line"></div><div class="line">        return pa # only 2 ways to get out of the loop, they meet or the both hit the end=None</div><div class="line"></div><div class="line"># the idea is if you switch head, the possible difference between length would be countered.</div><div class="line"># On the second traversal, they either hit or miss.</div><div class="line"># if they meet, pa or pb would be the node we are looking for,</div><div class="line"># if they didn&apos;t meet, they will hit the end at the same iteration, pa == pb == None, return either one of them is the same,None</div></pre></td></tr></table></figure></p>
<h2 id="链表复制-138-Copy-List-with-Random-Pointer"><a href="#链表复制-138-Copy-List-with-Random-Pointer" class="headerlink" title="链表复制(138. Copy List with Random Pointer)"></a>链表复制(138. Copy List with Random Pointer)</h2><h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem"></a>Problem</h3><p>A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null.</p>
<p>Return a deep copy of the list.</p>
<h3 id="Solution-13"><a href="#Solution-13" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;</div><div class="line">Solution:</div><div class="line">- two-pass: consider normal copy of linkedlist, do similar stuff. first copy nodes and next pointer, then copy random pointer</div><div class="line">            how to find new nodes in second pass? find with O(1) time =&gt; hashmap</div><div class="line">- one-pass: first copy all nodes and store them into hashmap, then connect all nodes. time complexity: O(n), space complexity: O(n)</div><div class="line"></div><div class="line">Followup:</div><div class="line">- without hashmap?</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line"># Definition for singly-linked list with a random pointer.</div><div class="line"># class RandomListNode(object):</div><div class="line">#     def __init__(self, x):</div><div class="line">#         self.label = x</div><div class="line">#         self.next = None</div><div class="line">#         self.random = None</div><div class="line"></div><div class="line">class Solution(object):</div><div class="line">    # copy nodes and next pointer</div><div class="line">    def copyRandomList(self, head):</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        :type head: RandomListNode</div><div class="line">        :rtype: RandomListNode</div><div class="line">        &quot;&quot;&quot;</div><div class="line">        if not head: return None</div><div class="line">        cur=head</div><div class="line">        hashmap=&#123;&#125;</div><div class="line">        while cur:</div><div class="line">            # create nodes</div><div class="line">            if cur not in hashmap:</div><div class="line">                curCopy=RandomListNode(cur.label)</div><div class="line">                hashmap[cur]=curCopy</div><div class="line">            if cur.next and cur.next not in hashmap:</div><div class="line">                nextCopy=RandomListNode(cur.next.label)</div><div class="line">                hashmap[cur.next]=nextCopy</div><div class="line">            if cur.random and cur.random not in hashmap:</div><div class="line">                randomCopy=RandomListNode(cur.random.label)</div><div class="line">                hashmap[cur.random]=randomCopy</div><div class="line">            # connect nodes</div><div class="line">            if cur.next: hashmap[cur].next=hashmap[cur.next]</div><div class="line">            if cur.random: hashmap[cur].random=hashmap[cur.random]</div><div class="line">            # next round</div><div class="line">            cur=cur.next</div><div class="line">        return hashmap[head]</div></pre></td></tr></table></figure>
<blockquote>
<p>参考链接：<br><a href="http://wdxtub.com/2016/01/22/programmer-startline-5/" target="_blank" rel="external">编程起跑线 第 5 课 链表</a><br><a href="https://www.codefellows.org/blog/implementing-a-singly-linked-list-in-python/" target="_blank" rel="external">Implementing a Singly Linked List in Python</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Data Structure </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 链表 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[聊天机器人和智能客服(笔记)]]></title>
      <url>http://www.shuang0420.com/2016/08/20/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%92%8C%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D(%E7%AC%94%E8%AE%B0)/</url>
      <content type="html"><![CDATA[<p>张辉智能客服分享会笔记以及三个月客服机器人实习的感悟。<br><a id="more"></a></p>
<h1 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h1><p>这一部分内容来自微信公众号机器之心《深度|Google Brain研究员详解聊天机器人：面临的深度学习技术问题以及基于TensorFlow的开发实践》，商业模式部分来自公众号大数据文摘《聊天机器人如何盈利？这里有七种可能的商业模式》</p>
<h2 id="模型分类"><a href="#模型分类" class="headerlink" title="模型分类"></a>模型分类</h2><h3 id="基于检索式模型-vs-生成式模型"><a href="#基于检索式模型-vs-生成式模型" class="headerlink" title="基于检索式模型 vs 生成式模型"></a>基于检索式模型 vs 生成式模型</h3><p><strong>基于检索式模型</strong>（更简单）使用了预定义回复库和某种启发式方法来根据输入和语境做出合适的回复。这种启发式方法可以像基于规则的表达式匹配一样简单，也可以像机器学习分类器集一样复杂。这些系统不会产生任何新文本，他们只是从固定的集合中挑选一种回复而已。</p>
<p><strong>生成式模型</strong>（更困难）不依赖于预定义回复库。他们从零开始生成新回复。生成式模型通常基于机器翻译技术，但区别于语言翻译，我们把一个输入「翻译」成一个输出「回复」。</p>
<p>两种方式都有明显的优势和劣势。由于采用人工制作的回复库，基于检索式方法不会犯语法错误。然而它们可能无法处理没见过的情况，因为它们没有合适的预定义回复。同样，这些模型不能重新提到上下文中的实体信息，如先前对话中提到过的名字。生成式模型更「聪明」。它们可以重新提及输入中的实体并带给你一种正和人类对话的感觉。然而，这些模型很难训练，很可能会犯语法错误（特别是长句），而且通常要求大量的训练数据。</p>
<p>基于检索式模型或生成式模型都可以应用深度学习技术，但是相关研究似乎正转向生成式方向。像序列到序列（Sequence to Sequence）这样的深度学习架构是唯一可以适用于产生文本的，并且研究者希望在这个领域取得快速进步。然而，我们仍处于构建工作良好的生成式模型的早期阶段。现在的生产系统更可能是基于检索式的。</p>
<h3 id="长对话-vs-短对话"><a href="#长对话-vs-短对话" class="headerlink" title="长对话 vs 短对话"></a>长对话 vs 短对话</h3><p>对话越长，就越难使它自动化。一方面，短文本对话（更简单）的目标是单独回复一个简单的输入。例如，你可能收到一个用户的特定问题并回复合适的答案。而长对话（更困难）要求你经历多个转折并需要记录说过什么话。客户支持类对话通常是包含多个问题的长对话流。</p>
<h3 id="开域-open-domain-vs-闭域-closeddomain"><a href="#开域-open-domain-vs-闭域-closeddomain" class="headerlink" title="开域(open domain) vs 闭域(closeddomain)"></a>开域(open domain) vs 闭域(closeddomain)</h3><p>在开域（更困难）环境中，用户可以进行任何对话。不需要明确定义的目标或意图。像Twitter 和 Reddit 这种社交媒体网站上的对话通常是开域的——它们可以是任何主题。话题的无限数量和用于产生合理回复的一定量的知识使开域成为了一个艰难的问题。</p>
<p>在闭域（更简单）设定中，因为系统试图达成一个非常明确的目标，可能输入和输出的空间会有所限制。例如客户技术支持或购物助手就属于闭域的范畴。这些系统不需要能谈论政治，它们只需要尽可能高效地完成它们特定的任务。当然，用户仍然可以进行任何他们想要的对话，但是这样的系统不需要能处理所有情况，并且用户也不期望它能处理。</p>
<h2 id="普遍难题"><a href="#普遍难题" class="headerlink" title="普遍难题"></a>普遍难题</h2><p>在构建大部分属于活跃研究领域的会话代理方面存在着许多明显和不明显的难题。</p>
<ol>
<li><p>整合语境<br>为了生成明智的回复，系统可能需要整合语言语境（linguistic context）和物理语境（physical context）。在长对话中，人们记录已经被说过的话和已经交换过的信息。这是结合语言语境的例子。最普遍的方法是将对话嵌入一个向量中，但在长对话上进行这样的操作是很有挑战性的。「使用生成式分层神经网络模型构建端到端对话系统」和「神经网络对话模型的注意与意图」两个实验中都选择了这个研究方向。此外还可能需要整合其它类型的语境数据，例如日期/时间、位置或用户信息。</p>
</li>
<li><p>一致人格<br>当生成回复时，对于语义相同的输入，代理应该生成相同的回答。例如，你想在「你多大了？」和「你的年龄是多少？」上得到同样的回答。这听起来很简单，但是将固定的知识或者「人格」整合进模型是非常困难的研究难题。许多系统学习如何生成语义合理的回复，但是它们没有被训练如何生成语义上一致的回复。这一般是因为它们是基于多个不同用户的数据训练的。「基于个人的神经对话模型」这样的模型是明确的对人格建模的方向上的第一步。</p>
</li>
<li><p>模型评估<br>评估一个对话代理的理想方式是衡量它是否完成了它的任务，例如，在给定对话中解决客户支持问题。但是获取这样的标签成本高昂，因为它们要求人类的判断和评估。某些时候并不存在明确定义的目标，比如开域模型中的情况。通常像 BLEU 这样被用于机器翻译且是基于文本匹配的标准并不能胜任，因为智能的回复可以包括完全不同的单词或短语。实际上，在 How NOT To Evaluate Your Dialogue System: An Empirical Study of UnsupervisedEvaluation Metrics for Dialogue Response Generation 中，研究者发现没有一个通用的度量能真正与人类判断一一对应。</p>
</li>
<li><p>意图和多样性<br>生成式系统的普遍问题是它们往往能生成像「太好了！」或「我不知道」这样的能适用于许多输入情况的普遍回复。谷歌的智能回复（Smart Reply ）早期版本常常用「我爱你」回复一切。一定程度上这是系统根据数据和实际训练目标/算法训练的结果。然而，人类通常使用针对输入的回复并带有意图。因为生成系统（特别是开域系统）是不被训练成有特定意图的，所以它们缺乏这种多样性。</p>
</li>
</ol>
<h2 id="实际工作情况"><a href="#实际工作情况" class="headerlink" title="实际工作情况"></a>实际工作情况</h2><p>纵观现在所有最前沿的研究，我们发展到哪里了？这些系统的实际工作情况如何？让我们再看看我们的分类法。一个基于检索式开域系统显然是不可能实现的，因为你不能人工制作出足够的回复来覆盖所有情况。生成式开域系统几乎是人工通用智能（AGI: Artificial General Intelligence），因为它需要处理所有可能的场景。我们离 AGI 还非常遥远（但是这个领域有许多研究正在进行）。</p>
<p>这就让我们的问题进入了生成式和基于检索式方法都适用的受限的领域。对话越长，语境就越重要，问题就变得越困难。</p>
<p>现任百度首席科学家吴恩达说得很好：</p>
<blockquote>
<p>当今深度学习的价值在你可以获得许多数据的狭窄领域内。有一件事它做不到：进行有意义的对话。存在一些演示，并且如果你仔细挑选这些对话，看起来就像它正在进行有意义的对话，但是如果你亲自尝试，它就会快速偏离轨道。</p>
</blockquote>
<p>许多公司从外包他们的对话业务给人类工作者开始，并承诺一旦他们收集到了足够的数据，他们就会使其「自动化」。只有当他们在一个相当狭窄的领域中这样操作时，这才有可能发生——比如呼叫 Uber 的聊天界面。任何稍微多点开域的事（像销售邮件）就超出了我们现在的能力范围。然而，我们也可以利用这些系统建议和改正回复来辅助人类工作者。这就更符合实际了。</p>
<p>生产系统的语法错误成本很高并会赶走用户。所以，大多数系统可能最好还是使用不会有语法错误和不礼貌回答的基于检索式方法。如果公司能想办法得到大量的数据，那么生成式模型就将是可行的——但是，必须需要其它技术的辅助来防止它们像微软的 Tay 一样脱轨。</p>
<h2 id="商业模式"><a href="#商业模式" class="headerlink" title="商业模式"></a>商业模式</h2><h3 id="商业模式一：BaaS（Bots-as-a-Services，聊天机器人即服务）"><a href="#商业模式一：BaaS（Bots-as-a-Services，聊天机器人即服务）" class="headerlink" title="商业模式一：BaaS（Bots as a Services，聊天机器人即服务）"></a>商业模式一：BaaS（Bots as a Services，聊天机器人即服务）</h3><p>B2B 领域的聊天机器人主要是帮助用户和团队更有效率地开展工作、管理任务或解决团队沟通方面出现的问题。所以 B2B 领域的聊天机器人可能会复制目前已经存在的 B2B 软件领域的商业模式。</p>
<p>对于 B2B 聊天机器人而言，我个人坚信，SaaS 式的免费增值模式可能会成为它最可行的商业模式。对于一些聊天机器人来说，根据你购买的增值服务的不同，那么你能使用到的聊天机器人的功能也是不同的。根据市场调研公司 Forrester 发布的数据，在 2016年，SaaS 和基于云的商业应用服务的营收有望达到 328 亿美元。因此可以想象，B2B 聊天机器人市场的营收应该也不会低。Slack 平台上的大部分应用基本都是基础功能免费，要想使用更高级的功能则需要付费。</p>
<p>SaaS 产品的商业模式是 B2B 领域的客户都非常熟悉的。在此基础上，聊天机器人未来可能会采取更为复杂的定价模式。</p>
<h3 id="商业模式二：聊天机器人-赞助内容和原生内容"><a href="#商业模式二：聊天机器人-赞助内容和原生内容" class="headerlink" title="商业模式二：聊天机器人 + 赞助内容和原生内容"></a>商业模式二：聊天机器人 + 赞助内容和原生内容</h3><p>因为 BuzzFeed、VICE 等的出现，原生内容和原生广告在过去几年里慢慢变成了一个大趋势。原生内容或赞助内容是这样一种模式：媒体公司（如 BuzzFeed）将那些付费品牌商家的赞助内容直接发布到自己的内容频道上，让读者阅读的时候感觉这篇内容好像是媒体自己创作发布的内容而非品牌商发布的广告。下面这个例子就是杜蕾斯在 BuzzFeed 上发布的原生广告内容：</p>
<p>现在设想一下你正在咨询一个烹饪方面的聊天机器人，聊天机器人基于自己的原生功能可能会回答你说，在某些菜谱中，使用香菜代替茴香是可以的，然后会发给你一篇文章《这五道用 ‘是拉差辣椒酱’（一种泰式料理常用的香甜辣椒酱）烹饪的菜，吃后绝对让你惊叹不已》。当然了，这里的是拉差辣椒酱就是赞助内容。</p>
<p>这种原生广告的效果要比传统的横幅广告的效果要好很多。这种类型的广告对品牌商和出版商都有益。未来，你可能会看到这种广告形式将被出版商应用到聊天机器人里。</p>
<h3 id="商业模式三：利用聊天机器人做联盟网络营销"><a href="#商业模式三：利用聊天机器人做联盟网络营销" class="headerlink" title="商业模式三：利用聊天机器人做联盟网络营销"></a>商业模式三：利用聊天机器人做联盟网络营销</h3><p>联盟广告营销最近很多年已经成为一种非常流行的商业化策略。联盟广告营销指的是一种网站 A 为网站 B 设置推广链接，然后从为网站 B 带来的销售额中获取一定提成的一种广告系统。</p>
<p>Forrester 发布的数据报告显示，2016年，美国在联盟广告营销上的花费将达到 45 亿美元。联盟广告营销也可以作为聊天机器人的一种商业模式。举个例子，对于聊天机器人的开发商，你可以开发一款健身方面的聊天机器人，在如何保持健康的身体方面为用户提供专业的建议，然后给用户发送一些附有商业推广链接的健身方面的产品。</p>
<p>购物聊天机器人 Kip 现在已经开始采用这种商业化策略了。用户可以问 Kip “巧克力” 或 “咖啡” 等很多产品方面的问题，然后它会回复一些产品的购买链接，如下图所示：<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%92%8C%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%28%E7%AC%94%E8%AE%B0%29/kip.jpg" class="ful-image" alt="kip.jpg"><br>用户如果通过 Kip 发的链接购买产品，那么 Kip 团队就能从销售收入中收取一定的提成。</p>
<h3 id="商业模式四：用聊天机器人做用户调研"><a href="#商业模式四：用聊天机器人做用户调研" class="headerlink" title="商业模式四：用聊天机器人做用户调研"></a>商业模式四：用聊天机器人做用户调研</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%92%8C%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%28%E7%AC%94%E8%AE%B0%29/DisOrDatBot.jpg" class="ful-image" alt="DisOrDatBot.jpg">
<p>（DisOrDatBot 截图）<br>最近美国总统大选正在如火如荼地进行中，想了解千禧一代都是怎么看待美国总统大选的吗？你可以付费使用一些聊天机器人来进行这方面的调研。虽然我现在还没有看到有人利用聊天机器人做这方面的事，但我觉得如果有专门的 Q&amp;A 聊天机器人来专门帮助人们做调研的话还是非常靠谱的。</p>
<p>目前像 DisOrDatBot 这样的聊天机器人已经开始向用户问一下调研类问题了。想象一下你作为一次活动的策划者，现在正在发愁究竟邀请哪支乐队来你所在的城市进行表演，是邀请电台司令乐队（Radiahead）还是五分钱乐队（Nickelback，加拿大的著名乐队），这时，与其花很多钱请调研公司帮你做调研，还不如使用 DisOrDatBot 进行调研，看你所在城市的用户到底喜欢哪支乐队。</p>
<p>如果你已经开发了一个定期给一群小众用户提供有价值内容的聊天机器人，那些想触及这些用户或是想向这群用户销售产品的公司可能会比较有兴趣通过你的聊天机器人做调研。</p>
<h3 id="商业模式五：将聊天机器人用于潜在客户开发中"><a href="#商业模式五：将聊天机器人用于潜在客户开发中" class="headerlink" title="商业模式五：将聊天机器人用于潜在客户开发中"></a>商业模式五：将聊天机器人用于潜在客户开发中</h3><p>我预测，聊天机器人未来将会被应用到潜在客户开发中，一开始主要利用内容去开发潜在客户。通过在房产所有权、保险、婚礼和理财等方面为用户提供专业的信息、想法和见解，聊天机器人然后将自己获得的这些用户信息给到那些销售相关产品和服务的公司。</p>
<p>举个例子，加入你正在和一个 “生活聊天机器人” 聊天，向聊天机器人咨询一些购房方面的问题，随着聊天的深入，聊天机器人搜集了更多有关你的信息，包括你手头有多少首付资金、你想在哪里购房定居、你是否在职、你购买的是否是你的第一套房产等等。在和聊天机器人建立起一定的关系后，聊天机器人于是问你下面这个问题：</p>
<p>“你是否介意我介绍一家比较适合你的房产公司和你联系？”</p>
<p>在经过你的同意之后，聊天机器人就会将你的信息给到你所在地区的一家房地产公司。这家房产公司第二天就会和你联系沟通你的购房需求。然后这家房产公司会给聊天机器人开发商一定的佣金作为为其开发潜在客户的报酬。</p>
<h3 id="商业模式六：纯粹用于零售销售的聊天机器人"><a href="#商业模式六：纯粹用于零售销售的聊天机器人" class="headerlink" title="商业模式六：纯粹用于零售销售的聊天机器人"></a>商业模式六：纯粹用于零售销售的聊天机器人</h3><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%92%8C%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%28%E7%AC%94%E8%AE%B0%29/hm.jpg" class="ful-image" alt="hm.jpg">
<p>（Kit 上的 H&amp;M 零售聊天机器人）<br>聊天机器人最直接的一种使用场景是商家直接向消费者（B2C）销售产品。想象一下，沃尔玛、Harry’ s、Target、Amazon 和京东等开发了这样一种聊天机器人，你可以问聊天机器人是否销售 “牙膏” 或 “刮胡刀” 等，然后聊天机器人会直接回复你这些商品的购买链接，所以用户在于聊天机器人的交流中就能直接完成商品的购买。</p>
<h3 id="商业模式七：按完成的咨询次数或任务收费"><a href="#商业模式七：按完成的咨询次数或任务收费" class="headerlink" title="商业模式七：按完成的咨询次数或任务收费"></a>商业模式七：按完成的咨询次数或任务收费</h3><p>人们都希望得到专业的好建议，也愿意为好建议付费。随着聊天机器人变得越来越专业和智能，我认为未来人们会在生活中的很多方面都希望得到聊天机器人的建议和帮助，并愿意为这些建议付费。例如，如果你需要生活方面的建议，你可以和 “Oprah 聊天机器人” 交流，如果你需要获得汽车方面的信息，那么你可以和 “机械聊天机器人” 交流，如果你希望获得匿名的婚姻咨询，你可以和 “婚姻聊天机器人” 交流咨询。当然了，为了得到聊天机器人的建议，你是需要支付一定的费用的。</p>
<h2 id="论文推荐"><a href="#论文推荐" class="headerlink" title="论文推荐"></a>论文推荐</h2><p>Neural Responding Machine for Short-Text Conversation (2015-03)<br>A Neural Conversational Model (2015-06)<br>A Neural Network Approach to Context-Sensitive Generation of Conversational Responses (2015-06)<br>The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems (2015-06)<br>Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models (2015-07)<br>A Diversity-Promoting Objective Function for Neural Conversation Models (2015-10)<br>Attention with Intention for a Neural Network Conversation Model (2015-10)<br>Improved Deep Learning Baselines for Ubuntu Corpus Dialogs (2015-10)<br>A Survey of Available Corpora for Building Data-Driven Dialogue Systems (2015-12)<br>Incorporating Copying Mechanism in Sequence-to-Sequence Learning (2016-03)<br>A Persona-Based Neural Conversation Model (2016-03)<br>How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation (2016-03)</p>
<h1 id="客服机器人"><a href="#客服机器人" class="headerlink" title="客服机器人"></a>客服机器人</h1><h2 id="与聊天机器人的不同"><a href="#与聊天机器人的不同" class="headerlink" title="与聊天机器人的不同"></a>与聊天机器人的不同</h2><h3 id="回答问题的范围和方式不同"><a href="#回答问题的范围和方式不同" class="headerlink" title="回答问题的范围和方式不同"></a>回答问题的范围和方式不同</h3><p><strong>聊天机器人：</strong> 发散。可以回答广泛的问题，回答也是发散的。<br><strong>客服机器人：</strong> 收敛。关注焦点在于聚焦的业务范围内有多大的问题处理能力。</p>
<h3 id="使用门槛不同"><a href="#使用门槛不同" class="headerlink" title="使用门槛不同"></a>使用门槛不同</h3><p><strong>聊天机器人：</strong> 不需要给他准备非常多的专业知识，多放一些常识库、问答库和寒暄库的知识，多放一些笑话和段子然后推出去为大家做一些即时的服务就可以了，当然也可以让他继续学习和优化。<br><strong>客服机器人：</strong> 专业知识库。</p>
<h2 id="特质"><a href="#特质" class="headerlink" title="特质"></a>特质</h2><ol>
<li><p>开放问题的收敛能力<br>智能客服机器人应有归纳问题的能力。能够尽量把客户的问题引导到正确的轨道上来。</p>
</li>
<li><p>自学习的能力<br>智能客服机器人应有自学习的能力。它可以根据知识库以及工单进行批量学习，同时能够根据人工服务过程进行单次学习。</p>
</li>
<li><p>人工指导学习的能力<br>我们可以对智能客服机器人进行相应的测试，并可以对其进行调教。</p>
</li>
<li><p>归纳聚类的能力<br>智能客服机器人可以根据客户问的问题与现有知识进行匹配，并能够将匹配度低的问题聚类交由人工来进行回答及维护。人工回答后智能客服机器人能够同步学习，并生成新的问答类型，在下次回到中可以自行解决。</p>
</li>
</ol>
<h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><h3 id="基于业务"><a href="#基于业务" class="headerlink" title="基于业务"></a>基于业务</h3><ol>
<li><p>不靠谱问题<br>–&gt; 收敛，回归。eg.提供相似问题1、2、3<br>–&gt; 无法收敛？转人工</p>
</li>
<li><p>靠谱问题<br>–&gt; 定位，回答</p>
</li>
</ol>
<p>总之一句话：<strong>不离本行不废话！</strong></p>
<h3 id="快速收敛"><a href="#快速收敛" class="headerlink" title="快速收敛"></a>快速收敛</h3><ol>
<li>输入问题的时候就能快速收敛(显示提示列表让用户选择)</li>
<li>对不靠谱问题即时收敛，提供可能的相似问题</li>
</ol>
<h3 id="二八原则"><a href="#二八原则" class="headerlink" title="二八原则"></a>二八原则</h3><p>对于很多企业的客户服务来说，80%的客户问题集中在20%的问题类型里。所以要集中攻克的是 – <strong>不停训练智能客服机器人让它能够应对这10%的问题类型中各种刁钻的问法</strong>。</p>
<h3 id="知识支撑与自学习"><a href="#知识支撑与自学习" class="headerlink" title="知识支撑与自学习"></a>知识支撑与自学习</h3><p>客服机器人至少要有的3个知识库:</p>
<ul>
<li>寒暄库</li>
<li>行业知识库</li>
<li>基于用户体验的知识库</li>
</ul>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="多少情感含量？"><a href="#多少情感含量？" class="headerlink" title="多少情感含量？"></a>多少情感含量？</h3><p>客服机器人与人工服务应该是能进行无缝对接的，因此它的感性层面应该等同于人工客服，在不能回答问题的时候也要彬彬有礼，在寒暄的时候要含蓄内敛不能太过浮夸、奔放。客服机器人掌握的度应该是和人工客服一样，能让客户“如沐春风”的。</p>
<h3 id="是否拒绝谩骂类客户？"><a href="#是否拒绝谩骂类客户？" class="headerlink" title="是否拒绝谩骂类客户？"></a>是否拒绝谩骂类客户？</h3><p>然而接触了这么多用户问题，也发现有一部分人纯粹是来发牢骚甚至是骂人的，并不期待回答，对于这种情况如何处理？个人认为，“客户是上帝”，然而企业也必须尊重员工，不能任由人工客服承受无理的客服的谩骂，既然机器人和人工客服无缝对接，那么机器人也不应该“承受”这些，而应巧妙的拒绝客户，结束对话。</p>
<h3 id="是否上下文关联？"><a href="#是否上下文关联？" class="headerlink" title="是否上下文关联？"></a>是否上下文关联？</h3><p>尽可能在一次交互里解决问题。因为通过多次询问后客户的场景往往是混乱的，难以识别。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-31%20%E4%B8%8B%E5%8D%883.40.23.png" alt=""></p>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>简化的原理、流程<br><strong>输入：客户问题</strong><br>–&gt; 分词、权重<br>–&gt; 语义分析<br>–&gt; 匹配知识<br>–&gt; 要素补足<br>–&gt; 精确匹配<br>–&gt; 回复内容<br>输出: 标准问题</p>
<p>中间可能有的意外是匹配程度低，机器人难以给出准确回复，这时要走另一条线，来发现新的问题，完善服务。<br><strong>精确匹配</strong><br>–&gt; 匹配度低<br>–&gt; 传递到人工<br>–&gt; 人工解答<br>–&gt; 产生新的知识点<br>–&gt; 转交给客服机器人</p>
<h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><p>一代：关键词<br>二代：规则+搜索<br>三代：语义网+自学习</p>
<p>目前停留在二代，三代大多还在实验室阶段。</p>
<p>如何精确匹配？</p>
<ul>
<li><p>局部（文本相关）<br>维度设计： 命中核心词？其他字段？<br>权重设计：如何给各维度分配权重？</p>
</li>
<li><p>全局<br>检索排序：哪个最相关？哪个最有可能相关？</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[实习总结之 sentence embedding]]></title>
      <url>http://www.shuang0420.com/2016/08/05/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%20sentence%20embedding/</url>
      <content type="html"><![CDATA[<p>5-7月的实习，总的来说主要做了三件事情，一是语料的补充，具体表现是通过编写分布式爬虫从各种渠道爬取相关语料，二是特征提取，这一阶段测试了各种模型，doc2vec, lda, LSI, RNN, CNN 等等，试图在 word2vec 词向量基础上，产生质量更高的 sentence embedding,这也是本篇的重点所在。三是新问题发现，主要是通过聚类算法的实验实现。难点在于用什么语料聚类以及如何产生自动化标签。其他的工作也就是打打杂，处理、过滤、验证各种数据，产生训练集、测试集，以及做各种 demo 界面，比较简单。</p>
<p>最终还是发现 doc2vec, lda 产生的 sentence embedding 质量太低，在充足并相关的语料下，用 word2vec 得到的词向量效果还是很不错的，sentence embedding 最终的产生还是通过 RNN。这篇重点是 CNN，因为这是我自己负责的，之后会上一篇 RNN 的版本。</p>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>基本介绍见 <a href="http://www.shuang0420.com/2017/01/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/">卷积神经网络 CNN 笔记</a></p>
<p>来源：Kim Y. Convolutional neural networks for sentence classification[J]. arXiv preprint arXiv:1408.5882, 2014.(<a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="external">http://www.aclweb.org/anthology/D14-1181</a>)</p>
<p>达到了 94.5% 的准确率，并不如 RNN，因为 CNN 模型的 focus 通常是长文本而不是短句，这里的情景是短句 FAQ，这也是效果不如 RNN 的一个原因。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-20%20%E4%B8%8B%E5%8D%884.30.27.png" alt=""></p>
<p>CNN 一些入门介绍见 <a href="http://www.shuang0420.com/2016/06/20/TensorFlow%E5%AE%9E%E6%88%98-MNIST/">CNN 及 TensorFlow 实战 MINST</a></p>
<h2 id="Why-CNN"><a href="#Why-CNN" class="headerlink" title="Why CNN"></a>Why CNN</h2><p>句子可以切分为很多词，词和词组合之后会产生局部语意，句子可以分成若干个有『局部语意』的小块。nlp 里面一个很重要的矛盾就是粒度和语意的矛盾。如果粒度过大，则太稀疏没法玩，粒度过小则意思就不对了。CNN 通过卷积，把每 k 个词组合之后的语意放在一起，得到更为准确的句向量。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>论文提出的输入类型有以下四种：</p>
<ul>
<li>CNN-rand: 所有的 word vector 都是随机初始化的，同时当做训练过程中优化的参数；</li>
<li>CNN-static: 所有的 word vector 直接使用 Word2Vec 工具得到的结果，并且是固定不变的；</li>
<li>CNN-non-static: 所有的 word vector 直接使用 Word2Vec 工具得到的结果，这些 word vector 也当做是可优化的参数，在训练过程中被 Fine tuned；</li>
<li>CNN-multichannel: CNN-static 和 CNN-non-static 的混合版本，即两种类型的输入；</li>
</ul>
<p>一般来说 non-static vector 要优于 static vector。我们实验用的是随机初始化方法，数据 format：</p>
<pre>用户问题\t标准问题id</pre>

<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>输入层是句子中的词语对应的 word vector 依次（从上到下）排列的矩阵，句子有 n 个词，vector的维数为 k ，矩阵就是 n×k。</p>
<h3 id="第一层卷积层"><a href="#第一层卷积层" class="headerlink" title="第一层卷积层"></a>第一层卷积层</h3><p>输入层通过卷积操作得到若干个 Feature Map，卷积窗口的大小为 h×k ，其中 h 表示纵向词语的个数，而 k 表示 word vector 的维数。如果 h=2，就是相邻的两个word做一次卷积。通过这样一个大型的卷积窗口，得到若干个列数为 1 的Feature Map。</p>
<p>卷积之后的结果经过激活函数 f 得到 feature，记为$c_i$。它是由$x_{i:i+h−1}$相邻的 h 个 words 卷积得到的值，再 activation 之后的值，也是当前层的输出。</p>
<p>卷积之后的值：$w⋅x_{i:i+h−1}+b$<br>输出的 feature 值 $c_i=f(w⋅x_{i:i+h−1}+b)$,这就是我们的 sentence embedding<br>窗口大小：h<br>这样之后，一个 n 长度的sentence就有$[x_{1:h}， x_{2:h+1}，x_{3:h+2}，…，x_{n−h+1:n}]$这些 word windows，卷积后的结果就是 c = $[c1,c2,…,c_{n−h+1}]$，维度为(1，n-h+1)<br>然后进行池化 max pooling，选出最重要的 feature。pooling scheme可以根据句子的长度来选择。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>接下来的池化层，才用 Max-over-time Pooling 的方法。从之前一维的 Feature Map 中提出最大的值，论文中解释最大值代表着最重要的信号。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。</p>
<p>最终池化层的输出为各个Feature Map的最大值们，即一个一维的向量。</p>
<h3 id="全连接-Softmax层"><a href="#全连接-Softmax层" class="headerlink" title="全连接 + Softmax层"></a>全连接 + Softmax层</h3><p>池化层的一维向量的输出通过全连接的方式，连接一个Softmax层，Softmax层可根据任务的需要设置（通常反映着最终类别上的概率分布）。</p>
<h2 id="调参建议"><a href="#调参建议" class="headerlink" title="调参建议"></a>调参建议</h2><p>对 Ye Zhang 等人基于 Kim Y 的模型做了大量的调参实验之后的结论：</p>
<ul>
<li>由于模型训练过程中的随机性因素，如随机初始化的权重参数，mini-batch，随机梯度下降优化算法等，造成模型在数据集上的结果有一定的浮动，如准确率(accuracy)能达到 1.5% 的浮动，而AUC 则有 3.4% 的浮动；</li>
<li>词向量是使用 word2vec 还是 GloVe，对实验结果有一定的影响，具体哪个更好依赖于任务本身；</li>
<li>Filter的大小对模型性能有较大的影响，并且Filter的参数应该是可以更新的；</li>
<li>Feature Map的数量也有一定影响，但是需要兼顾模型的训练效率；</li>
<li>1-max pooling的方式已经足够好了，相比于其他的pooling方式而言；</li>
<li>dropout 非常重要，能够带来 2-4% 的效果提升</li>
<li>multichannel 的效果没有预期的好</li>
</ul>
<p>调参建议如下：</p>
<ul>
<li>使用non-static版本的 word2vec 或者 GloVe 要比单纯的 one-hot representation 取得的效果好得多；</li>
<li>为了找到最优的过滤器(Filter)大小，可以使用线性搜索的方法。通常过滤器的大小范围在1-10之间，当然对于长句，使用更大的过滤器也是有必要的；</li>
<li>Feature Map的数量在100-600之间；</li>
<li>可以尽量多尝试激活函数，实验发现 ReLU 和 tanh 两种激活函数表现较佳；</li>
<li>使用简单的 1-max pooling 就已经足够了，可以没必要设置太复杂的 pooling 方式；</li>
<li>当发现增加 Feature Map 的数量使得模型的性能下降时，可以考虑增大正则的力度，如调高dropout的概率；</li>
<li>为了检验模型的性能水平，多次反复的交叉验证是必要的，这可以确保模型的高性能并不是偶然。</li>
</ul>
<p>需要确定的参数：</p>
<ul>
<li>input word vector representations;</li>
<li>filter region size(s);</li>
<li>the number of feature maps;</li>
<li>the activation function(s);</li>
<li>the pooling strategy;</li>
<li>regularization terms (dropout/l2).</li>
</ul>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>tanh</p>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>交叉熵</p>
<h2 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h2><p>还想着能不能用字向量取代词向量，一可以避免分词的麻烦，二可以解决未登录词的问题，这样在测试的时候就很少会遇到Unknown的字向量的问题。另外由于卷积的作用，字向量效果并不一定比词向量差。之后有时间做实验后会更新。</p>
<h1 id="其他实验"><a href="#其他实验" class="headerlink" title="其他实验"></a>其他实验</h1><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>LDA 最大的特点是需要大量的语料，否则数据维度远大于样本数，效果会很差。另外，LDA 适合比较高层次的主题，对稍微细一点的粒度，效果可能就没那么好了。</p>
<h2 id="Doc2vec"><a href="#Doc2vec" class="headerlink" title="Doc2vec"></a>Doc2vec</h2><p>能产生很好的词向量，却不能产生很好的句向量。推测原因是句子太短。</p>
<blockquote>
<p>参考链接<br><a href="http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html#" target="_blank" rel="external">卷积神经网络(CNN)在句子建模上的应用</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Tensorflow </tag>
            
            <tag> doc2vec </tag>
            
            <tag> 句向量 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[项目实战--云计算Twitter Analytics Web Service]]></title>
      <url>http://www.shuang0420.com/2016/07/28/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98--%E4%BA%91%E8%AE%A1%E7%AE%97Twitter%20Analytics%20Web%20Service/</url>
      <content type="html"><![CDATA[<p>CMU 15619 Cloud Computing 的 team project，拖了很久，最终还是鼓起勇气整理了。时隔三个多月，回头来看，找到了更多可以优化的点，本篇内容许多是和同伴讨论整理后得出，借鉴了小土刀的博客，然而现在找不到具体地址了抱歉。<br><a id="more"></a></p>
<h1 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>搭建性能高、可靠性好的 web 服务，前端负责处理请求，处理较高负载（大约每秒至少30000次请求）</p>
<ul>
<li>数据预处理：对大数据集（约1TB）进行预处理，包括过滤脏数据、敏感词、处理停顿词、计算情感分析权重值等，在 hadoop 平台上实现ETL；</li>
<li>后端：保存清理后的 twitter 数据，评估 SQL (MySQL) 和 NoSQL (HBase) 在不同类型的数据、不同大小的数据集下的 performance。</li>
<li>前端：接收并响应不同类型的 HTTP GET 请求</li>
<li>要求：给定预算，最优化性能。</li>
</ul>
<h2 id="四种-query-类型"><a href="#四种-query-类型" class="headerlink" title="四种 query 类型"></a>四种 query 类型</h2><h3 id="Query1"><a href="#Query1" class="headerlink" title="Query1"></a>Query1</h3><p><strong>要求：</strong> 对加密信息进行破译，返回正确信息。<br><strong>难度系数：</strong> 低。<br>这一阶段主要是用来熟悉各种 web 框架，包括 Undertow, Vert.X, Netty 等，比较性能选定合适框架。</p>
<p><strong>Request:</strong></p>
<pre>GET /q1?key=<large_number>&message=<uppercase_ciphertext_message_c></uppercase_ciphertext_message_c></large_number></pre>

<p><strong>Response:</strong></p>
<pre>TEAMID,TEAM_AWS_ACCOUNT_ID\n
yyyy-MM-dd HH:mm:ss\n
[The decrypted message M]\n</pre>

<h3 id="Query2"><a href="#Query2" class="headerlink" title="Query2"></a>Query2</h3><p><strong>要求：</strong> 处理大量的读请求。<br><strong>难度系数：</strong> 高。<br>对数据格式有较高要求，如何处理不同语言，各种特殊符号，如何处理敏感词、停顿词、计算情感分析权重值、过滤脏数据等。<br>对性能有较高要求，如何设计前后端来处理高并发的读请求。</p>
<p><strong>Request:</strong></p>
<pre>GET /q2?userid=uid&hashtag=hashtag</pre>

<p><strong>Response:</strong></p>
<pre>TEAMID,TEAM_AWS_ACCOUNT_ID\n
Sentiment_density1:Tweet_time1:Tweet_id1:Censored_text1\n
Sentiment_density2:Tweet_time2:Tweet_id2:Censored_text2\n
Sentiment_density3:Tweet_time3:Tweet_id3:Censored_text3\n
...</pre>

<h3 id="Query3"><a href="#Query3" class="headerlink" title="Query3"></a>Query3</h3><p><strong>要求：</strong> 给定一定范围的 user id 和 日期，计算关键词出现的次数。处理大量的写请求。<br><strong>难度系数：</strong> 中。<br>依旧是性能的要求。</p>
<p><strong>Request:</strong></p>
<pre>GET/q3?start_date=yyyy-mm-dd&end_date=yyyy-mm-dd&start_userid=uid&end_userid=uid&words=w1,w2,w3</pre>

<p><strong>Response:</strong></p>
<pre>TEAMID,TEAM_AWS_ACCOUNT_ID\n
w1:count1\n
w2:count2\n
w3:count3\n</pre>


<h3 id="Query4"><a href="#Query4" class="headerlink" title="Query4"></a>Query4</h3><p><strong>要求：</strong> 处理高并发读写请求，有一致性要求。<br><strong>难度系数：</strong> 高。</p>
<p><strong>Request:</strong><br><strong>GET:</strong></p>
<pre>q4?tweetid=<tweetid>&op=set&seq=<sequence number="">&fields=<comma separated="" list="" of="" fields="">&payload=<comma separated="" list="" of="" base64*="" encoded="" fields=""></comma></comma></sequence></tweetid></pre>

<p><strong>SET:</strong></p>
<pre>q4?tweetid=<tweetid>&op=set&seq=<sequence number="">&fields=<comma separated="" list="" of="" fields="">&payload=<comma separated="" list="" of="" base64*="" encoded="" fields=""></comma></comma></sequence></tweetid></pre>

<p><strong>Response:</strong><br><strong>GET:</strong></p>
<pre>q4?tweetid=<tweetid>&op=get&seq=<sequence number="">&fields=<comma separated="" list="" of="" fields="">&payload=<empty></empty></comma></sequence></tweetid></pre>

<p><strong>SET：</strong></p>
<pre>TEAMID,TEAM_AWS_ACCOUNT_ID\n
success\n</pre>


<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>这篇博客是针对 query 2 进行的反思，进一步明确应用场景。</p>
<ul>
<li>数据是 5100W 条左右带 tag 的 tweet</li>
<li>只有读请求，每次需要返回指定用户用指定 tag 发送的 tweet</li>
<li>有一定的预算限制（不能任意开机器来凑性能）</li>
<li>前端使用 Undertow，后端是部署在 Amazon 的 MySQL 和 Hbase</li>
</ul>
<p>充分理解应用场景非常非常非常重要，至少需要明确服务具体接收的请求的格式和具体需要返回的内容是什么；是偏向读还是偏向写，还是读写比较均衡；数据量大概是多少…之后才能进行针对性的优化设计。</p>
<h1 id="通用优化"><a href="#通用优化" class="headerlink" title="通用优化"></a>通用优化</h1><p>纵观整个 Request-response 流程，分以下几个步骤：<br></p>
<p><strong>a.    Load Generator to Load Balancer (if any, else merge with b.)<br>b.    Load Balancer to Web Service<br>c.    Parsing request<br>d.    Web Service to DB<br>e.    At DB (execution)<br>f.    DB to Web Service<br>g.    Parsing DB response<br>h.    Web Service to LB<br>i.    LB to LG<br></strong></p>
<p>i 部分我们不需要考虑，因为这个场景并没有用任何渲染引擎，就是单纯返回一段数据而已；c 部分设涉及了解密算法，优化从代码层面入手。数据库（e）与网络传输（b、d、f、h）部分，是主要的瓶颈所在，我们一点一点来分析。</p>
<h2 id="网络传输优化"><a href="#网络传输优化" class="headerlink" title="网络传输优化"></a>网络传输优化</h2><p>b、d、f、h 部分，实际就是网络部分优化，b、h 涉及 front-end，d、f 涉及 back-end。对后端而言，因为是 <strong>只读操作，所以不需要考虑一致性问题</strong>，可以做的是</p>
<ol>
<li><p>努力增加并发数</p>
<ul>
<li>使用 ELB 增加多台前端，多台机器并发请求；</li>
<li>同理，多台后端分发数据库请求。</li>
<li>每台机器增加线程数（当然要在内存的允许范围内），但是加到一定程度也就足够了（毕竟带宽是有限的）；</li>
</ul>
</li>
<li><p>减少每次传输所需要的带宽<br>如在后端对数据进行压缩，在前端进行解压缩，以减少数据传输所需带宽。</p>
</li>
</ol>
<p>这一部分优化的另一个方向是设计缓存，减少对后台的请求。这里的缓存不是说数据库的缓存查询，而是前端对 response 的缓存，目的是对一些请求不用查询数据库就能返回 response，采用的方式通常是 temporal and spatial locality。</p>
<p>有缓存，那么肯定就有预热，预热的重要性在于，把常用的记录缓存下来，为了多一些的缓存，可以是开一个内存优化的机器，比其他系列多一倍内存。</p>
<p>对于 g 的优化，很简单，联系业务场景，读比较多，因此可以对数据进行预处理后（整理成 response 的格式）再存入数据库，用空间换时间。</p>
<h2 id="数据库优化"><a href="#数据库优化" class="headerlink" title="数据库优化"></a>数据库优化</h2><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><ol>
<li><p>选择合适的存储引擎<br>在 MySQL 中有两个存储引擎 MyISAM 和 InnoDB，每个引擎都有利有弊。<br>MyISAM 偏好读操作。适合于需要大量查询的应用，对于有大量写操作并不是很好。update一个字段，整个表都会被锁起来，而其他进程包括读进程都无法操作。MyISAM 对于SELECT COUNT(*) 这类的计算是超快无比的。<br>InnoDB 偏好写操作。是一个非常复杂的存储引擎，对于一些小的应用，它会比 MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务。</p>
</li>
<li><p>sharding and replication<br>sharding 是对数据分区存储在不同数据库里，需要考虑的是怎么分区，需要设定规则，可能导致的问题是请求不均衡；<br>replication 比较简单，同一份数据多备份几分，最简单采用 round-robin 分配请求就可以了。</p>
</li>
<li><p>建立 index<br>根据使用频率决定哪些字段需要建立索引，选择经常作为连接条件、筛选条件、聚合查询、排序的字段作为索引的候选字段。</p>
</li>
<li><p>字段设计<br>尽量不要允许 NULL，除非必要，可以用 NOT NULL+DEFAULT 代替。<br>少用TEXT和IMAGE，二进制字段的读写是比较慢的，而且，读取的方法也不多，大部分情况下最好不用。<br>另外，mysql有一个analyse query的功能，可以来帮助你加快速度，比如数据类型的调整，不过这是建立在数据量很大的情况下</p>
</li>
<li><p>查询语句<br>横向来看，不要写 SELECT *的语句，而是选择需要的字段；纵向来看，合理写 WHERE 子句，不要写没有WHERE的SQL语句；<br>开启查询缓存；</p>
</li>
<li><p>参数配置<br>配置文件 /etc/my.cnf</p>
<ul>
<li><p>max_connections  默认的151，可以修改为3000（750M）<br>max_connections 是指 MySql 的最大连接数，如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，当然这建立在机器能支撑的情况下，因为如果连接数越多，介于MySql会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。可以过’conn%’通配符查看当前状态的连接数量，以定夺该值的大小。MySQL服务器允许的最大连接数16384；查看系统当前最大连接数 show variables like ‘max_connections’;</p>
</li>
<li><p>thread_concurrency  应该设定为CPU核数的2倍<br>thread_concurrency 的值的正确与否, 对mysql的性能影响很大, 在多个cpu(或多核)的情况下，错误设置了 thread_concurrency 的值, 会导致mysql不能充分利用多cpu(或多核), 出现同一时刻只能一个cpu(或核)在工作的情况。</p>
</li>
<li><p>back_log   默认的50，可以修改为500.（每个连接256kb,占用：125M）<br>back_log 值指出在 MySQL 暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到 max_connections 时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过 back_log，将不被授予连接资源。<br>back_log 值不能超过TCP/IP连接的侦听队列的大小。若超过则无效，查看当前系统的TCP/IP连接的侦听队列的大小命令：cat /proc/sys/net/ipv4/tcp_max_syn_backlog。对于 Linux 系统推荐设置为小于 512 的整数。<br>show variables like ‘back_log’; 查看当前数量</p>
</li>
</ul>
</li>
</ol>
<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><ol>
<li><p>Rowkey 设计<br>Rowkey 一定要尽量短 （如：时间用时间戳整数表示、编码压缩）</p>
</li>
<li><p>key-value 的设计<br>把一些重要的筛选信息左移到合适的位置，从而在不改变数据量的情况下，提高查询性能，尽量把查询维度或信息存储在行健中，因为它筛选数据的效率最高。<br><br>理由：HBase 的 Rowkey 是数据行的唯一标识，必须通过它进行数据行访问，目前有三种方式，单行键访问、行键范围访问、全表扫描访问。数据按行键的方式排序存储，依次按位比较，数值较大的排列在后，例如 int 方式的排序：1，10，100，11，12，2，20…，906，…。</p>
</li>
<li><p>增加数据节点<br>Hbase 在行方向上水平划分成 N 个 Region，每个表一开始只有一个 Region，数据量增多，Region 自动分裂为两个，不同 Region 分布在不同 Server 上，但同一个不会拆分到不同 Server。一个 region 只能由一个服务器管理，所以总是添加到同一个 region 上，会造成读写热点，从而使集群性能下降。解决方法，比如我们有9台服务器，那么我们就把0-9均匀加到行健前缀，这样就会被平均的分到不同的 region 服务器上了，好处是，因为相连的数据都分布到不同的服务器上了，用户可以多线程并行的读取数据，这样查询的吞吐量会提高。</p>
</li>
<li><p>参数配置</p>
<ul>
<li><p>分配合适的内存给 RegionServer 服务<br>在不影响其他服务的情况下，越大越好。例如在 HBase 的 conf 目录下的 hbase-env.sh 的最后添加 export HBASE_REGIONSERVER_OPTS=”-Xmx16000m $HBASE_REGIONSERVER_OPTS”<br>其中 16000m 为分配给 RegionServer 的内存大小。</p>
</li>
<li><p>RegionServer 的请求处理 IO 线程数<br>较少的 IO 线程适用于处理单次请求内存消耗较高的 Big Put 场景 (大容量单次 Put 或设置了较大 cache 的 Scan，均属于 Big Put) 或 ReigonServer 的内存比较紧张的场景。<br>较多的 IO 线程，适用于单次请求内存消耗低，TPS 要求 (每秒事务处理量 (TransactionPerSecond)) 非常高的场景。设置该值的时候，以监控内存为主要参考。<br>在 hbase-site.xml 配置文件中配置项为 hbase.regionserver.handler.count。<br>200</p>
</li>
<li><p>调整 Block Cache<br>hfile.block.cache.size：RS的block cache的内存大小限制，默认值0.25，在偏向读的业务中，可以适当调大该值，具体配置时需试hbase集群服务的业务特征，结合memstore的内存占比进行综合考虑。</p>
</li>
</ul>
</li>
</ol>
<h1 id="进一步优化-–-架构设计"><a href="#进一步优化-–-架构设计" class="headerlink" title="进一步优化 – 架构设计"></a>进一步优化 – 架构设计</h1><p>上一部分的优化并不能解决所有问题，至少不能解决多少 front-end 机器，多少 back-end 机器。进一步的优化更多的是依靠实验、依靠监控和日志分析。</p>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>给定预算，优化性能。<br><br>一般思路：auto-scaling 方式，在高峰期增加机器，低谷期减少机器</p>
<h2 id="困难"><a href="#困难" class="headerlink" title="困难"></a>困难</h2><p>模拟 2 分钟 ＝ 现实 1 小时    –&gt; 30 倍缩放<br><br>现实 2 小时的高峰期对应只有 4 分钟，然而坑爹的是！！！aws 申请机器到使用，有 3-5 分钟的延迟，这个延迟并不会被缩放，也就是说你看到高峰立即响应、增加机器，等到机器投入使用了，高峰就过了。。。<br><br>所以，要么提前 3-5 分钟预测到高峰期，要么，简单粗暴开够机器，等着。</p>
<p>怎么预测？可以通过观察监控数据，拟合流量曲线。然而设想是美好的，现实是残酷的。模拟实验有时会让服务器假装『挂掉』，这样有一段时间就无法处理任何请求，所以……所以难以拟合。</p>
<p>能做的，只有设定 baseline，调整参数（前后端各有几台机器），不断实验，找最优解了，这里的原则是 <strong>充分利用硬件资源。</strong> 在预算条件下，对每台机器，其 CPU，内存，带宽等资源都要尽可能的使用，如果资源不平衡，就说明钱没有花在刀刃上，可以考虑更换不同类型的机器，Amazon 提供了『通用』，『内存优化』和『计算优化』这几种不同的机器，可以根据监控的数据，根据前后端不同的任务来决定具体使用什么类型的机器。</p>
<h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><p>怎么调整参数？答案就是看监控。<br>重要指标：<br><strong>CPU / 内存 / 网络</strong><br>从这三个角度的数据观察规律，或者用 aws 的 cloud watch 设定一些阈值，设置自动提醒，当然也可以自己写脚本，省钱！</p>
<h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><p>一般来说，不同的 web 服务，用户的请求模式总体来说是有一定规律的。对于 Twitter 数据的分析，就有热门/冷门的用户/hashtag/单词/时间段（比方说有重大事件发生的日子，tweet 的数量可能会更多）<br></p>
<p>我们应该根据具体的需求，通过统计大致了解数据分布。比方说其中一个请求是返回某用户包含某 hashtag 的 tweet，那么我们最好需要了解哪些用户热门，哪些 hashtag 热门，然后根据这些特点来设计数据库结构、设计缓存。就 Hbase 而言，可以根据这些日志，进行数据库中不同 region 在不同 regionserver 的平衡，充分利用 HBase 的能力。</p>
<h1 id="设计举例"><a href="#设计举例" class="headerlink" title="设计举例"></a>设计举例</h1><p>前端使用 ELB(负载均衡) + 2 台机器，后端使用 1(master) + 3(slave) 的模式可能是最科学的，这样可以尽可能得减轻前端单机压力。</p>
<blockquote>
<p>参考链接：<br><a href="http://coolshell.cn/articles/1846.html" target="_blank" rel="external">MySQL性能优化的最佳20+条经验</a><br><a href="http://5434718.blog.51cto.com/5424718/1207526" target="_blank" rel="external">MySQL性能优化之参数配置</a><br><a href="https://www.ibm.com/developerworks/cn/java/j-lo-HBase/" target="_blank" rel="external">HBase 数据库检索性能优化策略</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Projects </category>
            
        </categories>
        
        
        <tags>
            
            <tag> mysql </tag>
            
            <tag> hbase </tag>
            
            <tag> webserver </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[项目实战--App Recommender System]]></title>
      <url>http://www.shuang0420.com/2016/07/22/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98--App%20Recommender%20System/</url>
      <content type="html"><![CDATA[<p>该项目从华为应用市场爬取 app 数据，存到 MongoDB 中，再经过推荐算法更新数据，展示到前端。项目综合了之前讲到的所有爬虫技巧，来源自 BitTiger 的组织。项目共分为 <strong>爬虫模块、推荐模块、网站模块</strong> 三部分。<br><a id="more"></a></p>
<h1 id="技术需求文档"><a href="#技术需求文档" class="headerlink" title="技术需求文档"></a>技术需求文档</h1><h2 id="爬虫模块"><a href="#爬虫模块" class="headerlink" title="爬虫模块"></a>爬虫模块</h2><p>从 <a href="http://appstore.huawei.com/more/all" target="_blank" rel="external">http://appstore.huawei.com/more/all</a> 爬取总排行榜所有APP数据</p>
<ul>
<li>Title</li>
<li>AppId</li>
<li>缩略图</li>
<li>介绍</li>
</ul>
<p>从 <a href="http://appstore.huawei.com/topics/" target="_blank" rel="external">http://appstore.huawei.com/topics/</a> 爬取所有专题<br>每个专题包括</p>
<ul>
<li>Title</li>
<li>AppId List</li>
</ul>
<p>每个包含在专题中的APP，都爬取</p>
<ul>
<li>Title</li>
<li>AppId</li>
<li>缩略图</li>
<li>介绍</li>
</ul>
<p>从<a href="http://appstore.huawei.com/" target="_blank" rel="external">http://appstore.huawei.com/</a> 做为种子，抓取所有App信息</p>
<ul>
<li>App List的信息</li>
<li>App 的信息</li>
</ul>
<h2 id="推荐模块"><a href="#推荐模块" class="headerlink" title="推荐模块"></a>推荐模块</h2><p>根据输入的App List，输出与这个List最相关的App List</p>
<ul>
<li>基础数据：AppId List的集合</li>
<li>输入AppId List</li>
<li>输出AppId List</li>
</ul>
<h2 id="网站模块"><a href="#网站模块" class="headerlink" title="网站模块"></a>网站模块</h2><p>首页 - A List of most popular Apps（类似于AppStore的Top Charts）</p>
<ul>
<li>Title</li>
<li>缩略图</li>
<li>介绍的前20个字符</li>
</ul>
<p>详情页</p>
<ul>
<li>Title</li>
<li>缩略图</li>
<li>完整介绍</li>
<li>相关推荐的App List</li>
</ul>
<h1 id="爬虫模块-1"><a href="#爬虫模块-1" class="headerlink" title="爬虫模块"></a>爬虫模块</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Skill</p>
<ul>
<li>Python</li>
<li>scrapy</li>
<li>mongodb</li>
<li>proxy</li>
<li>scrapyjs</li>
</ul>
<p>Performance</p>
<ul>
<li>100 pages/second(vs 30k/second)</li>
</ul>
<h2 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h2><p>要点：</p>
<ol>
<li><p>利用 Proxy 更换 user-agent<br>srcapy 发出请求的 user agent 的默认值为Scrapy／version，和普通浏览器的不同，所以网站就会识别出这不是真的用户而是爬虫，就会屏蔽这些请求。所以我们就使用 proxy 随机模拟了不同的 user agent 来“欺骗”网站，才能成功地爬取页面。这一部分是使用 Scrapy middleware 实现的。<br><a href="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/">博客</a></p>
</li>
<li><p>利用 scrapy-redis 分布式架构加快爬取速度<br><a href="http://www.shuang0420.com/2016/06/17/爬虫总结-四-分布式爬虫/">博客</a></p>
</li>
<li><p>怎样获取更多数据</p>
<ul>
<li>通过相似 app</li>
<li>通过搜索</li>
<li>扩展链接</li>
</ul>
</li>
</ol>
<h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><p>涉及到 ‘下一页’ 的爬取，见<a href="http://www.shuang0420.com/2016/06/20/爬虫总结-五-其他技巧/">爬虫总结-五-其他技巧</a> 使用 scrapy-splash 部分的实例。</p>
<h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><p>appstore.dat 存储基本信息，格式</p>
<pre>id \t title \t intro</pre>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">C5683	微信	华为手机服务:激活华为会员，立享精彩权益！ 更新到手机服务最新版本、免费激活会员，立享高级专属售后服务特权、VIP客服热线，还有160G免费空间、海量衣食住行礼包、华为商城优惠券等你抢！ “手机服务”为华为终端用户提供一站式售后服务，集成华为会员、在线寄修、预约维修、在线客服等十多项丰富的在线服务，旨在为亿万华为终端用户提供优质、省心、快捷的服务。 主要</div><div class="line">C262	去哪儿旅行	旅游 旅游，去哪儿 去哪儿，酒店 酒店，机票 机票、火车票火车票，门票门票，度假度假，线路线路，快到碗里来！ 去哪儿旅行—总有你要的低价！  作为中国领先的无线和在线旅游平台，去哪儿旅行支持用户低价购买近60万条国内国际航线，42万家国内酒店、16万家国际酒店；超100万条国内外度假线路和2万种景点门票；享受国内160座城市，以及国外86座城市的</div><div class="line">C10059090	华为游戏中心	华为游戏中心是华为公司提供的安卓游戏下载平台，所有游戏都经过专业的测评团队层层检测，为您推荐最安全质量最高的游戏内容， 目前我们有20万游戏聚众平台，个性化专题定制，和丰富多元的游戏礼包，还有游戏论坛供大家互动，华为游戏中心不单服务所有华为用户，还支持国内主流安卓机型。 新版本特性： 1、新增   资讯频道，最新热点、最强攻略、最全视频，完</div><div class="line">C57236	今日头条	今天，看今日头条 今日头条，超过 4 亿用户选择的新闻资讯 App ！ 单用户每日使用时长超过60分钟 每天社交平台分享量达550万次 颠覆传统阅读——人“搜索”资讯的模式，运用大数据算法，精准推荐你感兴趣的内容，从此不受冗杂信息困扰，畅游个性化信息海洋，让你的阅读更加有用高效。   【海量内容源】 聚合超过5000</div><div class="line">C10217244	华为钱包	华为钱包:1.“红酒”频道，精选莫塞尔多款进口红酒，享受华为特惠价格，让你足不出户品尝世界各地的葡萄佳酿！  2. “钱包”有生活服务、花币中心、红酒等频道，聚合优质服务，提供超值体验。  3. NFC线下支付，集手机支付和线下刷卡为一体，享受安全快捷的支付体验，省去了随身携带钱包、现金和银行卡的麻烦。（NFC支付功能当前支持荣耀V8全网通版、Mate S</div></pre></td></tr></table></figure>
<p>appstore_re.dat 存储推荐信息，格式</p>
<pre>id \t url \t title \t recommended_appid</pre>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">C5683	http://appstore.huawei.com:80/app/C5683	微信	C2543:QQ同步助手,C9319:QQ,C5373:支付宝,C2682:搜狗输入法,C3466:酷狗音乐,C54626:铃声多多,C2861:同花顺炒股,C36902:WiFi万能钥匙,C104688:腾讯新闻,C21976:UC浏览器,C5683:微信,C9319:QQ,C7166:微博,C10085602:花粉俱乐部,C23563:陌陌,C10154337:易信,C37549:189邮箱,C2543:QQ同步助手,C10405418:QQ邮箱,C19185:百度贴吧,</div><div class="line">C262	http://appstore.huawei.com:80/app/C262	去哪儿旅行	C6006:蚂蜂窝自由行,C5157:携程旅行,C30591:途牛旅游,C10770:阿里旅行,C69056:驴妈妈旅游,C10027123:航旅纵横,C12192:同程旅游,C10141560:飞常准,C10017070:高铁管家,C10226376:神州专车,C3382:百度地图,C20911:高德地图,C262:去哪儿旅行,C5157:携程旅行,C10047107:滴滴出行,C5745:天翼导航,C3403:高德导航,C10239309:优步 - Uber,C39196:艺龙旅行,C10043914:和地图,</div><div class="line">C10059090	http://appstore.huawei.com:80/app/C10059090	华为游戏中心	C10217244:华为钱包,C10085602:花粉俱乐部,C10173884:天龙八部3D,C10055832:华为文件管理,C10207207:华为云服务,C10049053:华为商城,C10242764:梦幻西游,C27162:华为应用市场,C10126869:刀塔传奇,C66323:华为备份,C36902:WiFi万能钥匙,C27162:华为应用市场,C10055832:华为文件管理,C66323:华为备份,C21976:UC浏览器,C10067631:华为手机服务,C20679:QQ浏览器,C10207207:华为云服务,C10059090:华为游戏中心,C10132067:华为帐号,</div><div class="line">C57236	http://appstore.huawei.com:80/app/C57236	今日头条	C19168:凤凰新闻,C9147:网易新闻,C2022:搜狐新闻,C104688:腾讯新闻,C31975:墨迹天气,C40238:内涵段子,C3386:汽车之家,C20960:搜狐视频,C3382:百度地图,C2217:我查查,C57236:今日头条,C9147:网易新闻,C2022:搜狐新闻,C104688:腾讯新闻,C179773:爱读掌阅,C149006:塔读文学,C28837:ZAKER,C86189:爱动漫,C2034:天翼阅读,C10084466:咪咕阅读,</div></pre></td></tr></table></figure>
<p>app_info.json 存储 app 所有信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;title&quot;: &quot;华为商城&quot;, &quot;url&quot;: &quot;http://appstore.huawei.com:80/app/C10049053&quot;, &quot;app_id&quot;: &quot;C10049053&quot;, &quot;recommended&quot;: &quot;C10085602:花粉俱乐部,C10217244:华为钱包,C10168550:亲情关怀,C10055832:华为文件管理,C10060708:华为支付,C10067631:华为手机服务,C10132067:华为帐号,C10465316:华为众测,C10207207:华为云服务,C66323:华为备份,C34075:手机淘宝,C10049053:华为商城,C5206:美团团购,C10608:大众点评,C57804:天猫,C20252:手机京东,C9136:唯品会,C10116109:百度糯米,C41277:苏宁易购,C10284106:美丽说,&quot;, &quot;score&quot;: &quot;8&quot;, &quot;thumbnail_url&quot;: &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/ea05c760d27f4b58bd6e8b8e88fdc127.png&quot;, &quot;intro&quot;: &quot;【华为商城客户端 我的掌上购机神器】  预约抢购华为、荣耀新品，华为官方商城，值得信赖！HUAWEI P9、荣耀V8、荣耀畅玩5X、HUAWEI Mate8、荣耀7等众多热门手机及丰富的配件，每月促销活动不断，红包疯狂送，抽奖玩不停！  1、掌上购机神器 每周三移动端专场，手机抢购更便捷； 闹钟提醒，手机抢购不再错过；&lt;b&quot;, &quot;developer&quot;: &quot;华为终端有限公司&quot;&#125;</div><div class="line">&#123;&quot;title&quot;: &quot;微博&quot;, &quot;url&quot;: &quot;http://appstore.huawei.com:80/app/C7166&quot;, &quot;app_id&quot;: &quot;C7166&quot;, &quot;recommended&quot;: &quot;C10084137:新浪微博(G3版…,C6056298:VSCO Cam™,C19185:百度贴吧,C10159988:in,C10047082:知乎,C10204517:MIX,C10125085:哔哩哔哩动画,C10318669:PicsArt,C10231827:快看漫画,C10168892:网易云音乐,C5683:微信,C9319:QQ,C7166:微博,C10085602:花粉俱乐部,C23563:陌陌,C10154337:易信,C37549:189邮箱,C2543:QQ同步助手,C10405418:QQ邮箱,C19185:百度贴吧,&quot;, &quot;score&quot;: &quot;10&quot;, &quot;thumbnail_url&quot;: &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/a885314b6da5496084f009a43226dabf.png&quot;, &quot;intro&quot;: &quot;关于百果园： 百果园，深耕水果行业15年水果连锁品牌，首创水果分级标准。  从2001年百果园创立之日起，信守“一生只做一件事，一心一意做水果”的承诺，15年辛勤耕耘，只为让全天下人享受水果好生活。  目前已经在云南、四川、陕西、山西、山东、江苏、海南等省份以及美国、新西兰等国家建立了100多个水果生产基地，同时配有从果园到门店&quot;, &quot;developer&quot;: &quot;新浪网技术（中国）有限公司&quot;&#125;</div></pre></td></tr></table></figure></p>
<h3 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h3><p>MongoDB 中新建 database appstore，新建 collections user_download_history 和 app_info，导入相应文件。<br>将文件导入 MongoDB。</p>
<pre>
$ mongoimport --db appstore --collection user_download_history --drop --file user_download_history.json
$ mongoimport --db appstore --collection app_info --drop --file app_info.json
</pre>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; db.app_info.find()</div><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;577cbfade677be6b09d8dc2c&quot;), &quot;score&quot; : &quot;8&quot;, &quot;title&quot; : &quot;果汁四溅&quot;, &quot;url&quot; : &quot;http://appstore.huawei.com:80/app/C10204319&quot;, &quot;app_id&quot; : &quot;C10204319&quot;, &quot;thumbnail_url&quot; : &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/f986a241d80d46fcb4fecc1e85121a60.png&quot;, &quot;intro&quot; : &quot;【果汁四溅】年度最“溅”的消除游戏！首发大奖10台iphone6溅到你手！  ★全球50多个国家APP商店推荐★ ★30个国家桌面游戏排名第一★ ★5000万玩家五星好评★  游戏特色： 【七大新玩法，酷炫拽！叼咋天！】  【250关挑战极限！救救水果君】  【果汁爆溅，萌翻全场！】   首发&quot;, &quot;developer&quot; : &quot;深圳市唯变科技开发有限公司&quot; &#125;</div><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;577cbfade677be6b09d8dc2d&quot;), &quot;score&quot; : &quot;9&quot;, &quot;title&quot; : &quot;泡泡龙亚特兰蒂斯&quot;, &quot;url&quot; : &quot;http://appstore.huawei.com:80/app/C10145675&quot;, &quot;app_id&quot; : &quot;C10145675&quot;, &quot;thumbnail_url&quot; : &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/cb3c6ce12b73424990921097fe20a7b1.png&quot;, &quot;intro&quot; : &quot;泡泡龙亚特兰蒂斯:一款令人着迷的泡泡龙游戏。在经典的游戏模式中增加了独有的BOSS战，每个场景都 有独特的守护者等待你的挑战。多种多样的奇趣道具供你使用。打BOSS，秀操作，耍道具让你爱不释手。欢乐之旅由一段美丽的故事带你进入。前所未有的体验，带给你神奇的亚特兰蒂斯之旅。&quot;, &quot;developer&quot; : &quot;深圳市灵游科技有限公司&quot; &#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; db.user_download_history.find()</div><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;5691f793f0fe47e651ba1a52&quot;), &quot;user_id&quot; : 3, &quot;download_history&quot; : [ &quot;C10249215&quot;, &quot;C10221865&quot;, &quot;C10269239&quot;, &quot;C10157957&quot;, &quot;C10148546&quot;, &quot;C10241662&quot;, &quot;C10203747&quot;, &quot;C10144080&quot;, &quot;C10136202&quot;, &quot;C10271994&quot; ] &#125;</div><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;5691f793f0fe47e651ba1a53&quot;), &quot;user_id&quot; : 4, &quot;download_history&quot; : [ &quot;C10026769&quot;, &quot;C10053551&quot;, &quot;C10237091&quot;, &quot;C10141383&quot;, &quot;C10162014&quot;, &quot;C10148546&quot; ] &#125;</div></pre></td></tr></table></figure>
<h1 id="推荐模块-1"><a href="#推荐模块-1" class="headerlink" title="推荐模块"></a>推荐模块</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><p>Skill</p>
<ul>
<li>Python</li>
<li>collaborative-filtering algorithm</li>
<li>cosine_similarity</li>
</ul>
<p>Performance</p>
<ul>
<li>1 second/app(vs with 10ms/app)</li>
</ul>
<h2 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h2><h3 id="协同过滤算法简述"><a href="#协同过滤算法简述" class="headerlink" title="协同过滤算法简述"></a>协同过滤算法简述</h3><h4 id="基于用户的协同过滤算法（User-CF）"><a href="#基于用户的协同过滤算法（User-CF）" class="headerlink" title="基于用户的协同过滤算法（User CF）"></a>基于用户的协同过滤算法（User CF）</h4><p>强调 <strong>把和你有相似爱好的其他的用户的物品推荐给你</strong><br>基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。</p>
<p><strong>过程：</strong></p>
<ol>
<li>将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到和目标用户兴趣相似的用户集合；</li>
<li>找到这个集合中的用户喜欢的，且目标用户没有访问过的物品，计算得到一个排序的物品列表作为推荐。</li>
</ol>
<p><strong>优点和适用场景：</strong></p>
<ul>
<li>可以发现用户感兴趣的热门物品</li>
<li>用户有新行为，不一定造成推荐结果的立即变化</li>
<li>适用于用户较少的场合，否则用户相似度矩阵计算代价很大</li>
<li>适合时效性较强，用户个性化兴趣不太明显的领域</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>数据稀疏性。一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。</li>
<li>算法扩展性。最近邻居算法的计算量随着用户和物品数量的增加而增加，不适合数据量大的情况使用。</li>
<li>对新用户不友好，对新物品友好，因为用户相似度矩阵不能实时计算</li>
<li>很难提供令用户信服的推荐解释</li>
</ul>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-01%20%E4%B8%8B%E5%8D%887.31.02.png" alt=""></p>
<h4 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h4><p>强调 <strong>把和你喜欢的物品相似的物品推荐给你</strong><br>基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度。在京东、天猫上看到「购买了该商品的用户也经常购买的其他商品」，就是主要基于 ItemBasedCF。</p>
<p><strong>过程：</strong></p>
<ol>
<li>基于用户对物品的偏好计算相似度，找到相似的物品；</li>
<li>根据物品的相似度和用户的历史行为预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。</li>
</ol>
<p>因为物品直接的相似性相对比较固定，所以可以预先在线下计算好不同物品之间的相似度，把结果存在表中，当推荐时进行查表，计算用户可能的打分值。</p>
<p><strong>优点和适用场景：</strong></p>
<ul>
<li>可以发现用户潜在的但自己尚未发现的兴趣爱好</li>
<li>有效的进行长尾挖掘</li>
<li>利用用户的历史行为给用户做推荐解释，使用户比较信服</li>
<li>适用于物品数明显小于用户数的场合，否则物品相似度矩阵计算代价很大</li>
<li>适合长尾物品丰富，用户个性化需求强的领域</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对新用户友好，对新物品不友好，因为物品相似度矩阵不需要很强的实时性</li>
</ul>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-01%20%E4%B8%8B%E5%8D%887.31.21.png" alt=""></p>
<p>Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，但这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。</p>
<p>两个例子：</p>
<p>非社交网络：在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。</p>
<p>社交网络：在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。</p>
<p>更多见<a href="http://blog.csdn.net/ygrx/article/details/15501679" target="_blank" rel="external">[推荐算法]基于用户的协同过滤算法</a></p>
<h3 id="项目算法"><a href="#项目算法" class="headerlink" title="项目算法"></a>项目算法</h3><p>采用的是基于物品的协同过滤算法，相似度算法用的是 cosine similarity。通过计算 similarity between a1 and all user download history 来推导 similarity between a1 and all other apps，前提是假定每个 user download history list 里的 app 是相互关联的。看下面的具体例子。</p>
<p>cosine similarity<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-06%20%E4%B8%8B%E5%8D%888.00.56.png" alt=""></p>
<p>similarity between a3 and a5<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-06%20%E4%B8%8B%E5%8D%888.05.07.png" alt=""></p>
<p>a3’s top-5 related apps<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-06%20%E4%B8%8B%E5%8D%888.05.56.png" alt=""></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>首先读取 MongoDB 的数据，过程略。这里主要展示算法实现。<br>cosine similarity<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">@classmethod</div><div class="line">def cosine_similarity(cls, app_list1, app_list2):</div><div class="line">    match_count = cls.__count_match(app_list1, app_list2)</div><div class="line">    return float(match_count) / math.sqrt( len(app_list1) * len(app_list2))</div><div class="line"></div><div class="line">@classmethod</div><div class="line">def __count_match(cls, list1, list2):</div><div class="line">    count = 0</div><div class="line">    for element in list1:</div><div class="line">        if element in list2:</div><div class="line">            count += 1</div><div class="line">    return count</div></pre></td></tr></table></figure></p>
<p>top-5 related app_list1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def calculate_top_5(app, user_download_history):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    cosine_similarity between an App and user&apos;s history</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    #create a dict to store each other app and its similarity to this app</div><div class="line">    app_similarity = collections.defaultdict(float) #&#123;app_id: similarity&#125;</div><div class="line">    for apps in user_download_history:</div><div class="line">        #calculate the similarity</div><div class="line">        similarity = Helper.cosine_similarity([app], apps)</div><div class="line">        # accumluate similarity</div><div class="line">        for other_app in apps:</div><div class="line">            app_similarity[other_app] += similarity</div><div class="line"></div><div class="line">    # There could be app without related apps (not in any download history)</div><div class="line">    if not app in app_similarity:</div><div class="line">        return</div><div class="line"></div><div class="line">    #sort app_similarity dict by value and get the top 5 as recommendation</div><div class="line">    app_similarity.pop(app)</div><div class="line">    sorted_tups = sorted(app_similarity.items(), key=operator.itemgetter(1), reverse=True)#sort by similarity</div><div class="line">    top_5_app = [sorted_tups[0][0], sorted_tups[1][0], sorted_tups[2][0], sorted_tups[3][0], sorted_tups[4][0]]</div><div class="line">    #print(&quot;top_5_app for &quot; + str(app) + &quot;:\t&quot; + str(top_5_app))</div></pre></td></tr></table></figure></p>
<p>最后要更新 MongoDB 中的数据。更新后的数据如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;577cbfade677be6b09d8dc2c&quot;), &quot;score&quot; : &quot;8&quot;, &quot;title&quot; : &quot;果汁四溅&quot;, &quot;url&quot; : &quot;http://appstore.huawei.com:80/app/C10204319&quot;, &quot;app_id&quot; : &quot;C10204319&quot;, &quot;thumbnail_url&quot; : &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/f986a241d80d46fcb4fecc1e85121a60.png&quot;, &quot;intro&quot; : &quot;【果汁四溅】年度最“溅”的消除游戏！首发大奖10台iphone6溅到你手！  ★全球50多个国家APP商店推荐★ ★30个国家桌面游戏排名第一★ ★5000万玩家五星好评★  游戏特色： 【七大新玩法，酷炫拽！叼咋天！】  【250关挑战极限！救救水果君】  【果汁爆溅，萌翻全场！】   首发&quot;, &quot;developer&quot; : &quot;深圳市唯变科技开发有限公司&quot;, &quot;top_5_app&quot; : [ &quot;C10053551&quot;, &quot;C10148546&quot;, &quot;C10141383&quot;, &quot;C10189589&quot;, &quot;C10026769&quot; ] &#125;</div><div class="line">&#123; &quot;_id&quot; : ObjectId(&quot;577cbfade677be6b09d8dc2d&quot;), &quot;score&quot; : &quot;9&quot;, &quot;title&quot; : &quot;泡泡龙亚特兰蒂斯&quot;, &quot;url&quot; : &quot;http://appstore.huawei.com:80/app/C10145675&quot;, &quot;app_id&quot; : &quot;C10145675&quot;, &quot;thumbnail_url&quot; : &quot;http://appimg.hicloud.com/hwmarket/files/application/icon144/cb3c6ce12b73424990921097fe20a7b1.png&quot;, &quot;intro&quot; : &quot;泡泡龙亚特兰蒂斯:一款令人着迷的泡泡龙游戏。在经典的游戏模式中增加了独有的BOSS战，每个场景都 有独特的守护者等待你的挑战。多种多样的奇趣道具供你使用。打BOSS，秀操作，耍道具让你爱不释手。欢乐之旅由一段美丽的故事带你进入。前所未有的体验，带给你神奇的亚特兰蒂斯之旅。&quot;, &quot;developer&quot; : &quot;深圳市灵游科技有限公司&quot;, &quot;top_5_app&quot; : [ &quot;C2217&quot;, &quot;C40224&quot;, &quot;C10196888&quot;, &quot;C10197446&quot;, &quot;C10047107&quot; ] &#125;</div></pre></td></tr></table></figure></p>
<h1 id="网站模块-1"><a href="#网站模块-1" class="headerlink" title="网站模块"></a>网站模块</h1><p>Skill</p>
<ul>
<li>javascript</li>
<li>node.js</li>
<li>meteor</li>
<li>mongodb</li>
</ul>
<p>Performance</p>
<ul>
<li>1k QPS (vs 10k QPS)</li>
</ul>
<p>具体实现见 <a href="http://www.shuang0420.com/2016/07/07/Meteor-App-Recommender-System/">Meteor – App Recommender System</a></p>
]]></content>
      
        <categories>
            
            <category> Projects </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python-cgi + ajax 实现异步响应表单]]></title>
      <url>http://www.shuang0420.com/2016/07/18/socket%20+%20python-cgi%20+%20ajax%20%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E5%93%8D%E5%BA%94%E8%A1%A8%E5%8D%95/</url>
      <content type="html"><![CDATA[<p>简单的界面用于个人 or 公司内部 demo。<br><a id="more"></a></p>
<h1 id="cgi"><a href="#cgi" class="headerlink" title="cgi"></a>cgi</h1><p>CGI 目前由NCSA维护，NCSA定义CGI如下：<br>CGI(Common Gateway Interface),通用网关接口,它是一段程序,运行在服务器上如：HTTP服务器，提供同客户端HTML页面的接口。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%888.23.12.png" alt=""></p>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><p>cgi 实在是…… 太！简！单！太！方！便了！不废话，上个教程。<br><a href="http://www.runoob.com/python/python-cgi.html" target="_blank" rel="external">Python CGI编程</a></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p><a href="http://www.runoob.com/python/python-cgi.html" target="_blank" rel="external">apache cgi 配置教程</a>网上都有，上面的教程也有介绍，值得一提的是，我遵照了若干教程，然而并没能用 mac 自带的 apache 配置 cgi 成功，最后是用 xampp 配置好的，推测是原有的 xampp 对本机的 apache 有改动。</p>
<p>下面记录下 xampp 配置 cgi 过程。<br>在 /Application/XAMPP/etc/httpd.conf 改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;Directory &quot;/Applications/XAMPP/xamppfiles/cgi-bin/&quot;&gt;</div><div class="line">    AllowOverride None</div><div class="line">    Options Indexes FollowSymLinks MultiViews ExecCGI</div><div class="line">    Order allow,deny</div><div class="line">    Allow from all</div><div class="line">&lt;/Directory&gt;</div></pre></td></tr></table></figure></p>
<p>在 /Applications/XAMPP/xamppfiles/apache2/conf/httpd.conf 改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;Directory &quot;/Applications/XAMPP/xamppfiles/apache2/htdocs&quot;&gt;</div><div class="line">    Options Indexes FollowSymLinks MultiViews ExecCGI</div><div class="line">    AllowOverride All</div><div class="line">    Order allow,deny</div><div class="line">    Allow from all</div><div class="line">&lt;/Directory&gt;</div></pre></td></tr></table></figure></p>
<p>重启，把 cgi 代码放到 /Applications/XAMPP/xamppfiles/cgi-bin/ 目录下运行，如果无法运行，改权限</p>
<pre>chmod 755 test.py</pre>

<p>附测试代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div><div class="line"># -*- coding: UTF-8 -*-</div><div class="line"></div><div class="line">print &quot;Content-type:text/html&quot;</div><div class="line">print                               # 空行，告诉服务器结束头部</div><div class="line">print &apos;&lt;html&gt;&apos;</div><div class="line">print &apos;&lt;head&gt;&apos;</div><div class="line">print &apos;&lt;meta charset=&quot;utf-8&quot;&gt;&apos;</div><div class="line">print &apos;&lt;title&gt;Hello Word - 我的第一个 CGI 程序！&lt;/title&gt;&apos;</div><div class="line">print &apos;&lt;/head&gt;&apos;</div><div class="line">print &apos;&lt;body&gt;&apos;</div><div class="line">print &apos;&lt;h2&gt;Hello Word! 我是来自菜鸟教程的第一CGI程序&lt;/h2&gt;&apos;</div><div class="line">print &apos;&lt;/body&gt;&apos;</div><div class="line">print &apos;&lt;/html&gt;&apos;</div></pre></td></tr></table></figure></p>
<p>浏览器打开 <a href="http://localhost/cgi-bin/test.py，" target="_blank" rel="external">http://localhost/cgi-bin/test.py，</a> 测试成功。</p>
<h2 id="cgi-调试"><a href="#cgi-调试" class="headerlink" title="cgi 调试"></a>cgi 调试</h2><p>为了浏览器调试，在 cgi 文件中加上下面的代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import cgitb</div><div class="line">cgitb.enable()</div></pre></td></tr></table></figure></p>
<p>这时，debug信息会显示在浏览器上面，方便调试。生产环境中出于安全的考虑，一般会关掉debug的功能。</p>
<h1 id="ajax"><a href="#ajax" class="headerlink" title="ajax"></a>ajax</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>AJAX 全称 Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。它并非一种新的技术，而是以下几种原有技术的结合体。</p>
<ol>
<li>使用 CSS 和 XHTML 来表示。</li>
<li>使用 DOM 模型来交互和动态显示。</li>
<li>使用 XMLHttpRequest 来和服务器进行异步通信。</li>
<li>使用 javascript 来绑定和调用。</li>
</ol>
<p>通过 AJAX 异步技术，可以在客户端脚本与 web 服务器交互数据的过程中使用 XMLHttpRequest 对象来完成 HTTP 请求(Request)/应答(Response)模型：</p>
<ol>
<li>不需要用户等待服务端响应。在异步派发 XMLHttpRequest 请求后控制权马上就被返回到浏览器。界面不会出现白板，在得到服务器响应之前还可以友好的给出一个加载提示。</li>
<li>不需要重新加载整个页面。为 XMLHttpRequest 注册一个回调函数，待服务器响应到达时，触发回调函数，并且传递所需的少量数据。“按需取数据”也降低了服务器的压力。</li>
<li>不需要使用隐藏或内嵌的框架。在 XHR 对象之前，模拟 Ajax 通信通常使用 hack 手段，如使用隐藏的或内嵌的框架(iframe标签)。</li>
</ol>
<h2 id="重要对象：XMLHttpRequest"><a href="#重要对象：XMLHttpRequest" class="headerlink" title="重要对象：XMLHttpRequest"></a>重要对象：XMLHttpRequest</h2><p>XMLHttpRequest 是一套可以在 Javascript、VbScript、Jscript 等脚本语言中通过http协议传送或接收 XML 及其他数据的一套API。</p>
<h3 id="主要函数-客户端"><a href="#主要函数-客户端" class="headerlink" title="主要函数(客户端)"></a>主要函数(客户端)</h3><ul>
<li><p>open(method,url,async, bstrUser, bstrPassword)<br>规定请求的类型、URL 以及是否异步处理请求。</p>
</li>
<li><p>setRequestHeader(name,value)<br>自定义HTTP头部信息。需在open()方法之后和send()之前调用，才能成功发送请求头部信息。<br>onreadystatchange,</p>
</li>
<li><p>send(string)<br>将请求发送到服务器。参数string仅用于POST请求；对于GET请求的参数写在url后面，所以string参数传递null。</p>
</li>
</ul>
<h2 id="简单实例"><a href="#简单实例" class="headerlink" title="简单实例"></a>简单实例</h2><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">var myAjax = &#123;</div><div class="line">    // XMLHttpRequest IE7+, Firefox, Chrome, Opera, Safari ；  ActiveXObject IE6, IE5</div><div class="line">    xhr: window.XMLHttpRequest ? new XMLHttpRequest() : new ActiveXObject(&apos;Microsoft.XMLHTTP&apos;),</div><div class="line">    get: function (url, callback) &#123;</div><div class="line">        this.xhr.open(&apos;get&apos;, url);</div><div class="line">        this.onreadystatechange(callback, this.xhr);</div><div class="line">        this.xhr.send(null);        </div><div class="line">    &#125;,</div><div class="line">    post: function (url, data, callback) &#123;</div><div class="line">        this.xhr.open(&apos;post&apos;, url);</div><div class="line">        this.xhr.setRequestHeader(&apos;Content-Type&apos;, &apos;application/x-www-form-urlencoded&apos;);</div><div class="line">        this.onreadystatechange(callback, this.xhr);</div><div class="line">        this.xhr.send(data);</div><div class="line">    &#125;,</div><div class="line">    onreadystatechange: function (func, _xhr) &#123;</div><div class="line">        _xhr.onreadystatechange = function () &#123;</div><div class="line">            if (_xhr.readyState == 4) &#123;</div><div class="line">                if (_xhr.status == 200) &#123;</div><div class="line">                    func(_xhr.responseText);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$(&apos;#btn_nowTime1&apos;).bind(&apos;click&apos;, null</div><div class="line">    , function () &#123;</div><div class="line">        myAjax.post(&apos;AjaxHandler.ashx&apos;, &apos;func=GetServerTime&apos;</div><div class="line">            , function (data) &#123;</div><div class="line">                if (data)</div><div class="line">                    alert(data);</div><div class="line">            &#125;</div><div class="line">        );</div><div class="line">    &#125;);</div></pre></td></tr></table></figure>
<h1 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h1><p>Socket 是任何一种计算机网络通讯中最基础的内容。例如当你在浏览器地址栏中输入 www.oschina.net 时，你会打开一个套接字，然后连接到 www.oschina.net 并读取响应的页面然后然后显示出来。而其他一些聊天客户端如 gtalk 和 skype 也是类似。任何网络通讯都是通过 Socket 来完成的。<br>下面简单介绍下 python 的 socket 编程。</p>
<h2 id="server"><a href="#server" class="headerlink" title="server"></a>server</h2><h3 id="主要函数"><a href="#主要函数" class="headerlink" title="主要函数"></a>主要函数</h3><ul>
<li><p>s.bind(address)<br>将 socket 绑定到地址, 在 AF_INET 下,以元组（host,port）的形式表示地址.</p>
</li>
<li><p>s.listen(backlog)<br>开始监听 TCP 传入连接。backlog 指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。</p>
</li>
<li><p>s.accept()<br>接受TCP连接并返回（conn,address）,其中 conn 是新的 socket 对象，可以用来接收和发送数据。address是连接客户端的地址。</p>
</li>
</ul>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ol>
<li><p>创建 socket</p>
<pre>socket.socket(socket.AF_INET,socket.SOCK_STREAM) </pre>
</li>
<li><p>绑定到本地IP与端口</p>
<pre>s.bind()</pre>
</li>
<li><p>开始监听连接</p>
<pre>s.listen()</pre>
</li>
<li><p>进入循环，不断接受客户端的连接请求              </p>
<pre>s.accept()</pre>
</li>
<li><p>然后接收传来的数据，并发送给对方数据         </p>
<pre>
# 接收数据
s.recv()
# 发送数据
s.sendall()</pre>
</li>
<li><p>传输完毕后，关闭socket                     </p>
<pre>s.close()</pre>


</li>
</ol>
<h2 id="client"><a href="#client" class="headerlink" title="client"></a>client</h2><h3 id="主要函数-1"><a href="#主要函数-1" class="headerlink" title="主要函数"></a>主要函数</h3><ul>
<li><p>s.connect(address)<br>连接到 address 处的 socket。一般 address 的格式为元组（hostname,port），如果连接出错，返回 socket.error 错误。</p>
</li>
<li><p>s.connect_ex(adddress)<br>功能与 connect(address) 相同，但是成功返回0，失败返回errno的值。</p>
</li>
</ul>
<h3 id="过程-1"><a href="#过程-1" class="headerlink" title="过程"></a>过程</h3><ol>
<li><p>创建 socket，连接远端地址</p>
<pre>socket.socket(socket.AF_INET,socket.SOCK_STREAM)  
s.connect()</pre>
</li>
<li><p>连接后发送数据和接收数据          </p>
<pre>s.sendall()
s.recv()</pre>
</li>
<li><p>传输完毕后，关闭 socket         </p>
<pre>s.close()</pre>

</li>
</ol>
<h2 id="server-client-主要公共函数"><a href="#server-client-主要公共函数" class="headerlink" title="server, client 主要公共函数"></a>server, client 主要公共函数</h2><ul>
<li><p>s.recv(bufsize[,flag])<br>接受 TCP socket 的数据。数据以字符串形式返回，bufsize 指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。</p>
</li>
<li><p>s.send(string[,flag])<br>发送TCP数据。将string中的数据发送到连接的 socket 。返回值是要发送的字节数量，该数量可能小于 string 的字节大小。</p>
</li>
<li><p>s.sendall(string[,flag])<br>完整发送TCP数据。将string中的数据发送到连接的 socket ，但在返回之前会尝试发送所有数据。成功返回 None，失败则抛出异常。</p>
</li>
<li><p>s.close()<br>关闭 socket。</p>
</li>
</ul>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>实现一个简单的 socket，用户输入问题，刷新页面响应。</p>
<h2 id="server-完整代码"><a href="#server-完整代码" class="headerlink" title="server 完整代码"></a>server 完整代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div><div class="line"># -*- coding: UTF-8 -*-</div><div class="line">import socket</div><div class="line">import sys</div><div class="line"></div><div class="line">HOST = &apos;localhost&apos;   # Symbolic name meaning all available interfaces</div><div class="line">PORT = 9000 # Arbitrary non-privileged port</div><div class="line"></div><div class="line"># create tcp socket</div><div class="line">s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</div><div class="line">print &apos;Socket created&apos;</div><div class="line"></div><div class="line">try:</div><div class="line">    s.bind((HOST, PORT))</div><div class="line">except socket.error , msg:</div><div class="line">    print &apos;Bind failed. Error Code : &apos; + str(msg[0]) + &apos; Message &apos; + msg[1]</div><div class="line">    sys.exit()</div><div class="line"></div><div class="line">print &apos;Socket bind complete&apos;</div><div class="line"></div><div class="line"></div><div class="line">s.listen(10)</div><div class="line">print &apos;Socket now listening&apos;</div><div class="line"></div><div class="line"></div><div class="line">while 1:</div><div class="line">    #wait to accept a connection - blocking call</div><div class="line">    conn, addr = s.accept()</div><div class="line">    print &apos;Connected with &apos; + addr[0] + &apos;:&apos; + str(addr[1])</div><div class="line">    data = conn.recv(1024)</div><div class="line">    reply = &apos;&#123;&quot;search&quot;:&#123;&quot;cost&quot;:0.01,&quot;data&quot;:[&#123;&quot;ask&quot;:&quot;在哪里&quot;,&quot;answer&quot;:[&quot;不是在这里&quot;,&quot;你在这里&quot;,&quot;好久不见&quot;],&quot;score&quot;:99&#125;]&#125;,&quot;model&quot;:&#123;&quot;cost&quot;:0.02,&quot;answer&quot;:&quot;在这里哈哈哈&quot;&#125;&#125;&apos;</div><div class="line">    print reply</div><div class="line">    if not data:</div><div class="line">        break</div><div class="line">    conn.sendall(reply)</div><div class="line"></div><div class="line">conn.close()</div><div class="line">s.close()</div></pre></td></tr></table></figure>
<h2 id="client-完整代码"><a href="#client-完整代码" class="headerlink" title="client 完整代码"></a>client 完整代码</h2><h3 id="client-响应表单"><a href="#client-响应表单" class="headerlink" title="client 响应表单"></a>client 响应表单</h3><p>mysocket.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div><div class="line"># -*- coding: UTF-8 -*-</div><div class="line">import json</div><div class="line">import socket</div><div class="line">import cgitb</div><div class="line">import cgi</div><div class="line"></div><div class="line">cgitb.enable()</div><div class="line">form = cgi.FieldStorage()</div><div class="line">question = form.getvalue(&apos;question&apos;)</div><div class="line"></div><div class="line"></div><div class="line">sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</div><div class="line">sock.connect((&apos;localhost&apos;, 9000))</div><div class="line">message = question</div><div class="line">try:</div><div class="line">    # Set the whole string</div><div class="line">    sock.sendall(message)</div><div class="line">except socket.error:</div><div class="line">    # Send failed</div><div class="line">    print &apos;Send failed&apos;</div><div class="line">    sys.exit()</div><div class="line"></div><div class="line"># sock.send(data)</div><div class="line">result = json.loads(sock.recv(10240))</div><div class="line"></div><div class="line"># 以下代码处理 json 并显示表单</div><div class="line">search_cost = result[&apos;search&apos;][&apos;cost&apos;]</div><div class="line">answers = result[&apos;search&apos;][&apos;data&apos;][0][&apos;answer&apos;]</div><div class="line">ans = answers[0]</div><div class="line">search_score = result[&apos;search&apos;][&apos;data&apos;][0][&apos;score&apos;]</div><div class="line">rnnAns = result[&apos;model&apos;]</div><div class="line"></div><div class="line">print &quot;Content-type:text/html&quot;</div><div class="line">print</div><div class="line">print &apos;&lt;html&gt;&apos;</div><div class="line">print &apos;&lt;head&gt;&apos;</div><div class="line">print &apos;&lt;meta charset=&quot;utf-8&quot;&gt;&apos;</div><div class="line">print &apos;&lt;title&gt;TEST&lt;/title&gt;&apos;</div><div class="line">print &apos;&lt;/head&gt;&apos;</div><div class="line"></div><div class="line"></div><div class="line">print &apos;&lt;body align=&quot;center&quot;&gt;&lt;div width=&quot;980&quot; align=&quot;center&quot;&gt;&apos;</div><div class="line">print &quot;&lt;h2&gt;Question: &quot; + question + &quot;&lt;/h2&gt;&quot;</div><div class="line">print &quot;&lt;hr &gt;&lt;h2&gt; Top 3&lt;/h2&gt;&quot;</div><div class="line">print &quot;Cost: &quot;</div><div class="line">print search_cost</div><div class="line">print &quot;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&quot;</div><div class="line">print &quot;Score: &quot;</div><div class="line">print search_score</div><div class="line">print &quot;&lt;br &gt;&quot;</div><div class="line"></div><div class="line"></div><div class="line">for i in range(1, len(answers) + 1):</div><div class="line">    print &quot;&lt;p&gt;&quot;</div><div class="line">    print i</div><div class="line">    print &quot;:&amp;nbsp;&quot;</div><div class="line">    print answers[i - 1].encode(&apos;utf8&apos;)</div><div class="line">    print &quot;&lt;br&gt;&quot;</div><div class="line"></div><div class="line"></div><div class="line">print &quot;&lt;hr /&gt;&lt;h2&gt; RNN &amp;nbsp; Model &lt;/h2&gt;&quot;</div><div class="line">print &quot;Cost: &quot;</div><div class="line">print rnnAns[&apos;cost&apos;]</div><div class="line">print &quot;&lt;br&gt;&lt;p&gt;&quot;</div><div class="line">print rnnAns[&apos;answer&apos;].encode(&apos;utf8&apos;)</div><div class="line">print &quot;&lt;/p&gt;&quot;</div><div class="line">print &quot;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&quot;</div><div class="line">print &apos;&lt;/div&gt;&lt;/body&gt;&apos;</div><div class="line">print &apos;&lt;/html&gt;&apos;</div></pre></td></tr></table></figure></p>
<h3 id="client-主页面"><a href="#client-主页面" class="headerlink" title="client 主页面"></a>client 主页面</h3><p>index.html<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line">&lt;html&gt;</div><div class="line"></div><div class="line">&lt;head&gt;</div><div class="line">    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;</div><div class="line">    &lt;style&gt;</div><div class="line">    body &#123;</div><div class="line">            font-family: &quot;arial&quot;;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        table,</div><div class="line">        td,</div><div class="line">        th &#123;</div><div class="line">            border: 1px solid #ddd;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        th,</div><div class="line">        td &#123;</div><div class="line">            border-bottom: 1px solid #ddd;</div><div class="line">            height: 50px;</div><div class="line">            text-align: left;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        table &#123;</div><div class="line">            border-collapse: collapse;</div><div class="line">            width: 80%;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    &lt;/style&gt;</div><div class="line">    &lt;title&gt;Simple Evaluation Check&lt;/title&gt;</div><div class="line">    &lt;script language=&quot;Javascript&quot;&gt;</div><div class="line">        function xmlhttpPost(strURL) &#123;</div><div class="line">            var xmlHttpReq = false;</div><div class="line">            var self = this;</div><div class="line">            // Mozilla/Safari</div><div class="line">            if (window.XMLHttpRequest) &#123;</div><div class="line">                self.xmlHttpReq = new XMLHttpRequest();</div><div class="line">            &#125;</div><div class="line">            // IE</div><div class="line">            else if (window.ActiveXObject) &#123;</div><div class="line">                self.xmlHttpReq = new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;);</div><div class="line">            &#125;</div><div class="line">            self.xmlHttpReq.open(&apos;POST&apos;, strURL, true);</div><div class="line">            # 设置 content-type</div><div class="line">            self.xmlHttpReq.setRequestHeader(&apos;Content-Type&apos;, &apos;application/x-www-form-urlencoded&apos;);</div><div class="line">            self.xmlHttpReq.onreadystatechange = function() &#123;</div><div class="line">                if (self.xmlHttpReq.readyState == 4) &#123;</div><div class="line">                    updatepage(self.xmlHttpReq.responseText);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            self.xmlHttpReq.send(getquerystring());</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        function getquerystring() &#123;</div><div class="line">            var form = document.forms[&apos;f1&apos;];</div><div class="line">            var question = form.question.value;</div><div class="line">            qstr = &apos;question=&apos; + question; // NOTE: no &apos;?&apos; before querystring</div><div class="line">            return qstr;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        function updatepage(str) &#123;</div><div class="line">            document.getElementById(&quot;result&quot;).innerHTML = str;</div><div class="line">        &#125;</div><div class="line">    &lt;/script&gt;</div><div class="line">&lt;/head&gt;</div><div class="line"></div><div class="line">&lt;body&gt;</div><div class="line">    &lt;form name=&quot;f1&quot;&gt;</div><div class="line">        &lt;p&gt;Question:</div><div class="line">            &lt;input name=&quot;question&quot; type=&quot;text&quot;&gt;</div><div class="line">            &lt;input value=&quot;Go&quot; type=&quot;button&quot; onclick=&apos;JavaScript:xmlhttpPost(&quot;cgi-bin/mysocket.py&quot;)&apos;&gt;</div><div class="line">        &lt;/p&gt;</div><div class="line">        &lt;div id=&quot;result&quot;&gt;&lt;/div&gt;</div><div class="line">    &lt;/form&gt;</div><div class="line">&lt;/body&gt;</div><div class="line"></div><div class="line">&lt;/html&gt;</div></pre></td></tr></table></figure></p>
<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>运行 server.py，浏览器输入 <a href="http://localhost/index.html" target="_blank" rel="external">http://localhost/index.html</a> 查看效果<br>主页<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%889.01.17.png" alt=""></p>
<p>异步响应<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%889.01.24.png" alt=""></p>
<p>server<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%889.44.45.png" alt=""></p>
<blockquote>
<p>参考链接<br><a href="http://www.runoob.com/python/python-cgi.html" target="_blank" rel="external">Python CGI编程</a><br><a href="http://ssiddique.info/writing-your-first-python-cgi-ajax-script.html" target="_blank" rel="external">Writing Your First Python CGI – Ajax Script</a><br><a href="http://www.cnblogs.com/heyuquan/archive/2013/05/13/js-jquery-ajax.html" target="_blank" rel="external">触碰jQuery：AJAX异步详解</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Web Application </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> cgi </tag>
            
            <tag> ajax </tag>
            
            <tag> socket </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Tf-idf 总结笔记]]></title>
      <url>http://www.shuang0420.com/2016/07/10/Tfidf%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>简单的概念，强大的效用。附 sklearn 和 nltk 实现代码。<br><a id="more"></a></p>
<p>部分截图、概念描述来自 CMU 95 - 865 Text Analytics，上课的时候没有好好做笔记，到实习的时候发现，有些概念虽然很简单，但确实很实用，理解透彻才能发挥无穷效力。是在评估聚类算法的时候偶然想到的方法，确！实！很！有！用！</p>
<p>TF-IDF（term frequency–inverse document frequency），一种常用的加权技术，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。主要逻辑是：<strong>字词的重要性随著它在文件中出现的次数成正比增加，但同时会随著它在语料库中出现的频率成反比下降。</strong> TF-IDF加权的各种形式常被搜寻引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，因特网上的搜寻引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。</p>
<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><p>Heap’s law 通常用于预测 vocabulary size，Zipf’s law 则用于描述 term frequency 的分布。</p>
<h2 id="Heaps’-Law"><a href="#Heaps’-Law" class="headerlink" title="Heaps’ Law"></a>Heaps’ Law</h2><p>一张图解释。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-09-06%20%E4%B8%8B%E5%8D%881.13.51.png" alt=""></p>
<h2 id="Zipf’s-Law"><a href="#Zipf’s-Law" class="headerlink" title="Zipf’s Law"></a>Zipf’s Law</h2><p>齐普夫定律，本科信息计量学就学过，大致是说，在一个自然语言的语料库中，一个词的出现频数和这个词在这个语料中的排名（这个排名是基于出现次数的）成反比，频数和排名的乘积是常数。即 Rank <em> Frequency = Constant (Constant = 0.1 </em> N)，或者 P(tR) = 0.1/N。<br><br><br>Zipf’s law 告诉我们以下几点</p>
<ul>
<li>‘A few terms are very common’，这些大部分是 stopwords。</li>
<li>‘Most terms are very rare’，多数 term 只出现两三次，完全可以忽略。</li>
<li>very common 的，very rare 的在文本分析中都可以忽略，真正重要的是在中间的一部分 terms。</li>
</ul>
<p>两幅图概括。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%883.05.56.png" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-09-06%20%E4%B8%8B%E5%8D%881.18.36.png" alt=""></p>
<p>从 Zipf’s law 得到的一些数字是：</p>
<ul>
<li>排名第一的词占全部词的 10%</li>
<li>排名前5的词占全部词的 23%</li>
<li>前100 的词占全部词的 52%</li>
<li>50% 的 term 只出现了一次</li>
<li>91% 的 term 出现的次数小于 10</li>
</ul>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>合理使用 Zipf’s law，我们可以大大减少字典存储内存。<br>另外， Zipf’s law 还可以为特定的任务制作特定的 stopwords list。</p>
<p>Example:  “trading” and “prices” are frequent Wall Street Journal terms<br>• They are candidate stopwords<br>• They also are important terms for financial analysis<br>• If your task is financial analysis, leave them in<br>• If your task is analysis of technology products, maybe discard them</p>
<h1 id="Tf-idf-笔记"><a href="#Tf-idf-笔记" class="headerlink" title="Tf-idf 笔记"></a>Tf-idf 笔记</h1><h2 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h2><p>Zipf’s law 给出了自然语言的统计性质，term frequence is highly skewed.<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%883.21.34.png" alt=""></p>
<p>由此发展出了更能代表 term weight 的方法，tf - idf，用中文来表述没那么直观，且看英文解释。简单来讲就是说，如果某个词或短语在一篇文章中出现的频率 TF 高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%883.24.19.png" alt=""></p>
<h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%883.32.54.png" alt=""></p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-30%20%E4%B8%8B%E5%8D%883.33.01.png" alt=""></p>
<p>tf 通常会被正规化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><ul>
<li>Reward words that better represent each document</li>
<li>Reward words that discriminate among different documents</li>
<li>Scale for document length</li>
</ul>
<p>当然也有些情境下 tf 已经足够，并不需要 tfidf，如</p>
<ul>
<li>文件都是同一长度</li>
<li>并不需要区分文件（也就不需要idf）</li>
<li>机器学习算法可以学习特征等</li>
</ul>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="sklearn-方法"><a href="#sklearn-方法" class="headerlink" title="sklearn 方法"></a>sklearn 方法</h2><p>首先看下要用到的两个类，CountVectorizer 和 TfidfTransformer。参数一目了然，之后对比与 nltk 结果差异时再做解释。<br>CountVectorizer<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; vectorizer = CountVectorizer()</div><div class="line">&gt;&gt;&gt; vectorizer</div><div class="line">CountVectorizer(analyzer=u&apos;word&apos;, binary=False, decode_error=u&apos;strict&apos;,</div><div class="line">        dtype=&lt;type &apos;numpy.int64&apos;&gt;, encoding=u&apos;utf-8&apos;, input=u&apos;content&apos;,</div><div class="line">        lowercase=True, max_df=1.0, max_features=None, min_df=1,</div><div class="line">        ngram_range=(1, 1), preprocessor=None, stop_words=None,</div><div class="line">        strip_accents=None, token_pattern=u&apos;(?u)\\b\\w\\w+\\b&apos;,</div><div class="line">        tokenizer=None, vocabulary=None)</div></pre></td></tr></table></figure></p>
<p>TfidfTransformer<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; tfidfTransformer = TfidfTransformer()</div><div class="line">&gt;&gt;&gt; tfidfTransformer</div><div class="line">TfidfTransformer(norm=u&apos;l2&apos;, smooth_idf=True, sublinear_tf=False,</div><div class="line">         use_idf=True)</div></pre></td></tr></table></figure></p>
<p>计算 tfidf 代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer</div><div class="line"></div><div class="line">corpus = []</div><div class="line">doc1 = &quot;I love Chinese food&quot;</div><div class="line">doc2 = &quot;I love American spirits&quot;</div><div class="line">corpus.append(doc1)</div><div class="line">corpus.append(doc2)</div><div class="line"></div><div class="line"></div><div class="line">def calTfidf(corpus):</div><div class="line">    # 该转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频</div><div class="line">    vectorizer = CountVectorizer()</div><div class="line">    # vectorizer = CountVectorizer(token_pattern=&apos;(?u)\\b\\w+\\b&apos;)</div><div class="line">    # 统计每个词语的tf-idf权值</div><div class="line">    transformer = TfidfTransformer()</div><div class="line">    # transformer = TfidfTransformer(norm=None)</div><div class="line">    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))</div><div class="line">    tfidf_weight = tfidf.toarray()</div><div class="line">    idf_weight = transformer.idf_  # idf np.log(float(n_samples) / df) + 1.0</div><div class="line">    word = vectorizer.get_feature_names()  # 获取词袋模型中的所有词语</div><div class="line">    termWeights = dict()</div><div class="line">    for i in range(len(tfidf_weight)):</div><div class="line">        lwords = [(word[j], float(tfidf_weight[i][j]), float(idf_weight[j]))</div><div class="line">                  for j in range(len(word))]</div><div class="line">        lwords = sorted(lwords, key=lambda m: -m[1])</div><div class="line">        termWeights[i] = lwords</div><div class="line">    for key, items in termWeights.items():</div><div class="line">        print u&quot;-------第&quot;, key, u&quot;类文本的词语tf-idf权重------&quot;</div><div class="line">        for item in items:</div><div class="line">            print &apos;Word: &#123;&#125;\tTfidf: &#123;&#125;\tIdf: &#123;&#125;&apos;.format(item[0], item[1], item[2])</div></pre></td></tr></table></figure></p>
<p>输出</p>
<pre>
-------第 0 类文本的词语tf-idf权重------
Word: chinese    Tfidf: 0.631667201738    Idf: 1.40546510811
Word: food    Tfidf: 0.631667201738    Idf: 1.40546510811
Word: love    Tfidf: 0.449436416524    Idf: 1.0
Word: american    Tfidf: 0.0    Idf: 1.40546510811
Word: spirits    Tfidf: 0.0    Idf: 1.40546510811
-------第 1 类文本的词语tf-idf权重------
Word: american    Tfidf: 0.631667201738    Idf: 1.40546510811
Word: spirits    Tfidf: 0.631667201738    Idf: 1.40546510811
Word: love    Tfidf: 0.449436416524    Idf: 1.0
Word: chinese    Tfidf: 0.0    Idf: 1.40546510811
Word: food    Tfidf: 0.0    Idf: 1.40546510811
</pre>


<p>为什么少了个 I ？看 CountVectorizer 构造器有一个参数是 token_pattern，忽略了单个字母的 word。</p>
<pre>
token_pattern (default u'(?u)\b\w\w+\b'), regular expression identifying tokens–by default words that consist of a single character (e.g., ‘a’, ‘2’) are ignored, setting token_pattern to '(?u)\b\w+\b' will include these tokens
</pre>

<p>改成 vectorizer = CountVectorizer(token_pattern=’(?u)\b\w+\b’) 就会包括 I</p>
<pre>
-------第 0 类文本的词语tf-idf权重------
Word: chinese    Tfidf: 0.576152355165    Idf: 1.40546510811
Word: food    Tfidf: 0.576152355165    Idf: 1.40546510811
Word: i    Tfidf: 0.40993714596    Idf: 1.0
Word: love    Tfidf: 0.40993714596    Idf: 1.0
Word: american    Tfidf: 0.0    Idf: 1.40546510811
Word: spirits    Tfidf: 0.0    Idf: 1.40546510811
-------第 1 类文本的词语tf-idf权重------
Word: american    Tfidf: 0.576152355165    Idf: 1.40546510811
Word: spirits    Tfidf: 0.576152355165    Idf: 1.40546510811
Word: i    Tfidf: 0.40993714596    Idf: 1.0
Word: love    Tfidf: 0.40993714596    Idf: 1.0
Word: chinese    Tfidf: 0.0    Idf: 1.40546510811
Word: food    Tfidf: 0.0    Idf: 1.40546510811
</pre>

<h2 id="nltk-方法"><a href="#nltk-方法" class="headerlink" title="nltk 方法"></a>nltk 方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">from __future__ import division, unicode_literals</div><div class="line">from textblob import TextBlob as tb</div><div class="line">import math</div><div class="line"></div><div class="line">corpus = []</div><div class="line">doc1 = &quot;I love Chinese food&quot;</div><div class="line">doc2 = &quot;I love American spirits&quot;</div><div class="line">corpus.append(doc1)</div><div class="line">corpus.append(doc2)</div><div class="line"></div><div class="line">def tf(word, blob):</div><div class="line">    return blob.words.count(word) / len(blob.words)</div><div class="line"></div><div class="line"></div><div class="line">def n_containing(word, bloblist):</div><div class="line">    return sum(1 for blob in bloblist if word in blob.words)</div><div class="line"></div><div class="line"></div><div class="line">def idf(word, bloblist):</div><div class="line">    return math.log(len(bloblist) / n_containing(word, bloblist)) + 1</div><div class="line"></div><div class="line"></div><div class="line">def tfidf(word, blob, bloblist): return tf(word, blob) * idf(word, bloblist)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def calTfidf(corpus):</div><div class="line">    bloblist = [tb(doc.strip(&apos;\n&apos;)) for doc in corpus]</div><div class="line">    for i, blob in enumerate(bloblist):</div><div class="line">        tf_scores = &#123;word: tf(word, blob) for word in blob.words&#125;</div><div class="line">        idf_scores = &#123;word: idf(word, bloblist) for word in blob.words&#125;</div><div class="line">        tfidf_scores = &#123;word: tfidf(word, blob, bloblist)</div><div class="line">                        for word in blob.words&#125;</div><div class="line">        sorted_words = sorted(tfidf_scores.items(),</div><div class="line">                              key=lambda x: x[1], reverse=True)</div><div class="line">        print &apos;u&quot;-------第&quot;, key, u&quot;类文本的词语tf-idf权重------&quot;&apos;</div><div class="line">        for word, score in sorted_words[:50]:</div><div class="line">            print (&quot;Word: &#123;&#125;\t TF-IDF: &#123;&#125;\t TF: &#123;&#125;\t IDF: &#123;&#125;&quot;.format(word, round(score, 5), tf_scores[word], round(idf_scores[word], 5)))</div></pre></td></tr></table></figure>
<p>输出</p>
<pre>
u"-------第", key, u"类文本的词语tf-idf权重------"
Word: food     TF-IDF: 0.42329     TF: 0.25     IDF: 1.69315
Word: Chinese     TF-IDF: 0.42329     TF: 0.25     IDF: 1.69315
Word: I     TF-IDF: 0.25     TF: 0.25     IDF: 1.0
Word: love     TF-IDF: 0.25     TF: 0.25     IDF: 1.0
u"-------第", key, u"类文本的词语tf-idf权重------"
Word: American     TF-IDF: 0.42329     TF: 0.25     IDF: 1.69315
Word: spirits     TF-IDF: 0.42329     TF: 0.25     IDF: 1.69315
Word: I     TF-IDF: 0.25     TF: 0.25     IDF: 1.0
Word: love     TF-IDF: 0.25     TF: 0.25     IDF: 1.0
</pre>

<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><p>发现 sklearn 和 nltk 两种方法计算的结果不同，研究一下发现是对 tf 的处理不同。</p>
<ul>
<li>看 sklearn 的 TfidfTransformer，发现进行了正则化(L2)，norm=u’l2’。</li>
<li>而 nltk，我们用的是 Linear scaling 的方法。</li>
</ul>
<p>为什么要进行这样的比较，实际是因为当时需要看 tf-idf, tf, idf 各自的情况，却发现 sklearn 得不到正确的 tf，于是才折腾的用了 nltk，实际上通过 nltk 自己写函数会更灵活，具体问题具体分析吧。</p>
<h1 id="缺陷与不足"><a href="#缺陷与不足" class="headerlink" title="缺陷与不足"></a>缺陷与不足</h1><p>TF-IDF 并不是万能的，它单纯地认为文本频数小的单词就越重要，文本频数大的单词就越无用，显然这并不是完全正确的。可能会出现的结果：</p>
<ul>
<li>被忽略的高频词。高频词 != 无意义的词。引入 idf，初衷是抑制无意义的高频词（通常是 stopwords）的影响，如上文提到的 Wall Street Journal 例子，如果在金融分析的场景下，“trading” 和 “prices” 这类高频词本不该被忽略。</li>
<li>被强调的低频词。</li>
</ul>
<p>这也是为什么我在计算了 tf-idf 的同时，还要观察 tf、idf 的值的原因。根据不同的场景，可能需要引入阈值，限制IDF值过大的词语导入。另外 tfidf 这种基于词袋的算法还有个与生俱来的硬伤 – 位置信息被忽略。</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Meteor--App_Recommender_System]]></title>
      <url>http://www.shuang0420.com/2016/07/07/Meteor-App-Recommender-System/</url>
      <content type="html"><![CDATA[<p>Meteor 是一个构建在 Node.js 之上的平台，用来开发实时网页程序，这一篇用 Meteor 搭建 App_Recommender_System 的 front-end.<br><a id="more"></a></p>
<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="Meteor-的创新"><a href="#Meteor-的创新" class="headerlink" title="Meteor 的创新"></a>Meteor 的创新</h2><p>Meteor 的关键性创新在于 Rails 程序只跑在服务器上，而一个 Meteor App 还包括在客户端（浏览器）上运行的客户端组件。这就相当于书店的伙计不仅仅在书店里帮你找书，还跟你回家，每天晚上读给你听（这听起来怪怪的）。<br>这种架构让 Meteor 变得数据库无处不在。简单说，Meteor 把你的数据拿出一部分子集复制到客户端。这样后两个主要结果：第一，服务器不再发送 HTML 代码到客户端，而是发送真实的原始数据，让客户端决定如何处理线传数据。第二，你可以不必等待服务器传回数据，而是立即访问甚至修改数据（延迟补偿 latency compensation）。</p>
<h2 id="Meteor-特点"><a href="#Meteor-特点" class="headerlink" title="Meteor 特点"></a>Meteor 特点</h2><p>Meteor 位于程序数据库和用户界面之间，保持二者之间的数据同步更新。除此之外，Meteor 还有以下特点：</p>
<ol>
<li>纯数据对话。服务器与客户端初始化后只传输数据，由客户端决定如何渲染。</li>
<li>一种语言。前后端统一使用 JavaScript 进行开发。</li>
<li>无处不在的数据库。浏览器端使用与服务器端一致的 API 访问本地数据库。</li>
<li>延迟补偿。在客户端使用预取和数据模型模拟技术，提供接近零延迟的数据库连接体验。</li>
<li>全栈响应式。实时作为默认模式，从数据库到模版的所有层面上，都应当具备可用的事件驱动接口。</li>
<li>社区生态友好。Meteor 开放源代码并能与现有的开源工具和框架整合，而非取代它们</li>
<li>简单即生产力。让事情看起来简单的最佳方式就是让它真正变得简单，通过干净且具古典美的 API 来实现。</li>
</ol>
<h2 id="Meteor-结构"><a href="#Meteor-结构" class="headerlink" title="Meteor 结构"></a>Meteor 结构</h2><p>一般来说需要新建四个子文件夹：/client，/server，/public 和 /lib。然后在 /client 文件夹中新建 main.html 和 main.js 文件。这些文件夹中有一些拥有特别的作用。</p>
<h3 id="Meteor-文件规则"><a href="#Meteor-文件规则" class="headerlink" title="Meteor 文件规则"></a>Meteor 文件规则</h3><ul>
<li>在 /server 文件夹中的代码只会在服务器端运行。</li>
<li>在 /client 文件夹中的代码只会在客户端运行。</li>
<li>其它代码则将同时运行于服务器端和客户端上。</li>
<li>请将所有的静态文件（字体，图片等）放置在 /public 文件夹中。</li>
</ul>
<h3 id="Meteor-加载文件顺序"><a href="#Meteor-加载文件顺序" class="headerlink" title="Meteor 加载文件顺序"></a>Meteor 加载文件顺序</h3><ul>
<li>在 /lib 文件夹中的文件将被优先载入。</li>
<li>所有以 main.* 命名的文件将在其他文件载入后载入。</li>
<li>其他文件以文件名的字母顺序载入。</li>
</ul>
<h2 id="Meteor-核心概念"><a href="#Meteor-核心概念" class="headerlink" title="Meteor 核心概念"></a>Meteor 核心概念</h2><h3 id="模板系统-Spacebars"><a href="#模板系统-Spacebars" class="headerlink" title="模板系统 Spacebars"></a>模板系统 Spacebars</h3><p>Spacebar 就是简单的 HTML 加上三件事情：Inclusion （有时也称作 “partial”）、Expression 和 Block Helper。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">- Inclusion ：通过 &#123;&#123;&gt; templateName&#125;&#125; 标记，简单直接地告诉 Meteor 这部分需要用相同名称的模板来取代</div><div class="line">- Expression ：比如 &#123;&#123;title&#125;&#125; 标记，它要么是调用当前对象的属性，要么就是对应到当前模板管理器中定义的 helper 方法，并返回其方法值。</div><div class="line">- Block Helper ：在模板中控制流程的特殊标签，如 &#123;&#123;#each&#125;&#125;…&#123;&#123;/each&#125;&#125; 或 &#123;&#123;#if&#125;&#125;…&#123;&#123;/if&#125;&#125; 。</div></pre></td></tr></table></figure></p>
<p>模板的作用局限于显示或循环变量，而 helper 则扮演着一个相当重要的角色：把值分配给每个变量。</p>
<p><a href="https://github.com/meteor/meteor/blob/devel/packages/spacebars/README.md" target="_blank" rel="external">Spacebars 文档</a></p>
<h3 id="collection"><a href="#collection" class="headerlink" title="collection"></a>collection</h3><p>meteor collection 是 MongoDB Collections 的扩展，在 server 下插入 collection 的数据将会在 MongoDB 自动更新。任何在 client 下 publish 的数据将会在客户端页面实时显示。</p>
<blockquote>
<p>All    collections    are    reactively updated on    both the client    and    the server.    If a new app were    published    to our store,    and    thousands    of clients were    connected, all of    their    app    stores would update immediately    without    any    extra    work on    our    end!</p>
</blockquote>
<h4 id="服务器端"><a href="#服务器端" class="headerlink" title="服务器端"></a>服务器端</h4><p>在服务器，collection 有一个任务就是和 Mongo 数据库联络，读取任何数据变化。 在这种情况下，它可以比对标准的数据库。<br>collection 还可以像 API 一样操作 Mongo 数据库。在服务器端的代码，你可以写像 Posts.insert() 或 Posts.update() 这样的 Mongo 命令，来对 Mongo 数据库中的 posts 集合进行操作。</p>
<h4 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h4><p>在客户端，collection 是一个安全拷贝来自于实时一致的数据子集。客户端的 collection 总是（通常）透明地实时更新数据子集。当你在客户端申明 Posts = new Mongo.Collection(‘posts’); 你实际上是创建了一个本地的，在浏览器缓存中的真实的 Mongo 集合。 当我们说客户端 collection 被”缓存”是指它保存了你数据的一个子集，而且对这些数据提供了十分快速的访问。</p>
<p>有一点我们必须要明白，因为这是 Meteor 工作的一个基础: 通常说来，客户端的集合的数据是你 Mongo 数据库的所有数据的一个子集（毕竟我们不会想把整个数据库的数据全传到客户端来）。</p>
<p>另外，那些数据是被存储在浏览器内存中的，也就是说访问这些数据几乎不需要时间，不像去服务器访问 Posts.find() 那样需要等待，因为数据事实上已经载入了。</p>
<h4 id="客户端-服务器通讯"><a href="#客户端-服务器通讯" class="headerlink" title="客户端-服务器通讯"></a>客户端-服务器通讯</h4><p>实际情况是服务器端的 collection 被客户端的 collection 通知说有一个新 item，然后执行了一个任务把这个 item 放入 Mongo 数据库，进而送到所有连接着的客户端。<br>在浏览器的控制台取出所有的 item 没什么用处。需要把这些数据显示在模板中，并把这个简单的 HTML 原型变成一个有用的实时 Web 应用。</p>
<h3 id="发布（Publication）和订阅（Subscription）"><a href="#发布（Publication）和订阅（Subscription）" class="headerlink" title="发布（Publication）和订阅（Subscription）"></a>发布（Publication）和订阅（Subscription）</h3><p>collection 通过发布（publications）和订阅（subscriptions）机制把数据实时同步上行或者下行到连接着的各个用户的浏览器或者Mongo数据库中。Meteor App 保证只发布你让这个当前用户看到的数据。autopublish 的目的是让 Meteor 应用有个简单的起步阶段，它简单地直接把服务器上的全部数据镜像到客户端，因此你就不用管发布和订阅了。然而在实际工程中，我们需要删除它。</p>
<pre>meteor remove autopublish</pre>

<h4 id="发布"><a href="#发布" class="headerlink" title="发布"></a>发布</h4><p>一个 App 的数据库可能用上万条数据，其中一些还可能是私用和保密敏感数据。显而易见我们不能简单地把数据库镜像到客户端去，无论是安全原因还是扩展性原因。<strong>发布</strong> 就是告诉 Meteor 哪些数据子集是需要送到客户端。<br>为达到这个目的，我们建立一个简单的 Publish() 函数，只发布没有打标记的帖子</p>
<pre>
// 在服务器端
Meteor.publish('posts', function() {
  return Posts.find({flagged: false});
});</pre>

<h4 id="订阅"><a href="#订阅" class="headerlink" title="订阅"></a>订阅</h4><p><strong>订阅</strong> 就是让客户端来确定哪些子集是他们在某个特别时候特别需要的。</p>
<p>在客户端我们需要订阅这个发布。我们仅仅需要增加这样一行到 main.js 文件中</p>
<pre>Meteor.subscribe('posts');</pre>

<p>Meteor 程序在客户端能够具有可伸缩性：不去订阅全部数据，而是指选择你现在需要的数据去订阅。这样的话，你就可以避免消耗大量的客户端内存，无论服务器端的总数据量有多大。</p>
<h4 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h4><p> 基本上我们可以把发布/订阅模式想象成为一个漏斗，从服务器端（数据源）过滤数据传送到客户端（目标）。<br> 这个漏斗的专属协议叫做 DDP（分布式数据协议 Distributed Data Protocol 的缩写）。如果想了解 DDP 的更多细节，可以通过看 Matt DeBergalis（Meteor 创始人之一）在 <a href="https://2012.realtimeconf.com/video/matt-debergalis" target="_blank" rel="external">Real-time 大会上的讲演视频</a>，或者来自 Chris Mather 的这个<a href="https://www.eventedmind.com/items/meteor-subscriptions-and-ddp" target="_blank" rel="external">截屏视频</a>，来学习关于这个概念更多的细节。</p>
 <pre>DDP
 distributed data protocol. the stateful websocket protocol.(under "Publish and subscribe", "Methods", and "Server connections")

 `ddp` can be configured to use a randomly generated subdomain for each long polling connection(Web browsers put a limit on the total number of HTTP connections that can be open to a particular domain at any one time, across all browser tabs.) 所谓 wire up(Mongo driver will automatically register with `ddp` to receive incoming data for `mycollection` and use it to keep `MyCollection` up to date.)

 特性：
 database driver integration
 automatic latency compensation（client's screen update instantly when they make changes 不用 wait for server round trip. 和 full-stack db drivers to snapshot and restore records??）
 transparent reconnect
 authentication (authentication hooks work great with Meteor Account.)
 input sanitization（audit-argument-checks， match's check）
 tracker-aware（connection status, subscription readiness, currently logged-in user 都是 reactive variables）
 default connect（meteor tools 构建会自动set up server, 这样就可以直接 meteor.subscribe，而不用 myconn = DPP.connect(url), myconn.subscribe 了）
 connection lifecycle hooks (当connection建立或关闭时，实现用户在线统计功能)
 CRUD bilerplate and quickstart packages （The `insecure` package turns off `allow`/`deny` rule checking for the generic `create`, `update`, and `delete` methods. The `autopublish` package automatically subscribes every connected client to the full contents of every database collection.）</pre>


<h3 id="Router"><a href="#Router" class="headerlink" title="Router"></a>Router</h3><p>假设有一个帖子列表页面，我们还希望可以通过固定链接访问到每个单独的帖子页面，URL 形式是 <a href="http://myapp.com/posts/xyz（这里的" target="_blank" rel="external">http://myapp.com/posts/xyz（这里的</a> xyz 是 MongoDB 的 _id 标识符），对于每个帖子来说是唯一的。这意味着我们需要某些路由来看看浏览器的地址栏里面的路径是什么，并相应地显示正确的内容。这就是 router 的作用。</p>
<h4 id="添加-Iron-Router-包"><a href="#添加-Iron-Router-包" class="headerlink" title="添加 Iron Router 包"></a>添加 Iron Router 包</h4><pre>meteor add iron:router</pre>

<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul>
<li>路由规则（Route）：路由规则是路由的基本元素。它的工作就是当用户访问 App 的某个 URL 的时候，告诉 App 应该做什么，返回什么东西。</li>
<li>路径（Path）：路径是访问 App 的 URL。它可以是静态的（/terms_of_service）或者动态的（/posts/xyz），甚至还可以包含查询参数（- /search?keyword=meteor）。</li>
<li>目录（Segment）：路径的一部分，使用正斜杠（/）进行分隔。</li>
<li>Hooks：Hooks 是可以执行在路由之前，之后，甚至是路由正在进行的时候。一个典型的例子是，在显示一个页面之前检测用户是否拥有这个权限。</li>
<li>过滤器（Filter）：过滤器类似于 Hooks ，为一个或者多个路由规则定义的全局过滤器。</li>
<li>路由模板（Route Template）：每个路由规则指向的 Meteor 模板。如果你不指定，路由器将会默认去寻找一个具有相同名称的模板。</li>
<li>布局（Layout）：你可以想象成一个数码相框的布局。它们包含所有的 HTML 代码放置在当前的模板中，即使模板发生改变它们也不会变。</li>
<li>控制器（Controller）：有时候，你会发现很多你的模板都在重复使用一些参数。为了不重复你的代码，你可以让这些路由规则继承一个路由控制器（Routing Controller）去包含所有的路由逻辑。</li>
</ul>
<h4 id="路由：把-URL-映射到模板"><a href="#路由：把-URL-映射到模板" class="headerlink" title="路由：把 URL 映射到模板"></a>路由：把 URL 映射到模板</h4><p>默认情况下，Iron Router 会为路由规则，指定相同名字的模板。而如果路径（path 参数）没有指定，它也会根据路由规则的名字，去指定同样名字的路径。</p>
<p>你可能想知道为什么我们需要在一开始去制定路由规则。这是因为 Iron Router 的部分功能需要使用路由规则去生成 App 的链接信息。其中最常见的一个是  的 Spacebars helper，它需要返回路由规则的 URL 路径。</p>
<p>除了指定静态的 / URL ，我们还可以使用 Spacebars helper。虽然它们的效果是一样的，不过这给了我们更多的灵活性，如果我们更改了路由规则的映射路径，helper 仍然可以输出正确的 URL 。</p>
<h4 id="等待数据"><a href="#等待数据" class="headerlink" title="等待数据"></a>等待数据</h4><p>如果你要部署当前版本的 App（或启动起来去使用上面的链接），你会注意到在所有帖子完全出现之前，列表里面会空了一段时间。这是因为在第一次加载页面的时候，要等到 posts 订阅完成后，即从服务器抓取完帖子的数据，才能有帖子显示在页面上。</p>
<p>这应该要有一个更好的用户体验，比如提供一些视觉上的反馈让用户知道正在读取数据，这样用户才会去继续等待。Iron Router 给了我们一个简单的方法去实现它。我们把订阅放到 waitOn 的返回上。</p>
<h2 id="Meteor-部署"><a href="#Meteor-部署" class="headerlink" title="Meteor 部署"></a>Meteor 部署</h2><pre>meteor deploy myapp.meteor.com</pre>

<p>当然，你要把“myapp”替换成你想要的名称，最好是命名一个没有被使用的。如果你的名称已经被使用，Meteor 会提示你去输入密码。如果发生这样的情况，只需通过 ctrl+c 来取消当前操作，然后用另一个不同的名称再试一次。</p>
<p>如果顺利地部署成功了，几秒钟后你就能够在 <a href="http://myapp.meteor.com" target="_blank" rel="external">http://myapp.meteor.com</a> 上访问到你的应用了。</p>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><h2 id="安装-meteorjs"><a href="#安装-meteorjs" class="headerlink" title="安装 meteorjs"></a>安装 meteorjs</h2><pre>$ curl https://install.meteor.com/ | sh</pre>

<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><pre>
$ meteor create app_store
</pre>

<pre>
$ cd App-Recommender-System/
$ meteor
</pre>

<p>浏览器打开 <a href="http://localhost:3000/，就可以看到" target="_blank" rel="external">http://localhost:3000/，就可以看到</a> Meteor App 的状态。</p>
<h2 id="第三方-package"><a href="#第三方-package" class="headerlink" title="第三方 package"></a>第三方 package</h2><p>Add    three    packages:</p>
<ol>
<li>twbs:bootstrap - Twitter    Bootstrap    packaged     for    Meteor</li>
<li>iron:router – A    Meteor    package    that    handles    routing    between     pages</li>
<li>barbatus:stars-rating – A    small    library    to    give    us    nice    rating    stars    for    the    app    store.</li>
</ol>
<pre>meteor add iron:router twbs:bootstrap barbatus:stars-rating</pre>


<p>Remove    two    default    packages:</p>
<ol>
<li>autopublish – a    development    package    that    publishes    all    of    our    MongoDB data    to    the    client.        Great     for    prototyping,    insecure     for    production!        </li>
<li>insecure    – The    package    name    says    it    all.        This    package    allows    the    user    client     to    create,    update,    read    and    delete    any    data    in    our database.        It’s     another    package    meant     to    make    development     easier,    but    we    won’t    have    a    need     for    this    in    our    project.</li>
</ol>
<pre>meteor remove autopublish insecure</pre>


<h2 id="server"><a href="#server" class="headerlink" title="server"></a>server</h2><h3 id="Create-app-collection"><a href="#Create-app-collection" class="headerlink" title="Create app collection"></a>Create app collection</h3><p>新建 lib 文件夹，创建 apps.js 文件，添加如下代码</p>
<pre>Apps = new Meteor.Collection('apps');</pre>

<h3 id="Put-json-files"><a href="#Put-json-files" class="headerlink" title="Put json files"></a>Put json files</h3><p>这里衔接上一个部分 crawler 的工作，将我们保存在 MongoDB 的数据导出来放在 server 文件夹下。</p>
<pre>mongoexport -d appstore -c app_info -o ./app_info_new.json</pre>

<h3 id="Populate-app-collection"><a href="#Populate-app-collection" class="headerlink" title="Populate app collection"></a>Populate app collection</h3><p>server 文件夹下新建 fixures.js 来 load data.</p>
<pre>
// checks if app collection is empty so we don't call this code on every run
if(Apps.find({}).count() < 1){

    // read in json file using Npm filesystem package
    var fs = Npm.require('fs');
    fs.readFile('../../../../../server/app_info.json', 'utf8', Meteor.bindEnvironment(function(err, data) {
        if (err) throw err;
        var appData = data.split("\n");

        for (var i = 0; i < appData.length - 1; i++) {
            var rawAppData = JSON.parse(appData[i]);
            var app = {};

            app.name = rawAppData.title;
            app.app_id = rawAppData.app_id;
            app.developer = rawAppData.developer;
            app.description = rawAppData.intro;
            app.avgRating = parseInt(rawAppData.score) / 2;
            app.iconUrl = rawAppData.thumbnail_url;
            app.recommendedApps = rawAppData.top_5_app;
            app.numberOfRecommendations = 0;
            // insert app into collection
            Apps.insert(app);
        }

    }, function(err){
        throw err;
    }));
}
</pre>


<h3 id="Publish-app-collection"><a href="#Publish-app-collection" class="headerlink" title="Publish app collection"></a>Publish app collection</h3><p>server 文件夹下新建 publications.js 文件 publish data<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">/** returns all apps in the collection, takes an options object that is used to push</div><div class="line">sorting and filtering operations onto the server side, this publication will be used</div><div class="line">in top charts list*/</div><div class="line">Meteor.publish(&apos;apps&apos;,function(options)&#123;</div><div class="line">  return Apps.find(&#123;&#125;,options);</div><div class="line">&#125;);</div><div class="line"></div><div class="line">/** takes an appid as a parameter and returns just one app that matches the appid,</div><div class="line">this publication will be used by our app details page for a single app */</div><div class="line">Meteor.publish(&apos;singleApp&apos;,function(id)&#123;</div><div class="line">  return Apps.find(&#123;_id:id&#125;);</div><div class="line">&#125;);</div><div class="line"></div><div class="line">/** this publication will be used to look up the recommended apps*/</div><div class="line">Meteor.publish(&apos;singleAppByAppId&apos;,function(appId)&#123;</div><div class="line">  return Apps.find(&#123;app_id:appId&#125;);</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<h2 id="client"><a href="#client" class="headerlink" title="client"></a>client</h2><h3 id="新建-index-html-文件"><a href="#新建-index-html-文件" class="headerlink" title="新建 index.html 文件"></a>新建 index.html 文件</h3><p>在 client 目录下新建 index.html 文件。添加 head，meteor 的 Blaze template engine 会产生 body。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;head&gt;</div><div class="line">  &lt;title&gt;App Store&lt;/title&gt;</div><div class="line">&lt;/head&gt;</div><div class="line"></div><div class="line">&lt;body&gt;</div><div class="line">&lt;/body&gt;</div></pre></td></tr></table></figure></p>
<h3 id="添加-css-文件"><a href="#添加-css-文件" class="headerlink" title="添加 css 文件"></a>添加 css 文件</h3><p>这里就不贴代码了</p>
<h3 id="Tell-iron-router-where-to-rend-templates"><a href="#Tell-iron-router-where-to-rend-templates" class="headerlink" title="Tell iron:router where to rend templates"></a>Tell iron:router where to rend templates</h3><p>在 layouts 里新建 master_layout.html 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;template name=&quot;masterLayout&quot;&gt;</div><div class="line"></div><div class="line">    &lt;div class=&quot;container&quot;&gt;</div><div class="line">        &lt;div class=&quot;row&quot;&gt;</div><div class="line">            &lt;div class=&quot;col-md-4 col-md-offset-4 col-sm-12 col-xs-12 mainContainer&quot;&gt;</div><div class="line">                 &#123;&#123;&gt; yield &#125;&#125; &lt;!-- tells iron:router where to render our view templates inside of this layout--&gt;</div><div class="line">            &lt;/div&gt;</div><div class="line">        &lt;/div&gt;</div><div class="line">    &lt;/div&gt;</div><div class="line"></div><div class="line">&lt;/template&gt;</div></pre></td></tr></table></figure></p>
<h3 id="topChart-template"><a href="#topChart-template" class="headerlink" title="topChart template"></a>topChart template</h3><p>新建 views 文件夹，新建 topChart.html 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;!-- define a template named topChart --&gt;</div><div class="line">&lt;template name=&quot;topChart&quot;&gt;</div><div class="line">  &lt;!-- utilize the bootstrap navbar class to give us the App	Store	top	bar --&gt;</div><div class="line">    &lt;nav class=&quot;navbar navbar-default&quot;&gt;</div><div class="line">        &lt;div class=&quot;container-fluid&quot;&gt;</div><div class="line">            &lt;div class=&quot;text-center&quot; id=&quot;navTitle&quot;&gt;</div><div class="line">                &lt;strong&gt;Top Charts&lt;/strong&gt;</div><div class="line">            &lt;/div&gt;</div><div class="line">        &lt;/div&gt;</div><div class="line">    &lt;/nav&gt;</div><div class="line">&lt;/template&gt;</div></pre></td></tr></table></figure></p>
<h3 id="为-topChart-template-填充数据"><a href="#为-topChart-template-填充数据" class="headerlink" title="为 topChart template 填充数据"></a>为 topChart template 填充数据</h3><p>之前我们加了 “iron:router” 包，用这个包，我们可以提供一个得到 topChart template 的路径，为这个 template 设置数据<br>在 lib 下新建 routing 文件夹，添加 router.js 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">Router.configure(&#123;</div><div class="line">    // tell iron router to render our individual templates inside of our masterLayout template</div><div class="line">    layoutTemplate: &quot;masterLayout&quot;</div><div class="line">&#125;);</div><div class="line"></div><div class="line">Router.route(&apos;/&apos;, &#123;// describe the route path, &apos;/&apos; - root path of our web app</div><div class="line">    name: &apos;topChart&apos;,// the template this route will use</div><div class="line">    waitOn: function() &#123;</div><div class="line">      // iron router will hold off rendering our page until the waitOn function completes</div><div class="line">      // returns a subscription to our “apps” publication</div><div class="line">      // passes	a	set	of options to	the	server,</div><div class="line">      // tells	the	server	to	sort	our	apps	based	off	of	avgRating and	app_id.	And	to only	give us	a	max	of	20	results.</div><div class="line">        Meteor.subscribe(&apos;apps&apos;, &#123;sort: &#123;avgRating: -1, app_id: -1&#125;, limit: 50&#125;);</div><div class="line">    &#125;,</div><div class="line">    // The return	value	of the data function becomes the Blaze template’s	“data	context”.</div><div class="line">    // returns an	object with	a	single property	“apps” that	contains all the Apps returned via	our	subscription.</div><div class="line">    data: function () &#123;</div><div class="line">        return &#123;</div><div class="line">            apps: Apps.find(&#123;&#125;, &#123;sort: &#123;avgRating: -1, app_id: -1&#125;, limit: 50&#125;)</div><div class="line">        &#125;;</div><div class="line">    &#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<h3 id="更新-topChart-html-文件"><a href="#更新-topChart-html-文件" class="headerlink" title="更新 topChart.html 文件"></a>更新 topChart.html 文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;ul class=&quot;list-group&quot;&gt;</div><div class="line">  &lt;!-- use the built in SpaceBars &#123;&#123;#each&#125;&#125;	to iterate through the “apps” property that	we set inside	our	route’s	”data”	function.</div><div class="line">  Inside the	&#123;&#123;#each&#125;&#125;&#123;&#123;/each&#125;&#125; block the data context	changes	from “apps”	to be	the	app	in scope for each	iteration.--&gt;</div><div class="line">    &#123;&#123;#each apps&#125;&#125;</div><div class="line">    &lt;!-- call	the	“appPreview” template. Since it	is inside	the	&#123;&#123;#each&#125;&#125;	block	it will	be passed	the	data for each	individual	app	as	its</div><div class="line">    context	as we	iterate	through	our	parent data	context. --&gt;</div><div class="line">        &#123;&#123;&gt; appPreview this&#125;&#125;</div><div class="line">    &#123;&#123;/each&#125;&#125;</div><div class="line">&lt;/ul&gt;</div></pre></td></tr></table></figure>
<h3 id="appPreview-template"><a href="#appPreview-template" class="headerlink" title="appPreview template"></a>appPreview template</h3><p>views 里添加 appPreview.html 文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&lt;template name=&quot;appPreview&quot;&gt;</div><div class="line">  &lt;!-- each app preview will be wrapped in an &lt;li&gt; element, as the topChart template will insert each app into a &lt;ul&gt;--&gt;</div><div class="line">    &lt;li class=&quot;list-group-item&quot;&gt;</div><div class="line">        &lt;div class=&quot;row&quot;&gt;</div><div class="line">            &lt;div class=&quot;col-xs-1 appRank text-muted&quot;&gt;</div><div class="line">                &#123;&#123;rank&#125;&#125;</div><div class="line">            &lt;/div&gt;</div><div class="line">            &lt;!-- set the img src to the iconUrl property that we scraped--&gt;</div><div class="line">            &lt;div class=&quot;col-xs-3 appIconPreview&quot;&gt;</div><div class="line">                &lt;img height=50 width=50 src=&quot;&#123;&#123;iconUrl&#125;&#125;&quot;/&gt;</div><div class="line">            &lt;/div&gt;</div><div class="line">            &lt;div class=&quot;col-xs-6 nameColumn&quot;&gt;</div><div class="line">                &lt;a href=&quot;&#123;&#123;pathFor &apos;appPage&apos;&#125;&#125;&quot;&gt;&#123;&#123;name&#125;&#125;&lt;/a&gt;&lt;br/&gt;</div><div class="line">                &lt;div style=&quot;display:flex&quot;&gt;</div><div class="line">                  &lt;!-- the stars-rating package with this template. pass apps avgRating property to the templates &quot;rating&quot; property</div><div class="line">                        so that it can color the correct # of stars--&gt;</div><div class="line">                    &#123;&#123;&gt; starsRating rating=avgRating class=&apos;mystar&apos; size=&apos;sm&apos;&#125;&#125;</div><div class="line">                &lt;/div&gt;</div><div class="line">            &lt;/div&gt;</div><div class="line">            &lt;div class=&quot;col-xs-2 text-right getApp&quot;&gt;</div><div class="line">              &lt;!-- pathFor is a built in helper that takes the name of the route and returns a correct path. --&gt;</div><div class="line">                &lt;a href=&quot;&#123;&#123;pathFor &apos;appPage&apos;&#125;&#125;&quot; class=&quot;btn btn-primary&quot;&gt;+ Get&lt;/a&gt;</div><div class="line">            &lt;/div&gt;</div><div class="line">        &lt;/div&gt;</div><div class="line">    &lt;/li&gt;</div><div class="line">&lt;/template&gt;</div></pre></td></tr></table></figure></p>
<h3 id="appPage-template"><a href="#appPage-template" class="headerlink" title="appPage template"></a>appPage template</h3><p>和上面是同样的道理，在 views 里添加 appPage.html 文件，然后为 appPage template 填充数据，不同的是这里多了中间一步，通过遍历 recommendedApps，我们只能从数据库里得到 recommended app 的 appid, 然而并没有 data context，也就是说，我们还需要有一个 template helper 来设置 recommended apps 的 data context，所以需要新建一个 appPage.js 文件来完成这个工作。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">// Template.&#123;&#123;templateName&#125;&#125;.helpers() takes a JSON	object where the keys are the name of hte helper function</div><div class="line">// these functions can be called directly from a blaze template</div><div class="line">Template.appPage.helpers(&#123;</div><div class="line">    getSuggestedApp: function(appId) &#123;</div><div class="line">      // use singleAppByAppId subscription, then use Apps.findOne() to retrieve the app object and return it to our	Blaze</div><div class="line">      // template, the pass the object to suggestedApp templates as the data context</div><div class="line">        Meteor.subscribe(&apos;singleAppByAppId&apos;, appId);</div><div class="line">        return Apps.findOne(&#123;app_id: appId&#125;);</div><div class="line">    &#125;</div><div class="line">&#125;);</div><div class="line"></div><div class="line"></div><div class="line">// attach JQuerystyle event listeners on a template by passing a json object to Template.&#123;&#123;templateName&#125;&#125;.events().</div><div class="line">// the key here is the event type(click in this case) followed by the CSS selecter(#backLink in this case)</div><div class="line">// values are the functions	to be executed on event click. here we are using a feature of iron router “history.back()” to</div><div class="line">// bring us back to the previous page</div><div class="line">Template.appPage.events(&#123;</div><div class="line">    &quot;click #backLink&quot; : function(evt) &#123;</div><div class="line">        history.back();</div><div class="line">    &#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<p>之后再修改 routers.js 文件，加上<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Router.route(&apos;/app/:_id&apos;, &#123;//’:’ tells Iron	Router that this is a variable that will be bound to the _id parameter</div><div class="line">    name: &apos;appPage&apos;,</div><div class="line">    waitOn: function() &#123;</div><div class="line">      // pass the app id passed in the url by using this.params._id</div><div class="line">        Meteor.subscribe(&apos;singleApp&apos;, this.params._id);</div><div class="line">    &#125;,</div><div class="line">    data: function () &#123;</div><div class="line">      // bind the app with the given id to the data context of template</div><div class="line">        return Apps.findOne(this.params._id);</div><div class="line">    &#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-07%20%E4%B8%8A%E5%8D%8810.38.31.png" alt=""></p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-07%20%E4%B8%8B%E5%8D%886.03.40.png" alt=""></p>
<p><a href="https://github.com/Shuang0420/App-Recommender-System/tree/master/app_store" target="_blank" rel="external">代码</a></p>
<blockquote>
<p>参考链接<br><br><a href="http://zh.discovermeteor.com/pdf" target="_blank" rel="external">DISCOVER METEOR</a><br></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Web Application </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> Meteor </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[项目实战--云计算Social Networking Timeline]]></title>
      <url>http://www.shuang0420.com/2016/07/05/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98--%E4%BA%91%E8%AE%A1%E7%AE%97Social%20Networking%20Timeline/</url>
      <content type="html"><![CDATA[<p>CMU 15619 Cloud Computing 的 individual project，项目全名是 Social Networking Timeline with Heterogeneous Back-ends，通过 MySQL/HBase/MongoDB 实现简化版 twitter 的后端。<br><a id="more"></a></p>
<h1 id="Implementing-Basic-Login-with-MySQL-on-RDS"><a href="#Implementing-Basic-Login-with-MySQL-on-RDS" class="headerlink" title="Implementing Basic Login with MySQL on RDS"></a>Implementing Basic Login with MySQL on RDS</h1><p>AWS RDS 配置 MySQL 并导入 users.csv and userinfo.csv 数据集，</p>
<p>数据集：</p>
<ul>
<li>users.csv [UserID, Password]</li>
<li>userinfo.csv  [UserID, Name, Profile Image URL]</li>
</ul>
<p>如果用户名密码正确，返回 user name and Profile Image Url，<br>如果不正确，name 返回 “Unauthorized”，Profile Image URL 返回 “#”.</p>
<p>Request:</p>
<pre>GET /task1?id=[UserID]&pwd=[Password]</pre>

<p>Response:</p>
<pre>returnRes({"name":"my_name", "profile":"profile_image_url"})</pre>



<p>效果：<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-11%20%E4%B8%8B%E5%8D%887.40.29.png" alt=""></p>
<h1 id="Storing-Social-Graph-using-HBase"><a href="#Storing-Social-Graph-using-HBase" class="headerlink" title="Storing Social Graph using HBase"></a>Storing Social Graph using HBase</h1><p>数据集：</p>
<ul>
<li>links.csv [Followee, Follower]</li>
</ul>
<p>对 followers 进行排序。<strong>排序规则：</strong></p>
<ol>
<li>按姓名进行升序排序</li>
<li>按 Profile Image URL 进行升序排序</li>
</ol>
<p>实现：<br>从 HBase 中根据 userid 找出 followers，再从 MySQL 中根据 follower userid 找出 name 和 profile url 并进行排序。</p>
<p>这里的问题是 HBase 的表如何设计能最大化性能。可以采用的方式为：<br>对数据集进行处理，按 followee 排序然后按 followers 排序，并进行合并，得到 [Followee, FollowerList]，followee 作为 rowkey。</p>
<p>Request:</p>
<pre>GET /task2?id=[UserID]</pre>

<p>Response:</p>
<pre>{"followers":[{"name":"follower_name_1", "profile":"profile_image_url_1"}, {"name":"follower_name_2", "profile":"profile_image_url_2"}, ...]}</pre>


<h1 id="Build-Homepage-using-MongoDB"><a href="#Build-Homepage-using-MongoDB" class="headerlink" title="Build Homepage using MongoDB"></a>Build Homepage using MongoDB</h1><p>同样是对 HBase 表的设计。这里要求的是根据 userid 找到 followees，然后再找到 followees 的 posts。为了提高性能，可以做的是：<br>对数据集进行处理，按 follower 排序然后按 followees 排序，并进行合并，得到 [Follower, FolloweeList]</p>
<p>数据集：<br>posts.csv<br>{<br>    “pid”:xxx,                                      // PostID<br>    “uid”:xxx,                                      // UserID of poster<br>    “name”:”xxx”,                                   // User name of poster<br>    “profile”:”xxx”,                                // Poster profile image URL<br>    “timestamp”:”YYYY-MM-DD HH:MM:SS”,              // When post is posted<br>    “image”:”xxx”,                                  // Post image<br>    “content”:”xxx”,                                // Post text content<br>    “comments”:[                                    // comments json array<br>        {<br>            “uid”:xxx,                              // UserID of commenter<br>            “name”:”xxx”,                           // User name of commenter<br>            “profile”:”xxx”,                        // Commenter profile image URL<br>            “timestamp”:”YYYY-MM-DD HH:MM:SS”,      // When comment is made<br>            “content”:”xxx”                         // Comment text content<br>        },<br>        {<br>            “uid”:xxx,<br>            …….<br>        },<br>        ……<br>    ]<br>}</p>
<p>Request:</p>
<pre>GET /task3?id=[UserID]</pre>

<p>Response:</p>
<pre>{"posts":[{post1_json}, {post2_json}, ...]}</pre>


<h1 id="Put-Everything-Together"><a href="#Put-Everything-Together" class="headerlink" title="Put Everything Together"></a>Put Everything Together</h1><p>显示 user 关注的人的最新 30 篇 posts<br><strong>排序规则：</strong><br>对 followers 进行排序。排序规则：</p>
<ol>
<li>按姓名进行升序排序</li>
<li>按 Profile Image URL 进行升序排序</li>
</ol>
<p>对最新 30 篇 posts 排序：</p>
<ol>
<li>按 timestamp 升序排序</li>
<li>按 pid (PostID) 升序排序</li>
</ol>
<p>不满 30 篇 posts 返回全部。</p>
<p>Sample Request:</p>
<pre>http://backend-public-dns:8080/MiniSite/task4?id=99</pre>

<p>Sample Response:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">   &quot;followers&quot;: [</div><div class="line">      &#123;</div><div class="line">         &quot;name&quot;: &quot;Alastair Moock&quot;,</div><div class="line">         &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/61ed34b1f6bcd5498d888e3c2a1768.png&quot;</div><div class="line">      &#125;,</div><div class="line">      &#123;</div><div class="line">         &quot;name&quot;: &quot;Amr Diab&quot;,</div><div class="line">         &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/507e08e097e49ffaa584b988748180.png&quot;</div><div class="line">      &#125;,</div><div class="line">      &#123;</div><div class="line">         &quot;name&quot;: &quot;CJ Bolland&quot;,</div><div class="line">         &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/699bb8d37e61d66750e58fd1513637.png&quot;</div><div class="line">      &#125;,</div><div class="line">	... (more followers ommitted)</div><div class="line">   ],</div><div class="line">   &quot;name&quot;: &quot;Accent&quot;,</div><div class="line">   &quot;posts&quot;: [</div><div class="line">      &#123;</div><div class="line">         &quot;content&quot;: &quot;Wow, just experienced Screamers (2006)&quot;,</div><div class="line">         &quot;timestamp&quot;: &quot;2015-08-07 19:06:57&quot;,</div><div class="line">         &quot;uid&quot;: 2587,</div><div class="line">         &quot;_id&quot;: &#123;</div><div class="line">            &quot;$oid&quot;: &quot;56b06bde2fa550d2061f30c2&quot;</div><div class="line">         &#125;,</div><div class="line">         &quot;name&quot;: &quot;Beggars Opera&quot;,</div><div class="line">         &quot;image&quot;: &quot;http://cmucloudsocial.s3.amazonaws.com/posts/Screamers_2006_.png&quot;,</div><div class="line">         &quot;pid&quot;: 156154,</div><div class="line">         &quot;comments&quot;: [</div><div class="line">            &#123;</div><div class="line">               &quot;uid&quot;: 34190,</div><div class="line">               &quot;timestamp&quot;: &quot;2015-10-21 13:57:59&quot;,</div><div class="line">               &quot;content&quot;: &quot;I have seen this movie on starz, I regret to say that I was not lucky enough to have watched it while the screening took place. This documentary follows the one and only System Of A Down, throughout several locations from LA to Europe, while diggin deep in history and showing the truth about the forgotten genocide. This movie includes interviews with experts and some of the survivors of the genocide. This sad story of human history also follows the massacre that took place in africa during 2004 while the whole world stood watching, idle in the face of massive death. To conclude this, several SOAD fans won&apos;t be dissapointed by the extensive repertoire of songs played throughout the film. I&apos;m glad I&apos;m finally getting the DVD after a long wait, I hope you feel the same way.&quot;,</div><div class="line">               &quot;name&quot;: &quot;Youngster&quot;,</div><div class="line">               &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/6a75f0eea78b8cd2c0ae4da2f85f34.png&quot;</div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">               &quot;uid&quot;: 27184,</div><div class="line">               &quot;timestamp&quot;: &quot;2015-10-21 21:36:12&quot;,</div><div class="line">               &quot;content&quot;: &quot;Great movie for fans of System of a Down. Better yet, this is a great movie documenting genocide in general, and the Armenian genocide in particular. I highly recommend this movie. It is a must see. Share the movie with friends, family, and members of your local community. Everyone will thank you for it. Very eye-opening experience.&quot;,</div><div class="line">               &quot;name&quot;: &quot;Shirobon&quot;,</div><div class="line">               &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/539869e8b863511771c3b0b5e13d94.png&quot;</div><div class="line">            &#125;,</div><div class="line">			... (more comments omitted)</div><div class="line">         ],</div><div class="line">         &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/8f64d02d77cf734ddde87b7832ca76.png&quot;</div><div class="line">      &#125;,</div><div class="line">      &#123;</div><div class="line">         &quot;content&quot;: &quot;Wow, just experienced The Tube (2004)&quot;,</div><div class="line">         &quot;timestamp&quot;: &quot;2015-08-11 02:42:40&quot;,</div><div class="line">         &quot;uid&quot;: 357,</div><div class="line">         &quot;_id&quot;: &#123;</div><div class="line">            &quot;$oid&quot;: &quot;56b06be12fa550d2061f706a&quot;</div><div class="line">         &#125;,</div><div class="line">         &quot;name&quot;: &quot;Agoria&quot;,</div><div class="line">         &quot;image&quot;: &quot;http://cmucloudsocial.s3.amazonaws.com/posts/The_Tube_2004_.png&quot;,</div><div class="line">         &quot;pid&quot;: 175927,</div><div class="line">         &quot;comments&quot;: [</div><div class="line">            &#123;</div><div class="line">               &quot;uid&quot;: 29700,</div><div class="line">               &quot;timestamp&quot;: &quot;2015-09-15 10:20:44&quot;,</div><div class="line">               &quot;content&quot;: &quot;A FORMER GOVERNMENT AGENT HOLDS A TRAIN HOSTAGE WITH A BOMB THAT&apos;LL BLOW UP IF THE TRAIN STOPS AND IT&apos;S UP TO A DETECTIVE TO STOP HIM AND FIND A WAY TO SAVE THE LIVES OF THE PASSENGERS. WHAT WE HAVE HERE IS BASICALLY ANOTHER IMITATION OF &apos;&apos;SPEED&apos;&apos;. THE DIALOGUE IS LAUGHABLE AND THE ACTION [WHICH THERE IS PLENTY OF] IS NOT REALLY THAT ENTERTAINING. THE ACTING IS ALSO PRETTY BAD, BUT THE MOVIE TENDS TO SHOW A FEW SIGNS OF LIFE IN THE LAST 30 MINUTES. IF YOU&apos;RE AN ACTION FAN [LIKE ME] AND YOU&apos;RE CURIOUS ABOUT THIS MOVIE, RENT IT. BUT DON&apos;T BUY IT. ON THIS DVD, YOU HAVE THE CHOICE OF WATCHING THIS MOVIE DUBBED IN EITHER ENGLISH OR FRENCH OR YOU CAN WATCH THIS MOVIE IN ITS ORIGINAL LANGUAGE, WHICH IS KOREAN.&quot;,</div><div class="line">               &quot;name&quot;: &quot;Teimoso&quot;,</div><div class="line">               &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/a3a08f80ee44be2438293a04d2f9f2.png&quot;</div><div class="line">            &#125;,</div><div class="line">            &#123;</div><div class="line">               &quot;uid&quot;: 31346,</div><div class="line">               &quot;timestamp&quot;: &quot;2015-10-12 06:16:52&quot;,</div><div class="line">               &quot;content&quot;: &quot;A Hollywood movie went out into the world, traveled to Korea, got assimilated and regurgitated, and now it returns to our shores as this. The studios know it and advertise it using reviews that cast it as the Korean version of Speed. It also &quot;borrows&quot; a score straight from Hans Zimmer&apos;s work for The Rock, and the main actor looks and acts like Chow Yun Fat light. It&apos;s discouraging to see Korean cinema paying homage to American action flicks when it has so many more interesting stories to tell. At least Woon-Hak Baek&apos;s first feature, Shiri, spoke in a unique voice and told a story personal to the Korean experience. This is a step backwards for him.On the other hand, this movie composite of so many action movies we&apos;ve seen before is fascinating in its skewed familiarity. It&apos;s not terrible; the production values are high, the acting occasionally thrilling, the one-liners sometimes amusing. It&apos;s no more or less diverting than the average Hollywood Die Hard knockoff. I think of it as top notch karaoke, like American Idol. In the proper context, it&apos;s impressive.In the grand scheme of things, though, it&apos;s depressing, especially when Korean directors like Chan-wook Park are producing such unique and energetic work.&quot;,</div><div class="line">               &quot;name&quot;: &quot;The Veronicas&quot;,</div><div class="line">               &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/1bd4dc1a4ca7a49daf53ca9a03735e.png&quot;</div><div class="line">            &#125;,</div><div class="line">			... (more comments omitted)</div><div class="line">         ],</div><div class="line">         &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/329125b52f0db7d04ed8828b5eccac.png&quot;</div><div class="line">      &#125;,</div><div class="line">	... (more posts ommitted)</div><div class="line">   ],</div><div class="line">   &quot;profile&quot;: &quot;https://cmucloudsocial.s3.amazonaws.com/profiles/8e8a1b156037ed1ecfba40b917084e.png&quot;</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-11%20%E4%B8%8B%E5%8D%888.01.42.png" alt=""></p>
<h1 id="Basic-Recommendation"><a href="#Basic-Recommendation" class="headerlink" title="Basic Recommendation"></a>Basic Recommendation</h1><p>根据 userid 推荐 10 个 user。<br><strong>算法：</strong> 基于用户的协同过滤算法。<br>Eg.<br>assume A follows {B, C, D}.<br>Followee B follows {C, E, A},<br>followee C follows {F, G} and<br>followee D follows {G, H}.</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-11%20%E4%B8%8B%E5%8D%888.54.32.png" alt=""></p>
<p>得分：{G: 2, E: 1, F: 1, H: 1}</p>
<p>排序规则：</p>
<ol>
<li>按得分降序排序</li>
<li>按 user id 升序排序</li>
</ol>
<p>少于 10 个用户返回全部。</p>
<p>Request:</p>
<pre>GET /task2?id=[UserID]</pre>

<p>Response:</p>
<pre>returnRes({"recommendation":[{name:<name1>, profile:<profile1>},{name:<name2>, profile:<profile2>},...,{name:<name10>, profile:<profile10>]})</profile10></name10></profile2></name2></profile1></name1></pre>

<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-08-11%20%E4%B8%8B%E5%8D%888.28.48.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Projects </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Back-ends </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结--汇总贴]]></title>
      <url>http://www.shuang0420.com/2016/06/27/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93--%E6%B1%87%E6%80%BB%E8%B4%B4/</url>
      <content type="html"><![CDATA[<p>从 抓取 –&gt; 分析 –&gt; 存储 三个部分对之前5篇博客和代码进行重组。<br><a id="more"></a></p>
<h1 id="抓取"><a href="#抓取" class="headerlink" title="抓取"></a>抓取</h1><h2 id="cloud-scrapy"><a href="#cloud-scrapy" class="headerlink" title="cloud scrapy"></a>cloud scrapy</h2><p><a href="http://www.shuang0420.com/2016/06/15/爬虫总结-三-scrapinghub/">博客</a></p>
<h2 id="防止爬虫被-ban"><a href="#防止爬虫被-ban" class="headerlink" title="防止爬虫被 ban"></a>防止爬虫被 ban</h2><p>设置 user-agent 和 ip 代理来防止爬虫被 ban 的测试<br><a href="https://github.com/Shuang0420/Crawler_examples/tree/master/blogCrawler" target="_blank" rel="external">代码</a><br><a href="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/">博客</a></p>
<h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p><a href="https://github.com/Shuang0420/Crawler_examples/blob/master/examples/examples/spiders/GithubSpider.py" target="_blank" rel="external">代码</a><br><a href="http://www.shuang0420.com/2016/06/20/爬虫总结-五-其他技巧/">博客</a></p>
<h2 id="scrapy-splash-爬取js交互式表格数据"><a href="#scrapy-splash-爬取js交互式表格数据" class="headerlink" title="scrapy-splash 爬取js交互式表格数据"></a>scrapy-splash 爬取js交互式表格数据</h2><p><a href="https://github.com/Shuang0420/Crawler_examples/blob/master/examples/examples/spiders/JsSpider.py" target="_blank" rel="external">代码</a><br><a href="http://www.shuang0420.com/2016/06/20/爬虫总结-五-其他技巧/">博客</a></p>
<h2 id="同时运行多个爬虫"><a href="#同时运行多个爬虫" class="headerlink" title="同时运行多个爬虫"></a>同时运行多个爬虫</h2><p><a href="https://github.com/Shuang0420/Crawler_examples/blob/master/FAQscrapy/FAQscrapy/run.py" target="_blank" rel="external">代码</a></p>
<h2 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h2><p><a href="http://www.shuang0420.com/2016/06/17/爬虫总结-四-分布式爬虫/">博客</a><br><a href="https://github.com/Shuang0420/distributed_crawler" target="_blank" rel="external">代码</a></p>
<h2 id="增量爬取"><a href="#增量爬取" class="headerlink" title="增量爬取"></a>增量爬取</h2><p>利用 redis<br><a href="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/">博客</a></p>
<h2 id="处理验证码"><a href="#处理验证码" class="headerlink" title="处理验证码"></a>处理验证码</h2><ul>
<li>更换ip地址</li>
<li>使用cookie登陆</li>
<li>验证码识别手段</li>
</ul>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="不规则的-html"><a href="#不规则的-html" class="headerlink" title="不规则的 html"></a>不规则的 html</h2><p><a href="https://github.com/Shuang0420/Crawler_examples/blob/master/FAQscrapy/FAQscrapy/spiders/FAQ_jingdong.py" target="_blank" rel="external">代码</a><br><a href="http://www.shuang0420.com/2016/06/17/爬虫总结-五-其他技巧/">博客</a></p>
<h1 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h1><h2 id="如何进行网页去重？"><a href="#如何进行网页去重？" class="headerlink" title="如何进行网页去重？"></a>如何进行网页去重？</h2><p>将 url 抽象为关键特征相似度的计算。比如可以把站点抽象为一维特征，目录深度抽象为一维特征，一级目录、二级目录、尾部页面的名字也都可以抽象为一维特征。得到各个维度的特征，定义每个特征的重要程度，给出公式，把这个问题简化成一个机器学习的问题，只需要人为判断出一批url是否相似，用svm训练一下就可以达到机器判断的目的。<br><a href="http://www.shuang0420.com/2016/06/11/爬虫总结（一）/">博客</a></p>
<h2 id="内容以什么形式存储？"><a href="#内容以什么形式存储？" class="headerlink" title="内容以什么形式存储？"></a>内容以什么形式存储？</h2><p>关系数据库 or NoSQL? (待补充)</p>
<h1 id="代码快速通道"><a href="#代码快速通道" class="headerlink" title="代码快速通道"></a>代码快速通道</h1><p><a href="https://github.com/Shuang0420/Crawler/tree/master/tieba" target="_blank" rel="external">百度贴吧</a><br><a href="https://github.com/Shuang0420/Crawler/tree/master/zhidao" target="_blank" rel="external">百度知道</a><br><a href="https://github.com/Shuang0420/Crawler/tree/master/wangyi" target="_blank" rel="external">网易新闻</a><br><a href="https://github.com/Shuang0420/distributed_crawler" target="_blank" rel="external">百度搜索、贴吧、知道分布式爬虫</a><br><a href="https://github.com/Shuang0420/Crawler_examples/tree/master/FAQscrapy/FAQscrapy" target="_blank" rel="external">京东、苏宁FAQ</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[numpy－理解 keepdims=True]]></title>
      <url>http://www.shuang0420.com/2016/06/26/numpy%EF%BC%8D%E7%90%86%E8%A7%A3keepdims=True/</url>
      <content type="html"><![CDATA[<p>理解 numpy 中的 keepdims。实现 softmax 时遇到的坑。<br><a id="more"></a></p>
<h2 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h2><p>文档对 numpy.sum 里 keepdims 的说明如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">numpy.sum(a, axis=None, dtype=None, out=None, keepdims=False)[source]</div><div class="line"></div><div class="line">keepdims : bool, optional</div><div class="line">If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</div></pre></td></tr></table></figure></p>
<p>边做实验边来解释。</p>
<h2 id="np-max-x"><a href="#np-max-x" class="headerlink" title="np.max(x)"></a>np.max(x)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x=np.array([[1001, 1002], [3, 4]])</div><div class="line">&gt;&gt;&gt; x -= np.max(x)</div><div class="line">&gt;&gt;&gt; x</div><div class="line">array([[  -1,    0],</div><div class="line">       [-999, -998]])</div></pre></td></tr></table></figure>
<p>np.max(x) 的结果一个数 1002, 矩阵的最大值。<br>因此 x -= np.max(x) 的效果是减去矩阵 x 中所有元素最大值</p>
<h2 id="np-max-x-axis-1"><a href="#np-max-x-axis-1" class="headerlink" title="np.max(x, axis=1)"></a>np.max(x, axis=1)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; x=np.array([[1001, 1002], [3, 4]])</div><div class="line">&gt;&gt;&gt; x -= np.max(x, axis=1)</div><div class="line">&gt;&gt;&gt; x</div><div class="line">array([[  -1,  998],</div><div class="line">       [-999,    0]])</div></pre></td></tr></table></figure>
<p>axis=1 代表以行为单位，因此 np.max(x, axis＝1) 求的是每行的最大值，然而注意它的结果默认是一个行向量，[1002, 4]<br>如果是 axis=0，代表以列为单位，求每一列最大值，结果是 [1001, 1002]<br>x -= np.max(x, axis=1) 在这里完全不 make sense，如果硬要解释的话，就是第 i 列减去 第 i 行的最大值，当然这就要求矩阵必须是方阵。（没有 broadcast）<br>所以我们要做的实际就是把这个行向量转化为列向量，然后 broadcast correctly against the input array，这也就是 keepdims 的功能。</p>
<h2 id="np-max-x-axis-1-keepdims-True-减去每一行最大值的正确打开方式"><a href="#np-max-x-axis-1-keepdims-True-减去每一行最大值的正确打开方式" class="headerlink" title="np.max(x, axis=1, keepdims=True) 减去每一行最大值的正确打开方式"></a>np.max(x, axis=1, keepdims=True) 减去每一行最大值的正确打开方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; x=np.array([[1001, 1002], [3, 4]])</div><div class="line">&gt;&gt;&gt; x -= np.max(x, axis=1, keepdims=True)</div><div class="line">&gt;&gt;&gt; x</div><div class="line">array([[-1,  0],</div><div class="line">       [-1,  0]])</div></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[论文笔记 - Distributed representations of sentences and documents]]></title>
      <url>http://www.shuang0420.com/2016/06/22/%E5%8F%A5%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>算法的 PV-DM 版本与 PV-DBOW 的核心理论，为了方便描述，这里的 paragraph 与句子同义。<br><a id="more"></a></p>
<p><strong>原文：</strong> LE, Quoc V.; MIKOLOV, Tomas. <a href="(http://arxiv.org/pdf/1405.4053.pdf">Distributed representations of sentences and documents</a>). arXiv preprint arXiv:1405.4053, 2014.</p>
<p><strong>译文：</strong> <a href="http://blog.csdn.net/liaocyintl/article/details/50369158" target="_blank" rel="external">Word2vec 句向量模型PV-DM与PV-DBOW原论文翻译</a></p>
<h1 id="分布记忆模型-PV-DM"><a href="#分布记忆模型-PV-DM" class="headerlink" title="分布记忆模型(PV-DM)"></a>分布记忆模型(PV-DM)</h1><p>用神经网络训练词向量的逻辑是，让网络去预测单词(目标词/上下文)这样的任务，句向量也是一样，我们给定从句子里的一些上下文，让网络去预测下一个单词。在句（Paragraph）向量模型中，每一个句子都被映射成一个独立的向量，这个句向量作为矩阵 D 的一列；同时，每一个词也被映射成一个独立的向量，这个词向量作为矩阵 W 的一列。对这个句向量和这些词向量求平均或者首尾相连，用来预测文本中的下一个词。这里，我们选用首尾相连来组合这些矩阵。</p>
<p>严格的说，与 Word2vec 的公式相比，唯一的不同点在于这里从 W 和D 两个矩阵中构造 h。句子的标识（Token）被当做另外一个“词”看待。它扮演一个“Memory”的角色，用来记忆当前文本或文章主题中漏掉了什么。因此，这个模型被称为“句向量的分布记忆模型”(PV-DM: Distributed Memory Model of Paragraph Vectors)。</p>
<p>上下文是固定长度的，从句子的一个滑动窗口中取样。句向量被这这个句子产生的所有上下文共享，但不超越句子。但是词向量矩阵 W 是超越句子，全局共享的。比如说，”powerful”的词向量也对所有的句子有效。</p>
<p>通过随机梯度下降法来训练这些句向量和词向量，在此过程中通过反向传播获得梯度。在随机梯度下降的每一步，都可以从一个随机的句子中抽取一个定长的上下文，从网络中计算出梯度误差，然后更新模型的参数。</p>
<p>在预测阶段，需要执行一个“推断（inference）”步骤计算新句子的句向量。也是通过梯度上升来获取。在这个阶段，其余的模型参数、词向量矩阵 W 和 softmax 权重是固定的。</p>
<p>假设语料库中有 N 个句子，字典里有 M 个词汇；我们试图将每一个句子映射到 p 维空间，每一个词映射到 q 维空间，于是这个模型就有总共 N×p+M×q 个参数（包括softmax参数）。即使句子的数量会随着 N 的增大而增大，训练中的更新还是稀疏且高效。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-08%20%E4%B8%8B%E5%8D%882.31.53.png" alt=""></p>
<p>经过训练，这些句向量就可以当做句子的特征使用。我们可以把这些特征直接用于传统的机器学习技术，比如逻辑回归、支持向量机或者 K-means 聚类。<br>总而言之，这个算法有两个关键阶段：</p>
<ol>
<li>通过训练获得词向量矩阵 W, softmax 权重 U, b 以及句向量 D；</li>
<li>第二个阶段是推断阶段，用于取得一个新句子（没有出现过）的句向量 D，通过增加更多的列在矩阵 D 里，并保持 W, U, b 不变的情况下在矩阵 D 上进行梯度下降。我们使用 D 通过一个基础的分类器给句子加上标签</li>
</ol>
<p>句向量有两个显著的优点：</p>
<ol>
<li>它的训练集是没有被标签的数据，因此它可以被用于一些训练样本标签不足的任务</li>
<li>句向量也解决了词袋模型的一些关键的弱点。第一，它继承了词向量的一个重要特性——词和词之间的语义。在语义里，“强有力”比起“巴黎”来说，和“强壮”更接近。第二，它考虑到了“词序（word order）”，n-gram 模型则需要设置一个较大的 n 才能做到。这一点很重要，因为模型保存了句子中大量的信息，包括词序。也就是说，我们的模型优于词袋 n-gram 模型，因为后者会表现出一个极高的维度，影响效率而且很难泛化</li>
</ol>
<h1 id="分布词袋模型-PV-DBOW-无词序句向量"><a href="#分布词袋模型-PV-DBOW-无词序句向量" class="headerlink" title="分布词袋模型(PV-DBOW)-无词序句向量"></a>分布词袋模型(PV-DBOW)-无词序句向量</h1><p>上面的方法讨论了在一个文本窗口内，通过句向量和词向量的首尾相接来预测下一个词。另一种方法不把上下文中的词作为输入，而是强制这个模型在输出中从句子中随机抽取词汇来进行预测。实际上，其意义在于在每一个随机梯度下降的迭代中，我们抽取一个文本窗口，然后从这个文本窗口中抽取一个词，然后通过一个分类任务得到句向量。这项技术如图所示。我们把这个版本称为句向量的分布词袋(PV-DBOW): Distributed Bag of Words version of Paragraph Vector）。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-08%20%E4%B8%8B%E5%8D%882.32.01.png" alt=""></p>
<p>句向量被训练出来，用来预测在一个小窗口中的词汇。</p>
<p>除了在概念上简单以外，这个模型只需要存储少量的数据。相比于上一个模型需要存储 softmax 权重和词向量，这个模型只需要存储 softmax 权重。同样的，这个模型也近似于 Skip-gram 模型。</p>
<h1 id="PV-DM-amp-PV-DBOW-结合"><a href="#PV-DM-amp-PV-DBOW-结合" class="headerlink" title="PV-DM &amp; PV-DBOW 结合"></a>PV-DM &amp; PV-DBOW 结合</h1><p>可以把每一个句向量当作两个向量的组合：一个通过 PV-DM 训练，另一个通过 PV-DBOW 训练。PV-DM 能够很好地执行多种任务，但是它结合 PV-DBOW 后，常常能够更加出色完成任务。</p>
<blockquote>
<p>参考链接<br><br><a href="http://blog.csdn.net/liaocyintl/article/details/50369158" target="_blank" rel="external">Word2vec 句向量模型PV-DM与PV-DBOW原论文翻译</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
            <tag> Deep learning </tag>
            
            <tag> doc2vec </tag>
            
            <tag> 句向量 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[词向量总结笔记（简洁版）]]></title>
      <url>http://www.shuang0420.com/2016/06/21/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>综合各家博客整理的词向量总结笔记。<br><a id="more"></a></p>
<h1 id="词向量模型"><a href="#词向量模型" class="headerlink" title="词向量模型"></a>词向量模型</h1><h2 id="one-hot-Vector"><a href="#one-hot-Vector" class="headerlink" title="one-hot Vector"></a>one-hot Vector</h2><h3 id="one-hot-vector"><a href="#one-hot-vector" class="headerlink" title="one-hot vector"></a>one-hot vector</h3><p>最简单的编码方式：假设我们的词库总共有n个词，那我们开一个1*n的高维向量，而每个词都会在某个索引index下取到1，其余位置全部都取值为0。</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>这种词向量编码方式简单粗暴，我们将每一个词作为一个完全独立的个体来表达。遗憾的是，这种方式下，我们的词向量没办法给我们任何形式的词组相似性权衡。因为你开了一个极高维度的空间，然后每个词语都会占据一个维度，因此没有办法在空间中关联起来。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>可以把词向量的维度降低一些，在这样一个子空间中，可能原本没有关联的词就关联起来了。</p>
<h2 id="基于-SVD-的方法"><a href="#基于-SVD-的方法" class="headerlink" title="基于 SVD 的方法"></a>基于 SVD 的方法</h2><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>这是一种构造词嵌入（即词向量）的方法，我们首先会遍历所有的文本数据集，然后统计词出现的次数，接着用一个矩阵 X 来表示所有的次数情况，紧接着对X进行奇异值分解得到一个 USVT 的分解。然后用 U 的行（rows）作为所有词表中词的词向量。对于矩阵 X ，有2种选择：全文或者窗口长度。</p>
<ul>
<li><p>词-文档矩阵<br>建立一个词组文档矩阵 X，具体是这么做的：遍历海量的文件，每次词组 i 出现在文件 j 中时，将 Xij 的值加1。不过大家可想而知，这会是个很大的矩阵R|V|×M，而且矩阵大小还和文档个数M有关系。所以咱们最好想办法处理和优化一下。word-document的共现矩阵最终会得到泛化的主题（例如体育类词汇会有相似的标记），这就是浅层语义分析(LSA, Latent Semantic Analysis)</p>
</li>
<li><p>基于窗口的共现矩阵 X<br>把矩阵X记录的词频变成一个相关性矩阵，对 X 做奇异值分解，观察奇异值（矩阵的对角元素），并根据我们期待保留的百分比来进行截断（只保留前k个维度），把子矩阵 U1:|V|,1:k 视作我们的词嵌入矩阵。也就是说，对于词表中的每一个词，我们都用一个 k 维的向量来表达了。窗口长度容易捕获语法（POS）和语义信息</p>
</li>
</ul>
<h3 id="对共现矩阵X进行奇异值分解"><a href="#对共现矩阵X进行奇异值分解" class="headerlink" title="对共现矩阵X进行奇异值分解"></a>对共现矩阵X进行奇异值分解</h3><p><a href="http://www.flickering.cn/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/2015/01/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%88we-recommend-a-singular-value-decomposition%EF%BC%89/" target="_blank" rel="external">特征值分解与奇异值分解</a><br>特征值分解只适用于方阵。当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。我们通过特征值分解得到的前N个特征向量，对应了这个矩阵最主要的N个变化方向。利用这前N个变化方向，可以近似这个矩阵（变换）。也就是 – 提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。<br>奇异值分解是一个能适用于任意的矩阵的一种分解的方法，可以通过求特征值得到。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%A5%87%E5%BC%82%E5%80%BC.png" alt=""></p>
<p><a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F" target="_blank" rel="external">Python中简单的词向量SVD分解</a></p>
<h3 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h3><ul>
<li>矩阵的维度会经常变化（新的词语经常会增加，语料库的大小也会随时变化）。</li>
<li>矩阵是非常稀疏的，因为大多数词并不同时出现。</li>
<li>矩阵的维度通常非常高（≈106×106），需要大量的存储</li>
<li>训练需要O(n2)的复杂度</li>
<li>对于新词或者新的文档很难及时更新</li>
<li>需要专门对矩阵X进行特殊处理，以应对词组频率的极度不平衡的状况</li>
</ul>
<h3 id="解决方案：直接学习低维度的词向量"><a href="#解决方案：直接学习低维度的词向量" class="headerlink" title="解决方案：直接学习低维度的词向量"></a>解决方案：直接学习低维度的词向量</h3><p>idea: 将最重要的信息存储在固定的，低维度的向量里：密集向量（dense vector)，维数通常是25-1000<br>然而，如何降维？</p>
<h2 id="基于迭代的方法"><a href="#基于迭代的方法" class="headerlink" title="基于迭代的方法"></a>基于迭代的方法</h2><p>创建一个模型，它能够一步步迭代地进行学习，并最终得出每个单词基于其上下文的条件概率。</p>
<h3 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a>n-gram</h3><p><strong>基本思想：</strong> 一个词出现的概率只与它前面固定数目的词相关。<br>主要工作是在预料中统计各种词串出现的次数以及平滑化处理，概率值计算号之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。<br>建立一个概率模型，它包含已知和未知参数。每增加一个训练样本，它就能从模型的输入、输出和期望输出（标签），多学到一点点未知参数的信息。<br>在每次迭代过程中，这个模型都能够评估其误差，并按照一定的更新规则，惩罚那些导致误差的参数。(误差“反向传播”法)。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>以 {“The”, “cat”, “over”, “the”, “puddle”} 为上下文，能够预测或产生它们中心的词语”jumped”。模型输入为 x(c)，模型输出为 y，y 就是中心词 ‘jumped’。对于每个词语 wi 学习了两个向量。</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/2.jpg" class="ful-image" alt="2.jpg">
<p>连续词袋模型（CBOW）中的各个记号：</p>
<ul>
<li>$W_i$：单词表 V 中的第 i 个单词, one-hot 向量</li>
<li>$v∈R^{n∗|V|}$：输入词矩阵</li>
<li>$v_i$：V的第i列，n 维 $W_i$ 的输入向量</li>
<li>$U∈R^{|V|∗n}$：输出词矩阵</li>
<li>$U_i$：U 的第 i 行，n 维 $W_i$ 的输出向量</li>
</ul>
<p>把整个过程拆分成以下几步：</p>
<ul>
<li>对于 m 个词长度的输入上下文，我们产生它们的 one-hot 向量 $(x^{c−m},⋯,x^{c−1},x^{c+1},⋯,x^{c+m})$，作为模型输入。</li>
<li>我们得到上下文的嵌入词向量 $(v_{c−m+1}=Vx^{c−m+1},⋯,v_{c+m}=Vx^{c+m})$</li>
<li>将这些向量取平均 $\widehat{v}=\frac {V_{c−m} +V_{c−m+1} +⋯+V_{c+m}}{2m}$</li>
<li>产生一个得分向量 $z=U\widehat{v}$</li>
<li>将得分向量转换成概率分布形式 $\widehat{y}=softmax(z)$</li>
<li>我们希望我们产生的概率分布 ,与真实概率分布 $\widehat{y}$ 相匹配。而 y 刚好也就是我们期望的真实词语的one-hot向量。</li>
</ul>
<p><strong> 怎样找到矩阵U、V？</strong><br>目标函数选交叉熵，用梯度下降法去更新每一个相关的词向量 $U_c$ 和 $V_j$.</p>
<p>当我们试图从已知概率学习一个新的概率时，最常见的是从信息论的角度寻找方法来评估两个概率分布的差距。其中广受好评又广泛应用的一个评估差异/损失的函数是交叉熵：<br>$H(\widehat{y},y) = -\sum_{j=1}^{|V|}y_jlog(\widehat{y}_j)$</p>
<p>结合我们当下的例子，y 只是一个one-hot向量，于是上面的损失函数就可以简化为：<br>$H(\widehat{y},y) = -y_jlog(\widehat{y}_j)$</p>
<p>我们用 c 表示 y 这个 one-hot 向量取值为 1 的那个维度的下标。所以在我们预测为准确值的情况下 $\widehat{y}_c=1$。于是损失为 −1 log(1) = 0。所以对于一个理想的预测值，因为预测得到的概率分布和真实概率分布完全一样，因此损失为0。相反,当我们的预测结果非常不理想， $\widehat{y}_c=0.01$。计算得到的损失为−1 log(0.01) ≈ 4.605，损失非常大，原本这才是标准结果，可是你给了一个非常低的概率，因此会拿到一个非常大的loss。最终的优化函数为：</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/cnn%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0.png" alt=""></p>
<h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p>与上面提到的模型对应的另一种思路，是以中心的词语 ”jumped” 为输入，能够预测或产生它周围的词语 ”The”, “cat”, “over”, “the”, “puddle” 等。这里我们叫 ”jumped” 为上下文。我们把它叫做Skip-Gram 模型。</p>
<p>这个模型的建立与连续词袋模型（CBOW）非常相似，但本质上是交换了输入和输出的位置。我们令输入的 one-hot 向量（中心词）为 x（因为它只有一个），输出向量为 y(j)。U 和 V 的定义与连续词袋模型一样。看一下网络结构图：</p>
<img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/1.jpg" class="ful-image" alt="1.jpg">
<p>举个例子，假设现在的数据集如下：</p>
<pre>the quick brown fox jumped over the lazy dog</pre>

<p>这个数据集中包含了词语及其上下文信息。上下文信息(Context)是一个比较宽泛的概念，有多种不同的理解：例如，词语周边的句法结构，词语的左边部分的若干个词语信息，对应的右半部分等。这里，我们使用最原始和基本的定义，即认为词语左右相邻的若干个词汇是该词对应的上下文信息。例如，取左右的词窗口为1，下面是数据集中的(上下文信息，对应的词)的pairs：</p>
<pre>
([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</pre>

<p>Skip-Gram模型是通过输入的目标词来预测其对应的上下文信息，所以目标是通过[quick]来预测[the]和[brown]，通过[brown]来预测[quick]和[fox]… 将上面的pair转换为(inpUt, output)的形式如下：</p>
<pre>
(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</pre>


<p>对应到上面部分，我们可以把 Skip-Gram 模型的运作方式拆分成以下几步：</p>
<ul>
<li>生成 one-hot 输入向量 x。</li>
<li>得到上下文的嵌入词向量 $V_c$=$V_x$。</li>
<li>因为这里不需要取平均值的操作，所以直接是$\widehat{v}=v_c$。</li>
<li>通过$U=UV_c$产生 2m 个得分向量 $U_{c−m},⋯,U_{c−1},U_{c+1},⋯,U_{c+m}$，如果上下文各取一个词，就是 $U_{c-1}$, $U_{c+1}$</li>
<li>将得分向量转换成概率分布形式 $y=softmax(u)$。</li>
<li>我们希望我们产生的概率分布与真实概率分布 $y^{c−m},⋯,y^{c−1},,y^{c+1}⋯,y^{c+m}$ 相匹配，也就是我们真实输出结果的 one-hot 向量。</li>
</ul>
<p>为模型设定一个目标/损失函数。不过不同的地方是我们这里需要引入朴素贝叶斯假设来将联合概率拆分成独立概率相乘。只要给出了中心词，所有的输出词是完全独立的。使用随机梯度下降算法(SGD)来进行最优化求解，并且使用mini-batch方法 (通常batch_size在16到512之间)。可以用随机梯度下降法去更新未知参数的梯度。<br>对应的优化函数是<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/skipgram.png" alt=""></p>
<p>这里值得一提的是，skipgram 和 PMI 之间是有联系的，Levy and Goldberg(2014) 提到过，skip-gram 在矩阵成为 PMI 的一个 shifted version 时($WW^{‘T}=M^{PMI}-logk$)，得到最优解，也就是说，</p>
<blockquote>
<p>Skip-gram is implicitly factoring a shifted version of the PMI matrix into the two embedding matrices.</p>
</blockquote>
<p>我们再次观察一下目标函数，注意到对整个单词表|V|求和的计算量是非常巨大的，任何一个对目标函数的更新和求值操作都会有O(|V|)的时间复杂度。我们需要一个思路去简化一下，我们想办法去求它的近似，可以参照负面采样（Negative Sampling）</p>
<h4 id="why-skip-gram"><a href="#why-skip-gram" class="headerlink" title="why skip-gram"></a>why skip-gram</h4><p>在NLP中，语料的选取是一个相当重要的问题。<br>首先，语料必须充分。一方面词典的词量要足够大，另一方面尽可能地包含反映词语之间关系的句子，如“鱼在水中游”这种句式在语料中尽可能地多，模型才能学习到该句中的语义和语法关系，这和人类学习自然语言是一个道理，重复次数多了，也就会模型了。<br>其次，语料必须准确。所选取的语料能够正确反映该语言的语义和语法关系。如中文的《人民日报》比较准确。但更多时候不是语料选取引发准确性问题，而是处理的方法。<br>由于窗口大小的限制，这会导致超出窗口的词语与当前词之间的关系不能正确地反映到模型中，如果单纯扩大窗口大小会增加训练的复杂度。Skip-gram模型的提出很好解决了这些问题。</p>
<p>我们来看看 skip-gram 的定义。</p>
<p>Skip-gram 实际上的定义很简单，就是允许跳几个字的意思。依照原论文里的定义，这个句子：</p>
<pre>Insurgents killed in ongoing fighting.</pre>

<p>在 bi-grams 的时候是拆成：</p>
<pre>{ insurgents killed, killed in, in ongoing, ongoing fighting }</pre>

<p>在 2-skip-bi-grams 的时候拆成：</p>
<pre>{ insurgents killed, insurgents in, insurgents ongoing, killed in, killed ongoing, killed fighting, in ongoing, in fighting, ongoing fighting }</pre>

<p>在 tri-grams 的时候是：</p>
<pre>{ insurgents killed in, killed in ongoing, in ongoing fighting }</pre>

<p>在 2-skip-tri-grams 的时候是：</p>
<pre>{ insurgents killed in, insurgents killed ongoing, insurgents killed fighting, insurgentsin ongoing, insurgents in fighting, insurgents ongoing fighting, killed in ongoing, killed in fighting, killed ongoing fighting, in ongoing fighting }</pre>

<p>这样就有办法在整篇文章都是用“台湾大学”的情况下以“台大”找到文章，解决一些“同义词”想要解决的问题。Skip-gram 一方面反映了句子的真实意思，另一方面还扩大了语料，2元词组由原来的4个扩展到了9个，3元词组由原来的3个扩展到了10个。</p>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word2Vec 是一个典型的预测模型，用于高效地学习Word Embedding，实现的模型就是上面提到的两种：连续词袋模型(CBOW)和Skip-Gram模型。算法上这两个模型是相似的</p>
<ul>
<li>CBOW 从输入的上下文信息来预测目标词</li>
<li>Skip-gram 模型则是相反的，从目标词来预测上下文信息<br>一般而言，这种方式上的区别使得 CBOW 模型更适合应用在小规模的数据集上，能够对很多的分布式信息进行平滑处理；而 Skip-Gram 模型则比较适合用于大规模的数据集上。</li>
</ul>
<p>另外一点是，embeddings 可以来捕捉关系！<br><img src="http://ox5l2b8f4.bkt.clouddn.com/images/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AE%80%E6%B4%81%E7%89%88%EF%BC%89/3.jpg" class="ful-image" alt="3.jpg"></p>
<h2 id="向量空间模型-Vector-space-models-VSMs"><a href="#向量空间模型-Vector-space-models-VSMs" class="headerlink" title="向量空间模型(Vector space models, VSMs)"></a>向量空间模型(Vector space models, VSMs)</h2><p>将词语表示为一个连续的词向量，并且语义接近的词语对应的词向量在空间上也是接近的。<br><strong>分布式假说理论：</strong> 该假说的思想是如果两个词的上下文(context)相同，那么这两个词所表达的语义也是一样的；换言之，两个词的语义是否相同或相似，取决于两个词的上下文内容，上下文相同表示两个词是可以等价替换的。</p>
<p>词向量生成方法主要分两大类：</p>
<ul>
<li>计数法(count-based methods, e.g. Latent Semantic Analysis)<br>在大型语料中统计词语及邻近的词的共现频率，然后将之为每个词都映射为一个稠密的向量表示；</li>
<li>预测法(predictive methods, e.g. neural probabilistic language models)。<br>直接利用词语的邻近词信息来得到预测词的词向量（词向量通常作为模型的训练参数）。</li>
</ul>
<h1 id="词向量任务评价"><a href="#词向量任务评价" class="headerlink" title="词向量任务评价"></a>词向量任务评价</h1><h2 id="内部任务评价"><a href="#内部任务评价" class="headerlink" title="内部任务评价"></a>内部任务评价</h2><p>内部任务评价的特点如下：</p>
<ul>
<li>一般是在一个特定的子任务中进行评测</li>
<li>计算很快</li>
<li>有助于理解相关的系统</li>
<li>在实际的NLP任务中表现好坏，可能需要外部关联实际应用</li>
</ul>
<h3 id="方法：词向量类比"><a href="#方法：词向量类比" class="headerlink" title="方法：词向量类比"></a>方法：词向量类比</h3><p>我们先输入一组不完整的类比 a:b::c:? 内部任务评价系统找出最大化余弦相似度的词向量<br>理想情况下，我们想得到xb−xa=xd−xc(例如，王后–国王 = 女演员 – 男演员)。于是xb−xa+xc=xd, 所以我们只需要找出一个与xb−xa+xc的标准化内积（比如余弦相似度）取最大值的词向量就可以了。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E4%BB%BB%E5%8A%A1%E8%AF%84%E4%BB%B7.png" alt=""></p>
<p>类比语料示例：</p>
<pre>
首都城市1 : 国家1 : : 首都城市2 : 国家2
Beijing:China::Astana       Kazakhstan
Beijing:China::Asmara       Eritrea
...

比较级
bad:worst::big            biggest
bad:worst::easy           easiest
...

时态
dancing:danced::decreased         decreased
dancing:danced::falling           fell
...
</pre>

<p><a href="https://code.google.com/archive/p/word2vec/source" target="_blank" rel="external">评测语料</a></p>
<h3 id="方法：相关性评价"><a href="#方法：相关性评价" class="headerlink" title="方法：相关性评价"></a>方法：相关性评价</h3><p>另外一个评测词向量质量的简单方法是人为对两个词的相似度在一个固定区间内打分(比如说 0-10)，再跟对应向量的余弦相适度进行对比。<br><a href="https://code.google.com/archive/p/word2vec/source/default/source" target="_blank" rel="external">评测语料</a></p>
<h3 id="考虑参数"><a href="#考虑参数" class="headerlink" title="考虑参数"></a>考虑参数</h3><ul>
<li>词向量的维度</li>
<li>资料库的大小</li>
<li>资料源/类型</li>
<li>上下文窗口的大小</li>
<li>上下文的对称性</li>
</ul>
<p>一般而言，</p>
<ul>
<li>精度和使用的模型高度相关，因为这些生成词向量的方法所依据的特性是完全不同的(如同时出现的次数，奇异向量等。)</li>
<li>文集量越大，精度越高，因为例子越多，生成的系统学习到的经验就更丰富。比如在完成词汇类比的例子中，系统如果之前没有接触测试词，就可能会生成错误的结果。</li>
<li>如果维度特别低或特别高，精度就会比较低。低维度词向量无法捕捉文集中不同词语的不同意义。这可以视为我们模型复杂度过低而导致的高偏差。比如 “king”, “queen”, “man”, “woman” 这几个词，我们需要至少2个维度像”gender” 如 “leadership” 来把它们编译成 2-字节 词向量。 过低的维度将无法捕捉四个词之间的语义差别，而过高的维度将捕捉到一些对泛化能力没有用的噪音– 即高方差的问题。</li>
</ul>
<h2 id="外部任务评价"><a href="#外部任务评价" class="headerlink" title="外部任务评价"></a>外部任务评价</h2><p>外部任务评价的特点如下：</p>
<ul>
<li>在一个实际任务中进行评测</li>
<li>需要花很长的时间来计算精度</li>
<li>不太清楚是否是某个子系统或者其他子系统，又或是几个子系统互相作用引起的问题</li>
<li>如果替换原有的子系统后获得精度提升，则说明替换很可能是有效的</li>
</ul>
<blockquote>
<p>参考链接：<br><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651641662&amp;idx=1&amp;sn=5cfba5999c1dd436af43523f53497094&amp;scene=21#wechat_redirect" target="_blank" rel="external">斯坦福大学课程：深度学习与自然语言处理</a><br><a href="http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F" target="_blank" rel="external">斯坦福大学深度学习与自然语言处理第二讲：词向量</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word2vec </tag>
            
            <tag> Deep learning </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 实战 MINST]]></title>
      <url>http://www.shuang0420.com/2016/06/20/TensorFlow%E5%AE%9E%E6%88%98-MNIST/</url>
      <content type="html"><![CDATA[<p>工作中需要实现 CNN、RNN 模型，于是开始学习 TensorFlow。这是第一篇，MNIST的实战。官方文档讲的很详细，这里我不过是用我的思路整理一遍，方便日后的查阅。<br><a id="more"></a></p>
<h1 id="TensorFlow-介绍"><a href="#TensorFlow-介绍" class="headerlink" title="TensorFlow 介绍"></a>TensorFlow 介绍</h1><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].</p>
<p>一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor 实例.</p>
<h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo pip install —upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl</div></pre></td></tr></table></figure>
<h2 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h2><ul>
<li>使用图 (graph) 来表示计算任务.</li>
<li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li>
<li>使用 tensor 表示数据.</li>
<li>通过 变量 (Variable) 维护状态.</li>
<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"># 常量</div><div class="line">a = tf.constant([3.0, 3.0])</div><div class="line"></div><div class="line"># 变量，变量要进行初始化</div><div class="line">x = tf.Variable([1.0, 2.0])</div><div class="line"># 变量初始化</div><div class="line">init_op = tf.initialize_all_variables()</div><div class="line"></div><div class="line"># 矩阵乘法</div><div class="line">product = tf.matmul(matrix1, matrix2)</div><div class="line"># 减法</div><div class="line">sub = tf.sub(x, a)</div><div class="line"># 加法</div><div class="line">new_value = tf.add(state, one)</div><div class="line"></div><div class="line"># Fetch</div><div class="line"># 启动默认图</div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line"># 执行矩阵乘法。函数调用 &apos;run(product)&apos; 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行</div><div class="line">result = sess.run(product)</div><div class="line"></div><div class="line"># 任务完成, 关闭会话.</div><div class="line">sess.close()</div><div class="line"></div><div class="line"># Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 &quot;with&quot; 代码块 来自动完成关闭动作.</div><div class="line">with tf.Session() as sess:</div><div class="line">  result = sess.run([product])</div><div class="line">  print result</div><div class="line"></div><div class="line"># 取回多个 tensor:</div><div class="line">input1 = tf.constant(3.0)</div><div class="line">input2 = tf.constant(2.0)</div><div class="line">input3 = tf.constant(5.0)</div><div class="line">intermed = tf.add(input2, input3)</div><div class="line">mul = tf.mul(input1, intermed)</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">  result = sess.run([mul, intermed])</div><div class="line">  print result</div><div class="line"></div><div class="line"># Feed</div><div class="line">#feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 run() 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 &quot;feed&quot; 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.</div><div class="line">input1 = tf.placeholder(tf.float32)</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line">output = tf.mul(input1, input2)</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">  print sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)</div></pre></td></tr></table></figure>
<h1 id="单层-SoftMax-神经网络模型"><a href="#单层-SoftMax-神经网络模型" class="headerlink" title="单层 SoftMax 神经网络模型"></a>单层 SoftMax 神经网络模型</h1><h2 id="加载-MNIST-数据"><a href="#加载-MNIST-数据" class="headerlink" title="加载 MNIST 数据"></a>加载 MNIST 数据</h2><p>60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。</p>
<p>每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。图片设为“xs”，标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。</p>
<p>每一张图片包含28X28个像素点。我们把这个数组展开成一个向量，长度是 28x28 = 784。</p>
<p>在 MNIST 训练数据集中，mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。</p>
<p>相对应的 MNIST 数据集的标签是介于0到9的数字，用来描述给定图片里表示的数字。为了用于这个教程，我们使标签数据是”one-hot vectors”。 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0。所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此， mnist.train.labels 是一个 [60000, 10] 的数字矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import tensorflow.examples.tutorials.mnist.input_data as input_data</div><div class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</div></pre></td></tr></table></figure>
<h2 id="运行-TensorFlow-的-InteractiveSession"><a href="#运行-TensorFlow-的-InteractiveSession" class="headerlink" title="运行 TensorFlow 的 InteractiveSession"></a>运行 TensorFlow 的 InteractiveSession</h2><p>Tensorflow 依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">sess = tf.InteractiveSession()</div></pre></td></tr></table></figure></p>
<h2 id="构建-Softmax-回归模型"><a href="#构建-Softmax-回归模型" class="headerlink" title="构建 Softmax 回归模型"></a>构建 Softmax 回归模型</h2><p>y = softmax(Wx + b)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># x 是一个占位符placeholder，在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。我们用2维的浮点数张量来表示这些图，这个张量的形状是[None，784 ]。（这里的None表示此张量的第一个维度可以是任何长度的。）</div><div class="line">x = tf.placeholder(tf.float32, [None, 784])</div><div class="line"></div><div class="line"># 权重值</div><div class="line">W = tf.Variable(tf.zeros([784,10]))</div><div class="line"></div><div class="line"># 偏离值</div><div class="line">b = tf.Variable(tf.zeros([10]))</div><div class="line"></div><div class="line"># 类别预测 － softmax 模型。（激活函数，线性输出-&gt;概率分布）</div><div class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</div></pre></td></tr></table></figure>
<p>我们在调用tf.Variable的时候传入初始值。在这个例子里，我们把 W 和 b 都初始化为零向量。W 是一个784x10的矩阵（因为我们有784个特征和10个输出值）。b 是一个10维的向量（因为我们有10个分类）。</p>
<h2 id="构建代价函数"><a href="#构建代价函数" class="headerlink" title="构建代价函数"></a>构建代价函数</h2><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s1.html" target="_blank" rel="external">交叉熵</a><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/mnist10.png" alt=""></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 正确值</div><div class="line">y_ = tf.placeholder(&quot;float&quot;, [None,10])</div><div class="line"></div><div class="line"># 损失函数</div><div class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</div></pre></td></tr></table></figure>
<p>注意，tf.reduce_sum把minibatch里的每张图片的交叉熵值都加起来了。我们计算的交叉熵是指整个minibatch的。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵。梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow只需将每个变量一点点地往使成本不断降低的方向移动。</p>
<p>TensorFlow在这里实际上所做的是，它会在后台给计算图增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>
<p>返回的train_step操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行train_step来完成。</p>
<p>训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 初始化变量</div><div class="line">init = tf.initialize_all_variables()</div><div class="line"></div><div class="line"># 在session里启动模型</div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"># 开始训练模型，这里我们让模型循环训练1000次！</div><div class="line">for i in range(1000):</div><div class="line">  batch = mnist.train.next_batch(50)</div><div class="line">  sess.run(train_step,feed_dict=&#123;x: batch[0], y_: batch[1]&#125;)</div></pre></td></tr></table></figure></p>
<p>每一步迭代，我们都会随机加载50个训练样本，然后执行一次train_step，并通过feed<em>dict将x 和 y</em>张量占位符用训练训练数据替代。</p>
<p>使用一小部分的随机数据来进行训练被称为随机训练（stochastic training）- 在这里更确切的说是随机梯度下降训练。在理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>找出预测正确的标签<br>tf.argmax 给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签。tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值<br>tf.argmax(y_,1) 代表正确的标签<br>tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</div></pre></td></tr></table></figure></p>
<p>这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</div></pre></td></tr></table></figure></p>
<p>最后，我们可以计算出在测试数据上的准确率，大概是90.92%。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print sess.run(accuracy,feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</div></pre></td></tr></table></figure></p>
<h1 id="多层卷积网络模型"><a href="#多层卷积网络模型" class="headerlink" title="多层卷积网络模型"></a>多层卷积网络模型</h1><p>CNN 对模式分类非常适合，最初是为识别二维形状而特殊设计的，这种二维形状对平移、比例缩放、倾斜或其他形式对变形有高度不变性。详见 <a href="http://www.shuang0420.com/2017/01/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0/">卷积神经网络 CNN 笔记</a></p>
<h2 id="TensorFlow-实现"><a href="#TensorFlow-实现" class="headerlink" title="TensorFlow 实现"></a>TensorFlow 实现</h2><h3 id="初始化权重和偏置项"><a href="#初始化权重和偏置项" class="headerlink" title="初始化权重和偏置项"></a>初始化权重和偏置项</h3><p>为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免 0 梯度。由于我们使用的是 ReLU 神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为 0 的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def weight_variable(shape):</div><div class="line">  initial = tf.truncated_normal(shape, stddev=0.1)</div><div class="line">  return tf.Variable(initial)</div><div class="line"></div><div class="line">def bias_variable(shape):</div><div class="line">  initial = tf.constant(0.1, shape=shape)</div><div class="line">  return tf.Variable(initial)</div></pre></td></tr></table></figure></p>
<h3 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h3><p>我们的卷积使用 1 步长（stride size），0 边距（padding size）的模板，保证输出和输入是同一个大小。<br>我们的池化用简单传统的 2x2 大小的模板做 max pooling。为了代码更简洁，我们把这部分抽象成一个函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 卷积函数</div><div class="line">def conv2d(x, W):</div><div class="line">  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)</div><div class="line"></div><div class="line"># 池化函数</div><div class="line">def max_pool_2x2(x):</div><div class="line">  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],</div><div class="line">                        strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)</div></pre></td></tr></table></figure></p>
<h3 id="第一层卷积"><a href="#第一层卷积" class="headerlink" title="第一层卷积"></a>第一层卷积</h3><p>第一层卷积由一个卷积接一个 max pooling 完成。卷积在每个 5x5 的 patch 中算出 32 个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是 patch 的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([5, 5, 1, 32])</div><div class="line">b_conv1 = bias_variable([32])</div><div class="line"></div><div class="line"># 把 x 变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是 rgb 彩色图，则为3)。</div><div class="line">x_image = tf.reshape(x, [-1,28,28,1])</div><div class="line"></div><div class="line"># 把 x_image 和权值向量进行卷积，加上偏置项，然后应用 ReLU 激活函数，最后进行 max pooling。</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure></p>
<h3 id="第二层卷积"><a href="#第二层卷积" class="headerlink" title="第二层卷积"></a>第二层卷积</h3><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个 5x5 的 patch 会得到64个特征。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_conv2 = weight_variable([5, 5, 32, 64])</div><div class="line">b_conv2 = bias_variable([64])</div><div class="line"></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure></p>
<h3 id="密集连接层-全连接层"><a href="#密集连接层-全连接层" class="headerlink" title="密集连接层(全连接层)"></a>密集连接层(全连接层)</h3><p>经过第一次池化，图片尺寸减小到 14*14，经过第二次池化，图片尺寸减小到 7x7。我们加入一个有 1024 个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([7 * 7 * 64, 1024])</div><div class="line">b_fc1 = bias_variable([1024])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>为了减少过拟合，我们在输出层之前加入 dropout。我们用一个 placeholder 来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用 dropout，在测试过程中关闭 dropout。 TensorFlow的tf.nn.dropout 操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(&quot;float&quot;)</div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure></p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>最后，我们添加一个softmax层，就像前面的单层softmax regression一样。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">W_fc2 = weight_variable([1024, 10])</div><div class="line">b_fc2 = bias_variable([10])</div><div class="line"></div><div class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</div></pre></td></tr></table></figure></p>
<h3 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h3><p>使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，只是我们会用更加复杂的ADAM优化器来做梯度最速下降，在feed_dict中加入额外的参数keep_prob来控制dropout比例。然后每100次迭代输出一次日志。最后的准确率是 99.2％<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line"></div><div class="line"></div><div class="line">for i in range(20000):</div><div class="line">    batch = mnist.train.next_batch(50)</div><div class="line">    if i % 100 == 0:</div><div class="line">        train_accuracy = sess.run(</div><div class="line">            accuracy, feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 1.0&#125;)</div><div class="line">        print &quot;step %d, training accuracy %g&quot; % (i, train_accuracy)</div><div class="line">    sess.run(train_step, feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)</div><div class="line"></div><div class="line">print &quot;test accuracy %g&quot; % sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/Shuang0420/TensorFlow_Study" target="_blank" rel="external">代码</a></p>
<blockquote>
<p>参考链接:</p>
<blockquote>
<p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/os_setup.html" target="_blank" rel="external">中文文档</a><br><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="external">github 项目地址</a><br><a href="http://blog.csdn.net/zouxy09/article/details/8781543" target="_blank" rel="external">http://blog.csdn.net/zouxy09/article/details/8781543</a><br><a href="http://blog.csdn.net/celerychen2009/article/details/8973218" target="_blank" rel="external">http://blog.csdn.net/celerychen2009/article/details/8973218</a><br><a href="http://www.jeyzhang.com/cnn-learning-notes-1.html" target="_blank" rel="external">卷积神经网络(CNN)学习笔记 - LeNet5网络详解</a></p>
</blockquote>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结(五)-- 其他技巧]]></title>
      <url>http://www.shuang0420.com/2016/06/20/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%BA%94-%E5%85%B6%E4%BB%96%E6%8A%80%E5%B7%A7/</url>
      <content type="html"><![CDATA[<p>补充前面没有提到的一些技巧。<br><a id="more"></a></p>
<h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><h3 id="研究源码"><a href="#研究源码" class="headerlink" title="研究源码"></a>研究源码</h3><p>以 github 登录（<a href="https://github.com/login）" target="_blank" rel="external">https://github.com/login）</a> 为例，查看html源码会发现表单里面有个隐藏的authenticity_token值，这个是需要先获取然后跟用户名和密码一起提交的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&lt;div class=&quot;auth-form p-3&quot; id=&quot;login&quot;&gt;</div><div class="line"></div><div class="line">    &lt;!-- &lt;/textarea&gt; --&gt;&lt;!-- &apos;&quot;` --&gt;&lt;form accept-charset=&quot;UTF-8&quot; action=&quot;/session&quot; data-form-nonce=&quot;b2e0b5f779ddbb5dbf93b903a82e5fc5204da96b&quot; method=&quot;post&quot;&gt;&lt;div style=&quot;margin:0;padding:0;display:inline&quot;&gt;&lt;input name=&quot;utf8&quot; type=&quot;hidden&quot; value=&quot;&amp;#x2713;&quot; /&gt;&lt;input name=&quot;authenticity_token&quot; type=&quot;hidden&quot; value=&quot;MDOLdxNeNMPn2sjrj51G+v/yMYpikLru8QWiLI170WRME4UBfvGItiAhzZWFujZVUSoT7SFygFcjE8pMfRcMHQ==&quot; /&gt;&lt;/div&gt;      &lt;div class=&quot;auth-form-header&quot;&gt;</div><div class="line">        &lt;h1&gt;Sign in to GitHub&lt;/h1&gt;</div><div class="line">      &lt;/div&gt;</div><div class="line"></div><div class="line"></div><div class="line">      &lt;div id=&quot;js-flash-container&quot;&gt;</div><div class="line">&lt;/div&gt;</div><div class="line"></div><div class="line"></div><div class="line">      &lt;div class=&quot;auth-form-body mt-4&quot;&gt;</div><div class="line"></div><div class="line">        &lt;label for=&quot;login_field&quot;&gt;</div><div class="line">          Username or email address</div><div class="line">        &lt;/label&gt;</div><div class="line">        &lt;input autocapitalize=&quot;off&quot; autocorrect=&quot;off&quot; autofocus=&quot;autofocus&quot; class=&quot;form-control input-block&quot; id=&quot;login_field&quot; name=&quot;login&quot; tabindex=&quot;1&quot; type=&quot;text&quot; /&gt;</div><div class="line"></div><div class="line">        &lt;label for=&quot;password&quot;&gt;</div><div class="line">          Password &lt;a href=&quot;/password_reset&quot; class=&quot;label-link&quot;&gt;Forgot password?&lt;/a&gt;</div><div class="line">        &lt;/label&gt;</div><div class="line">        &lt;input class=&quot;form-control form-control input-block&quot; id=&quot;password&quot; name=&quot;password&quot; tabindex=&quot;2&quot; type=&quot;password&quot; /&gt;</div><div class="line"></div><div class="line">        &lt;input class=&quot;btn btn-primary btn-block&quot; data-disable-with=&quot;Signing in…&quot; name=&quot;commit&quot; tabindex=&quot;3&quot; type=&quot;submit&quot; value=&quot;Sign in&quot; /&gt;</div><div class="line">      &lt;/div&gt;</div><div class="line">&lt;/form&gt;</div></pre></td></tr></table></figure></p>
<h3 id="重写start-requests方法"><a href="#重写start-requests方法" class="headerlink" title="重写start_requests方法"></a>重写start_requests方法</h3><p>首先确保 cookie 打开</p>
<pre>COOKIES_ENABLES = True
</pre>

<p>重写start_requests方法</p>
<pre>
# 重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数
def start_requests(self):
    return [Request("https://github.com/login",
                    meta={'cookiejar': 1}, callback=self.post_login)]

# FormRequeset
def post_login(self, response):
    # 先去拿隐藏的表单参数authenticity_token
    authenticity_token = response.xpath(
        '//input[@name="authenticity_token"]/@value').extract_first()
    logging.info('authenticity_token=' + authenticity_token)
    pass
</pre>

<p>start_requests方法指定了回调函数，用来获取隐藏表单值authenticity_token，同时我们还给Request指定了cookiejar的元数据，用来往回调函数传递cookie标识。</p>
<h3 id="使用FormRequest"><a href="#使用FormRequest" class="headerlink" title="使用FormRequest"></a>使用FormRequest</h3><p>Scrapy为我们准备了FormRequest类专门用来进行Form表单提交的。</p>
<pre>
# FormRequeset
def post_login(self, response):
    # 先去拿隐藏的表单参数authenticity_token
    authenticity_token = response.xpath(
        '//input[@name="authenticity_token"]/@value').extract_first()
    logging.info('authenticity_token=' + authenticity_token)
    # FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单
    # 登陆成功后, 会调用after_login回调函数，如果url跟Request页面的一样就省略掉
    return [FormRequest.from_response(response,
                                      url='https://github.com/session',
                                      meta={'cookiejar': response.meta['cookiejar']},
                                      #headers=self.post_headers,  
                                      formdata={
                                          'login': 'shuang0420',
                                          'password': 'XXXXXXXXXXXXXXXXX',
                                          'authenticity_token': authenticity_token
                                      },
                                      callback=self.after_login,
                                      dont_filter=True
                                      )]
</pre>

<p>FormRequest.from_response()方法让你指定提交的url，请求头还有form表单值，注意我们还通过meta传递了cookie标识。它同样有个回调函数，登录成功后调用。下面我们来实现它。注意这里我继续传递cookiejar，访问初始页面时带上cookie信息。</p>
<pre>
def after_login(self, response):
    # 登录之后，开始进入我要爬取的私信页面
    for url in self.start_urls:
        logging.info('letter url=' + url)
        yield Request(url, meta={'cookiejar': response.meta['cookiejar']},callback=self.parse_page)
</pre>

<h3 id="页面处理"><a href="#页面处理" class="headerlink" title="页面处理"></a>页面处理</h3><p>这个例子的主要任务是模拟登录，在登录 github 后爬取主页的 comments 内容。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-05%20%E4%B8%8B%E5%8D%884.19.03.png" alt=""></p>
<p>代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def parse_page(self, response):</div><div class="line">    &quot;&quot;&quot;comments 内容&quot;&quot;&quot;</div><div class="line">    logging.info(u&apos;--------------消息分割线-----------------&apos;)</div><div class="line">    logging.info(response.url)</div><div class="line">    replaceTags = re.compile(&apos;&lt;.*?&gt;&apos;)</div><div class="line">    replaceLine = re.compile(&apos;\r|\n|\t&apos;)</div><div class="line">    message = response.xpath(</div><div class="line">        &apos;//div[@class=&quot;details&quot;]/div[@class=&quot;message markdown-body&quot;]|div[@class=&quot;message markdown-body&quot;]/blockquote&apos;).extract()</div><div class="line">    for m in message:</div><div class="line">        m = replaceTags.sub(&quot;&quot;, m)</div><div class="line">        m = replaceLine.sub(&quot;&quot;, m)</div><div class="line">        print m</div></pre></td></tr></table></figure></p>
<p>爬取结果</p>
<pre>
I like topn (or perhaps top_n) a little better, because it's not dependent on what the features represent (words, phrases, entities, characters...). …      
Note: as of now, the classes and methods are not well arranged, and there are a few mock classes (which will be removed) to help me with testing. O…      
Hello @gojomo thank you for replying fast.I have used save() to save the model and load_word2vec_format() to load the model. Thats where the probl…      
The unicode_errors='ignore' option should make it impossible for the exact same error to occur; perhaps you're getting some other very-similar error?       
(Nevermind, #758 added annoy.)      
It looks like the tests don't run on Travis, since Annoy is not installed there. Not sure how to fix the test failure in Python 2.6 either.      
Hello,Sorry for posting after even you have created the FAQ.I trained a model with tweets which had some undecodable unicode characters. When i t…      
dtto      
Misleading comment: there is no "training", the model is transferred from Mallet. These parameters only affect inference, model is unchanged.      
PEP8: Hanging indent of 4 spaces.      
@piskvorky I've addressed the comments. Could you please check?      
Thanks, that was quick :)      
@piskvorky , @tmylk , could you review?      
Added comment, made change in changelog.       
No, this was after that in 0.13.2. I noticed it because when I was testing the #768 solution, print_topics was failing.      
@tmylk how do you review these PRs before merging? There are too many errors, we cannot merge code so carelessly.      
Looks good to me... except still needs a comment explaining why the alias is there. And maybe a mention in the changelog, so we can deprecate the o…      
Yes, assign self.wordtopics = self.word_topics, with a big fat comment explaining why this alias is there.      
I don't understand how this version with storing unicode to binary files even worked. It means our unit tests must be faulty / incomplete.     
</pre>

<p><a href="">代码</a></p>
<h2 id="识别验证码"><a href="#识别验证码" class="headerlink" title="识别验证码"></a>识别验证码</h2><p>验证码是一种非常有效的反爬虫机制，它能阻止大部分的暴力抓取，在电商类、投票类以及社交类等网站上应用广泛。如果破解验证码，成为了数据抓取工作者必须要面对的问题。下面介绍3种常用的方法。</p>
<h3 id="更换ip地址"><a href="#更换ip地址" class="headerlink" title="更换ip地址"></a>更换ip地址</h3><p>在访问某些网站时，我们最初只是需要提供用户名密码就可以登陆的，比如说豆瓣网，如果我们要是频繁登陆访问，可能这时网站就会出现一个验证码图片，要求我们输入验证码才能登陆，这样在保证用户方便访问的同时，又防止了机器的恶意频繁访问。对于这种情况，我们可以使用代理服务器访问，只需要换个ip地址再次访问，验证码就不会出现了，当然，当验证码再次出现的时候，我们只能再更换ip地址。</p>
<h3 id="使用cookie登陆"><a href="#使用cookie登陆" class="headerlink" title="使用cookie登陆"></a>使用cookie登陆</h3><p>如果采用cookie登陆，可以这样实现：首先需要手动登陆网站一次，获取服务器返回的cookie，这里就带有了用户的登陆信息，当然也可以采用获取的cookie登陆该网站的其他页面，而不用再次登陆。具体代码已经实现，详见ZhihuSpider。我们只需要在配置文件中提供用户名密码，及相应的cookie即可。对于不出现验证码的情况，爬虫会提交用户名密码实现post请求登陆，如果失败，才会使用事先提供的cookie信息。</p>
<p>需要说明的是，判断爬虫登陆与否，我们只需要看一下爬取的信息里面是否带有用户信息即可。在使用cookie登陆的时候，还需要不定期更新cookie，以保证爬取顺利进行。</p>
<h3 id="验证码识别手段"><a href="#验证码识别手段" class="headerlink" title="验证码识别手段"></a>验证码识别手段</h3><p>使用cookie登陆比较简单，但是有时效性问题。验证码识别是个很好的思路，然而识别的精度又限制了抓取的效率。</p>
<h2 id="爬取js交互式表格数据"><a href="#爬取js交互式表格数据" class="headerlink" title="爬取js交互式表格数据"></a>爬取js交互式表格数据</h2><p>这里，若使用Google Chrome分析”请求“对应的链接(方法：右键→审查元素→Network→清空，点击”加载更多“，出现对应的GET链接寻找Type为text/html的，点击，查看get参数或者复制Request URL)，循环过程。</p>
<h3 id="启动-splash-容器"><a href="#启动-splash-容器" class="headerlink" title="启动 splash 容器"></a>启动 splash 容器</h3><pre>
$ docker run -p 8050:8050 scrapinghub/splash</pre>

<h3 id="配置-scrapy-splash"><a href="#配置-scrapy-splash" class="headerlink" title="配置 scrapy-splash"></a>配置 scrapy-splash</h3><p>在你的 scrapy 工程的配置文件settings.py中添加</p>
<pre>
SPLASH_URL = 'http://192.168.59.103:8050'

# 添加Splash中间件，还是在settings.py中通过DOWNLOADER_MIDDLEWARES指定，并且修改HttpCompressionMiddleware的优先级
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

# 默认情况下，HttpProxyMiddleware的优先级是750，要把它放在Splash中间件后面

# 设置Splash自己的去重过滤器
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# 如果你使用Splash的Http缓存，那么还要指定一个自定义的缓存后台存储介质，scrapy-splash提供了一个scrapy.contrib.httpcache.FilesystemCacheStorage的子类

HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# 如果你要使用其他的缓存存储，那么需要继承这个类并且将所有的scrapy.util.request.request_fingerprint调用替换成scrapy_splash.splash_request_fingerprint
</pre>

<h3 id="使用-scrapy-splash"><a href="#使用-scrapy-splash" class="headerlink" title="使用 scrapy-splash"></a>使用 scrapy-splash</h3><h4 id="SplashRequest"><a href="#SplashRequest" class="headerlink" title="SplashRequest"></a>SplashRequest</h4><p>最简单的渲染请求的方式是使用scrapy_splash.SplashRequest，通常你应该选择使用这个<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">yield SplashRequest(url, self.parse_result,</div><div class="line">    args=&#123;</div><div class="line">        # optional; parameters passed to Splash HTTP API</div><div class="line">        &apos;wait&apos;: 0.5,</div><div class="line"></div><div class="line">        # &apos;url&apos; is prefilled from request url</div><div class="line">        # &apos;http_method&apos; is set to &apos;POST&apos; for POST requests</div><div class="line">        # &apos;body&apos; is set to request body for POST requests</div><div class="line">    &#125;,</div><div class="line">    endpoint=&apos;render.json&apos;, # optional; default is render.html</div><div class="line">    splash_url=&apos;&lt;url&gt;&apos;,     # optional; overrides SPLASH_URL</div><div class="line">    slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  # optional</div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>另外，你还可以在普通的scrapy请求中传递splash请求meta关键字达到同样的效果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">yield scrapy.Request(url, self.parse_result, meta=&#123;</div><div class="line">    &apos;splash&apos;: &#123;</div><div class="line">        &apos;args&apos;: &#123;</div><div class="line">            # set rendering arguments here</div><div class="line">            &apos;html&apos;: 1,</div><div class="line">            &apos;png&apos;: 1,</div><div class="line"></div><div class="line">            # &apos;url&apos; is prefilled from request url</div><div class="line">            # &apos;http_method&apos; is set to &apos;POST&apos; for POST requests</div><div class="line">            # &apos;body&apos; is set to request body for POST requests</div><div class="line">        &#125;,</div><div class="line"></div><div class="line">        # optional parameters</div><div class="line">        &apos;endpoint&apos;: &apos;render.json&apos;,  # optional; default is render.json</div><div class="line">        &apos;splash_url&apos;: &apos;&lt;url&gt;&apos;,      # optional; overrides SPLASH_URL</div><div class="line">        &apos;slot_policy&apos;: scrapy_splash.SlotPolicy.PER_DOMAIN,</div><div class="line">        &apos;splash_headers&apos;: &#123;&#125;,       # optional; a dict with headers sent to Splash</div><div class="line">        &apos;dont_process_response&apos;: True, # optional, default is False</div><div class="line">        &apos;dont_send_headers&apos;: True,  # optional, default is False</div><div class="line">        &apos;magic_response&apos;: False,    # optional, default is True</div><div class="line">    &#125;</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
<p>Splash API说明，使用SplashRequest是一个非常便利的工具来填充request.meta[‘splash’]里的数据</p>
<ul>
<li>meta[‘splash’][‘args’] 包含了发往Splash的参数。</li>
<li>meta[‘splash’][‘endpoint’] 指定了Splash所使用的endpoint，默认是render.html</li>
<li>meta[‘splash’][‘splash_url’] 覆盖了settings.py文件中配置的Splash URL</li>
<li>meta[‘splash’][‘splash_headers’] 运行你增加或修改发往Splash服务器的HTTP头部信息，注意这个不是修改发往远程web站点的HTTP头部</li>
<li>meta[‘splash’][‘dont_send_headers’] 如果你不想传递headers给Splash，将它设置成True</li>
<li>meta[‘splash’][‘slot_policy’] 让你自定义Splash请求的同步设置</li>
<li>meta[‘splash’][‘dont_process_response’] 当你设置成True后，SplashMiddleware不会修改默认的scrapy.Response请求。默认是会返回SplashResponse子类响应比如SplashTextResponse</li>
<li>meta[‘splash’][‘magic_response’] 默认为True，Splash会自动设置Response的一些属性，比如response.headers,response.body等<br>如果你想通过Splash来提交Form请求，可以使用scrapy_splash.SplashFormRequest，它跟SplashRequest使用是一样的。</li>
</ul>
<h4 id="Responses"><a href="#Responses" class="headerlink" title="Responses"></a>Responses</h4><p>对于不同的Splash请求，scrapy-splash返回不同的Response子类</p>
<ul>
<li>SplashResponse 二进制响应，比如对/render.png的响应</li>
<li>SplashTextResponse 文本响应，比如对/render.html的响应</li>
<li>SplashJsonResponse JSON响应，比如对/render.json或使用Lua脚本的/execute的响应</li>
</ul>
<p>如果你只想使用标准的Response对象，就设置meta[‘splash’][‘dont_process_response’]=True</p>
<p>所有这些Response会把response.url设置成原始请求URL(也就是你要渲染的页面URL)，而不是Splash endpoint的URL地址。实际地址通过response.real_url得到</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>爬取华为应用市场( <a href="http://appstore.huawei.com/more/all" target="_blank" rel="external">http://appstore.huawei.com/more/all</a> )的“下一页” url 链接。</p>
<h4 id="查看网页源代码"><a href="#查看网页源代码" class="headerlink" title="查看网页源代码"></a>查看网页源代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://app.vmall.com/js/core/jquery.min.js&quot;&gt;&lt;/script&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">    var jsResource = new Array();</div><div class="line">    jsResource[&apos;cloud.page.count&apos;] = &quot;共&quot;;</div><div class="line">    jsResource[&apos;cloud.page.numbers&apos;] = &quot;条记录&quot;;</div><div class="line">    jsResource[&apos;cloud.page.last_page&apos;] = &quot;上一页&quot;;</div><div class="line">    jsResource[&apos;cloud.page.next_page&apos;] = &quot;下一页&quot;;</div><div class="line">    jsResource[&apos;cloud.page.pages&apos;] = &quot;页&quot;;</div><div class="line">    jsResource[&apos;cloud.page.first&apos;] = &quot;首页&quot;;</div><div class="line">    jsResource[&apos;cloud.page.last&apos;] = &quot;尾页&quot;;</div><div class="line">    jsResource[&apos;cloud.downAppError&apos;]=&quot;您的请求正在处理中，请不要重复提交。&quot;</div><div class="line">    jsResource[&apos;cloud.msg.ok&apos;]=&quot;确定&quot;</div><div class="line">    jsResource[&apos;cloud.msg.message&apos;]=&quot;提示&quot;</div><div class="line">    jsResource[&apos;cloud.detail.close&apos;]=&quot;关闭&quot;</div><div class="line">&lt;/script&gt;</div><div class="line"></div><div class="line"></div><div class="line">    &lt;script type=&quot;text/javascript&quot; src=&quot;http://app.vmall.com/js/all/more.js?version=2.9.5.20150418&quot;&gt;&lt;/script&gt;</div></pre></td></tr></table></figure>
<h4 id="查看渲染后的代码"><a href="#查看渲染后的代码" class="headerlink" title="查看渲染后的代码"></a>查看渲染后的代码</h4><p>启动 splash 容器，在浏览器打开 <a href="http://192.168.59.103:8050/" target="_blank" rel="external">http://192.168.59.103:8050/</a> ， 输入网址进行 render，查看渲染后的代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;div class=&quot;page-ctrl ctrl-app&quot; id=&quot;recommendListPage&quot;&gt;&lt;a href=&quot;http://appstore.huawei.com:80/more/all/1&quot;&gt;首页&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/1&quot;&gt;&lt;em class=&quot;arrow-grey-lt&quot;&gt;&amp;nbsp;&lt;/em&gt;上一页&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/1&quot;&gt;1&lt;/a&gt;&lt;span&gt;2&lt;/span&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/3&quot;&gt;3&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/4&quot;&gt;4&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/5&quot;&gt;5&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/3&quot;&gt;下一页&lt;em class=&quot;arrow-grey-rt&quot;&gt;&amp;nbsp;&lt;/em&gt;&lt;/a&gt; &lt;a href=&quot;http://appstore.huawei.com:80/more/all/41&quot;&gt;尾页&lt;/a&gt;</div></pre></td></tr></table></figure></p>
<h3 id="spider-部分代码"><a href="#spider-部分代码" class="headerlink" title="spider 部分代码"></a>spider 部分代码</h3><pre>
def parse(self, response):
    page = Selector(response)
    hrefs = page.xpath('//h4[@class="title"]/a/@href')
    if not hrefs:
        return
    for href in hrefs:
        url = href.extract()
        yield scrapy.Request(url, callback=self.parse_item)
# find next page
    nextpage = page.xpath('//div[@class="page-ctrl ctrl-app"]/a/em[@class="arrow-grey-rt"]/../@href').extract_first()
    print nextpage
    yield scrapy.Request(nextpage,callback=self.parse,meta={
    'splash': {
    'endpoint': 'render.html',
    'args': {'wait': 0.5}
    }
    })
</pre>

<p><a href="">完整代码</a></p>
<h2 id="分析不规则的-html"><a href="#分析不规则的-html" class="headerlink" title="分析不规则的 html"></a>分析不规则的 html</h2><p>之前的几个部分解决的都是 下载 Web 页面 的问题，这里补充下获取网页后分析过程的一些技巧。<br>以苏宁易购 help 页面为例。start_url 是 <a href="http://help.suning.com/faq/list.htm" target="_blank" rel="external">http://help.suning.com/faq/list.htm</a> ， 爬取的是左边侧栏每个大类的每个小类下右边的问题页面，如“权益介绍”、“等级权益介绍”这些 FAQ 页面，如何到达这些页面就不再多说，关键是到达这些页面后怎么获得信息。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-04%20%E4%B8%8B%E5%8D%883.37.07.png" alt=""></p>
<p>看一部分的网页源代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">&lt;div id=&quot;contentShow&quot;&gt;</div><div class="line">						 	&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;text-align:left;&quot; align=&quot;left&quot;&gt;</div><div class="line">	&lt;span style=&quot;font-size:9pt;font-family:宋体;color:black;&quot;&gt;&lt;/span&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;&quot; align=&quot;left&quot;&gt;</div><div class="line">	&lt;br /&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;&quot; align=&quot;left&quot;&gt;</div><div class="line">	&lt;b&gt;一、权益类型&lt;/b&gt;&lt;b&gt;&lt;/b&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;&quot; align=&quot;left&quot;&gt;</div><div class="line">	本次改版将上线&lt;span&gt;7&lt;/span&gt;个会员权益，涵盖价格优惠、资格抢先、服务优先等多个方面，会员等级越高，可享受到的会员权益越多。&lt;span&gt;&lt;/span&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;&quot; align=&quot;left&quot;&gt;</div><div class="line">	&lt;b&gt;二、具体详情：&lt;/b&gt;&lt;b&gt;&lt;/b&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">&lt;p class=&quot;MsoNormal&quot; style=&quot;background:white;&quot; align=&quot;left&quot;&gt;</div><div class="line">	&lt;b&gt;1&lt;/b&gt;&lt;b&gt;、生日红包&lt;/b&gt;&lt;br /&gt;</div><div class="line">特权内容：&lt;span&gt;&lt;br /&gt;</div><div class="line">&lt;/span&gt;已验证手机号的&lt;span&gt;V2&lt;/span&gt;及以上等级的会员，在实名认证或完善生日资料后，可在生日周期间获得生日红包。&lt;span&gt;&lt;br /&gt;</div><div class="line">&lt;/span&gt;&lt;span&gt;V2&lt;/span&gt;等级生日红包为&lt;span&gt;6&lt;/span&gt;元云券，&lt;span&gt;V3&lt;/span&gt;等级生日红包为&lt;span&gt;8&lt;/span&gt;元云券。（&lt;span&gt;2016&lt;/span&gt;年6月12日开始实施）&lt;span&gt;&lt;br /&gt;</div><div class="line">&lt;/span&gt;注意事项：&lt;span&gt;&lt;br /&gt;</div><div class="line">1&lt;/span&gt;）生日红包券为限品类云券，在生日周时自动发到会员账户，会员在成功收到生日红包券后会有短信提醒，并可登录“我的易购&lt;span&gt;-&lt;/span&gt;我的优惠券”【&lt;span&gt;&lt;a href=&quot;http://member.suning.com/emall/MyGiftTicket&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;点击查看&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;】，每个会员同一自然年内仅可获得一张生日红包券；&lt;span&gt;&lt;br /&gt;</div><div class="line">2&lt;/span&gt;）券使用规则：且限一次性使用、不找零、不兑现，不可以和云券叠加，可以和无敌券、易券叠加使用，不可使用自提；&lt;span&gt;&lt;br /&gt;</div><div class="line">3&lt;/span&gt;）券有效期：自券到账之日起&lt;span&gt;8&lt;/span&gt;日内有效；&lt;span&gt;&lt;br /&gt;</div><div class="line">4&lt;/span&gt;）券适用商品范围：仅限购买自营商品使用，也可以用于大聚惠、抢团购、手机专享价、名品特卖商品，但闪拍、秒杀、预售、海外购、虚拟商品、特殊类商品（一段奶粉等）及平台商户商品不可使用；&lt;span&gt;&lt;br /&gt;</div><div class="line">5&lt;/span&gt;）使用生日红包券的订单若发生退货，在有效期内券将返回至顾客账户，可再次使用；如用券订单退货时已超过券有效期，券将自动失效，不做延期；&lt;span&gt;&lt;/span&gt;</div><div class="line">&lt;/p&gt;</div><div class="line">......</div></pre></td></tr></table></figure></p>
<p>不难发现，有些文字分布在</p>
<ul>
<li>div[@id=”contentShow”]/p</li>
<li>div[@id=”contentShow”]/p/span</li>
<li>div[@id=”contentShow”]/p/b</li>
</ul>
<p>观察其他页面会发现还有些分布在 div[@id=”contentShow”]/h4 下或者 h3 下，有的甚至直接就在 div[@id=”contentShow”] 下。。<br>怎么办？<br>当然可以穷尽各种规则，也可以先把不需要的标签给去掉再 extract，这些我开始都傻傻的尝试过，结果总会忽略一些文字，后来在沮丧的看着 output 文件时福至心灵，直接取了 div[@id=”contentShow”] 再把所有的标签去掉不就行了？！</p>
<p>上代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">page = html.xpath(&apos;//div[@id=&quot;contentShow&quot;]&apos;).extract_first()</div><div class="line">replaceTags = re.compile(&apos;&lt;.*?&gt;&apos;)</div><div class="line">replaceLine = re.compile(&apos;\r|\n|\t&apos;)</div><div class="line">page = replaceTags.sub(&quot;&quot;, page)</div><div class="line">page = re.sub(replaceLine, &quot;&quot;, page)</div></pre></td></tr></table></figure></p>
<p>最后的结果非常干净<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;url&quot;: &quot;http://help.suning.com/page/id-26.htm&quot;, &quot;text&quot;: &quot; 一、账号注册目前注册个人用户仅支持：手机号方式进行注册。1、打开苏宁易购网站，点击页头“注册”，进入注册页面 2、进入注册页面，如果您是个人用户，可以用手机号进行注册；如果您是企业用户，可以点击“企业用户注册”，用单位名称进行注册，如果您有易购账号，可以点击“马上登录”3、填写注册信息，按照网页提示，填写手机号、验证码和密码 4、恭喜您，注册成功 &quot;, &quot;question&quot;: &quot;账户注册&quot;, &quot;title&quot;: &quot;易购注册登录&quot;&#125;</div></pre></td></tr></table></figure></p>
<p>掌握这个技巧，处理类似问题就很简单啦，如再爬京东的 help 网页，稍微改下代码5分钟就能搞定。</p>
<p><a href="">代码</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>回头谈点背景知识,scrapy使用了twisted.一个异步网络框架.因此要留意潜在的阻塞情况.但注意到settings中有个参数是设置ItemPipeline的并行度.由此推测pipeline不会阻塞,pipeline可能是在线程池中执行的(未验证).Pipeline一般用于将抓取到的信息保存(写数据库,写文件),因此这里你就不用担心耗时操作会阻塞整个框架了,也就不用在Pipeline中将这个写操作实现为异步.<br>除此之外框架的其他部分.都是异步的,简单说来就是,爬虫生成的请求交由调度器去下载,然后爬虫继续执行.调度器完成下载后会将响应交由爬虫解析.<br>网上找到的参考例子,部分将js支持写到了DownloaderMiddleware中,scrapy官网的code snippet也是这样 .若这样实现,就阻塞了整个框架,爬虫的工作模式变成了,下载-解析-下载-解析,而不在是并行的下载.在对效率要求不高的小规模爬取中问题不大.<br>更好的做法是将js支持写到scrapy的downloader里.网上有一个这样的实现(使用selenium+phantomjs).不过仅支持get请求.<br>在适配一个webkit给scrapy的downloader时,有各种细节需要处理.</p>
<blockquote>
<p>参考链接<br><br><a href="http://www.pythontab.com/html/2014/pythonweb_0311/712.html" target="_blank" rel="external">scrapy定制爬虫-爬取javascript内容</a><br><br><a href="http://www.pycoding.com/2016/04/12/scrapy-11.html" target="_blank" rel="external">Scrapy笔记（11）- 模拟登录</a><br><br><a href="http://www.lining0806.com/6-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E9%AA%8C%E8%AF%81%E7%A0%81%E7%99%BB%E9%99%86/" target="_blank" rel="external">网络爬虫-验证码登陆</a><br><br><a href="http://www.pycoding.com/2016/04/15/scrapy-12.html" target="_blank" rel="external">Scrapy笔记（12）- 抓取动态网站</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结(四)-- 分布式爬虫]]></title>
      <url>http://www.shuang0420.com/2016/06/17/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E5%9B%9B-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/</url>
      <content type="html"><![CDATA[<p>分布式爬虫的演习。<br><a id="more"></a><br>分布式爬虫问题其实也就是多台机器多个 spider 对 多个 url 的同时处理问题，怎样 schedule 这些 url，怎样汇总 spider 抓取的数据。最简单粗暴的方法就是将 url 进行分片，交给不同机器，最后对不同机器抓取的数据进行汇总。然而这样每个 spider 只能对自己处理的 url 去重，没办法全局的去重，另外性能也很难控制，可能有某台机器很早就跑完了，而别的机器还要跑很久。另一种思路就是把 url 存在某个地方，共享给所有的机器，总的调度器来分配请求，判断 spider 有没有闲置，闲置了就继续给它任务，直到所有的 url 都爬完，这种方法解决了去重问题（下面会具体讲到），也能提高性能，scrapy-redis 就实现了这样一个完整框架，总的来说，这更适合广度优先的爬取。</p>
<h1 id="Scrapyd"><a href="#Scrapyd" class="headerlink" title="Scrapyd"></a>Scrapyd</h1><p>Scrapy 并没有提供内置的分布式抓取功能，不过有很多方法可以帮你实现。</p>
<p>如果你有很多个spider，最简单的方式就是启动多个 Scrapyd 实例，然后将spider分布到各个机器上面。</p>
<p>如果你想多个机器运行同一个spider，可以将url分片后交给每个机器上面的spider。比如你把URL分成3份</p>
<pre>
http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list</pre>

<p>然后运行3个 Scrapyd 实例，分别启动它们，并传递part参数</p>
<pre>
curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 </pre>


<h1 id="Crawlera"><a href="#Crawlera" class="headerlink" title="Crawlera"></a>Crawlera</h1><p>这个，花钱就可以轻易解决～ <a href="http://doc.scrapinghub.com/crawlera.html" target="_blank" rel="external">直达</a></p>
<h1 id="Scrapy-redis"><a href="#Scrapy-redis" class="headerlink" title="Scrapy-redis"></a>Scrapy-redis</h1><p>Redis 是高性能的 key-value 数据库。我们知道 MongoDB 将数据保存在了硬盘里，而 Redis 的神奇之处在于它将数据保存在了内存中，因此带来了更高的性能。</p>
<h2 id="分布式原理"><a href="#分布式原理" class="headerlink" title="分布式原理"></a>分布式原理</h2><p>scrapy-redis实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的核心服务器称为 master，而把用于跑爬虫程序的机器称为 slave。</p>
<p>回顾 <a href="http://www.shuang0420.com/2016/06/12/爬虫总结-二-scrapy/">scrapy 框架</a>，我们首先给定一些start_urls，spider 最先访问 start_urls 里面的 url，再根据我们的 parse 函数，对里面的元素、或者是其他的二级、三级页面进行抓取。而要实现分布式，只需要在这个starts_urls里面做文章就行了。进一步描述如下：</p>
<ol>
<li><p>master 产生 starts_urls，url 会被封装成 request 放到 redis 中的 spider:requests，总的 scheduler 会从这里分配 request，当这里的 request 分配完后，会继续分配 start_urls 里的 url。</p>
</li>
<li><p>slave 从 master 的 redis 中取出待抓取的 request，下载完网页之后就把网页的内容发送回 master 的 redis，key 是 spider:items。scrapy 可以通过 settings 来让 spider 爬取结束之后不自动关闭，而是不断的去询问队列里有没有新的 url，如果有新的 url，那么继续获取 url 并进行爬取，所以这一过程将不断循环。</p>
</li>
<li><p>master 里的 reids 还有一个 key 是 “spider:dupefilter” 用来存储抓取过的 url 的 fingerprint（使用哈希函数将url运算后的结果），防止重复抓取，只要 redis 不清空，就可以进行断点续爬。</p>
</li>
</ol>
<p>对于已有的 scrapy 程序，对其扩展成分布式程序还是比较容易的。总的来说就是以下几步：</p>
<ol>
<li>找一台高性能服务器，用于 redis 队列的维护以及数据的存储。</li>
<li>扩展 scrapy 程序，让其通过服务器的 redis 来获取 start_urls，并改写 pipeline 里数据存储部分，把存储地址改为服务器地址。</li>
<li>在服务器上写一些生成url的脚本，并定期执行。</li>
</ol>
<p>关于 scheduler 到底是怎么进行调度的，需要看源码进行分析。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>可能上面的描述还是不够清楚，干脆看一下源码吧，scrapy-redis 主要要一下几个文件。</p>
<h3 id="零件分析"><a href="#零件分析" class="headerlink" title="零件分析"></a>零件分析</h3><ol>
<li><p>connection.py<br>根据 settings 里的配置实例化 redis 连接，被 dupefilter 和 scheduler 调用。</p>
</li>
<li><p>dupefilter.py<br>对 request 进行去重，使用了 redis 的 set。</p>
</li>
<li><p>queue.py<br>三种 queue, SpiderQueue（FIFO), SpiderPriorityQueue，以及 SpiderStack(LIFI)。默认使用的是第二种。</p>
</li>
<li><p>pipelines.py<br>分布式处理，将 item 存储在 redis 中。</p>
</li>
<li><p>scheduler.py<br>取代 scrapy 自带的 scheduler,实现分布式调度，数据结构来自 queue。</p>
</li>
<li><p>spider.py<br>定义 RedisSpider.py, 继承了 RedisMixin 和 CrawlSpider。</p>
</li>
</ol>
<p>由上可知，scrapy-redis 实现的 <strong>爬虫分布式</strong> 和 <strong>item处理分布式</strong> 就是由模块 scheduler 和模块 pipelines 实现。上述其它模块作为为二者辅助的功能模块。</p>
<h3 id="调度过程"><a href="#调度过程" class="headerlink" title="调度过程"></a>调度过程</h3><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>spider 被初始化时，同时会初始化一个对应的 scheduler 对象，这个调度器对象通过读取 settings，配置好自己的调度容器 queue 和判重工具dupefilter。</p>
<h4 id="判重-amp-进入调度池"><a href="#判重-amp-进入调度池" class="headerlink" title="判重 &amp; 进入调度池"></a>判重 &amp; 进入调度池</h4><p>每当一个 spider 产出一个 request 的时候，scrapy 内核会把这个 request 递交给这个 spider 对应的 scheduler 对象进行调度，scheduler 对象通过访问 redis 对 request 进行判重，如果不重复就把他添加进 redis 中的调度池。</p>
<h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><p>当调度条件满足时，scheduler 对象就从 redis 的调度池中取出一个 request 发送给spider，让 spider 爬取，若爬取过程中返回更多的url，那么继续进行直至所有的 request 完成。在这个过程中通过 connect signals.spider_idle 信号对 crawler 状态的监视，scheduler 对象发现 这个 spider 爬取了所有暂时可用 url，对应的 redis 的调度池空了，于是触发信号 spider_idle，spider收到这个信号之后，直接连接 redis 读取 strart_url池，拿去新的一批 url，返回新的 make_requests_from_url(url) 给引擎，进而交给调度器调度。</p>
<p>熟悉了原理其实可以自己来写 scheduler，自己定义调度优先级和顺序，👇<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/scheduler.png" alt=""></p>
<h2 id="Redis-配置"><a href="#Redis-配置" class="headerlink" title="Redis 配置"></a>Redis 配置</h2><h3 id="下载-Redis"><a href="#下载-Redis" class="headerlink" title="下载 Redis"></a>下载 Redis</h3><pre>wget http://download.redis.io/releases/redis-3.2.1.tar.gz</pre>

<h3 id="下载-scrapy-redis"><a href="#下载-scrapy-redis" class="headerlink" title="下载 scrapy-redis"></a>下载 scrapy-redis</h3><pre>pip install scrapy-redis</pre>

<h3 id="安装-Redis"><a href="#安装-Redis" class="headerlink" title="安装 Redis"></a>安装 Redis</h3><pre>make
make test</pre>

<h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>安装完成后，redis 默认是不能被远程连接的，此时要修改配置文件 redis.conf，修改后，重启 redis 服务器</p>
<pre>
#bind 127.0.0.1
bind 0.0.0.0
</pre>

<h3 id="任意目录下运行"><a href="#任意目录下运行" class="headerlink" title="任意目录下运行"></a>任意目录下运行</h3><pre>sudo cp redis.conf /etc/</pre>

<h3 id="可能错误"><a href="#可能错误" class="headerlink" title="可能错误"></a>可能错误</h3><p>如果因为 gcc 而不能 make</p>
<pre>sudo apt-get build-dep gcc</pre>

<p>如果遇到这个，</p>
<pre>make[1]: Entering directory `/opt/redis-2.6.14/src'
CC adlist.o
In file included from adlist.c:34:
zmalloc.h:50:31: error: jemalloc/jemalloc.h: No such file or directory
zmalloc.h:55:2: error: #error "Newer version of jemalloc required"
make[1]: *** [adlist.o] Error 1
make[1]: Leaving directory `/opt/redis-2.6.14/src'
make: *** [all] Error 2</pre>

<p>可以看<a href="http://blog.jboy1009.com/archives/150" target="_blank" rel="external">这里</a></p>
<p>用这个命令</p>
<pre>make MALLOC=libc</pre>

<p>如果遇到这个</p>
<pre>You need tcl 8.5 or newer in order to run the Redis test</pre>

<p>安装 tcl</p>
<pre>sudo apt-get install tcl</pre>

<p>(redis 更多安装配置)[<a href="https://testerhome.com/topics/3887" target="_blank" rel="external">https://testerhome.com/topics/3887</a>]</p>
<h2 id="Redis-常用命令"><a href="#Redis-常用命令" class="headerlink" title="Redis 常用命令"></a>Redis 常用命令</h2><h3 id="运行-Redis"><a href="#运行-Redis" class="headerlink" title="运行 Redis"></a>运行 Redis</h3><pre>redis-server redis.conf</pre>

<h3 id="进入命令行模式"><a href="#进入命令行模式" class="headerlink" title="进入命令行模式"></a>进入命令行模式</h3><pre>redis-cli</pre>

<h3 id="清空缓存"><a href="#清空缓存" class="headerlink" title="清空缓存"></a>清空缓存</h3><pre>flushdb</pre>

<h3 id="查看所有-key"><a href="#查看所有-key" class="headerlink" title="查看所有 key"></a>查看所有 key</h3><pre>
127.0.0.1:6379> keys *
1) "dmoz:items"
2) "dmoz:requests"
3) "dmoz:dupefilter"
</pre>

<h3 id="查看-list-item"><a href="#查看-list-item" class="headerlink" title="查看 list (item)"></a>查看 list (item)</h3><pre>
127.0.0.1:6379> LRANGE dmoz:items 0 3
1) "{\"spider\": \"dmoz\", \"crawled\": \"2016-07-12 11:18:35\", \"link\": \"http://feeds.abcnews.com/abcnews/topstories\", \"name\": \"ABC News: Top Stories \", \"description\": \"Collection of news headlines.\"}"
2) "{\"spider\": \"dmoz\", \"crawled\": \"2016-07-12 11:18:35\", \"link\": \"http://abcnews.go.com/\", \"name\": \"ABCNews.com \", \"description\": \"Includes American and world news headlines, articles, chatrooms, message boards, news alerts, video and audio webcasts, shopping, and wireless news service. As well as ABC television show information and content.\"}"
3) "{\"spider\": \"dmoz\", \"crawled\": \"2016-07-12 11:18:35\", \"link\": \"http://www.alarabiya.net/\", \"name\": \"Al Arabiya News Channel \", \"description\": \"Arabic-language news network. Breaking news and features along with videos, photo galleries and In-Focus sections on major news topics.  (Arabic, English, Persian, Urdu)\"}"
4) "{\"spider\": \"dmoz\", \"crawled\": \"2016-07-12 11:18:35\", \"link\": \"http://www.aljazeera.com/\", \"name\": \"Aljazeera \", \"description\": \"English version of the Arabic-language news network. Breaking news and features plus background material including profiles and global reactions.\"}"
</pre>

<h3 id="查看-set-dupefilter"><a href="#查看-set-dupefilter" class="headerlink" title="查看 set (dupefilter)"></a>查看 set (dupefilter)</h3><pre>
127.0.0.1:6379> SMEMBERS dmoz:dupefilter
   1) "28bf6cfa1409d6d2ad2852663a3751ae077a0b01"
   2) "6af16713d5d423a2e91c87085f277a810c690cfa"
   3) "c0ccfd767892b2bbb533a52c7cde55543aa4605b"
   4) "0ca88e614179c791f258d89a820449c91940c4d4"
   5) "546577e3457c55057c56985b71e6a142fe5a64e9"
   6) "d2af0f8cf72e394dc46a720ee620fd7cdb0b6ad6"
   7) "e0c1ab903b2a95f05bc8f5a5036b2f6f0b3fcbd0"
   8) "bf1290602aa0fd2deb7f8b582f855535ca151990"
   9) "c59f100b08e424352e6e368ff94d797c35fc5a4b"
  10) "5acf897c445b3dbba5b371f811b74e26c52cd5c6"
  </pre>


<h3 id="查看-sorted-set-requests"><a href="#查看-sorted-set-requests" class="headerlink" title="查看 sorted set (requests)"></a>查看 sorted set (requests)</h3><pre>
127.0.0.1:6379> ZRANGE dmoz:requests 0 3
1) "\x80\x02}q\x01(U\x04bodyq\x02U\x00U\t_encodingq\x03U\x05utf-8q\x04U\acookiesq\x05}q\x06U\x04metaq\a}q\b(U\x05depthq\tK\x01U\tlink_textq\nclxml.etree\n_ElementStringResult\nq\x0bU\tInvestingq\x0c\x85\x81q\r}q\x0e(U\a_parentq\x0fNU\x0cis_attributeq\x10\x89U\battrnameq\x11NU\ais_textq\x12\x89U\ais_tailq\x13\x89ubU\x04ruleq\x14K\x00uU\aheadersq\x15}q\x16U\aRefererq\x17]q\x18U\x14http://www.dmoz.org/q\x19asU\x03urlq\x1aX'\x00\x00\x00http://www.dmoz.org/Business/Investing/U\x0bdont_filterq\x1b\x89U\bpriorityq\x1cK\x00U\bcallbackq\x1dU\x14_response_downloadedq\x1eU\x06methodq\x1fU\x03GETq U\aerrbackq!Nu."
2) "\x80\x02}q\x01(U\x04bodyq\x02U\x00U\t_encodingq\x03U\x05utf-8q\x04U\acookiesq\x05}q\x06U\x04metaq\a}q\b(U\x05depthq\tK\x01U\tlink_textq\nclxml.etree\n_ElementStringResult\nq\x0bU\tLibrariesq\x0c\x85\x81q\r}q\x0e(U\a_parentq\x0fNU\x0cis_attributeq\x10\x89U\battrnameq\x11NU\ais_textq\x12\x89U\ais_tailq\x13\x89ubU\x04ruleq\x14K\x00uU\aheadersq\x15}q\x16U\aRefererq\x17]q\x18U\x14http://www.dmoz.org/q\x19asU\x03urlq\x1aX(\x00\x00\x00http://www.dmoz.org/Reference/Libraries/U\x0bdont_filterq\x1b\x89U\bpriorityq\x1cK\x00U\bcallbackq\x1dU\x14_response_downloadedq\x1eU\x06methodq\x1fU\x03GETq U\aerrbackq!Nu."
3) "\x80\x02}q\x01(U\x04bodyq\x02U\x00U\t_encodingq\x03U\x05utf-8q\x04U\acookiesq\x05}q\x06U\x04metaq\a}q\b(U\x05depthq\tK\x01U\tlink_textq\nclxml.etree\n_ElementStringResult\nq\x0bU\tTeen Lifeq\x0c\x85\x81q\r}q\x0e(U\a_parentq\x0fNU\x0cis_attributeq\x10\x89U\battrnameq\x11NU\ais_textq\x12\x89U\ais_tailq\x13\x89ubU\x04ruleq\x14K\x00uU\aheadersq\x15}q\x16U\aRefererq\x17]q\x18U\x14http://www.dmoz.org/q\x19asU\x03urlq\x1aX-\x00\x00\x00http://www.dmoz.org/Kids_and_Teens/Teen_Life/U\x0bdont_filterq\x1b\x89U\bpriorityq\x1cK\x00U\bcallbackq\x1dU\x14_response_downloadedq\x1eU\x06methodq\x1fU\x03GETq U\aerrbackq!Nu."
4) "\x80\x02}q\x01(U\x04bodyq\x02U\x00U\t_encodingq\x03U\x05utf-8q\x04U\acookiesq\x05}q\x06U\x04metaq\a}q\b(U\x05depthq\tK\x01U\tlink_textq\nclxml.etree\n_ElementStringResult\nq\x0bU\nBasketballq\x0c\x85\x81q\r}q\x0e(U\a_parentq\x0fNU\x0cis_attributeq\x10\x89U\battrnameq\x11NU\ais_textq\x12\x89U\ais_tailq\x13\x89ubU\x04ruleq\x14K\x00uU\aheadersq\x15}q\x16U\aRefererq\x17]q\x18U\x14http://www.dmoz.org/q\x19asU\x03urlq\x1aX&\x00\x00\x00http://www.dmoz.org/Sports/Basketball/U\x0bdont_filterq\x1b\x89U\bpriorityq\x1cK\x00U\bcallbackq\x1dU\x14_response_downloadedq\x1eU\x06methodq\x1fU\x03GETq U\aerrbackq!Nu."
</pre>


<h3 id="查看-list-items-长度"><a href="#查看-list-items-长度" class="headerlink" title="查看 list (items) 长度"></a>查看 list (items) 长度</h3><p>127.0.0.1:6379&gt; LLEN Search:items<br>(integer) 376</p>
<h3 id="查看-sorted-set-requests-长度"><a href="#查看-sorted-set-requests-长度" class="headerlink" title="查看 sorted set (requests) 长度"></a>查看 sorted set (requests) 长度</h3><p>127.0.0.1:6379&gt; ZCARD Search:requests<br>(integer) 1</p>
<h3 id="查看-set-dupefilter-长度"><a href="#查看-set-dupefilter-长度" class="headerlink" title="查看 set (dupefilter) 长度"></a>查看 set (dupefilter) 长度</h3><p>127.0.0.1:6379&gt; SCARD Search:dupefilter<br>(integer) 1</p>
<p><a href="http://www.runoob.com/redis/redis-tutorial.html" target="_blank" rel="external">Redis 教程</a></p>
<h2 id="scrapy-redis-配置"><a href="#scrapy-redis-配置" class="headerlink" title="scrapy_redis 配置"></a>scrapy_redis 配置</h2><p>从 github 上 下载 <a href="https://github.com/rolando/scrapy-redis" target="_blank" rel="external">example</a>，修改相应文件，items.py, settings.py, process_items.py 等。最重要的是改 settings.py</p>
<h3 id="通用配置"><a href="#通用配置" class="headerlink" title="通用配置"></a>通用配置</h3><pre>
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
SCHEDULER_PERSIST = True

# ITEM_PIPELINES
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
</pre>

<h3 id="master-配置"><a href="#master-配置" class="headerlink" title="master 配置"></a>master 配置</h3><p>settings.py 中添加</p>
<pre>
# redis
REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
</pre>

<h3 id="slave-配置"><a href="#slave-配置" class="headerlink" title="slave 配置"></a>slave 配置</h3><p>settings.py 中添加</p>
<pre>
# redis
REDIS_URL = 'redis://host_ip:6379'
</pre>

<h3 id="spider-改写"><a href="#spider-改写" class="headerlink" title="spider 改写"></a>spider 改写</h3><p>导入模块</p>
<pre>from scrapy_redis.spiders import RedisSpider</pre>

<p>继承 RedisSpider，并从 Redis 读取 url</p>
<pre>
class Search(RedisCrawlSpider):
    name = "Search"
    redis_key = 'Search:start_urls'</pre>

<h2 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h2><p>在 master 上启动 Redis</p>
<pre>redis-server</pre>


<p>启动 spider，任意顺序</p>
<pre>scrapy crawl Search</pre>

<p>可以看到 schedule 了多少 request</p>
<pre>
$ scrapy crawl Search
... [Search] DEBUG: Resuming crawl (8712 requests scheduled)</pre>


<h2 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h2><p>写到数据库里很简单，在 process_items.py 里添加代码，指定数据库 ip，插入同一个数据库。这里我们的数据不用导出到 mongodb 等数据库，只用把它转化为文本文件即可。</p>
<h3 id="redis-dump"><a href="#redis-dump" class="headerlink" title="redis-dump"></a>redis-dump</h3><p>安装 redis-dump</p>
<pre>gem install redis-dump</pre>

<p>导出</p>
<pre>redis-dump -u 127.0.0.1:6379 > db.json</pre>

<p>注意的是，它导出的是数据库里所有的 key-value，也就是说之后处理 items 的时候可能会有问题，item list 太大读取造成 memory error。</p>
<h3 id="python-连接-redis"><a href="#python-连接-redis" class="headerlink" title="python 连接 redis"></a>python 连接 redis</h3><p>很简单，导入模块，连接数据库，其他基本按照 redis 命令来。</p>
<pre>import redis
r = redis.Redis(host='106.75.136.128', port=6379)</pre>

<p>如</p>
<pre>for i in range(0, r.llen('Search:items'), 100):
    items = r.lrange('Search:items', start=0, end=100)</pre>

<p>然后把文件写到文件里。</p>
<h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><p>分布式系统还有一个问题，怎么监控 slave，知道哪台机器坏了，可以写个 socket 向 master 报告，或者用 email 告警。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>每次执行重新爬取，应该将redis中存储的数据清空，否则影响爬取现象。</p>
<p>另外，request 和 url 是不同的，前者是由后者经由函数make_request_from_url实现，并且这个过程由spider完成。spider会返回（return、yield）request给scrapy引擎进而交割调度器。url也是在spider中定义或由spider获取的。</p>
<blockquote>
<p>参考链接:<br><a href="https://www.douban.com/group/topic/38361104/" target="_blank" rel="external">使用scrapy,redis,mongodb实现的一个分布式网络爬虫</a><br><a href="http://blog.csdn.net/u012150179/article/details/38091411" target="_blank" rel="external">scrapy-redis实现爬虫分布式爬取分析与实现</a><br><a href="http://www.jikexueyuan.com/course/1556.html" target="_blank" rel="external">定向爬虫：Scrapy 与 Redis 入门</a><br><a href="http://www.pycoding.com/2016/03/19/scrapy-07.html" target="_blank" rel="external">Scrapy笔记（7）- 内置服务</a><br><a href="http://www.codexiu.cn/python/blog/6109/" target="_blank" rel="external">基于Redis的三种分布式爬虫策略</a><br><a href="http://ju.outofmemory.cn/entry/206756" target="_blank" rel="external">基于Python,scrapy,redis的分布式爬虫实现框架</a><br><a href="http://blog.csdn.net/u012150179/article/details/38226253" target="_blank" rel="external">scrapy-redis源码分析</a><br><a href="http://www.lxway.net/550248426.html" target="_blank" rel="external">Scrapy Redis源码 spider分析</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[讲座笔记 -- 腾讯应用宝]]></title>
      <url>http://www.shuang0420.com/2016/06/15/%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0%20--%20%E8%85%BE%E8%AE%AF%E5%BA%94%E7%94%A8%E5%AE%9D/</url>
      <content type="html"><![CDATA[<p>卓居超，2013年加入腾讯内部搜索部门，现负责腾讯应用宝搜索项目。近年来从事的科研工作集中在垂直领域的搜索、推荐技术研究。2015年代表腾讯公司在 WSDM 会议上做题为 “Semantic Matching in APP Search” 的主题报告，介绍腾讯应用宝语义搜索的技术实现。<br>今天他在公司做了一场关于腾讯应用宝的分享，这是一篇讲座笔记。<br><a id="more"></a></p>
<h1 id="应用宝-–-腾讯的安卓应用市场"><a href="#应用宝-–-腾讯的安卓应用市场" class="headerlink" title="应用宝 – 腾讯的安卓应用市场"></a>应用宝 – 腾讯的安卓应用市场</h1><ul>
<li>搜索是重要入口（新应用的分发）</li>
<li>app 快速的增长 一年增长几百万</li>
<li>二八原则 长尾大 0.1%的应用 80%的分发</li>
</ul>
<h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><ul>
<li>Downloads</li>
<li>QV</li>
<li>UV</li>
<li>CTR (Click-Through-Rate)</li>
<li>ROP (Rate-Of-Penetration)</li>
</ul>
<blockquote>
<p>CTR(Click-Through-Rate): 网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的点击量（严格的来说，可以是到达目标页面的数量）除以广告的浏览量（PV- Page View）。</p>
</blockquote>
<h2 id="语义计算策略"><a href="#语义计算策略" class="headerlink" title="语义计算策略"></a>语义计算策略</h2><h3 id="数据特征"><a href="#数据特征" class="headerlink" title="数据特征"></a>数据特征</h3><ul>
<li>数据量少 审核通过的应用数量只有数十万</li>
<li>文本信息少 附带文本信息少</li>
</ul>
<p>这就意味着能建索引的量少 –&gt; 所以要将信息泛化</p>
<h3 id="应对策略"><a href="#应对策略" class="headerlink" title="应对策略"></a>应对策略</h3><p>搜索＋推荐<br>用 <strong>词、主题、标签</strong> 来描述语义 (query –&gt; term + topic + tag –&gt; app)</p>
<h3 id="数据补充"><a href="#数据补充" class="headerlink" title="数据补充"></a>数据补充</h3><p>爬取全网资源</p>
<ul>
<li>游戏站点、用户评价</li>
<li>知识库：百度百科、百度知道</li>
<li>其他应用商店</li>
<li>搜索引擎 解析</li>
<li>应用宝用户行为</li>
</ul>
<p>容易出现的问题是噪音会很大，所以需要机器学习的方法进一步的过滤</p>
<p>过程就是 <strong>页面抓取 –&gt; 内容抓取 –&gt; 知识挖掘 –&gt; 标签 + 句法模板 + 标签集合 + 标签关联 –&gt; 标签关联净化 –&gt; 标签索引</strong></p>
<h3 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h3><p>利用用户行为来指导排序</p>
<ul>
<li>点击下载因子。赋予大的权重（增强鲁棒性）</li>
<li>entropy因子。entropy可以表示用户query的集中程度，，点击散，entropy高，区分精准query和模糊query</li>
</ul>
<h3 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h3><p>LDA 聚类，对 topic 进行人工标注，把 app 映射到 topic<br>LDA 在业界用法比较多。然而它最大的特点是需要大量的语料，语料少效果就不好。所以需要补充大量文本数据。</p>
<h3 id="标签挖掘"><a href="#标签挖掘" class="headerlink" title="标签挖掘"></a>标签挖掘</h3><ul>
<li>元搜方式挖掘 tag （通过搜app）</li>
<li>根据用户行为、画像挖掘 tag<br>对用户进行分群 地区／年龄／职业／性别等 生成代表用户属性的标签给app</li>
</ul>
<p>元搜，上大学的时候还学过来着，居然听讲座的时候没想起来😳</p>
<blockquote>
<p>元搜索引擎又称多搜索引擎，通过一个统一的用户界面帮助用户在多个搜索引擎中选择和利用合适的（甚至是同时利用若干个）搜索引擎来实现检索操作，是对分布于网络的多种检索工具的全局控制机制。（搜索引擎分类：全文搜索引擎、目录索引、元搜索引擎）</p>
</blockquote>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>方法：机器学习模型计算 confidence level<br>human editor + web data + qa (lda) user group tags  –&gt; <a href="http://blog.csdn.net/w28971023/article/details/8240756" target="_blank" rel="external">GBDT mode</a></p>
<blockquote>
<p>GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。</p>
</blockquote>
<h3 id="app-语义画像"><a href="#app-语义画像" class="headerlink" title="app 语义画像"></a>app 语义画像</h3><p>语义描述体系<br>分多个维度</p>
<h3 id="机器学习（LTR）"><a href="#机器学习（LTR）" class="headerlink" title="机器学习（LTR）"></a>机器学习（LTR）</h3><p>挑战：多来源检索结果不可比（类别／tag）<br>利器：lambdaMART 排序模型（GBRT的变种）</p>
<blockquote>
<p>LTR - Learning to rank：学习排序<br>用机器学习的方法进行排序，可用于相关性排序、推荐引擎等系统中。Learning to rank or machine-learned ranking (MLR) is a type of supervised or semi-supervised machine learning problem in which the goal is to automatically construct a ranking model from training data.</p>
</blockquote>
<h2 id="应用宝搜索商业化"><a href="#应用宝搜索商业化" class="headerlink" title="应用宝搜索商业化"></a>应用宝搜索商业化</h2><h3 id="分发升级"><a href="#分发升级" class="headerlink" title="分发升级"></a>分发升级</h3><p>应用+<br>应用分发 –&gt; 内容服务分发</p>
<ul>
<li>意图识别优化：什么时候出应用，什么时候出音乐，热度</li>
<li>多来源混排：机器学习＋运营系统优化异构排序，促进分发效率（应用、音乐等怎么混排）</li>
<li>多场景引导：在热词、直达区（搜索补充呈现）、联想词等场景引导用户，培养内容搜索习惯</li>
</ul>
<h3 id="应用搜索广告"><a href="#应用搜索广告" class="headerlink" title="应用搜索广告"></a>应用搜索广告</h3><p>技术核心</p>
<ul>
<li><p>app 画像<br>基于标签、主题、类别的 app 细粒度商业词</p>
</li>
<li><p>动态混排<br>根据 query 动态选择广告槽位</p>
</li>
</ul>
<p>利用相似应用打tag（confidence level –&gt; filter）</p>
<h1 id="机器学习的本质"><a href="#机器学习的本质" class="headerlink" title="机器学习的本质"></a>机器学习的本质</h1><p>已知数据 先验知识（专家系统） 未知数据的特征 –&gt; 求未知数据的优化分布<br>通用技术难点：空间搜索 函数泛化</p>
<p>实际工作：</p>
<ul>
<li>模型10% 其他90%</li>
<li>数据从哪来</li>
<li>特征如何抽取</li>
<li>领域先验知识</li>
</ul>
<p>大量噪音？维数灾难？<br><a href="http://blog.csdn.net/vividonly/article/details/50723852" target="_blank" rel="external">L0,L1,L2正则化</a> 剪枝<br>琐碎的准备工作很重要</p>
<blockquote>
<p>L0正则化的值是模型参数中非零参数的个数。<br>L1正则化表示各个参数绝对值之和。<br>L2正则化标识各个参数的平方的和的开方值。</p>
</blockquote>
<h1 id="大公司-vs-小公司"><a href="#大公司-vs-小公司" class="headerlink" title="大公司 vs 小公司"></a>大公司 vs 小公司</h1><h2 id="大公司"><a href="#大公司" class="headerlink" title="大公司"></a>大公司</h2><p>搜索 推荐 广告 都能接触到，可以和牛人接触<br>流程化 冗余 很多团队想做一件事</p>
<h2 id="小公司"><a href="#小公司" class="headerlink" title="小公司"></a>小公司</h2><ul>
<li><p>方向更对<br>不被商业价值束缚 不被同伴利益束缚 不被自己经验束缚</p>
</li>
<li><p>跑的更快<br>不被用户束缚 不被流程束缚 不被一般道德束缚</p>
</li>
</ul>
<h1 id="复利效应"><a href="#复利效应" class="headerlink" title="复利效应"></a>复利效应</h1><p>应用宝光是去噪就做了一年。<br>每天积累一点 –&gt; 复利效应 –&gt; 无法超越<br>腾讯去做搜索，做不过百度，为什么？技术团队不强？no！因为百度做了几十年的搜索，每天进步一点，复利效应无法超越。<br>我们要找到可以产生复利效应的点。算法是数学专家的事，我们可以做的是应用方面的复利效应，比如说聊天机器人。</p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 腾讯 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结(三)-- cloud scrapy]]></title>
      <url>http://www.shuang0420.com/2016/06/15/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%B8%89-scrapinghub/</url>
      <content type="html"><![CDATA[<p>发现了一个比较好玩的东西，scrapinghub，试着玩了一下 cloud scrapy，因为就它是免费的。。最大优点是可以将爬虫可视化。这里就简单记录一下它怎么用。<br><a id="more"></a></p>
<h1 id="注册账号-amp-新建-scrapy-cloud-project"><a href="#注册账号-amp-新建-scrapy-cloud-project" class="headerlink" title="注册账号 &amp; 新建 scrapy cloud project"></a>注册账号 &amp; 新建 scrapy cloud project</h1><p>在<a href="http://scrapinghub.com/scrapy-cloud/" target="_blank" rel="external">scrapyinghub 官网</a> 注册账号<br>登录后 create project，在新建的项目下，查看 Code &amp; Deploys，找到 API key 和 Project ID<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/scrapyinghub.png" alt=""></p>
<h1 id="Deploy-your-project"><a href="#Deploy-your-project" class="headerlink" title="Deploy your project"></a>Deploy your project</h1><pre>$ pip install shub</pre>

<p>login 并输入 API key</p>
<pre>$ shub login
Enter your API key from https://dash.scrapinghub.com/account/apikey
API key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Validating API key...
API key is OK, you are logged in now.</pre>

<p>deploy 并输入 Project ID</p>
<pre>$ shub deploy ProjectID
Packing version ed6b3b8-master
Deploying to Scrapy Cloud project "76180"
{"status": "ok", "project": 76180, "version": "ed6b3b8-master", "spiders": 1}
Run your spiders at: https://dash.scrapinghub.com/p/76180/</pre>


<h1 id="Schedule-your-spider"><a href="#Schedule-your-spider" class="headerlink" title="Schedule your spider"></a>Schedule your spider</h1><p>在自己的项目面板下选择 run spider 开启爬虫，也可以通过命令行开启。</p>
<pre>shub schedule Zhidao
Spider Zhidao scheduled, job ID: 76153/2/2
Watch the log on the command line:
    shub log -f 2/2
or print items as they are being scraped:
    shub items -f 2/2
or watch it running in Scrapinghub's web interface:
    https://dash.scrapinghub.com/p/76153/job/2/3</pre>

<p>看最新的 log 和 items<br>JOBID格式：2/2， 2/1 …</p>
<pre>shub log JOBID
shub items JOBID
</pre>

<p>或者 Dashboard 查看结果<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-24%20%E4%B8%8A%E5%8D%8811.00.23.png" alt=""></p>
<p>通过 Dashbord 还可以实时监控 crawler job 的情况，发出的请求数，抓取的 item 数，log 和 error 信息，执行的时间等，都一目了然。</p>
<h1 id="Save-items"><a href="#Save-items" class="headerlink" title="Save items"></a>Save items</h1><pre>curl -u APIkey: http://storage.scrapinghub.com/items/76153/2/2 > items.json </pre>


<h1 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h1><p>cloud scrapy 也提供了分布式爬虫的选择，当然是付费的。</p>
<h1 id="Crawlera"><a href="#Crawlera" class="headerlink" title="Crawlera"></a>Crawlera</h1><p>强悍的 Crawlera 提供了防止 ban 的机制，通过 ip、user-agent、cookie 等设置，防止爬虫被禁，详见 <a href="https://app.scrapinghub.com/o/64825/billing" target="_blank" rel="external">billing</a></p>
<p><a href="https://github.com/Shuang0420/Crawler" target="_blank" rel="external">完整代码</a></p>
<blockquote>
<p>参考链接:<br><a href="http://doc.scrapinghub.com/scrapy-cloud.html#deploying-a-scrapy-spider" target="_blank" rel="external">http://doc.scrapinghub.com/scrapy-cloud.html#deploying-a-scrapy-spider</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结(二)-- scrapy]]></title>
      <url>http://www.shuang0420.com/2016/06/12/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93-%E4%BA%8C-scrapy/</url>
      <content type="html"><![CDATA[<p>用现成的框架的好处就是不用担心 cookie、retry、频率限制、多线程的事。这一篇把上一篇的实例用 scrapy 框架重新实现一遍。主要步骤就是新建项目 (Project) –&gt; 定义目标（Items）–&gt; 制作爬虫（Spider）–&gt; 存储结果（Pipeline）<br><a id="more"></a></p>
<h1 id="Scrapy-概述"><a href="#Scrapy-概述" class="headerlink" title="Scrapy 概述"></a>Scrapy 概述</h1><blockquote>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试</p>
</blockquote>
<h2 id="Scrapy-架构"><a href="#Scrapy-架构" class="headerlink" title="Scrapy 架构"></a>Scrapy 架构</h2><p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/crawler.png" alt=""></p>
<p>绿线是数据流向，首先从初始 URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，Spider 分析出来的结果有两种：一种是需要进一步抓取的链接，例如之前分析的“下一页”的链接，这些东西会被传回 Scheduler ；另一种是需要保存的数据，它们则被送到 Item Pipeline 那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。</p>
<h2 id="Scrapy-组件"><a href="#Scrapy-组件" class="headerlink" title="Scrapy 组件"></a>Scrapy 组件</h2><ul>
<li>引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li>调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li>下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li>爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li>项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li>下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li>爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li>调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<h2 id="Scrapy-运行流程"><a href="#Scrapy-运行流程" class="headerlink" title="Scrapy 运行流程"></a>Scrapy 运行流程</h2><ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>若是解析出实体（Item）,则交给实体管道进行进一步的处理;若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取</li>
</ol>
<p>默认情况下，Scrapy使用 LIFO 队列来存储等待的请求。简单的说，就是 <strong>深度优先顺序</strong> 。如果想要 <strong>广度优先顺序</strong> 进行爬取，需要进行设定。</p>
<h2 id="Scrapy-存在的问题"><a href="#Scrapy-存在的问题" class="headerlink" title="Scrapy 存在的问题"></a>Scrapy 存在的问题</h2><p>爬虫是一个很依赖于网络io的应用，单机的处理能力有限，很快就变成瓶颈。而scrapy并不是一个分布式的设计，在需要大规模爬取的情况下就很成问题。当然可以通过修改Request队列来实现分布式爬取，而且工作量也不算特别大。</p>
<ul>
<li>scrapy的并行度不高。力图在爬虫里做一些计算性的操作就会影响抓取的速率。这主要是python里的线程机制造成的，因为Python使用了GIL(和Ruby一样)，多线程并不会带来太多速度上的提升(除非用Python的C扩展实现自己的模块，这样绕过了GIL)。Summary:Use Python threads if you need to run IO operations in parallel. Do not if you need to run computations in parallel.</li>
<li>scrapy的内存消耗很快。可能是出于性能方面的考虑，pending requests并不是序列化存储在硬盘中，而是放在内存中的(毕竟IO很费时)，而且所有Request都放在内存中。你抓取到 百万网页的时候，考虑到单个网页时产生很多链接的，pending request很可能就近千万了，加上脚本语言里的对象本来就有额外成本，再考虑到GC不会立即释放内存，内存占用就相当可观了。<br>归根到底，这两个问题是根植于语言之中的。</li>
</ul>
<h1 id="Scrapy-实例"><a href="#Scrapy-实例" class="headerlink" title="Scrapy 实例"></a>Scrapy 实例</h1><h2 id="新建项目-Project"><a href="#新建项目-Project" class="headerlink" title="新建项目 (Project)"></a>新建项目 (Project)</h2><pre>
scrapy startproject news_scrapy
</pre>

<p>输入以上命令之后，就会看见命令行运行的目录下多了一个名为 news_scrapy 的目录，目录的结构如下：</p>
<pre>
|---- news_scrapy
| |---- news_scrapy
|   |---- __init__.py
|   |---- items.py        #用来存储爬下来的数据结构（字典形式）
|    |---- pipelines.py    #用来对爬出来的item进行后续处理，如存入数据库等
|    |---- settings.py    #爬虫配置文件
|    |---- spiders        #此目录用来存放创建的新爬虫文件（爬虫主体）
|     |---- __init__.py
| |---- scrapy.cfg        #项目配置文件
</pre>

<h2 id="定义目标（Items）"><a href="#定义目标（Items）" class="headerlink" title="定义目标（Items）"></a>定义目标（Items）</h2><p>Items是装载抓取的数据的容器，工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误<br>通过创建scrapy.Item类, 并且定义类型为 scrapy.Field 的类属性来声明一个Item，通过将需要的item模型化，来控制站点数据。<br>编辑 items.py</p>
<pre>
# -*- coding: utf-8 -*-
import scrapy
class NewsScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    category = scrapy.Field()
    url = scrapy.Field()
    secondary_title = scrapy.Field()
    secondary_url = scrapy.Field()
    #text = Field()
</pre>


<h2 id="制作爬虫（Spider）"><a href="#制作爬虫（Spider）" class="headerlink" title="制作爬虫（Spider）"></a>制作爬虫（Spider）</h2><p>Spider 定义了用于下载的URL列表、跟踪链接的方案、解析网页内容的方式，以此来提取items。<br>要建立一个Spider，你必须用scrapy.spider.BaseSpider创建一个子类，并确定三个强制的属性：</p>
<ul>
<li>name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。</li>
<li>start_urls：爬取的URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li>
<li>parse()：解析的方法，调用的时候传入从每一个URL传回的Response对象作为唯一参数，负责解析并匹配抓取的数据(解析为item)，跟踪更多的URL。</li>
</ul>
<p>在 spiders 目录下新建 Wynews.py，代码如下。利用 yield Request(url=item[‘url’],meta={‘item_1’: item},callback=self.second_parse) 来进行第二层爬取。</p>
<pre>
class WynewsSpider(BaseSpider):
    name = "Wynews"
    start_urls = ['http://news.163.com/rank/']

    def parse(self,response):
        html = HtmlXPathSelector(response)
        page = html.xpath('//div[@class="subNav"]/a')
        for i in page:
            item = dict()
            item['category'] = i.xpath('text()').extract_first()
            item['url'] = i.xpath('@href').extract_first()
            print item['category'],item['url']
            yield Request(url=item['url'],meta={'item_1': item},callback=self.second_parse)

    def second_parse(self,response):
        item_1= response.meta['item_1']
        html = HtmlXPathSelector(response)
        #print 'response ',response
        page = html.xpath('//tr/td/a')
        #print 'page ',page
        items = []
        for i in page:
            item = DidiScrapyItem()
            item['category'] = item_1['category'].encode('utf8')
            item['url'] = item_1['url'].encode('utf8')
            item['secondary_title'] = i.xpath('text()').extract_first().encode('utf8')
            item['secondary_url'] = i.xpath('@href').extract_first().encode('utf8')
            #print i.xpath('text()').extract(),i.xpath('@href').extract()
            items.append(item)
        return items
</pre>

<h2 id="存储结果（Pipeline）"><a href="#存储结果（Pipeline）" class="headerlink" title="存储结果（Pipeline）"></a>存储结果（Pipeline）</h2><p>Item pipeline 的主要责任是负责处理 spider 抽取的 Item，主要任务是清理、验证和存储数据。当页面被 spider 解析后，将被发送到 pipeline，每个 pipeline 的组件都是由一个简单的方法组成的Python类。pipeline 获取Item，执行相应的方法，并确定是否需要在 pipeline中继续执行下一步或是直接丢弃掉不处理。</p>
<h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><ul>
<li>清理HTML数据</li>
<li>验证解析到的数据（检查Item是否包含必要的字段）</li>
<li>检查是否是重复数据（如果重复就删除）</li>
<li>将解析到的数据存储到 数据库/文件 中</li>
</ul>
<h3 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h3><ul>
<li><p>process_item(item, spider)<br>每一个item管道组件都会调用该方法，并且必须返回一个item对象实例或raise DropItem异常。<br>被丢掉的item将不会在管道组件进行执行</p>
</li>
<li><p>open_spider(spider)<br>当spider执行的时候将调用该方法</p>
</li>
<li><p>close_spider(spider)<br>当spider关闭的时候将调用该方法</p>
</li>
</ul>
<h3 id="编写自己的-Pipeline"><a href="#编写自己的-Pipeline" class="headerlink" title="编写自己的 Pipeline"></a>编写自己的 Pipeline</h3><p>编辑 pipelines.py。把抓取的 items 保存到 json 文件中。</p>
<pre>
import json
class NewsScrapyPipeline(object):
    def __init__(self):
        self.file = open('items.json', 'w')
    def process_item(self, item, spider):
        line = json.dumps(dict(item),ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
</pre>

<p>另外，如果不考虑编码（没有中文），可以在运行爬虫的时候直接通过下面的命令导出结果。</p>
<p>dump到JSON文件:</p>
<pre>scrapy crawl myspider -o items.json</pre>

<p>dump到CSV文件:</p>
<pre>scrapy crawl myspider -o items.csv</pre>

<p>dump到XML文件:</p>
<pre>scrapy crawl myspider -o items.xml</pre>


<h3 id="激活Item-Pipeline组件"><a href="#激活Item-Pipeline组件" class="headerlink" title="激活Item Pipeline组件"></a>激活Item Pipeline组件</h3><p>在settings.py文件中，往ITEM_PIPELINES中添加项目管道的类名，激活项目管道组件</p>
<pre>
ITEM_PIPELINES = {
    'news_scrapy.pipelines.NewsScrapyPipeline': 300,
}
</pre>

<h2 id="开启爬虫-Crawl"><a href="#开启爬虫-Crawl" class="headerlink" title="开启爬虫 (Crawl)"></a>开启爬虫 (Crawl)</h2><pre>scrapy crawl Wynews</pre>

<p><a href="https://github.com/Shuang0420/Crawler/tree/master/news_scrapy" target="_blank" rel="external">完整代码</a></p>
<h2 id="可能出现的问题-Problem"><a href="#可能出现的问题-Problem" class="headerlink" title="可能出现的问题 (Problem)"></a>可能出现的问题 (Problem)</h2><p>打开 items.json 文件，中文可能会出现文件乱码问题</p>
<pre>
[{"category": "\u93c2\u4f34\u6908", "url": "http://news.163.com/special/0001386F/rank_news.html", "secondary_title": "\u934b\u950b\u9422\u5cf0\u30b3\u95c3\u8e6d\u7b09\u9473\u6ec8\u69fb\u951b\u5c7e\u5d0f\u6fc2\u7a3f\u5dfb\u9359\u53c9\u7c2e\u6769\u6ec4\u7966\u95c0", "secondary_url": "http://caozhi.news.163.com/16/0615/09/BPJG6SB60001544E.html"},
</pre>

<p>这一行代码就能解决。</p>
<pre>
line = json.dumps(dict(item),ensure_ascii=False) + "\n"
</pre>

<p>结果</p>
<pre>
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股闯关MSCI再度失败 索罗斯们押注对冲胜出", "secondary_url": "http://money.163.com/16/0615/06/BPJ4T69300253B0H.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "湖北副省长担心房价下跌：泡沫若破裂后果很严重", "secondary_url": "http://money.163.com/16/0615/08/BPJBM36U00252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "马云:假货质量超过正品 打假很复杂", "secondary_url": "http://money.163.com/16/0615/08/BPJAIOVI00253G87.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股闯关未成功 纳入MSCI新兴市场指数被延迟", "secondary_url": "http://money.163.com/16/0615/07/BPJ7260D00252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "马云称许多假货比真品好 网友:怪不得淘宝假货多", "secondary_url": "http://money.163.com/16/0615/08/BPJC437N002526O3.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "贪官示意家人低价买地 拆迁后获赔近亿元", "secondary_url": "http://money.163.com/16/0615/08/BPJAT58400252G50.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "又是毒胶囊:浙江查获1亿多粒毒胶囊 6人被捕", "secondary_url": "http://money.163.com/16/0615/07/BPJ8NMRG00253B0H.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "还不起了？委内瑞拉寻求宽限1年偿还中国贷款", "secondary_url": "http://money.163.com/16/0615/07/BPJ9IH3400252C1E.html"}
{"category": "财经", "url": "http://money.163.com/special/002526BH/rank.html", "secondary_title": "A股频现清仓式减持 上半年十大减持王曝光", "secondary_url": "http://money.163.com/16/0615/07/BPJ7Q9BC00254IU4.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "《装X购车指南》 30-50万都能买到啥车？", "secondary_url": "http://auto.163.com/16/0615/07/BPJ6U1J900084TUP.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "看挡杆还以为是A8L 新款哈弗H9内饰曝光", "secondary_url": "http://auto.163.com/16/0615/00/BPIGTP4B00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "前脸/尾灯有变 新款捷达搭1.5L油耗更低", "secondary_url": "http://auto.163.com/16/0615/00/BPIGMEHE00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "主打车型不超10万良心价 远景SUV将8月上市", "secondary_url": "http://auto.163.com/16/0615/00/BPIHR2A500084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "Macan并不是我真姓 众泰SR8搭2.0T/D", "secondary_url": "http://auto.163.com/16/0613/00/BPDBPB0J00084TUO.html"}
{"category": "汽车", "url": "http://news.163.com/special/0001386F/rank_auto.html", "secondary_title": "上海福特翼搏优惠1.5万元", "secondary_url": "http://auto.163.com/16/0615/00/BPIHH8FF000857M6.html"}
</pre>

<h2 id="添加命令行参数"><a href="#添加命令行参数" class="headerlink" title="添加命令行参数"></a>添加命令行参数</h2><p>第一种方法，在命令行用crawl控制spider爬取的时候，加上-a选项，如</p>
<pre>scrapy crawl WangyiSpider -a category=打车</pre>

<p>然后在 spider 的构造函数里加上带入的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class WangyiSpider(BaseSpider):</div><div class="line">    name = &quot;Wangyi&quot;</div><div class="line"></div><div class="line">    def __init__(self, category=None, *args, **kwargs):</div><div class="line">        super(WangyiSpider, self).__init__(*args, **kwargs)</div><div class="line">        self.base_url = &apos;http://news.yodao.com/&apos;</div><div class="line">        self.start_urls = [&apos;http://news.yodao.com/search?q=&apos; +</div><div class="line">                           category]</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/Shuang0420/Crawler/blob/master/wangyi/wangyi/spiders/WangyiSpider.py" target="_blank" rel="external">代码</a><br><a href="https://github.com/Shuang0420/Crawler/tree/master/wangyi" target="_blank" rel="external">通过关键词爬取网易新闻－代码</a></p>
<h2 id="运行多个爬虫"><a href="#运行多个爬虫" class="headerlink" title="运行多个爬虫"></a>运行多个爬虫</h2><p>默认情况当你每次执行scrapy crawl命令时会创建一个新的进程。但我们可以使用核心API在同一个进程中同时运行多个spider，如下，在 settings.py 的同级目录下编辑 run.py，导入编写的 spider 类如 JingdongSpider, SuningSpider。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">from twisted.internet import reactor</div><div class="line">from scrapy.crawler import CrawlerRunner</div><div class="line">from scrapy.utils.log import configure_logging</div><div class="line">from scrapy.spiders import Spider</div><div class="line">from scrapy.selector import HtmlXPathSelector</div><div class="line">from items import FaqscrapyItem</div><div class="line">from scrapy.http import Request</div><div class="line">from scrapy.selector import Selector</div><div class="line">from scrapy.utils.project import get_project_settings</div><div class="line">from spiders.FAQ_jingdong import JingdongSpider</div><div class="line">from spiders.FAQ_suning import SuningSpider</div><div class="line">import re</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    settings = get_project_settings()</div><div class="line">    configure_logging(settings)</div><div class="line">    runner = CrawlerRunner(settings)</div><div class="line"></div><div class="line">    runner.crawl(JingdongSpider)</div><div class="line">    runner.crawl(SuningSpider)</div><div class="line"></div><div class="line">    d = runner.join()</div><div class="line">    d.addBoth(lambda _: reactor.stop())</div><div class="line"></div><div class="line">    # blocks process so always keep as the last statement</div><div class="line">    reactor.run()</div></pre></td></tr></table></figure></p>
<p>然而不幸的是同一进程内运行多个 spider 可能会出现数据丢失问题，影响进一步的数据使用。如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;&quot;url&quot;: &quot;http://help.jd.com/user/issue/231-213.html&quot;, &quot;text&quot;: &quot;订单已提交成功，如何付款？付款方式分为以下几种：（注：先款订单请您在订单提交后24小时内完成支付，否则订单会自动取消）1.货到付款：选择货到付款，在订单送达时您可选择现金、POS机刷卡、支票方式支付货款或通过京东APP手机客户端【扫一扫】功能扫描包裹单上的订单条形码方式用手机来完成订单的支付（扫码支付）；在订单未妥投之前您还可以进入“我的订单”在线支付货款。注意：货到付款的订单，如果一个ID帐号在一个月内有过1次以上或一年内有过3次以上，无理由不接收我司配送的商品，我司将在相应的ID帐户里按每单扣除500个京豆做为运费；时间计算方法为：成功提交订单后向前推算30天为一个月，成功提交订单后向前推算365天为一年，不以自然月和自然年计算。2.在线支付：选择在线支付，请您进入“我的订单”，点击“付款”，按提示进行操作；目前在线支付支持京东白条、余额、银行卡、网银+、微信、银联在线、网银钱包、信用卡等方式进行支付，可根据您的使用喜好进行选择。3.分期付款：目前不支持信用卡分期付款。4.公司转账：提交订单后选择线下公司转账会生成15位汇款识别码，请您按照提示到银行操作转账，然后进入“我的订单”填写付款确认；5.邮局汇款：订单提交成功后，请您按照提示到邮局操作汇款，然后进入“我的订单”填写付款确认。&quot;, &quot;question&quot;: &quot;订单已提交成功，如何付款？&quot;, &quot;title&quot;: &quot;支付流程&quot;&#125;</div><div class="line">�系统停机维护期间。（二） 电信设备出现故障不能进行数据传输的。（三） 由于黑客攻击、网络供应商技术调整或故障、网站升级、银行方面的问题等原因而造成的易付宝服务中断或延迟。（四） 因台风、地震、海啸、洪水、停电、战争、恐怖袭击等不可抗力之因素，造成易付宝系统障碍不能执行业务的。 第十三条  关于本协议条款和其他协议、告示或其他有关您使用本服务的通知，易付宝将以电子形式或纸张形式通知您，包括但不限于依据您向易付宝提供的电子邮件地址发送电子邮件的方式、依据投资者提供的联系地址寄送挂号信的方式、易付宝或合作伙伴网站公告、或发送手机短信、系统内通知和电话通知等方式。 第十四条  易付宝有权根据需要不时地修改本协议或制定、修改各类规则，但是，对于减少您权益或加重您义务的新增、变更或修改，易付宝将在生效日前提前至少7个日历日进行公示，如您不同意相关新增、变更或修改，您可以选择在公示期内终止本协议并停止使用本服务。如果相关新增、变更或修改生效后，您继续使用本服务则表示您接受修订后的权利义务条款。 第十五条 因本协议引起的或与本协议有关的争议，均适用中华人民共和国法律。 第十六条  因本协议引起的或与本协议有关的争议，易付宝与用户协商解决。协商不成的，任何一方均有权向被告住所地人民法院提起诉讼。 第十七条   本协议作为《易付宝余额理财服务协议》的有效补充，本协议未约定的内容，双方需按照《易付宝余额理财服务协议》相关约定。 &quot;, &quot;question&quot;: &quot;零钱宝定期转出服务协议&quot;, &quot;title&quot;: &quot;苏宁理财&quot;&#125;</div><div class="line">l&quot;: &quot;http://help.suning.com/page/id-536.htm&quot;, &quot;text&quot;: &quot;</div></pre></td></tr></table></figure></p>
<p><a href="">代码</a></p>
<h1 id="Scrapy-调优"><a href="#Scrapy-调优" class="headerlink" title="Scrapy 调优"></a>Scrapy 调优</h1><h2 id="提高并发能力"><a href="#提高并发能力" class="headerlink" title="提高并发能力"></a>提高并发能力</h2><h3 id="增加并发"><a href="#增加并发" class="headerlink" title="增加并发"></a>增加并发</h3><p>并发是指同时处理的request的数量。其有全局限制和局部(每个网站)的限制。Scrapy 默认的全局并发限制(16)对同时爬取大量网站的情况并不适用，因此需要增加这个值。 增加多少取决于爬虫能占用多少CPU。 一般开始可以设置为 100 。不过最好的方式是做一些测试，获得 Scrapy 进程占取CPU与并发数的关系。选择一个能使CPU占用率在80%-90%的并发数比较恰当。</p>
<pre>
# 增加全局并发数
CONCURRENT_REQUESTS = 100</pre>

<p>mac 下调试，运行程序后通过 top 监控，p 按 cpu 排序。观察发现，在 CONCURRENT_REQUESTS = 32 时，cpu 占用最多到 50% 左右，调整到 CONCURRENT_REQUESTS = 100，cpu 占用 90% 上下。</p>
<p>查看本机 cpu 信息，用 sysctl machdep.cpu 命令，如下，可以看到我的机子是双核、4线程的。</p>
<pre>
# cpu 信息
$ sysctl machdep.cpu
..........
machdep.cpu.core_count: 2
machdep.cpu.thread_count: 4
machdep.cpu.tsc_ccc.numerator: 0
machdep.cpu.tsc_ccc.denominator: 0
</pre>

<h3 id="降低log级别"><a href="#降低log级别" class="headerlink" title="降低log级别"></a>降低log级别</h3><p>为了减少CPU使用率(及记录log存储的要求), 当调试程序完毕后，可以不使用 DEBUG log级别。</p>
<pre>
# 设置Log级别:
LOG_LEVEL = 'INFO'</pre>


<h3 id="禁止cookies"><a href="#禁止cookies" class="headerlink" title="禁止cookies"></a>禁止cookies</h3><p>禁止cookies能减少CPU使用率及Scrapy爬虫在内存中记录的踪迹，提高性能。</p>
<pre># 禁止cookies:
COOKIES_ENABLED = False</pre>


<h3 id="禁止重试"><a href="#禁止重试" class="headerlink" title="禁止重试"></a>禁止重试</h3><p>对失败的HTTP请求进行重试会减慢爬取的效率，尤其是当站点响应很慢(甚至失败)时， 访问这样的站点会造成超时并重试多次。这是不必要的，同时也占用了爬虫爬取其他站点的能力。</p>
<pre>
# 禁止重试:
RETRY_ENABLED = False</pre>

<h3 id="减小下载超时"><a href="#减小下载超时" class="headerlink" title="减小下载超时"></a>减小下载超时</h3><p>对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。</p>
<pre>
# 减小下载超时:
DOWNLOAD_TIMEOUT = 15

# 可能会引发的错误
TimeoutError: User timeout caused connection failure: Getting http://homea.people.com.cn/n1/2016/0628/c69176-28504657.html took longer than 15.0 seconds..</pre>

<p>通过如上配置，我的爬虫每分钟响应的request是之前的4倍，然而值得注意的是，这些设置并不是在所有场景都适用，需要通过具体场景试验，具体问题具体分析。</p>
<h2 id="避免被禁止-ban"><a href="#避免被禁止-ban" class="headerlink" title="避免被禁止(ban)"></a>避免被禁止(ban)</h2><p>有些网站实现了特定的机制，以一定规则来避免被爬虫爬取。下面是些处理这些站点的建议(tips):</p>
<ul>
<li>使用user agent池，轮流选择之一来作为user agent。池中包含常见的浏览器的user agent(google一下一大堆)</li>
<li>禁止cookies(参考 COOKIES_ENABLED)，有些站点会使用cookies来发现爬虫的轨迹。</li>
<li>设置下载延迟(2或更高)。参考 DOWNLOAD_DELAY 设置。</li>
<li>如果可行，使用 Google cache 来爬取数据，而不是直接访问站点。</li>
<li>使用IP池。例如免费的 Tor项目 或付费服务(ProxyMesh)。</li>
<li>使用高度分布式的下载器(downloader)来绕过禁止(ban)，就只需要专注分析处理页面。这样的例子有: Crawlera</li>
</ul>
<p>如果仍然无法避免被ban，考虑商业支持.</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><ol>
<li><p>首先要有的是 <strong>user agent池</strong> 和 <strong>IP池</strong>。user agent池如下，添加在 settings.py 中。</p>
<pre>
USER_AGENTS = [
 "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
 "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
 "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
 "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
 "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
 "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
 "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
 "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
 "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
 "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
 "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
 "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
 "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
 "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
]
</pre>
</li>
<li><p>IP池 获取方式有多种，这里抓取的是<a href="http://www.xicidaili.com/" target="_blank" rel="external">西刺免费代理IP</a>的 IP，注意实时更新问题，否则很容易失败。将抓取的 IP 以 <a href="http://host1:port" target="_blank" rel="external">http://host1:port</a> 的格式存储于 list.txt 文本中。在 settings.py 里添加 PROXY_LIST = ‘/path/to/proxy/list.txt’。</p>
</li>
<li><p>有了 <strong>user agent池</strong> 和 <strong>IP池</strong>，接下来需要编写中间件，如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line">import random</div><div class="line">import base64</div><div class="line">from scrapy import log</div><div class="line"></div><div class="line">class RandomProxy(object):</div><div class="line">    def __init__(self, settings):</div><div class="line">        self.proxy_list = settings.get(&apos;PROXY_LIST&apos;)</div><div class="line">        fin = open(self.proxy_list)</div><div class="line"></div><div class="line">        self.proxies = &#123;&#125;</div><div class="line">        for line in fin.readlines():</div><div class="line">            parts = re.match(&apos;(\w+://)(\w+:\w+@)?(.+)&apos;, line)</div><div class="line">            if not parts:</div><div class="line">                continue</div><div class="line"></div><div class="line">            # Cut trailing @</div><div class="line">            if parts.group(2):</div><div class="line">                user_pass = parts.group(2)[:-1]</div><div class="line">            else:</div><div class="line">                user_pass = &apos;&apos;</div><div class="line"></div><div class="line">            self.proxies[parts.group(1) + parts.group(3)] = user_pass</div><div class="line"></div><div class="line">        fin.close()</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def from_crawler(cls, crawler):</div><div class="line">        return cls(crawler.settings)</div><div class="line"></div><div class="line">    def process_request(self, request, spider):</div><div class="line">        # Don&apos;t overwrite with a random one (server-side state for IP)</div><div class="line">        if &apos;proxy&apos; in request.meta:</div><div class="line">            return</div><div class="line"></div><div class="line">        proxy_address = random.choice(self.proxies.keys())</div><div class="line">        proxy_user_pass = self.proxies[proxy_address]</div><div class="line"></div><div class="line">        request.meta[&apos;proxy&apos;] = proxy_address</div><div class="line">        if proxy_user_pass:</div><div class="line">            basic_auth = &apos;Basic &apos; + base64.encodestring(proxy_user_pass)</div><div class="line">            request.headers[&apos;Proxy-Authorization&apos;] = basic_auth</div><div class="line">            print &quot;**************ProxyMiddleware have pass************&quot; + proxy[&apos;ip_port&apos;]</div><div class="line"></div><div class="line">    def process_exception(self, request, exception, spider):</div><div class="line">        proxy = request.meta[&apos;proxy&apos;]</div><div class="line">        log.msg(&apos;Removing failed proxy &lt;%s&gt;, %d proxies left&apos; % (</div><div class="line">                    proxy, len(self.proxies)))</div><div class="line">        try:</div><div class="line">            del self.proxies[proxy]</div><div class="line">        except ValueError:</div><div class="line">            pass</div><div class="line"></div><div class="line"></div><div class="line">class RandomUserAgent(object):</div><div class="line">    &quot;&quot;&quot;Randomly rotate user agents based on a list of predefined ones&quot;&quot;&quot;</div><div class="line"></div><div class="line">    def __init__(self, agents):</div><div class="line">        self.agents = agents</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def from_crawler(cls, crawler):</div><div class="line">        return cls(crawler.settings.getlist(&apos;USER_AGENTS&apos;))</div><div class="line"></div><div class="line">    def process_request(self, request, spider):</div><div class="line">        print &quot;**************************&quot; + random.choice(self.agents)</div><div class="line">        request.headers.setdefault(&apos;User-Agent&apos;, random.choice(self.agents))</div></pre></td></tr></table></figure>
</li>
<li><p>改写 spider，check 某个元素，确保 proxy 能够返回 target page。</p>
<pre>
if not pageUrls:
 yield Request(url=response.url, dont_filter=True)
</pre>
</li>
<li><p>配置 settings.py</p>
<pre>
# Retry many times since proxies often fail
RETRY_TIMES = 10
# Retry on most error codes since proxies fail for different reasons
RETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]
# Configure a delay for requests for the same website (default: 0)
DOWNLOAD_DELAY=3
# Disable cookies (enabled by default)
COOKIES_ENABLED=False
# Enable downloader middlewares
DOWNLOADER_MIDDLEWARES = {
 'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 90,
 # Fix path to this module
 'blogCrawler.middlewares.RandomProxy': 100,
 'blogCrawler.middlewares.RandomUserAgent': 1,
 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,
}
</pre>


</li>
</ol>
<p><a href="https://github.com/Shuang0420/Crawler/tree/master/blogCrawler" target="_blank" rel="external">这是一份简单的测试代码</a></p>
<h3 id="其他调优"><a href="#其他调优" class="headerlink" title="其他调优"></a>其他调优</h3><p>来自 <a href="http://ju.outofmemory.cn/entry/18981" target="_blank" rel="external">使用scrapy进行大规模抓取</a></p>
<ol>
<li><p>如果想要爬取的质量更高，尽量使用宽度优先的策略，在配置里设置 SCHEDULER_ORDER = ‘BFO’</p>
</li>
<li><p>修改单爬虫的最大并行请求数 CONCURRENT_REQUESTS_PER_SPIDER</p>
</li>
<li><p>修改twisted的线程池大小，默认值是10。参考<a href="http://twistedmatrix.com/documents/10.1.0/core/howto/threading.html" target="_blank" rel="external">Using Threads in Twisted</a>，在scrapy/core/manage.py爬虫启动前加上</p>
</li>
</ol>
<pre>reactor.suggestThreadPoolSize(poolsize)</pre>

<ol>
<li><p>可以开启dns cache来提高性能。在配置里面加上</p>
<pre>EXTENSIONS={’scrapy.contrib.resolver.CachingResolver’: 0,}</pre>
</li>
<li><p>如果自己实现duplicate filter的话注意要保证它是一直可用的，dupfilter里的异常是不会出现在日志文件中的，好像外面做了try-expect处理</p>
</li>
</ol>
<h2 id="去重与增量抓取"><a href="#去重与增量抓取" class="headerlink" title="去重与增量抓取"></a>去重与增量抓取</h2><h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h3><p>Scrapy支持通过RFPDupeFilter来完成页面的去重（防止重复抓取）。RFPDupeFilter实际是根据request_fingerprint实现过滤的，实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def request_fingerprint(request, include_headers=None):</div><div class="line">if include_headers:</div><div class="line">  include_headers = tuple([h.lower() for h in sorted(include_headers)])</div><div class="line">cache = _fingerprint_cache.setdefault(request, &#123;&#125;)</div><div class="line">if include_headers not in cache:</div><div class="line">  fp = hashlib.sha1()</div><div class="line">  fp.update(request.method)</div><div class="line">  fp.update(canonicalize_url(request.url))</div><div class="line">  fp.update(request.body or &apos;&apos;)</div><div class="line">  if include_headers:</div><div class="line">    for hdr in include_headers:</div><div class="line">      if hdr in request.headers:</div><div class="line">        fp.update(hdr)</div><div class="line">        for v in request.headers.getlist(hdr):</div><div class="line">          fp.update(v)</div><div class="line">  cache[include_headers] = fp.hexdigest()</div><div class="line">return cache[include_headers]</div></pre></td></tr></table></figure></p>
<p>我们可以看到，去重指纹是sha1(method + url + body + header)，所以，实际能够去掉重复的比例并不大。</p>
<p>如果我们需要自己提取去重的finger，需要自己实现Filter，并配置上它。</p>
<p>例如下面这个Filter只根据url去重：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">from scrapy.dupefilter import RFPDupeFilter</div><div class="line">class SeenURLFilter(RFPDupeFilter):</div><div class="line">  &quot;&quot;&quot;A dupe filter that considers the URL&quot;&quot;&quot;</div><div class="line">  def __init__(self, path=None):</div><div class="line">    self.urls_seen = set()</div><div class="line">    RFPDupeFilter.__init__(self, path)</div><div class="line">  def request_seen(self, request):</div><div class="line">    if request.url in self.urls_seen:</div><div class="line">      return True</div><div class="line">    else:</div><div class="line">      self.urls_seen.add(request.url)</div></pre></td></tr></table></figure></p>
<p>要在 settings 添加配置。<br>DUPEFILTER_CLASS =’scraper.custom_filters.SeenURLFilter’</p>
<h3 id="增量爬取"><a href="#增量爬取" class="headerlink" title="增量爬取"></a>增量爬取</h3><p>可以看这篇<a href="http://www.ihowandwhy.com/z/%E5%9F%BA%E4%BA%8Epython%E7%9A%84scrapy%E7%88%AC%E8%99%AB%EF%BC%8C%E5%85%B3%E4%BA%8E%E5%A2%9E%E9%87%8F%E7%88%AC%E5%8F%96%E6%98%AF%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%E7%9A%84%EF%BC%9F" target="_blank" rel="external">汇总贴</a></p>
<p>其实如果根据 url 判断的话有很多种方案，如下面这种（比起上面汇总贴的其他方案来说算是复杂的）。</p>
<blockquote>
<p>增量抓取。一个针对多个网站的爬虫很难一次性把所有网页爬取下来，并且网页也处于不断更新的状态中，爬取是一个动态的过程，爬虫支持增量的抓取是很必要的。大概的流程就是关闭爬虫时保存duplicate filter的数据，保存当前的request队列，爬虫启动时导入duplicate filter，并且用上次request队列的数据作为start url。这里还涉及scrapy一个称得上bug的问题，一旦抓取队列里url过多，关闭scrapy需要很久，有时候要花费几天的时间。我们hack了scrapy的代码，在接收到关闭命令后，保存duplicate filter数据和当前的request队列和已抓取的url列表，然后调用twisted的reactor.stop()强制退出。当前的request队列可以通过scrapy.core.scheduler的pending_requests成员得到。</p>
</blockquote>
<p>然而，如果使所有网站的动态过滤，比如是不是多了一个新回复，在url上的变化并不能体现出来，搜索引擎采用的是一系列的算法，判断某一个页面的更新时机。这个时候只能尝试用网页在进入下一级页面的时候都类似于最后更新时间、最后活动时间的参数进行判断了。</p>
<p>有机会会去尝试。</p>
<blockquote>
<p>参考资料<br><a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/faq.html" target="_blank" rel="external">scrapy 文档</a><br><a href="http://m.blog.csdn.net/article/details?id=50748700" target="_blank" rel="external">向scrapy中的spider传递参数的几种方法</a><br><a href="http://ju.outofmemory.cn/entry/18981" target="_blank" rel="external">使用scrapy进行大规模抓取</a><br><a href="http://www.pycoding.com/2016/04/10/scrapy-10.html" target="_blank" rel="external">Scrapy笔记（10）- 动态配置爬虫</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[爬虫总结(一)-- 爬虫基础 & python实现]]></title>
      <url>http://www.shuang0420.com/2016/06/11/%E7%88%AC%E8%99%AB%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>爬虫在平时也经常用，但一直没有系统的总结过，其实它涉及了许多的知识点。这一系列会理一遍这些知识点，不求详尽，只希望以点带面构建一个爬虫的知识框架。这一篇是概念性解释以及入门级爬虫介绍（以爬取网易新闻为例）。<br><a id="more"></a></p>
<h1 id="爬虫基础"><a href="#爬虫基础" class="headerlink" title="爬虫基础"></a>爬虫基础</h1><h2 id="什么是爬虫"><a href="#什么是爬虫" class="headerlink" title="什么是爬虫"></a>什么是爬虫</h2><p>爬虫说白了其实就是获取资源的程序。制作爬虫的总体分三步：爬－取－存。首先要获取整个网页的所有内容，然后再取出其中对你有用的部分，最后再保存有用的部分。</p>
<h2 id="爬虫类型"><a href="#爬虫类型" class="headerlink" title="爬虫类型"></a>爬虫类型</h2><ul>
<li>网络爬虫<br>网络爬虫，是一种按照一定的规则，<strong>自动的</strong> 抓取万维网信息的程序或者脚本。网络爬虫是搜索引擎系统中十分重要的组成部分，爬取的网页信息用于建立索引从而为搜索引擎提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，其性能的优劣直接影响着搜索引擎的效果。</li>
<li>传统爬虫<br>从一个或若干初始网页的URL开始，获得初始网页的URL，在抓取网页过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。</li>
</ul>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><ul>
<li>根据一定的网页分析算法过滤与主题无关的链接，保留有用链接并将其放入等待抓取的URL队列</li>
<li>根据一定的搜索策略从队列中选择下一步要抓取的网页URL，重复上述过程，直到达到指定条件才结束爬取</li>
<li>对所有抓取的网页进行一定的分析、过滤，并建立索引，以便之后的查询和检索。</li>
</ul>
<h2 id="爬取策略"><a href="#爬取策略" class="headerlink" title="爬取策略"></a>爬取策略</h2><h3 id="广度优先"><a href="#广度优先" class="headerlink" title="广度优先"></a>广度优先</h3><p>完成当前层次的搜索后才进行下一层次的搜索。一般的使用策略，一般通过队列来实现。</p>
<h3 id="最佳优先"><a href="#最佳优先" class="headerlink" title="最佳优先"></a>最佳优先</h3><p>会有评估算法，凡是被算法评估为有用的网页，先来爬取。</p>
<h3 id="深度优先"><a href="#深度优先" class="headerlink" title="深度优先"></a>深度优先</h3><p>实际应用很少。可能会导致trapped问题。通过栈来实现。</p>
<h2 id="URL（-Uniform-Resource-Locator-统一资源定位符）"><a href="#URL（-Uniform-Resource-Locator-统一资源定位符）" class="headerlink" title="URL（ Uniform Resource Locator: 统一资源定位符）"></a>URL（ Uniform Resource Locator: 统一资源定位符）</h2><p>互联网上资源均有其唯一的地址，由三部分组成。</p>
<ul>
<li>模式/协议</li>
<li>文件所在IP地址及端口号</li>
<li>主机上的资源位置</li>
<li>例子：<a href="http://www.example.com/index.html" target="_blank" rel="external">http://www.example.com/index.html</a></li>
</ul>
<h2 id="Web-Server／Socket如何建立连接和传输数据的"><a href="#Web-Server／Socket如何建立连接和传输数据的" class="headerlink" title="Web Server／Socket如何建立连接和传输数据的"></a>Web Server／Socket如何建立连接和传输数据的</h2><p>web server 的工作过程其实和打电话的过程差不多（买电话–&gt;注册号码–&gt;监听–&gt;排队接听–&gt;读写–&gt;关闭），经典的三步握手（有人在吗？我在呢，你呢？我也在）在排队接听时进行。下面一张图足以解释一切。</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/webserver.png" alt=""></p>
<p>Crawler端需要一个socket接口，向服务器端发起connect请求，完成连接后就可以和服务器交流了，操作完毕会关闭socket接口。服务器端更复杂一点，也需要一个socket接口，并且这个socket接口需要绑定一个地址（bind()），这就相当于有一个固定的电话号码，这样其他人拨打这个号码就可以找到这个服务器。绑定之后服务器的socket就开始监听（listen()）有没有用户请求，如果有，就接收请求（accept()），和用户建立连接，然后就可以交流。</p>
<h2 id="HTML-DOM"><a href="#HTML-DOM" class="headerlink" title="HTML DOM"></a>HTML DOM</h2><ul>
<li>DOM 将 HTML 文档表达为树结构</li>
<li>定义了访问和操作 HTML 文档的标准</li>
</ul>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.35.09.png" alt=""></p>
<h2 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h2><ul>
<li>由服务器端生成，发送给 User-Agent(一般是浏览器)，浏览器会将 Cookie 的 key/value 保存到某个目录下的文本文件哪，下次访问同一网站时就发送该 Cookie 给服务器。</li>
</ul>
<h2 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h2><ul>
<li>GET 直接以链接形式访问，链接中包含了所有的参数</li>
<li>PUT 把提交的数据放到 HTTP 包的包体中<pre>
eg.
import urllib
import urllib2
url='http://www.zhihu.com/#signin'
user_agent='MOZILLA/5.0'
values={'username':'252618408@qq.com','password':'xxx'}
headers={'User-Agent':user_agent}
data=urllib.urlencode(values) # urlencode 是 urllib 独有的方法
request=urllib2.Request(url,data,headers) # write a letter
response=urllib2.urlopen(request) # send the letter and get the reply
page=response.read() # read the reply
</pre>

</li>
</ul>
<p>urllib 仅可以接受 URL，这意味着你不可以伪装你的 User Agent 字符串等，但 urllib 提供了 urlencode 方法用来GET查询字符串等产生，而 urllib2 没有。<br>因此 urllib, urllib2经常一起使用。</p>
<h3 id="Headers-设置"><a href="#Headers-设置" class="headerlink" title="Headers 设置"></a>Headers 设置</h3><ul>
<li>User-Agent: 部分服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求</li>
<li>Content-Type: 使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析</li>
<li>application/xml: 在 XMl RPC, 如 RESTful/SOAP 调用时使用</li>
<li>application/json: 在 JSON RPC 调用时使用</li>
<li>application/x-www-form-urlencoded: 浏览器提交 Web 表单时使用</li>
</ul>
<h2 id="爬虫难点"><a href="#爬虫难点" class="headerlink" title="爬虫难点"></a>爬虫难点</h2><p>爬虫的两部分，一是下载 Web 页面，有许多问题需要考虑。如何最大程度地利用本地带宽,如何调度针对不同站点的 Web 请求以减轻对方服务器的负担等。一个高性能的 Web Crawler 系统里，DNS 查询也会成为急需优化的瓶颈，另外，还有一些“行规”需要遵循（例如robots.txt）。<br>而获取了网页之后的分析过程也是非常复杂的，Internet 上的东西千奇百怪，各种错误百出的 HTML 页面都有，要想全部分析清楚几乎是不可能的事；另外，随着 AJAX 的流行，如何获取由 Javascript 动态生成的内容成了一大难题；除此之外，Internet 上还有有各种有意或无意出现的 Spider Trap ，如果盲目的跟踪超链接的话，就会陷入 Trap 中万劫不复了，例如这个网站，据说是之前 Google 宣称 Internet 上的 Unique URL 数目已经达到了 1 trillion 个，因此这个人 is proud to announce the second trillion 。</p>
<h1 id="最简单的爬虫"><a href="#最简单的爬虫" class="headerlink" title="最简单的爬虫"></a>最简单的爬虫</h1><h2 id="requests-库"><a href="#requests-库" class="headerlink" title="requests 库"></a>requests 库</h2><pre>
import requests
url = "http://shuang0420.github.io/"
r = requests.get(url)
</pre>

<h2 id="urllib2-库"><a href="#urllib2-库" class="headerlink" title="urllib2 库"></a>urllib2 库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line"># request source file</div><div class="line">url = &quot;http://shuang0420.github.io/&quot;</div><div class="line">request = urllib2.Request(url)  # write a letter</div><div class="line">response = urllib2.urlopen(request)  # send the letter and get the reply</div><div class="line">page = response.read()  # read the reply</div><div class="line"></div><div class="line"># save source file</div><div class="line">webFile = open(&apos;webPage.html&apos;, &apos;wb&apos;)</div><div class="line">webFile.write(page)</div><div class="line">webFile.close()</div></pre></td></tr></table></figure>
<p>这是一个简单的爬虫，打开 webPage.html 是这样的显示，没有css.<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%881.44.20.png" alt=""></p>
<h1 id="实例：爬取网易新闻"><a href="#实例：爬取网易新闻" class="headerlink" title="实例：爬取网易新闻"></a>实例：爬取网易新闻</h1><p>爬取网易新闻 [代码示例]<br>– 使用 urllib2 的 requests包来爬取页面<br>– 使用正则表达式和 bs4 分析一级页面,使用 Xpath 来分析二级页面<br>– 将得到的标题和链接,保存为本地文件</p>
<h2 id="分析初始页面"><a href="#分析初始页面" class="headerlink" title="分析初始页面"></a>分析初始页面</h2><p>我们的初始页面是 <a href="http://news.163.com/rank" target="_blank" rel="external">http://news.163.com/rank</a><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.08.54.png" alt=""></p>
<p>查看源代码<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.10.28.png" alt=""></p>
<p>我们想要的是分类标题和URL，需要解析 DOM 文档树,这里使用了 BeautifulSoup 里的方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def Nav_Info(myPage):</div><div class="line">    # 二级导航的标题和页面</div><div class="line">    pageInfo = re.findall(r&apos;&lt;div class=&quot;subNav&quot;&gt;.*?&lt;div class=&quot;area areabg1&quot;&gt;&apos;, myPage, re.S)[</div><div class="line">        0].replace(&apos;&lt;div class=&quot;subNav&quot;&gt;&apos;, &apos;&apos;).replace(&apos;&lt;div class=&quot;area areabg1&quot;&gt;&apos;, &apos;&apos;)</div><div class="line">    soup = BeautifulSoup(pageInfo, &quot;lxml&quot;)</div><div class="line">    tags = soup(&apos;a&apos;)</div><div class="line">    topics = []</div><div class="line">    for tag in tags:</div><div class="line">        # 只要 科技、财经、体育 的新闻</div><div class="line">        # if (tag.string==&apos;科技&apos; or tag.string==&apos;财经&apos; or tag.string==&apos;体育&apos;):</div><div class="line">        topics.append((tag.string, tag.get(&apos;href&apos;, None)))</div><div class="line">    return topics</div></pre></td></tr></table></figure></p>
<p>然而，Beautiful Soup对文档的解析速度不会比它所依赖的解析器更快,如果对计算时间要求很高或者计算机的时间比程序员的时间更值钱,那么就应该直接使用 lxml。换句话说,还有提高Beautiful Soup效率的办法,使用lxml作为解析器。Beautiful Soup用lxml做解析器比用html5lib或Python内置解析器速度快很多。bs4 的默认解析器是 html.parser，使用lxml的代码如下：</p>
<pre>BeautifulSoup(markup, "lxml")</pre>

<h2 id="分析二级页面"><a href="#分析二级页面" class="headerlink" title="分析二级页面"></a>分析二级页面</h2><p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.14.06.png" alt=""></p>
<p>查看源代码<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-11%20%E4%B8%8B%E5%8D%889.11.23.png" alt=""></p>
<p>我们要爬取的是<td></td>之间的新闻标题和链接，同样需要解析文档树，可以通过以下代码实现，这里用了 lxml 解析器，效率更高。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def News_Info(newPage):</div><div class="line">    # xpath 使用路径表达式来选取文档中的节点或节点集</div><div class="line">    dom = etree.HTML(newPage)</div><div class="line">    news_titles = dom.xpath(&apos;//tr/td/a/text()&apos;)</div><div class="line">    news_urls = dom.xpath(&apos;//tr/td/a/@href&apos;)</div><div class="line">    return zip(news_titles, news_urls)</div></pre></td></tr></table></figure></p>
<p><a href="https://github.com/Shuang0420/Crawler/blob/master/news.py" target="_blank" rel="external">完整代码</a></p>
<h2 id="潜在问题"><a href="#潜在问题" class="headerlink" title="潜在问题"></a>潜在问题</h2><ol>
<li><p>我们的任务是爬取1万个网页，按上面这个程序，耗费时间长，我们可以考虑开启多个线程(池)去一起爬取，或者用分布式架构去并发的爬取网页。</p>
</li>
<li><p>种子URL和后续解析到的URL都放在一个列表里，我们应该设计一个更合理的数据结构来存放这些待爬取的URL才是，比如队列或者优先队列。</p>
</li>
<li><p>对各个网站的url，我们一视同仁，事实上，我们应当区别对待。大站好站优先原则应当予以考虑。</p>
</li>
<li><p>每次发起请求，我们都是根据url发起请求，而这个过程中会牵涉到DNS解析，将url转换成ip地址。一个网站通常由成千上万的URL，因此，我们可以考虑将这些网站域名的IP地址进行缓存，避免每次都发起DNS请求，费时费力。</p>
</li>
<li><p>解析到网页中的urls后，我们没有做任何去重处理，全部放入待爬取的列表中。事实上，可能有很多链接是重复的，我们做了很多重复劳动。</p>
</li>
<li><p>爬虫被封禁问题</p>
</li>
</ol>
<h2 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h2><ol>
<li><p>并行爬取问题</p>
<p> 关于并行爬取，首先我们想到的是多线程或者线程池方式，一个爬虫程序内部开启多个线程。同一台机器开启多个爬虫程序，这样，我们就有N多爬取线程在同时工作，大大提高了效率。</p>
<p> 当然，如果我们要爬取的任务特别多，一台机器、一个网点肯定是不够的，我们必须考虑分布式爬虫。分布式架构，考虑的问题有很多，我们需要一个scheduler来分配任务并排序，各个爬虫之间还需要通信合作，共同完成任务，不要重复爬取相同的网页。分配任务时我们还需要考虑负载均衡以做到公平。（可以通过Hash，比如根据网站域名进行hash）</p>
<p> 负载均衡分派完任务之后，千万不要以为万事大吉了，万一哪台机器挂了呢？原先指派给挂掉的哪台机器的任务指派给谁？又或者哪天要增加几台机器，任务有该如何进行重新分配呢？所以我们还要 task table 来纪录状态。</p>
</li>
<li><p>待爬取网页队列<br>如何对待待抓取队列，跟操作系统如何调度进程是类似的场景。<br>不同网站，重要程度不同，因此，可以设计一个优先级队列来存放待爬起的网页链接。如此一来，每次抓取时，我们都优先爬取重要的网页。<br>当然，你也可以效仿操作系统的进程调度策略之多级反馈队列调度算法。</p>
</li>
<li><p>DNS缓存<br>为了避免每次都发起DNS查询，我们可以将DNS进行缓存。DNS缓存当然是设计一个hash表来存储已有的域名及其IP。</p>
</li>
<li><p>网页去重<br>说到网页去重，第一个想到的是垃圾邮件过滤。垃圾邮件过滤一个经典的解决方案是Bloom Filter（布隆过滤器）。布隆过滤器原理简单来说就是：建立一个大的位数组，然后用多个Hash函数对同一个url进行hash得到多个数字，然后将位数组中这些数字对应的位置为1。下次再来一个url时，同样是用多个Hash函数进行hash，得到多个数字，我们只需要判断位数组中这些数字对应的为是全为1，如果全为1，那么说明这个url已经出现过。如此，便完成了url去重的问题。当然，这种方法会有误差，只要误差在我们的容忍范围之类，比如1万个网页，我只爬取到了9999个，并不会有太大的实际影响。<br>一种很不错的方法来自<a href="http://itindex.net/detail/39767-url-%E7%9B%B8%E4%BC%BC-%E8%AE%A1%E7%AE%97" target="_blank" rel="external">url相似度计算</a>，简单介绍下。<br>考虑到url本身的结构，对其相似度的计算就可以抽象为对其关键特征相似度的计算。比如可以把站点抽象为一维特征，目录深度抽象为一维特征，一级目录、二级目录、尾部页面的名字也都可以抽象为一维特征。比如下面两个url:<br>url1:  <a href="http://www.spongeliu.com/go/happy/1234.html" target="_blank" rel="external">http://www.spongeliu.com/go/happy/1234.html</a><br>url2:  <a href="http://www.spongeliu.com/snoopy/tree/abcd.html" target="_blank" rel="external">http://www.spongeliu.com/snoopy/tree/abcd.html</a></p>
<p>特征：</p>
<ul>
<li>站点特征：如果两个url站点一样，则特征取值1，否则取值0；</li>
<li>目录深度特征：特征取值分别是两个url的目录深度是否一致；</li>
<li>一级目录特征：在这维特征的取值上，可以采用多种方法，比如如果一级目录名字相同则特征取1，否则取0；或者根据目录名字的编辑距离算出一个特征值；或者根据目录名字的pattern，如是否数字、是否字母、是否字母数字穿插等。这取决于具体需求，这里示例仅仅根据目录名是否相同取1和0；</li>
<li>尾页面特征：这维特征的取值同一级目录，可以判断后缀是否相同、是否数字页、是否机器生成的随机字符串或者根据编辑长度来取值，具体也依赖于需求。这里示例仅仅判断最后一级目录的特征是否一致（比如是否都由数字组成、是否都有字母组成等）。</li>
</ul>
<p>这样，对于这两个url就获得了4个维度的特征，分别是：1 1 0 0 。有了这两个特征组合，就可以根据具体需求判断是否相似了。我们定义一下每个特征的重要程度，给出一个公式：</p>
<pre>similarity = feather1 * x1 + feather2*x2 + feather3*x3 + feather4*x4</pre>

<p>其中x表示对应特征的重要程度，比如我认为站点和目录都不重要，最后尾页面的特征才是最重要的，那么x1,x2,x3都可以取值为0，x4取值为1，这样根据similarity就能得出是否相似了。或者认为站点的重要性占10%，目录深度占50%，尾页面的特征占40%，那么系数分别取值为0.1\0.5\0\0.4即可。</p>
<p>其实这样找出需要的特征，可以把这个问题简化成一个机器学习的问题，只需要人为判断出一批url是否相似，用svm训练一下就可以达到机器判断的目的。<br>除了上面这种两个url相似度的判断，也可以将每一条url都抽象成一组特征，然后计算出一个url的得分，设置一个分数差的阈值，就可以达到从一大堆url中找出相似的url的目的。</p>
</li>
<li><p>数据存储的问题<br>数据存储同样是个很有技术含量的问题。用关系数据库存取还是用NoSQL，抑或是自己设计特定的文件格式进行存储，都大有文章可做。</p>
</li>
<li><p>进程间通信<br>分布式爬虫，就必然离不开进程间的通信。我们可以以规定的数据格式进行数据交互，完成进程间通信。</p>
</li>
<li><p>反爬虫机制问题<br>针对反爬虫机制，我们可以通过轮换IP地址、轮换Cookie、修改用户代理(User Agent)、限制速度、避免重复性爬行模式等方法解决。</p>
</li>
</ol>
<p>参考链接:</p>
<blockquote>
<p><a href="http://www.cnblogs.com/wawlian/archive/2012/06/18/2553061.html" target="_blank" rel="external">网络爬虫基本原理(一)</a><br><a href="http://www.chinahadoop.cn/course/596/learn#lesson/11986" target="_blank" rel="external">http://www.chinahadoop.cn/course/596/learn#lesson/11986</a><br><a href="https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys" target="_blank" rel="external">https://www.bittiger.io/blog/post/5pDTFcDwkmCvvmKys</a><br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="external">https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Crawler </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Crawler </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[gensim-doc2vec实战]]></title>
      <url>http://www.shuang0420.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/</url>
      <content type="html"><![CDATA[<p>gensim的doc2vec找不到多少资料，根据官方api探索性的做了些尝试。本文介绍了利用gensim的doc2vec来训练模型，infer新文档向量，infer相似度等方法，有一些不成熟的地方，后期会继续改进。</p>
<a id="more"></a>
<h3 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h3><pre>
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf8')
import gensim, logging
import os
import jieba

# logging information
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
</pre>

<h3 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h3><pre>
# get input file, text format
f = open('trainingdata.txt','r')
input = f.readlines()
count = len(input)
print count
</pre>

<h3 id="文件预处理，分词等"><a href="#文件预处理，分词等" class="headerlink" title="文件预处理，分词等"></a>文件预处理，分词等</h3><pre>
# read file and separate words
alldocs=[] # for the sake of check, can be removed
count=0 # for the sake of check, can be removed
for line in input:
    line=line.strip('\n')
    seg_list = jieba.cut(line)
    output.write(' '.join(seg_list) + '\n')
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count)) # for the sake of check, can be removed
    count+=1 # for the sake of check, can be removed
</pre>


<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>gensim Doc2Vec 提供了 DM 和 DBOW 两个模型。gensim 的说明文档建议多次训练数据集并调整学习速率或在每次训练中打乱输入信息的顺序以求获得最佳效果。</p>
<pre>
# PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size
Doc2Vec(sentences,dm=1, dm_concat=1, size=100, window=2, hs=0, min_count=2, workers=cores)
# PV-DBOW  
Doc2Vec(sentences,dm=0, size=100, hs=0, min_count=2, workers=cores)
# PV-DM w/average
Doc2Vec(sentences,dm=1, dm_mean=1, size=100, window=2, hs=0, min_count=2, workers=cores)
</pre>


<h3 id="训练并保存模型"><a href="#训练并保存模型" class="headerlink" title="训练并保存模型"></a>训练并保存模型</h3><pre>
# train and save the model
sentences= gensim.models.doc2vec.TaggedLineDocument('output.seq')
model = gensim.models.Doc2Vec(sentences,size=100, window=3)
model.train(sentences)
model.save('all_model.txt')
</pre>

<h3 id="保存文档向量"><a href="#保存文档向量" class="headerlink" title="保存文档向量"></a>保存文档向量</h3><pre>
# save vectors
out=open("all_vector.txt","wb")
for num in range(0,count):
    docvec =model.docvecs[num]
    out.write(docvec)
    #print num
    #print docvec
out.close()
</pre>

<h3 id="检验-计算训练文档中的文档相似度"><a href="#检验-计算训练文档中的文档相似度" class="headerlink" title="检验 计算训练文档中的文档相似度"></a>检验 计算训练文档中的文档相似度</h3><pre>
# test, calculate the similarity
# 注意 docid 是从0开始计数的
# 计算与训练集中第一篇文档最相似的文档
sims = model.docvecs.most_similar(0)
print sims
# get similarity between doc1 and doc2 in the training data
sims = model.docvecs.similarity(1,2)
print sims
</pre>

<h3 id="infer向量，比较相似度"><a href="#infer向量，比较相似度" class="headerlink" title="infer向量，比较相似度"></a>infer向量，比较相似度</h3><p>下面的代码用于检验模型正确性，随机挑一篇trained dataset中的文档，用模型重新infer，再计算与trained dataset中文档相似度，如果模型良好，相似度第一位应该就是挑出的文档。</p>
<pre>
# check
#############################################################################
# A good check is to re-infer a vector for a document already in the model. #
# if the model is well-trained,                                             #
# the nearest doc should (usually) be the same document.                    #
#############################################################################

print 'examing'
doc_id = np.random.randint(model.docvecs.count)  # pick random doc; re-run cell for more examples
print('for doc %d...' % doc_id)
inferred_docvec = model.infer_vector(alldocs[doc_id].words)
print('%s:\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))
</pre>

<h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><p>👇两个错误还在探索中，根据官方指南是可以运行的，然而我遇到了错误并没能解决。<br>第一段错误代码，关于train the model</p>
<pre>
alldocs=[]
count=0
for line in input:
    #print line
    line=line.strip('\n')
    seg_list = jieba.cut(line)
    #output.write(line)
    output.write(' '.join(seg_list) + '\n')
    alldocs.append(gensim.models.doc2vec.TaggedDocument(seg_list,count))
    count+=1

model = Doc2Vec(alldocs,size=100, window=2, min_count=5, workers=4)
model.train(alldocs)
</pre>

<p>报错信息</p>
<pre>
Traceback (most recent call last):
  File "d2vTestv5.py", line 59, in <module>
    model = Doc2Vec(alldocs[0],size=100, window=2, min_count=5, workers=4)
  File "/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py", line 596, in __init__
    self.build_vocab(documents, trim_rule=trim_rule)
  File "/usr/local/lib/python2.7/site-packages/gensim/models/word2vec.py", line 508, in build_vocab
    self.scan_vocab(sentences, trim_rule=trim_rule)  # initial survey
  File "/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py", line 639, in scan_vocab
    document_length = len(document.words)
AttributeError: 'generator' object has no attribute 'words'
</module></pre>

<p>第二段错误代码，关于infer</p>
<pre>
doc_words1=['验证','失败','验证码','未','收到']
doc_words2=['今天','奖励','有','哪些','呢']
# get infered vector
invec1 = model.infer_vector(doc_words1, alpha=0.1, min_alpha=0.0001, steps=5)
invec2 = model.infer_vector(doc_words2, alpha=0.1, min_alpha=0.0001, steps=5)
print invec1
print invec2

# get similarity
# the output docid is supposed to be 0
sims = model.docvecs.most_similar([invec1])
print sims

# according to official guide, the following codes are supposed to be fine, but it fails to run
sims= model.docvecs.similarity(invec1,invec2)
print model.similarity(['今天','有','啥','奖励'],['今天','奖励','有','哪些','呢'])
</pre>

<p>最后两行代码报错，错误信息</p>
<pre>
raceback (most recent call last):
  File "d2vTestv5.py", line 110, in <module>
    sims= model.docvecs.similarity(invec1,invec2)
  File "/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py", line 484, in similarity
    return dot(matutils.unitvec(self[d1]), matutils.unitvec(self[d2]))
  File "/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py", line 341, in __getitem__
    return vstack([self[i] for i in index])
  File "/usr/local/lib/python2.7/site-packages/gensim/models/doc2vec.py", line 341, in __getitem__
    return vstack([self[i] for i in index])
TypeError: 'numpy.float32' object is not iterable
</module></pre>

<p><a href="https://github.com/Shuang0420/doc2vec" target="_blank" rel="external">更多代码</a></p>
<h3 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h3><p>这里我们尝试了很多种方法作比较研究。</p>
<ul>
<li>纯 log 模型</li>
<li>纯 百科 模型</li>
<li>百科模型 + log 再训练模型</li>
<li>log 词库 + 百科模型 + log 再训练模型 (用到了 reset_weights 方法)</li>
</ul>
<ol>
<li>综合来讲，log 词库，百科数据训练模型，log 再训练的方法效果会更好些，然而增加百科数据并不会大幅提升效果。</li>
<li>对纯 log 模型而言，win=5，4的结果差不多，都要比 win=2 好很多。</li>
<li>log 模型对相近词的把握不是很好，前两个词非常准确，但是之后的词就没有多少代表性了，主要是因为词库里有大量噪音，加上百科数据训练，词的权重进行调整，会更偏向百科里的词，有人会有疑问，为什么 log 的词库百科训练会出现那么多百科的词，那是因为 log 里有新闻/百科的文本，包含了这些词，是谁这么无聊……</li>
<li>有效的语料库和干净的文本数据是模型分析的保证。有效的语料库和干净的文本数据是模型分析的保证。有效的语料库和干净的文本数据是模型分析的保证。重要的事情说三遍！</li>
</ol>
<p>eg. 与“奖励”最相近的词</p>
<pre>
# 纯 log 模型
奖    0.866039454937
奖金    0.838458776474
礼    0.698936760426
截止    0.662528753281
%    0.639326810837
周期    0.61717569828
1.8    0.609462141991
抽奖    0.581079006195
责    0.580395340919
消息    0.57931292057

# log 词库，百科训练模型
嘉奖    0.607903599739
奖赏    0.607445776463
报酬    0.59623169899
声望    0.580911517143
阴谋    0.557106971741
表扬    0.54744797945
奖品    0.543839931488
惩罚    0.540722668171
弱点    0.535359799862
俸禄    0.532780826092

# log 词库，百科训练模型，log 再训练
奖    0.86665225029
奖金    0.828586399555
补贴    0.731625974178
补助    0.640836119652
回事    0.638447761536
补偿    0.63090801239
账    0.630112946033
帐    0.605027675629
区别    0.58495759964
原因    0.584367990494
</pre>




<blockquote>
<p>参考链接<br><a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank" rel="external">https://radimrehurek.com/gensim/models/doc2vec.html</a><br><a href="https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb" target="_blank" rel="external">https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb</a><br><a href="http://blog.csdn.net/raycchou/article/details/50971599" target="_blank" rel="external">http://blog.csdn.net/raycchou/article/details/50971599</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> gensim </tag>
            
            <tag> doc2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[gensim - word2vec实战]]></title>
      <url>http://www.shuang0420.com/2016/05/30/gensim-word2vec%E5%AE%9E%E6%88%98/</url>
      <content type="html"><![CDATA[<p>介绍如何利用 gensim 库建立简单的 word2vec 模型。<br><a id="more"></a></p>
<pre>
# -*- coding: utf-8 -*-
import gensim
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import os
import logging
import jieba
import re
import multiprocessing
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

# logging information
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
logging.root.setLevel(level=logging.INFO)

# get input file, text format
inp = sys.argv[1]
input = open(inp, 'r')
output = open('output.seq', 'w')

if len(sys.argv) < 2:
    print(globals()['__doc__'] % locals())
    sys.exit(1)

# read file and separate words
for line in input.readlines():
    line=line.strip('\n')
    seg_list = jieba.cut(line)
    output.write(' '.join(seg_list) + '\n')

output.close()
output= open('output.seq', 'r')

# initialize the model
# size = the dimensionality of the feature vectors
# window = the maximum distance between the current and predicted word within a sentence
# min_count = ignore all words with total frequency lower than this.
model = Word2Vec(LineSentence(output), size=100, window=3, min_count=5,workers=multiprocessing.cpu_count())

# save model
model.save('output.model')
model.save_word2vec_format('output.vector', binary=False)

# test
model=gensim.models.Word2Vec.load('output.model')
x = model.most_similar([u'奖励'])
for i in x:
    print "Word: {}\t Similarity: {}".format(i[0], i[1])
</pre>

<p><a href="https://github.com/Shuang0420/word2vec_example" target="_blank" rel="external">更多代码</a></p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> gensim </tag>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之六 -- 若干源码细节]]></title>
      <url>http://www.shuang0420.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%85%AD-%E8%8B%A5%E5%B9%B2%E6%BA%90%E7%A0%81%E7%BB%86%E8%8A%82/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。<br><a id="more"></a></p>
<hr>


<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/61.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/62.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/63.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/64.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/65.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/66.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/67.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/68.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之五 -- 基于 Negative Sampling 的模型]]></title>
      <url>http://www.shuang0420.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%94-%E5%9F%BA%E4%BA%8E-Negative-Sampling-%E7%9A%84%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。<br><a id="more"></a></p>
<p><hr><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/51.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/52.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/53.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/54.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/55.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/56.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/57.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/58.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/59.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/510.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/511.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之四 -- 基于Hierarchical Softmax 的模型]]></title>
      <url>http://www.shuang0420.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E5%9B%9B-%E5%9F%BA%E4%BA%8EHierarchical-Softmax-%E7%9A%84%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。<br><a id="more"></a></p>
<hr>

<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/6.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/7.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/8.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/9.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/10.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/11.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/12.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/1.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/2.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/3.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/4.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/5.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之三 -- 背景知识]]></title>
      <url>http://www.shuang0420.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%89-%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。<br><a id="more"></a></p>
<hr>

<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/31.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/32.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/33.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/34.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/35.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/36.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/37.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/38.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/39.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/310.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/311.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之二 -- 预备知识]]></title>
      <url>http://www.shuang0420.com/2016/05/29/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%BA%8C-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。<br><a id="more"></a></p>
<hr>

<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/21.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/22.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/23.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/24.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/25.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/26.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/27.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[word2vec详解之一 -- 目录和前言]]></title>
      <url>http://www.shuang0420.com/2016/05/28/word2vec%E8%AF%A6%E8%A7%A3%E4%B9%8B%E4%B8%80-%E7%9B%AE%E5%BD%95%E5%92%8C%E5%89%8D%E8%A8%80/</url>
      <content type="html"><![CDATA[<p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。由于 word2vec 的作者 Tomas Mikolov 在两篇相关的论文 [3,4] 中并没有谈及太多算法细节，因而在一定程度上增加了这个工具包的神秘感。一些按捺不住的人于是选择了通过解剖源代码的方式来一窥究竟，出于好奇，我也成为了他们中的一员。读完代码后，觉得收获颇多，整理成文，给有需要的朋友参考。</p>
<a id="more"></a>
<p><hr><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/11.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/12.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/13.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/14.jpg" alt=""><br><img src="http://7xu83c.com1.z0.glb.clouddn.com/15.jpg" alt=""></p>
<p>作者: peghoty<br>出处: <a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969979</a><br>欢迎转载/分享, 但请务必声明文章出处.</p>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> word2vec </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PHP连接数据库js可视化数据]]></title>
      <url>http://www.shuang0420.com/2016/05/26/PHP%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93js%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE/</url>
      <content type="html"></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[短句归一化--LSI模型]]></title>
      <url>http://www.shuang0420.com/2016/05/25/%E7%9F%AD%E9%97%AE%E9%A2%98%E5%BD%92%E4%B8%80%E5%8C%96-LSI%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<h3 id="LSI-理解"><a href="#LSI-理解" class="headerlink" title="LSI 理解"></a>LSI 理解</h3><p>LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。</p>
<a id="more"></a>
<blockquote>
<p>潜在语义索引是一种用奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以可以从一篇文章中提取术语关系，从而建立起主要概念内容。</p>
</blockquote>
<h3 id="降维过程"><a href="#降维过程" class="headerlink" title="降维过程"></a>降维过程</h3><p>将文档库表示成VSM模型的词-文档矩阵Am×n(词-文档矩阵那就是词作为行，文档作为列，这是矩阵先行后列的表示决定的，当然如果表示成文档-词矩阵的话，后面的计算就要用该矩阵的转置了),其中m表示文档库中包含的所有不同的词的个(行数是不同词的个数)，即行向量表示一个词在不同文档出现的次数，n 表示文档库中的文档数(列数是不同文档的个数)，即列向量表示的是不同的文档.A表示为A = [α ij ],在此矩阵中 ,α ij为非负值 , 表示第 i 个词在第j 个文档中出现的频度。显然，A是稀疏矩阵(这是VSM和文档决定的)。</p>
<p>利用奇异值分解SVD(Singular Value Decomposition)求A的只有K个正交因子的降秩矩阵，该过程就是降维的过程。SVD的重要作用是把词和文档映射到同一个语义空间中，将词和文档表示为K个因子的形式。显然，这会丢失信息，但主要的信息却被保留了。为什么该过程可以降维呢？因为该过程解决了同义和多义现象。可以看出，K的取值对整个分类结果的影响很大。因为，K过小，则丢失信息就越多；K过大，信息虽然多，但可能有冗余且计算消耗大。K的选择也是值得研究的，不过一般取值为100-300，不绝对。</p>
<h3 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h3><p>对于 LSI/PLSI 来说，聚类的意义不在于文档，而在于单词。所以对于聚类的一种变型用法是，当 k 设的足够大时，LSI/PLSI 能够给出落在不同子空间的单词序列，基本上这些单词之间拥有较为紧密的语义联系。其实这种用法本质上还是在利用降维做单词相关度计算。</p>
<ol>
<li><p>特征降维<br>LSI 本质上是把每个特征映射到了一个更低维的子空间（sub space)，所以用来做降维可以说是天造地设。TFIDF是另一个通用的降维方法，通过一个简单的公式（两个整数相乘）得到不同单词的重要程度，并取前k个最重要的单词，而丢弃其它单词，只有信息的丢失，并没有信息的改变。从执行效率上 TFIDF 远远高于 LSI，不过从效果上（至少在学术界）LSI 要优于TFIDF。<br>不过必须提醒的是，无论是上述哪一种降维方法，都会造成信息的偏差，进而影响后续分类/聚类的准确率。 降维是希望以可接受的效果损失下，大大提高运行效率和节省内存空间。然而能不降维的时候还是不要降维（比如你只有几千篇文档要处理，那样真的没有必要降维）。</p>
</li>
<li><p>单词相关度计算<br>LSI 的结果通过简单变换就能得到不同单词之间的相关度( 0 ~ 1 之间的一个实数），相关度非常高的单词往往拥有相同的含义。不过不要被“潜在语义”的名称所迷惑，所谓的潜在语义只不过是统计意义上的相似，如果想得到同义词还是使用同义词词典靠谱。LSI 得到的近义词的特点是它们不一定是同义词（甚至词性都可能不同），但它们往往出现在同类情景下（比如“魔兽” 和 “dota”)。不过事实上直接使用LSI做单词相关度计算的并不多，一方面在于现在有一些灰常好用的同义词词典，另外相对无监督的学习大家还是更信任有监督的学习（分类）得到的结果。</p>
</li>
<li><p>聚类<br>直接用 LSI 聚类的情景还没有见过，但使用该系列算法的后续变种 PLSI, LDA 进行聚类的的确有一些。其中LDA聚类还有些道理（因为它本身就假设了潜在topic的联合概率分布），用 LSI 进行聚类其实并不合适。本质上 LSI 在找特征子空间，而聚类方法要找的是实例分组。 LSI 虽然能得到看起来貌似是聚类的结果，但其意义不见得是聚类所想得到的。一个明显的例子就是，对于分布不平均的样本集（比如新闻类的文章有1000篇，而文学类的文章只有10篇）， LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。</p>
</li>
</ol>
<h3 id="实验说明"><a href="#实验说明" class="headerlink" title="实验说明"></a>实验说明</h3><p>用了python的gensim包<br>现有的数据是438条标准问题以及3300条人工问题（可以转化为438条标准问题），现在需要对人工问题做一个归一化。<br>这里采用LSI模型进行建模实验，步骤如下。</p>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><pre>
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import logging
import jieba
import jieba.posseg as pseg
# 防止乱码
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
# 打印log信息
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</pre>


<h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><pre>
# 标准FAQ，一行对应一条问句
f = open('FAQuniq.txt', 'r')
# 对问句进行分词
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in f]

# 抽取一个bag-of-words，将文档的token映射为id
dictionary = corpora.Dictionary(texts)
# 保存词典
dictionary.save('LSI.dict')

# 产生文档向量，将用字符串表示的文档转换为用id和词频表示的文档向量
corpus = [dictionary.doc2bow(text) for text in texts]

# 基于这些“训练文档”计算一个TF-IDF模型
tfidf = models.TfidfModel(corpus)

# 转化文档向量，将用词频表示的文档向量表示为一个用tf-idf值表示的文档向量
corpus_tfidf = tfidf[corpus]

# 训练LSI模型 即将训练文档向量组成的矩阵SVD分解，并做一个秩为2的近似SVD分解
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)

# 保存模型
lsi.save('LSI.pkl')
lsi.print_topics(20)</pre>


<h3 id="初始化验证performance的文件"><a href="#初始化验证performance的文件" class="headerlink" title="初始化验证performance的文件"></a>初始化验证performance的文件</h3><p>checkFile的每行格式为：</p>
<pre>原始问题的docid：对应的标准问题的topicid</pre>

<p>把它存到checkDict这个dictionary中，key是docid，value是topicid。</p>
<pre>
checkDict=dict()
def getCheckId():
    fcheck=open('checkFile.txt')
    for line in fcheck:
        line=line.strip('\n')
        if (len(line)==0):
            continue
        docid=line.split(":")[0]
        topicid=line.split(":")[1]
        checkDict[int(docid)]=int(topicid)
getCheckId()</pre>

<h3 id="归一化／计算文档相似度"><a href="#归一化／计算文档相似度" class="headerlink" title="归一化／计算文档相似度"></a>归一化／计算文档相似度</h3><pre>
# 建索引
index = similarities.MatrixSimilarity(lsi[corpus])

# 初始化分数
score1=0
score2=0
score3=0

# 读取文件，文件的每行格式为一个原始问句
f2=open('ORIFAQ3330.txt','r')
# count的作用是和checkFile的docid，即checkDict的key对应
count=1
for query in f2:
    # 获取该原始问句本应对应的正确标准问句
    if (not checkDict.has_key(count)):
        count+=1
        continue
    checkId=checkDict[count]
    # 将问句向量化
    query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
    # 再用之前训练好的LSI模型将其映射到二维的topic空间：
    query_lsi = lsi[query_bow]
    # 计算其和index中doc的余弦相似度了：
    sims = index[query_lsi]
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    # 找出最相关的三篇文档，计算这三篇文档是否包括标准问句，如果文档就是标准问句，对应的分数加1
    if (checkId==sort_sims[0][0]):
        score1+=1
    elif (checkId==sort_sims[1][0]):
        score2+=1
    elif (checkId==sort_sims[2][0]):
        score3+=1
    count+=1</pre>

<h3 id="打印分数"><a href="#打印分数" class="headerlink" title="打印分数"></a>打印分数</h3><pre>
print "Score1: ".format(score1*1.0/count)
print "Score2: ".format(score2*1.0/count)
print "Score3: ".format(score3*1.0/count)</pre>

<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>其实这里的结果非常差，原因是文档（每一条问句）太短，只有十几个字，另外文档数太少，LSI降维牺牲了准确率，下一个实验LDA的准确率相比会高很多。<br>另外，本次实验所用的样本分布并不均匀，“未收到奖励”类似问题出现的频率比“软件无声音”类似问题出现的频率要高很多。<strong><em>重申：LSI/PLSI 得到的往往是相对平均的结果(A类500篇，B类600篇)，这种情况下根本无法得到好的聚类结果。相对传统聚类方法k-means， LSI 系列算法不仅存在信息的偏差（丢失和改变），而且不能处理分布不均的样本集。</em></strong></p>
<h3 id="LSI-缺陷"><a href="#LSI-缺陷" class="headerlink" title="LSI 缺陷"></a>LSI 缺陷</h3><p>常用的VSM文本表示模型中有两个主要的缺陷：</p>
<ol>
<li><p>该模型假设所有特征词条之间是相互独立、互不影响的（朴素贝叶斯也是这个思想），即该模型还是基于“词袋”模型（应该说所有利用VSM模型没有进行潜在语义分析的算法都是基于“词袋”假设）。</p>
</li>
<li><p>没有进行特征降维，特征维数可能会很高，向量空间可能很大，对存储和计算资源要求会比较高。</p>
</li>
</ol>
<p>LSI的基本思想是文本中的词与词之间不是孤立的，存在着某种潜在的语义关系，通过对样本数据的统计分析，让机器自动挖掘出这些潜在的语义关系，并把这些关系表示成计算机可以”理解”的模型。它可以消除词匹配过程中的同义和多义现象。它可以将传统的VSM降秩到一个低维的语义空间中，在该语义空间中计算文档的相似度等。总的说来，LSI就是利用词的语义关系对VSM模型进行降维，并提高分类的效果。</p>
<blockquote>
<p>参考链接<br><a href="http://www.zwbk.org/MyLemmaShow.aspx?lid=257113" target="_blank" rel="external">http://www.zwbk.org/MyLemmaShow.aspx?lid=257113</a><br><a href="http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C" target="_blank" rel="external">http://www.52nlp.cn/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%BA%8C</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LDA </tag>
            
            <tag> topic modeling </tag>
            
            <tag> cluster </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在c里调用python]]></title>
      <url>http://www.shuang0420.com/2016/05/22/%E5%9C%A8c%E9%87%8C%E8%B0%83%E7%94%A8python/</url>
      <content type="html"><![CDATA[<p>这一个例子是c调用了python的函数，函数返回值是list，包含了100个float值。</p>
<a id="more"></a>
<pre>
#include <python2.7 python.h="">
#include <stdio.h>
#include <stdlib.h>
void test1(){
  Py_Initialize();//初始化python
  char *test = "奖励";
  PyObject * pModule = NULL;
  PyObject * pModule1 = NULL;
  PyObject * pFunc = NULL;
  PyObject * pArg    = NULL;
  PyObject * result;
  pModule = PyImport_ImportModule("inferSingleDocVec");//引入模块
  pFunc = PyObject_GetAttrString(pModule, "getDocVec");//直接获取模块中的函数
  pArg= Py_BuildValue("(s)", test);
  result = PyEval_CallObject(pFunc, pArg); //调用直接获得的函数，并传递参数；这里得到的是一个list
  <code>for (int i = 0; i < PyList_Size(result); i++) {</code>
    printf("%f\t", PyFloat_AsDouble(PyList_GetItem(result, (Py_ssize_t)i)));//打印每一个元素
  }
  //下面代码适用于返回值为字符串的情况
  //char* s=NULL;
  //PyArg_Parse(result, "s", &s);
  //for (int i=0;s[i]!='\0';i++){
   // printf("%c",s[i]);
 // }
  Py_Finalize(); //释放python
//  return;
}
int main(int argc, char* argv[])
{
    test1();
    return 0;
}
</stdlib.h></stdio.h></python2.7></pre>

<p>编译运行</p>
<pre>
$ gcc -I/usr/local/lib/python2.7.11 -o inferDocVec inferDocVec.c -lpython2.7
$ ./inferDocVec
</pre>

<p>调用的inferSingleDocVec文件</p>
<pre>
#!/usr/bin/python
# -*- coding: utf-8 -*-
### for infer
import sys
reload(sys)
sys.setdefaultencoding('utf8')
import gensim, logging
from gensim.models import Doc2Vec
import os
import jieba
import multiprocessing
import numpy as np
import base64
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def getDocVec(doc_words):
    docwords=[word for word in jieba.cut(doc_words, cut_all = False)]
    model = Doc2Vec.load('all_model_v2.txt')
    invec = model.infer_vector(docwords, alpha=0.1, min_alpha=0.0001, steps=5)
    return (list)(invec)
</pre>

<p>关于如何将python文件转为模块，详见之前的一篇博文<a href="https://github.com/Shuang0420/Shuang0420.github.io/wiki/python----%E5%B0%86%E8%87%AA%E5%B7%B1%E5%86%99%E7%9A%84py%E6%96%87%E4%BB%B6%E4%BD%9C%E4%B8%BA%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5" target="_blank" rel="external">python 将自己写的py文件作为模块导入</a></p>
<blockquote>
<p>参考链接</p>
<blockquote>
<p><a href="https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail" target="_blank" rel="external">https://www.daniweb.com/programming/software-development/threads/237529/what-does-pyarg_parse-do-in-detail</a></p>
<p><a href="http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject" target="_blank" rel="external">http://stackoverflow.com/questions/5079570/writing-a-python-c-extension-how-to-correctly-load-a-pylistobject</a></p>
</blockquote>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> c </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AP聚类]]></title>
      <url>http://www.shuang0420.com/2016/05/19/AP%E8%81%9A%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>AP算法的具体工作过程如下：先计算N个点之间的相似度值，将值放在S矩阵中，再选取P值(一般取S的中值)。设置一个最大迭代次数(文中设默认值为1000)，迭代过程开始后，计算每一次的R值和A值，根据R(k,k)+A(k,k)值来判断是否为聚类中心(文中指定当(R(k,k)+A(k,k))＞0时认为是一个聚类中心)，当迭代次数超过最大值( 即maxits值)或者当聚类中心连续多少次迭代不发生改变( 即convits值)时终止计算(文中设定连续50次迭代过程不发生改变是终止计算)。<br><a id="more"></a></p>
<p>Affinity Propagation (AP) 聚类是最近在Science杂志上提出的一种新的聚类算法。它根据N个数据点之间的相似度进行聚类,这些相似度可以是对称的,即两个数据点互相之间的相似度一样(如欧氏距离);也可以是不对称的,即两个数据点互相之间的相似度不等。这些相似度组成N×N的相似度矩阵S(其中N为有N个数据点)。AP算法不需要事先指定聚类数目,相反它将所有的数据点都作为潜在的聚类中心,称之为exemplar。以S矩阵的对角线上的数值s(k, k)作为k点能否成为聚类中心的评判标准,这意味着该值越大,这个点成为聚类中心的可能性也就越大,这个值又称作参考度p (preference)。<br>在这里介绍几个文中常出现的名词：<br>exemplar：指的是聚类中心。<br>similarity：数据点i和点j的相似度记为S(i，j)。是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，如－|| ||。文中，所有点与点的相似度值全部取为负值。因为我们可以看到，相似度值越大说明点与点的距离越近，便于后面的比较计算。<br>preference：数据点i的参考度称为P(i)或S(i,i)。是指点i作为聚类中心的参考度。一般取S相似度值的中值。<br>Responsibility:R(i,k)用来描述点k适合作为数据点i的聚类中心的程度。<br>Availability:A(i,k)用来描述点i选择点k作为其聚类中心的适合程度。</p>
<p>Script output:<br>Estimated number of clusters: 3<br>Homogeneity: 0.872<br>Completeness: 0.872<br>V-measure: 0.872<br>Adjusted Rand Index: 0.912<br>Adjusted Mutual Information: 0.871<br>Silhouette Coefficient: 0.753</p>
<p>Python source code: plot_affinity_propagation.py<br>print(<strong>doc</strong>)</p>
<p>from sklearn.cluster import AffinityPropagation<br>from sklearn import metrics<br>from sklearn.datasets.samples_generator import make_blobs</p>
<p>##############################################################################</p>
<h1 id="Generate-sample-data"><a href="#Generate-sample-data" class="headerlink" title="Generate sample data"></a>Generate sample data</h1><p>centers = [[1, 1], [-1, -1], [1, -1]]<br>X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,<br>                            random_state=0)</p>
<p>##############################################################################</p>
<h1 id="Compute-Affinity-Propagation"><a href="#Compute-Affinity-Propagation" class="headerlink" title="Compute Affinity Propagation"></a>Compute Affinity Propagation</h1><p>af = AffinityPropagation(preference=-50).fit(X)<br>cluster_centers_indices = af.cluster_centers<em>indices</em><br>labels = af.labels_</p>
<p>n<em>clusters</em> = len(cluster_centers_indices)</p>
<p>print(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))<br>print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))<br>print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))<br>print(“Adjusted Rand Index: %0.3f”<br>      % metrics.adjusted_rand_score(labels_true, labels))<br>print(“Adjusted Mutual Information: %0.3f”<br>      % metrics.adjusted_mutual_info_score(labels_true, labels))<br>print(“Silhouette Coefficient: %0.3f”<br>      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))</p>
<p>##############################################################################</p>
<h1 id="Plot-result"><a href="#Plot-result" class="headerlink" title="Plot result"></a>Plot result</h1><p>import matplotlib.pyplot as plt<br>from itertools import cycle</p>
<p>plt.close(‘all’)<br>plt.figure(1)<br>plt.clf()</p>
<p>colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)<br>for k, col in zip(range(n<em>clusters</em>), colors):<br>    class_members = labels == k<br>    cluster_center = X[cluster_centers_indices[k]]<br>    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)<br>    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,<br>             markeredgecolor=’k’, markersize=14)<br>    for x in X[class_members]:<br>        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)</p>
<p>plt.title(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>plt.show()</p>
<blockquote>
<p>参考链接<br><a href="http://scikit-learn.org/stable/modules/clustering.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/clustering.html</a><br><a href="http://blog.csdn.net/u010695420/article/details/42239465" target="_blank" rel="external">http://blog.csdn.net/u010695420/article/details/42239465</a></p>
</blockquote>
<p>聚类算法Affinity Propagation(AP)</p>
<p>Affinity Propagation聚类算法简称AP，是一个在07年发表在Science上面比较新的算法。</p>
<p>AP算法的基本思想是将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>在AP算法中有一些特殊名词：</p>
<p>Exemplar：指的是聚类中心，K-Means中的质心。<br>Similarity：数据点i和点j的相似度记为s(i, j)，是指点j作为点i的聚类中心的相似度。一般使用欧氏距离来计算，一般点与点的相似度值全部取为负值；因此，相似度值越大说明点与点的距离越近，便于后面的比较计算。<br>Preference：数据点i的参考度称为p(i)或s(i,i)，是指点i作为聚类中心的参考度。一般取s相似度值的中值。<br>Responsibility：r(i,k)用来描述点k适合作为数据点i的聚类中心的程度。<br>Availability：a(i,k)用来描述点i选择点k作为其聚类中心的适合程度。<br>Damping factor(阻尼系数)：主要是起收敛作用的。<br>在实际计算应用中，最重要的两个参数（也是需要手动指定）是Preference和Damping factor。前者定了聚类数量的多少，值越大聚类数量越多；后者控制算法收敛效果。</p>
<p>AP聚类算法与经典的K-Means聚类算法相比，具有很多独特之处：</p>
<p>无需指定聚类“数量”参数。AP聚类不需要指定K（经典的K-Means）或者是其他描述聚类个数（SOM中的网络结构和规模）的参数，这使得先验经验成为应用的非必需条件，人群应用范围增加。<br>明确的质心（聚类中心点）。样本中的所有数据点都可能成为AP算法中的质心，叫做Examplar，而不是由多个数据点求平均而得到的聚类中心（如K-Means）。<br>对距离矩阵的对称性没要求。AP通过输入相似度矩阵来启动算法，因此允许数据呈非对称，数据适用范围非常大。<br>初始值不敏感。多次执行AP聚类算法，得到的结果是完全一样的，即不需要进行随机选取初值步骤（还是对比K-Means的随机初始值）。<br>算法复杂度较高，为O(N<em>N</em>logN)，而K-Means只是O(N*K)的复杂度。因此当N比较大时(N&gt;3000)，AP聚类算法往往需要算很久。<br>若以误差平方和来衡量算法间的优劣，AP聚类比其他方法的误差平方和都要低。（无论k-center clustering重复多少次，都达不到AP那么低的误差平方和）<br>AP算法相对K-Means鲁棒性强且准确度较高，但没有任何一个算法是完美的，AP聚类算法也不例外：</p>
<p>AP聚类应用中需要手动指定Preference和Damping factor，这其实是原有的聚类“数量”控制的变体。<br>算法较慢。由于AP算法复杂度较高，运行时间相对K-Means长，这会使得尤其在海量数据下运行时耗费的时间很多。<br>以下使用Python的机器学习库SKlearn应用AP（AffinityPropagation）算法进行案例演示。</p>
<p>案例中，我们会先对AP算法和K-Means聚类算法的运行时间做下对比，分别选取100,500,1000样本量下进行两种算法的聚类时间对比；然后，使用AP算法做聚类分析。</p>
<p>AP和K-Means运行时间对比</p>
<p>#coding:utf-8   </p>
<p>import numpy as np<br>import matplotlib.pyplot as plt<br>import time<br>from sklearn.cluster import KMeans,AffinityPropagation<br>from sklearn.datasets.samples_generator import make_blobs   </p>
<h1 id="生成测试数据"><a href="#生成测试数据" class="headerlink" title="生成测试数据"></a>生成测试数据</h1><p>np.random.seed(0)<br>centers = [[1, 1], [-1, -1], [1, -1]]<br>kmeans_time = []<br>ap_time = []<br>for n in [100,500,1000]:<br>    X, labels_true = make_blobs(n_samples=n, centers=centers, cluster_std=0.7)   </p>
<pre><code># 计算K-Means算法时间   
k_means = KMeans(init=&apos;k-means++&apos;, n_clusters=3, n_init=10)   
t0 = time.time()   
k_means.fit(X)   
kmeans_time.append([n,(time.time() - t0)])   

# 计算AP算法时间   
ap = AffinityPropagation()   
t0 = time.time()   
ap.fit(X)   
ap_time.append([n,(time.time() - t0)])   
</code></pre><p>print (‘K-Means time’,kmeans_time[:10])<br>print (‘AP time’,ap_time[:10])   </p>
<h1 id="图形展示"><a href="#图形展示" class="headerlink" title="图形展示"></a>图形展示</h1><p>km_mat = np.array(kmeans_time)<br>ap_mat = np.array(ap_time)<br>plt.figure()<br>plt.bar(np.arange(3), km_mat[:,1], width = 0.3, color = ‘b’, label = ‘K-Means’, log = ‘True’)<br>plt.bar(np.arange(3)+0.3, ap_mat[:,1], width = 0.3, color = ‘g’, label = ‘AffinityPropagation’, log = ‘True’)<br>plt.xlabel(‘Sample Number’)<br>plt.ylabel(‘Computing time’)<br>plt.title(‘K-Means and AffinityPropagation computing time ‘)<br>plt.legend(loc=’upper center’)<br>plt.show()   </p>
<p>运算结果如下：<br>bars11</p>
<p>(‘K-Means time’, [[100, 0.029999971389770508], [500, 0.029999971389770508], [1000, 0.0410001277923584]])<br>(‘AP time’, [[100, 0.03000020980834961], [500, 1.8999998569488525], [1000, 16.31499981880188]])  </p>
<p>图中为了更好的展示数据对比，已经对时间进行log处理，但可以从输出结果直接读取真实数据运算时间。由结果可以看到：当样本量为100时，AP的速度要大于K_Means；当数据增加到500甚至1000时，AP算法的运算时间要大大超过K-Means算法；甚至当我试图运算更大的数据量（100000）时，直接内存错误而被迫中止。</p>
<p>AP聚类示例</p>
<p>#coding:utf-8   </p>
<p>from sklearn.cluster import AffinityPropagation<br>from sklearn import metrics<br>from sklearn.datasets.samples_generator import make_blobs<br>import numpy as np   </p>
<h1 id="生成测试数据-1"><a href="#生成测试数据-1" class="headerlink" title="生成测试数据"></a>生成测试数据</h1><p>centers = [[1, 1], [-1, -1], [1, -1]]<br>X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)   </p>
<h1 id="AP模型拟合"><a href="#AP模型拟合" class="headerlink" title="AP模型拟合"></a>AP模型拟合</h1><p>af = AffinityPropagation(preference=-50).fit(X)<br>cluster_centers_indices = af.cluster_centers<em>indices</em><br>labels = af.labels_<br>new_X = np.column_stack((X, labels))   </p>
<p>n<em>clusters</em> = len(cluster_centers_indices)   </p>
<p>print(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>print(“Homogeneity: %0.3f” % metrics.homogeneity_score(labels_true, labels))<br>print(“Completeness: %0.3f” % metrics.completeness_score(labels_true, labels))<br>print(“V-measure: %0.3f” % metrics.v_measure_score(labels_true, labels))<br>print(“Adjusted Rand Index: %0.3f”<br>      % metrics.adjusted_rand_score(labels_true, labels))<br>print(“Adjusted Mutual Information: %0.3f”<br>      % metrics.adjusted_mutual_info_score(labels_true, labels))<br>print(“Silhouette Coefficient: %0.3f”<br>      % metrics.silhouette_score(X, labels, metric=’sqeuclidean’))<br>print(‘Top 10 sapmles:’,new_X[:10])   </p>
<h1 id="图形展示-1"><a href="#图形展示-1" class="headerlink" title="图形展示"></a>图形展示</h1><p>import matplotlib.pyplot as plt<br>from itertools import cycle   </p>
<p>plt.close(‘all’)<br>plt.figure(1)<br>plt.clf()   </p>
<p>colors = cycle(‘bgrcmykbgrcmykbgrcmykbgrcmyk’)<br>for k, col in zip(range(n<em>clusters</em>), colors):<br>    class_members = labels == k<br>    cluster_center = X[cluster_centers_indices[k]]<br>    plt.plot(X[class_members, 0], X[class_members, 1], col + ‘.’)<br>    plt.plot(cluster_center[0], cluster_center[1], ‘o’, markerfacecolor=col,<br>             markeredgecolor=’k’, markersize=14)<br>    for x in X[class_members]:<br>        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)   </p>
<p>plt.title(‘Estimated number of clusters: %d’ % n<em>clusters</em>)<br>plt.show()   </p>
<p>运行结果：<br>ap_clustering1111</p>
<p>Estimated number of clusters: 3<br>Homogeneity: 0.872<br>Completeness: 0.872<br>V-measure: 0.872<br>Adjusted Rand Index: 0.912<br>Adjusted Mutual Information: 0.871<br>Silhouette Coefficient: 0.753<br>(‘Top 10 sapmles:’, array([[ 1.47504421,  0.9243214 ,  0.        ],<br>       [-0.02204385, -0.80495334,  1.        ],<br>       [-1.17671587, -1.80823709,  2.        ],<br>       [ 0.77223375,  1.00873958,  0.        ],<br>       [ 1.23283122,  0.23187816,  0.        ],<br>       [-0.92174673, -0.88390948,  2.        ],<br>       [ 1.65956844, -1.44120941,  1.        ],<br>       [ 0.33389417, -1.98431234,  1.        ],<br>       [-1.27143074, -0.79197498,  2.        ],<br>       [ 1.33614738,  1.20373092,  0.        ]]))  </p>
<p>AffinityPropagation可配置的参数包括：（重点是damping和preference）</p>
<p>class sklearn.cluster.AffinityPropagation(damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity=’euclidean’, verbose=False)  </p>
<p>AP算法的应用场景：</p>
<p>图像、文本、生物信息学、人脸识别、基因发现、搜索最优航线、 码书设计以及实物图像识别等领域。</p>
<p>尾巴</p>
<p>综合来看，由于AP算法不适用均值做质心计算规则，因此对于离群点和异常值不敏感；同时其初始值不敏感的特性也能保持模型的较好鲁棒性。这两个突出特征使得它可以作为K-Means算法的一个有效补充，但在大数据量下的耗时过长，这导致它的适用范围只能是少量数据；虽然通过调整damping（收敛规则）可以在一定程度上提升运行速度（damping值调小），但由于算法本身的局限性决定了这也只是杯水车薪。<br><a href="http://www.dataivy.cn/blog/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95affinity-propagation_ap/" target="_blank" rel="external">聚类算法Affinity Propagation(AP)</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[LDA 以及 Gensim 实现]]></title>
      <url>http://www.shuang0420.com/2016/05/18/Gensim-and-LDA-Training-and-Prediction/</url>
      <content type="html"><![CDATA[<p>用 Gensim 实现 LDA，相比 JGibbLDA 的使用 Gensim 略为麻烦，然而感觉更清晰易懂，也就更灵活。<br><a id="more"></a></p>
<h1 id="LDA-介绍"><a href="#LDA-介绍" class="headerlink" title="LDA 介绍"></a>LDA 介绍</h1><p>LDA 是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。</p>
<p>需要理解的概念有：</p>
<ul>
<li>一个函数：gamma 函数</li>
<li>两个分布：beta分布、Dirichlet分布</li>
<li>一个模型：LDA（文档-主题，主题-词语）</li>
<li>一个采样：Gibbs采样</li>
</ul>
<p><strong>核心公式：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">p(w|d) = p(w|t)*p(t|d)</div></pre></td></tr></table></figure></p>
<p><strong>文档的生成过程</strong></p>
<ul>
<li>从狄利克雷分布中取样生成文档 i 的主题分布 $\theta_i$</li>
<li>从主题的多项式分布中取样生成文档i第 j 个词的主题 $z_{i,j}$</li>
<li>从狄利克雷分布中取样生成主题对应的词语分布 $\varnothing_{z_{i,j}}$</li>
<li>从词语的多项式分布 $\varnothing_{z_{i,j}}$ 中采样最终生成词语 $w_{i,j}$</li>
</ul>
<p><strong>怎么选择 topic 个数</strong></p>
<ul>
<li>最小化 topic 的相似度</li>
<li>perplexity</li>
</ul>
<h1 id="Python-gensim-实现"><a href="#Python-gensim-实现" class="headerlink" title="Python gensim 实现"></a>Python gensim 实现</h1><pre>
# install the related python packages
>>> pip install numpy
>>> pip install scipy
>>> pip install gensim
>>> pip install jieba

from gensim import corpora, models, similarities
import logging
import jieba

# configuration
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# load data from file
f = open('newfile.txt', 'r')
documents = f.readlines()

＃ tokenize
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]

# load id->word mapping (the dictionary)
dictionary = corpora.Dictionary(texts)

# word must appear >10 times, and no more than 40% documents
dictionary.filter_extremes(no_below=40, no_above=0.1)

# save dictionary
dictionary.save('dict_v1.dict')

# load corpus
corpus = [dictionary.doc2bow(text) for text in texts]

# initialize a model
tfidf = models.TfidfModel(corpus)

# use the model to transform vectors, apply a transformation to a whole corpus
corpus_tfidf = tfidf[corpus]

# extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents), using 500 iterations
lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100, iterations=500)

# save model to files
lda.save('mylda_v1.pkl')

# print topics composition, and their scores, for the first document. You will see that only few topics are represented; the others have a nil score.
for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, lda.print_topic(index, 10))

# print the most contributing words for 100 randomly selected topics
lda.print_topics(100)

# load model and dictionary
model = models.LdaModel.load('mylda_v1.pkl')
dictionary = corpora.Dictionary.load('dict_v1.dict')

# predict unseen data
query = "未收到奖励"
query_bow = dictionary.doc2bow(jieba.cut(query, cut_all = False))
for index, score in sorted(model[query_bow], key=lambda tup: -1*tup[1]):
    print "Score: {}\t Topic: {}".format(score, model.print_topic(index, 20))

# if you want to predict many lines of data in a file, do the followings
f = open('newfile.txt', 'r')
documents = f.readlines()
texts = [[word for word in jieba.cut(document, cut_all = False)] for document in documents]
corpus = [dictionary.doc2bow(text) for text in texts]

# only print the topic with the highest score
for c in corpus:
    flag = True
    for index, score in sorted(model[c], key=lambda tup: -1*tup[1]):
        if flag:
            print "Score: {}\t Topic: {}".format(score, model.print_topic(index, 20))</pre>

<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><p>If you occur encoding problems, you can try the following code</p>
<pre>
add it at the beginning of your python file
# -*- coding: utf-8 -*-

# also, do the followings
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

# the following code may lead to encoding problem when there're Chinese characters
model.show_topics(-1, 5)

# use this instead
model.print_topics(-1, 5)</pre>


<p>You can see step-by-step output by the following references.</p>
<blockquote>
<p>References:</p>
<blockquote>
<p><a href="https://radimrehurek.com/gensim/tut2.html" target="_blank" rel="external">https://radimrehurek.com/gensim/tut2.html</a> official guide (en)</p>
<p><a href="http://blog.csdn.net/questionfish/article/details/46725475" target="_blank" rel="external">http://blog.csdn.net/questionfish/article/details/46725475</a>  official guide (ch)</p>
<p><a href="https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation" target="_blank" rel="external">https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation</a></p>
</blockquote>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Gensim-用Python做主题模型]]></title>
      <url>http://www.shuang0420.com/2016/05/18/Gensim-%E7%94%A8Python%E5%81%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<h3 id="gensim-介绍"><a href="#gensim-介绍" class="headerlink" title="gensim 介绍"></a>gensim 介绍</h3><p>gemsim是一个免费python库，能够从文档中有效地自动抽取语义主题。gensim中的算法包括：LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation), RP (Random Projections), 通过在一个训练文档语料库中，检查词汇统计联合出现模式, 可以用来发掘文档语义结构，这些算法属于非监督学习，可以处理原始的，非结构化的文本（”plain text”）。<br><a id="more"></a></p>
<h3 id="gensim-特性"><a href="#gensim-特性" class="headerlink" title="gensim 特性"></a>gensim 特性</h3><ul>
<li>内存独立- 对于训练语料来说，没必要在任何时间将整个语料都驻留在RAM中</li>
<li>有效实现了许多流行的向量空间算法－包括tf-idf，分布式LSA, 分布式LDA 以及 RP；并且很容易添加新算法</li>
<li>对流行的数据格式进行了IO封装和转换</li>
<li>在其语义表达中，可以相似查询</li>
<li>gensim的创建的目的是，由于缺乏简单的（java很复杂）实现主题建模的可扩展软件框架.</li>
</ul>
<h3 id="gensim-设计原则"><a href="#gensim-设计原则" class="headerlink" title="gensim 设计原则"></a>gensim 设计原则</h3><ul>
<li>简单的接口，学习曲线低。对于原型实现很方便</li>
<li>根据输入的语料的size来说，内存各自独立；基于流的算法操作，一次访问一个文档.</li>
</ul>
<h3 id="gensim-核心概念"><a href="#gensim-核心概念" class="headerlink" title="gensim 核心概念"></a>gensim 核心概念</h3><p>gensim的整个package会涉及三个概念：corpus, vector, model.</p>
<ul>
<li>语库(corpus)<br>文档集合，用于自动推出文档结构，以及它们的主题等，也可称作训练语料。</li>
</ul>
<ul>
<li><p>向量(vector)</p>
<p>在向量空间模型(VSM)中，每个文档被表示成一个特征数组。例如，一个单一特征可以被表示成一个问答对(question-answer pair):</p>
<p>[1].在文档中单词”splonge”出现的次数？ 0个<br>[2].文档中包含了多少句子？ 2个<br>[3].文档中使用了多少字体? 5种<br>这里的问题可以表示成整型id (比如：1,2,3等), 因此，上面的文档可以表示成：(1, 0.0), (2, 2.0), (3, 5.0). 如果我们事先知道所有的问题，我们可以显式地写成这样：(0.0, 2.0, 5.0). 这个answer序列可以认为是一个多维矩阵（3维）. 对于实际目的，只有question对应的answer是一个实数.</p>
<p>对于每个文档来说，answer是类似的. 因而，对于两个向量来说（分别表示两个文档），我们希望可以下类似的结论：“如果两个向量中的实数是相似的，那么，原始的文档也可以认为是相似的”。当然，这样的结论依赖于我们如何去选取我们的question。</p>
</li>
</ul>
<ul>
<li><p>稀疏矩阵(Sparse vector)</p>
<p>通常，大多数answer的值都是0.0. 为了节省空间，我们需要从文档表示中忽略它们，只需要写：(2, 2.0), (3, 5.0) 即可(注意：这里忽略了(1, 0.0)). 由于所有的问题集事先都知道，那么在稀疏矩阵的文档表示中所有缺失的特性可以认为都是0.0.</p>
<p>gensim的特别之处在于，它没有限定任何特定的语料格式；语料可以是任何格式，当迭代时，通过稀疏矩阵来完成即可。例如，集合 ([(2, 2.0), (3, 5.0)], ([0, -1.0], [3, -1.0])) 是一个包含两个文档的语料，每个都有两个非零的 pair。</p>
</li>
</ul>
<ul>
<li><p>模型(model)</p>
<p>对于我们来说，一个模型就是一个变换(transformation)，将一种文档表示转换成另一种。初始和目标表示都是向量－－它们只在question和answer之间有区别。这个变换可以通过训练的语料进行自动学习，无需人工监督，最终的文档表示将更加紧凑和有用；相似的文档具有相似的表示。</p>
</li>
</ul>
<h3 id="演示代码"><a href="#演示代码" class="headerlink" title="演示代码"></a>演示代码</h3><p><a href="http://shuang0420.github.io/2016/05/18/Gensim-and-LDA-Training-and-Prediction/" target="_blank" rel="external">演示代码</a></p>
<blockquote>
<p>参考链接</p>
<blockquote>
<p><a href="http://d0evi1.github.io/gensim/" target="_blank" rel="external">http://d0evi1.github.io/gensim/</a></p>
</blockquote>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LDA </tag>
            
            <tag> gensim </tag>
            
            <tag> topic modeling </tag>
            
            <tag> cluster </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[JGibbLDA实战]]></title>
      <url>http://www.shuang0420.com/2016/05/16/JGibbLDA%E5%AE%9E%E6%88%98/</url>
      <content type="html"><![CDATA[<p>尝试了下JGibbLDA，发现按官方教程用以下命令直接运行jar包会出现错误。</p>
<a id="more"></a>
<h3 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h3><p>命令：</p>
<pre>java -mx512M -cp bin:lib/args4j-2.0.6.jar jgibblda.LDA -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models/casestudy/newdocs.dat</pre>

<p>错误信息：</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/err.png" alt=""></p>
<h3 id="手动配置"><a href="#手动配置" class="headerlink" title="手动配置"></a>手动配置</h3><p>于是尝试导入eclipse运行手动配置，成功，过程如下。</p>
<ol>
<li><p>下载JGibbLDA的jar包并解压；<br>网址：<a href="http://jgibblda.sourceforge.net/#Griffiths04" target="_blank" rel="external">http://jgibblda.sourceforge.net/#Griffiths04</a></p>
</li>
<li><p>导入eclipse，确保jar包在目录中</p>
</li>
<li><p>找到LDACmdOption.java文件， 修改部分代码</p>
<pre> @Option(name="-dir", usage="Specify directory")
 public String dir = "models/casestudy-en";

 @Option(name="-dfile", usage="Specify data file")
 public String dfile = "models/casestudy-en/newdocs.dat";</pre>

<p>值得注意的是，dfile的格式必须是👇这个样子：</p>
<pre>[M]
[document1]
[document2]
...
[documentM]</pre>

<p>第一行[M]是documents的总数，之后的每一行是一个document，每个document是一个word list，或者说是bag of words。</p>
<pre>[document i] = [word i1] [word i2] ... [word iNi]</pre>

<p>各参数含义：<br><strong>-est </strong>从训练语料中评估出LDA模型<br><strong>-alpha</strong> LDA模型中的alpha数值，默认为50/K(K是主题数目)<br><strong>-beta</strong> LDA模型中的beta数值，默认是0.1<br><strong>-ntopics</strong> 主题数目，默认值是100<br><strong>-niters</strong> GIbbs采样的迭代数目，默认值为2000<br><strong>-savestep</strong> 指定开始保存LDA模型的迭代次数<br><strong>-dir</strong> 训练语料目录<br><strong>-dfile</strong> 训练语料文件名称</p>
</li>
<li><p>修改项目的Run Configurations，在Java Application中选择LDA，点击(x)=Arguments，输入</p>
<pre>-est -alpha 0.2 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 100 -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile "newdocs.dat"</pre>

<p> 若利用已训练的LDA模型预测，输入以下参数：</p>
<pre>-inf -dir  Users\x\MyEclipse1\JGibbLDA-v.1.0\models\casestudy-en -dfile "test.txt"</pre>

<p> 注意，进行预测时，当前目录下必须包含已有的LDA训练输出文件，包括model-final.others、model-final.phi、model-final.tassign、model-final.theta、model-final.twords、wordmap.txt文件，如果运行报错，尝试修改LDACmdOption.java的modelName，确保和文件名的modelname部分一致。<br><pre>@Option(name=”-model”, usage=”Specify the model name”)<br> public String modelName = “model-final”;</pre>   </p>
<p> 如果出现java heap limited的问题，在VM arguments下添加</p>
<pre>-Xms1g -Xmx1g -Xmn512m</pre>
</li>
<li><p>Run<br>输出文件主要有：<br><strong><model_name>.others</model_name></strong>  文件存储LDA模型参数，如alpha、beta等。<br><strong><model_name>.phi </model_name></strong> 每个topic内对doc的分布情况。文件存储词语-主题分布，每一行是一个主题，列内容为词语。<br><strong><model_name>.theta </model_name></strong> 每个doc内对应上面的n个topic的分布情况。文件主题文档分布，每一行是一个文档，列内容是主题概率。<br><strong><model_name>.tassign</model_name></strong>  文件是训练预料中单词的主题指定（归属），每一行是一个语料文档。<br><strong><model_name>.twords</model_name></strong>  n个topic，以及每个topic下面包含的具体的字词<br><strong>wordmap.txt</strong>  词-id映射<br>其中<model_name>根据采样迭代次数来指定，如model-00800，最后一次采样名称命名为model-final。</model_name></p>
</li>
</ol>
<blockquote>
<p>参考链接：<br><a href="http://www.ithao123.cn/content-4208214.html" target="_blank" rel="external">http://www.ithao123.cn/content-4208214.html</a><br><a href="http://jgibblda.sourceforge.net/" target="_blank" rel="external">http://jgibblda.sourceforge.net/</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
            <category> Meaning Representation </category>
            
        </categories>
        
        
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LDA </tag>
            
            <tag> topic modeling </tag>
            
            <tag> cluster </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[让百度、google收录博客]]></title>
      <url>http://www.shuang0420.com/2016/05/13/%E8%AE%A9%E7%99%BE%E5%BA%A6%E3%80%81google%E6%94%B6%E5%BD%95%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>github 封了百度爬虫，因此百度索引不到我们在 github 上搭的博客（想知道自己的博客是否被索引可以这样查询，在搜索引擎中输入：site: 博客域名）解决github屏蔽百度爬虫的思路就是“迁出”我们的博客，让百度爬虫不直接访问github就行了。<br><a id="more"></a><br>建议看<a href="http://michael-j.net/2016/06/23/%E8%AE%A9%E7%99%BE%E5%BA%A6%E7%B4%A2%E5%BC%95%E4%BD%A0%E7%9A%84github%E7%9A%84%E5%8D%9A%E5%AE%A2/" target="_blank" rel="external">这篇文章</a>先进行尝试，若不行，尝试本文所用方法。</p>
<h1 id="百度收录"><a href="#百度收录" class="headerlink" title="百度收录"></a>百度收录</h1><p>让百度收录的第一问题是解决封禁，方案是迁出博客，让百度爬虫不直接访问 github.</p>
<h2 id="注册域名"><a href="#注册域名" class="headerlink" title="注册域名"></a>注册域名</h2><p>在<a href="https://sg.godaddy.com/zh/" target="_blank" rel="external">godaddy</a>注册并申请一个域名。55元／年。</p>
<h3 id="域名备案？no"><a href="#域名备案？no" class="headerlink" title="域名备案？no!"></a>域名备案？no!</h3><p>以下情况需要备案</p>
<ul>
<li>在国内申请的域名，如万网等服务商</li>
<li>凡是在中国大陆境内购买服务器的用户需备案，即你的网站空间在国内。</li>
</ul>
<p>但是！如果申请自国外服务商，如 Godaddy ，网站内容托管在国外服务商，如github或者买的国外的虚拟主机，那么你不需要备案！！这也就意味着，如果你想搭建个博客站点，还不想捣鼓这麻烦的备案流程，那么你唯一的做法就是，在国外服务商申请域名+购买国外的空间（或者使用国外免费的空间如github托管静态站点）</p>
<h2 id="将博客托管至Coding平台"><a href="#将博客托管至Coding平台" class="headerlink" title="将博客托管至Coding平台"></a>将博客托管至Coding平台</h2><h3 id="注册Coding账户并建立项目"><a href="#注册Coding账户并建立项目" class="headerlink" title="注册Coding账户并建立项目"></a>注册Coding账户并建立项目</h3><p>去<a href="https://coding.net/user" target="_blank" rel="external">Coding 官网</a>注册，并新建一个和账户名相同的账户。</p>
<h3 id="设置ssh"><a href="#设置ssh" class="headerlink" title="设置ssh"></a>设置ssh</h3><p>在Coding的个人主页的账户中，进入SSH公钥，添加你的公钥。（在本机 .ssh 目录下找到 id_rsa.pub，复制里面的内容在SSH-RSA公钥内容中即可。）</p>
<p>输入</p>
<pre>ssh -T git@git.coding.net</pre>

<p>进行测试，如果显示如下则SSH配置成功：</p>
<pre>Hello ...! You've conected to Coding.net by SSH successfully!</pre>

<h3 id="修改网站配置文件"><a href="#修改网站配置文件" class="headerlink" title="修改网站配置文件"></a>修改网站配置文件</h3><p>在你的 blog 根目录下的配置文件_config.yml，找到deploy的设置处，改为如下：</p>
<pre>
deploy:
  type: git
  repo:
    github: git@github.com:XXXXXXXXXXXXXXXXXXXXXXXXXXXX
    coding: git@git.coding.net:XXXXXXXXXXXXXXXXXXXXXXXXX,master</pre>
注意要改成你的项目地址。

### 将网站文件部署至Coding
在你的 github page 根目录下运行
<pre>hexo g -d</pre>

<p>成功之后，进入你的Coding对应的项目中应该能看到网站文件。</p>
<h3 id="配置Coding的Page服务"><a href="#配置Coding的Page服务" class="headerlink" title="配置Coding的Page服务"></a>配置Coding的Page服务</h3><p>进入你在Coding上的项目，点击左侧的代码可以看到Coding Pages服务。输入分支为master，点击开启服务。在自定义域名处填上你的网站域名。<br>如图<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-23%20%E4%B8%8B%E5%8D%888.51.31.png" alt=""><br>&lt;–&gt;</p>
<h2 id="配置DNS"><a href="#配置DNS" class="headerlink" title="配置DNS"></a>配置DNS</h2><p>在<a href="https://support.dnspod.cn/Kb/showarticle/?qtype=%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B&amp;tsid=42" target="_blank" rel="external">Godaddy注册商域名修改DNS地址</a>。登录<a href="https://sg.godaddy.com/zh/" target="_blank" rel="external">godaddy</a> 按 dnspod 要求更改 nameserver。</p>
<p>在<a href="https://www.dnspod.cn" target="_blank" rel="external">dnspod</a>进行网站的 dns 设置。将国内线路设置为CNAME的 pages.coding.me。之后就可以打开自己的域名啦～</p>
<p>github 也可以绑定域名。在本地网站根目录下的source文件夹下，新建一个文件名为CNAME的文件（无后缀名），填写你所绑定的域名地址，如 www.shuang0420.com，然后 generate &amp; deploy，就成功绑定域名啦。</p>
<p>如图<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-06-23%20%E4%B8%8B%E5%8D%889.46.38.png" alt=""></p>
<h2 id="让百度收录"><a href="#让百度收录" class="headerlink" title="让百度收录"></a>让百度收录</h2><p>用 <a href="http://zhanzhang.baidu.com" target="_blank" rel="external">百度站长工具</a> 的抓取诊断功能看是能进行抓取。<br>百度输入 site:www.shuang0420.com 检验。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-07-08%20%E4%B8%8A%E5%8D%889.53.06.png" alt=""></p>
<h1 id="google-收录"><a href="#google-收录" class="headerlink" title="google 收录"></a>google 收录</h1><p>google 收录就比较简单。</p>
<h2 id="添加站点并验证"><a href="#添加站点并验证" class="headerlink" title="添加站点并验证"></a>添加站点并验证</h2><p>用自己的google帐号登陆 <a href="https://www.google.com/webmasters/verification/home?hl=en" target="_blank" rel="external">Webmaster Central</a>，添加站点并验证。</p>
<p>将验证文件放到 source 文件下，在站点配置文件中加入</p>
<pre>skip_render: googled6054e120f1a1419.html</pre>


<h2 id="产生-sitemap"><a href="#产生-sitemap" class="headerlink" title="产生 sitemap"></a>产生 sitemap</h2><p>使用插件 hexo-generator-sitemap 能生成站点地图, 方法如下</p>
<pre>$ npm install hexo-generator-sitemap --save</pre>

<p>然后在 Hexo 根目录下的 config.yml 里配置一下</p>
<pre>
sitemap:
    path: sitemap.xml</pre>

<p>path 表示 Sitemap 的路径. 默认为 sitemap.xml.</p>
<p>然后重新生成</p>
<pre>hexo g</pre>

<h2 id="添加-sitemap"><a href="#添加-sitemap" class="headerlink" title="添加 sitemap"></a>添加 sitemap</h2><p>用 <a href="https://www.google.com/webmasters/tools" target="_blank" rel="external">google 站长工具</a>，在 <strong>抓取——站点地图</strong> 中就能看到 <strong>添加/测试站点地图</strong>，添加 sitemap.xml 即可。</p>
<blockquote>
<p>参考链接<br><a href="http://www.jeyzhang.com/blog-on-gitcafe-with-dns-settings.html" target="_blank" rel="external">如何将博客托管至Coding及相应的DNS设置</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[gollum/-github上搭建个人wiki]]></title>
      <url>http://www.shuang0420.com/2016/05/13/gollum:-github%E4%B8%8A%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAwiki/</url>
      <content type="html"><![CDATA[<p>博客凸显创作，维基则是整理的好工具，很多入门级别、复用别人的操作，如配置环境等，更适合发布在个人维基上，本文就以gollum+github搭建个人wiki做个示范。</p>
<a id="more"></a>
<h3 id="开通Wiki"><a href="#开通Wiki" class="headerlink" title="开通Wiki"></a>开通Wiki</h3><p>登陆Github，找到你所开通的Github项目的Settings栏目，开通Wikis，如果只希望别人可读不可写，勾选：Restrict edits to Collaborators only。如下图所示：</p>
<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%884.56.12.png" alt=""></p>
<h3 id="git-clone-相应维基的git地址"><a href="#git-clone-相应维基的git地址" class="headerlink" title="git clone 相应维基的git地址"></a>git clone 相应维基的git地址</h3><pre> git clone git@github.com:Shuang0420/Shuang0420.github.io.wiki.git wiki</pre>

<p>如果你之前没有设置git密钥，可以参照以下步骤先做配置，如果已经设置，请忽略。</p>
<ol>
<li><p>查看是否已经有了ssh密钥：<br><pre>cd ~/.ssh</pre><br>如果没有密钥则不会有此文件夹，有则备份删除</p>
</li>
<li><p>生成密钥，得到两个文件：id_rsa 和 id_rsa.pub</p>
<pre>ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”</pre>
</li>
<li><p>添加密钥到ssh：ssh-add id_rsa</p>
</li>
<li><p>在github上settings中添加ssh密钥，即“id_rsa.pub”里的公钥。</p>
</li>
<li><p>测试：ssh git@github.com</p>
</li>
</ol>
<h3 id="配置个人wiki"><a href="#配置个人wiki" class="headerlink" title="配置个人wiki"></a>配置个人wiki</h3><ol>
<li><p>在wiki目录下，安装bundler</p>
<pre>gem install bundler</pre>

<p>   如果安装没有问题，可以跳过以下错误解决。</p>
   <pre>
   ERROR:  Could not find a valid gem 'bundler' (>= 0), here is why:
             Unable to download data from https://rubygems.org/ - Errno::EPIPE: Broken pipe - SSL_connect (https://rubygems.org/latest_specs.4.8.gz)</pre>

<p>   解决：</p>
   <pre>gem source -a http://rubygems.org/
   gem install bundler</pre>

<p>   然而还是有错误：</p>
   <pre>Fetching: bundler-1.12.5.gem (100%)^[[A
   ERROR:  While executing gem ... (Gem::FilePermissionError)
       You don't have write permissions for the /Library/Ruby/Gems/2.0.0 directory.</pre>

<p>   因为没有sudo：</p>
   <pre>sudo gem install bundler</pre>
</li>
<li><p>新建Gemfile文件，内容如下：</p>
   <pre>
   source "http://rubygems.org"
   gem 'redcarpet'
   gem "grit", '~> 2.5.0', git: 'https://github.com/gitlabhq/grit.git', ref: '42297cdcee16284d2e4eff23d41377f52fc28b9d'
   gem 'gollum', git: 'https://github.com/gollum/gollum.git'</pre></li>
<li><p>运行：</p>
   <pre>
   # 安装项目依赖的所有gem包;此命令会尝试更新系统中已存在的gem包
   bundle install</pre>

<p>   时间有点久，耐心等待。<br>   然而最后出现error,</p>
   <pre>An error occurred while installing charlock_holmes (0.7.3), and Bundler cannot continue.
   Make sure that `gem install charlock_holmes -v '0.7.3'` succeeds before bundling.</pre>

<p>   好。那就按要求安装。</p>
   <pre>sudo gem install charlock_holmes -v '0.7.3'</pre>

<p>   <img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%885.25.54.png" alt=""></p>
<p>   好。继续按要求安装。</p>
   <pre>brew install icu4c</pre>

<p>   再重来</p>
   <pre>sudo gem install charlock_holmes -v '0.7.3'</pre>

   <pre>bundle install</pre>

<p>   <img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-27%20%E4%B8%8A%E5%8D%889.46.38.png" alt=""></p>
<p>   因为没有安装bundle</p>
   <pre>gem install bundle</pre>

<p>   安装后再次尝试运行</p>
   <pre>bundle install</pre>

<p>   error</p>
   <pre>Could not reach host index.rubygems.org. Check your network connection and try again.</pre>
   出现这种错误可以尝试把Gemfile里的https改成http（互相转化进行尝试）

   <pre>bundle install</pre>

<p>   终于成功！</p>
</li>
<li><p>已安装成功gollum等。然后运行：</p>
   <pre>gollum</pre>

<p>   走到这一步了本人还是遇到了万恶的失败。<br>   看着已经安装好了</p>
   <pre>
   Installing nokogiri 1.6.7.2 with native extensions
   Installing rack-protection 1.5.3
   Installing gollum-grit_adapter 1.0.1
   Installing sanitize 2.1.0
   Installing sinatra 1.4.7
   Installing gollum-lib 4.2.0
   Using gollum 4.0.1 from https://github.com/gollum/gollum.git (at master@5a5e56a)
   Bundle complete! 3 Gemfile dependencies, 24 gems now installed.
   Use `bundle show [gemname]` to see where a bundled gem is installed.
   </pre>

<p>   然而实际并没有</p>
   <pre>
   $ gollum
   -bash: gollum: command not found</pre>

<p>   大写的忧伤。最后通过直接安装gollum解决。</p>
   <pre>sudo gem install gollum</pre>

   <pre>gollum</pre>
</li>
<li><p>终于可以在本地启动成功维基。打开网址：<a href="http://0.0.0.0:4567/，可以直接在浏览器中编辑。" target="_blank" rel="external">http://0.0.0.0:4567/，可以直接在浏览器中编辑。</a></p>
<p>   <img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%886.26.05.png" alt=""></p>
<p>   如果发现不能如下错误，请尝试更新ruby。</p>
<p>   <img src="http://7xu83c.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-05-26%20%E4%B8%8B%E5%8D%889.31.07.png" alt=""></p>
<p>   更新ruby步骤<br>   安装 RVM<br>   RVM:Ruby Version Manager,Ruby版本管理器，包括Ruby的版本管理和Gem库管理(gemset)</p>
   <pre>curl -L get.rvm.io | bash -s stable</pre>

   <pre>source ~/.bashrc  
   source ~/.bash_profile </pre>

<p>   测试是否安装正常</p>
   <pre>rvm -v  </pre>

<p>   用RVM升级Ruby</p>
   <pre>
   #查看当前ruby版本  
   ruby -v  
   #列出已知的ruby版本  
   rvm list known  
   #安装ruby 2.3.0
   rvm install 2.3.0 </pre>

<p>   安装完成之后ruby -v查看是否安装成功。</p>
<p>   重新安装完毕后回到第3步。</p>
</li>
</ol>
<h3 id="github同步"><a href="#github同步" class="headerlink" title="github同步"></a>github同步</h3><p>在wiki目录下面，进行git库操作，提交本地对维基内容的修改。一切将自动保存在你的Github上的个人博客网站的wiki目录下面。</p>
<pre>
cd ~/wiki
git add .
git commit -am"first commit"
git push</pre>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> wiki </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hexo 主题配置]]></title>
      <url>http://www.shuang0420.com/2016/05/12/Github-Pages-Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>终于搭建完自己的博客站点啦，好有成就感✌️分享一些本站使用的 NexT 主题配置技巧。<br><a id="more"></a></p>
<h3 id="添加“标签”页面"><a href="#添加“标签”页面" class="headerlink" title="添加“标签”页面"></a>添加“标签”页面</h3><p>在终端窗口下，定位到 Hexo 站点目录下，新建一个页面，命名为 tags ：</p>
<pre>
$ cd your-hexo-site
$ hexo new page tags
</pre>

<p>注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如：</p>
<pre>
title: 标签
date: 2014-12-22 12:39:04
type: "tags"
comments: false
</pre>

<p>在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下:</p>
<pre>
menu:
  home: /
  archives: /archives
  tags: /tags
</pre>

<h3 id="添加“分类”页面"><a href="#添加“分类”页面" class="headerlink" title="添加“分类”页面"></a>添加“分类”页面</h3><p>在终端窗口下，定位到 Hexo 站点目录下，新建一个页面，命名为 categories ：</p>
<pre>
$ cd your-hexo-site
$ hexo new page categories
</pre>

<p>注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如：</p>
<pre>
title: 分类
date: 2014-12-22 12:39:04
type: "categories"
comments: false
</pre>

<p>在菜单中添加链接。编辑 主题配置文件 ， 添加 categories 到 menu 中，如下:</p>
<pre>
menu:
  home: /
  archives: /archives
  categories : /categories
</pre>


<h3 id="添加其它菜单页面"><a href="#添加其它菜单页面" class="headerlink" title="添加其它菜单页面"></a>添加其它菜单页面</h3><p>与添加分类、标签页面步骤相同，但还需要加一步，在主题文件夹下的 languages 文件夹中找到你使用的语言文件，打开后，在 menu 下新增刚刚添加的页面。否则在主页中显示的新页面是 Menu.xxx 形式而不是 xxx。</p>
<h3 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h3><p>感觉 DISUQS 比 多说 的设置简单一些。在 <a href="https://disqus.com/" target="_blank" rel="external">https://disqus.com/</a> 按要求注册，完成后在 admin/settings/general/ 下找到 Shortname。<br>编辑 站点配置文件， 添加 disqus_shortname 字段，设置如下：</p>
<pre>
disqus_shortname: your-disqus-shortname
</pre>


<h3 id="设置-RSS"><a href="#设置-RSS" class="headerlink" title="设置 RSS"></a>设置 RSS</h3><p>安装 hexo-generator-feed，在站点的根目录下执行以下命令：</p>
<pre>$ npm install hexo-generator-feed --save</pre>

<p>更改 主题配置文件，设定 rss 字段的值，留空表示使用 Hexo 生成的 Feed 链接。</p>
<h3 id="访问量统计"><a href="#访问量统计" class="headerlink" title="访问量统计"></a>访问量统计</h3><p>编辑 主题配置文件 中的 busuanzi_count 的配置项。<br>当enable: true时，代表开启全局开关。若site_uv、site_pv、page_pv的值均为false时，不蒜子仅作记录而不会在页面上显示。</p>
<h3 id="搜索服务"><a href="#搜索服务" class="headerlink" title="搜索服务"></a>搜索服务</h3><p>常用的是 local search 和 swiftype，local search，配置简单，但是个人试验可以搜内容，但无法跳转页面，swiftype 没有这种问题。<br><strong>Local search</strong><br>安装 hexo-generator-search，在站点的根目录下执行以下命令：</p>
<pre>$ npm install hexo-generator-search --save</pre>

<p>编辑 站点配置文件，新增以下内容到任意位置：</p>
<pre>
search:
  path: search.xml
  field: post
</pre>

<p><strong>Swiftype</strong></p>
<ol>
<li>前往 <a href="https://swiftype.com/users/sign_up" target="_blank" rel="external">Swiftype 注册页面</a>，注册一个新账户。</li>
<li>注册完成后，创建一个新的搜索引擎(create a search engine)，并按照提示完成创建步骤。</li>
<li>搜索引擎创建完成后，在菜单中选择 Integrate -&gt; Install Search 开启搜索定制，按照步骤完成定制。最后一步记得点击 Active 按钮。</li>
<li><p>返回定制引擎的第二个步骤 INSTALL CODE，复制出你的 swiftype_key,也就是下面的”xxxxxxxxx”部分。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">  (function(w,d,t,u,n,s,e)&#123;w[&apos;SwiftypeObject&apos;]=n;w[n]=w[n]||function()&#123;</div><div class="line">  (w[n].q=w[n].q||[]).push(arguments);&#125;;s=d.createElement(t);</div><div class="line">  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);</div><div class="line">  &#125;)(window,document,&apos;script&apos;,&apos;//s.swiftypecdn.com/install/v2/st.js&apos;,&apos;_st&apos;);</div><div class="line"></div><div class="line">  _st(&apos;install&apos;,&apos;xxxxxxxxx&apos;,&apos;2.0.0&apos;);</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>编辑 站点配置文件， 新增字段 swiftype_key，值设置成第四步中赋值出来的 key</p>
<pre>
# Swiftype Search Key
swiftype_key: xxxxxxxxx
</pre>

</li>
</ol>
<h3 id="开启打赏功能"><a href="#开启打赏功能" class="headerlink" title="开启打赏功能"></a>开启打赏功能</h3><p>只需要在 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。</p>
<pre>
reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！
wechatpay: /path/to/wechat-reward-image
alipay: /path/to/alipay-reward-image
</pre>

<h3 id="设置阅读全文"><a href="#设置阅读全文" class="headerlink" title="设置阅读全文"></a>设置阅读全文</h3><p>在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法：</p>
<p>在文章中使用 <!-- more --> 手动进行截断，这是 Hexo 提供的方式，推荐使用。<br>在文章的 front-matter 中添加 description，并提供文章摘录<br>自动形成摘要，在 主题配置文件 中添加：</p>
<pre>
auto_excerpt:
  enable: true
  length: 150</pre>

<p>默认截取的长度为 150 字符，可以根据需要自行设定</p>
<h3 id="MathJax"><a href="#MathJax" class="headerlink" title="MathJax"></a>MathJax</h3><p>只讲一种简单的方法 － 插件。<br>安装</p>
<pre>$ npm install hexo-math --save</pre>

<p>在 Hexo 文件夹中执行：</p>
<pre>$ hexo math install</pre>

<p>在 config.yml 文件中添加：</p>
<pre>plugins: hexo-math</pre>

<p>对于不含特殊符号的公式，可以直接使用 MathJax 的 inline math 表达式. 如果含有特殊符号，则需要人肉 escape，如 \ 之类的特殊符号在 LaTex 表达式中出现频率很高，这样就很麻烦，使用 tag 能够省不少事。</p>
<p>具体用法见 <a href="http://catx.me/2014/03/09/hexo-mathjax-plugin/" target="_blank" rel="external">Hexo MathJax插件</a>.<br><a href="https://www.zybuluo.com/yangfch3/note/267947#13" target="_blank" rel="external">MathJax用法总结</a></p>
<h3 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h3><ol>
<li><p>更改站点配置文件 config.yml</p>
<pre>post_asset_folder: true</pre>
</li>
<li><p>在hexo 目录中执行</p>
<pre>npm install https://github.com/CodeFalling/hexo-asset-image --save
</pre>
</li>
<li><p>新建博客，在 post 中会生成一个和博客名相同的文件夹和一个 .md 文件</p>
<pre>hexo new "newblog"</pre>
</li>
<li><p>把图片放入文件夹，在 .md 文件中使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% imgurl Github-Pages-Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/newblog/pict.jpg ful-image alt:newblog/pict.jpg %&#125;</div></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p>整理自 <a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="external">NexT 使用文档</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Github Pages+Hexo搭建个人博客]]></title>
      <url>http://www.shuang0420.com/2016/05/12/Github-Pages-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>一直想有个美美的博客，CSDN实在无法满足。看了一些大神的 github 博客，非常艳羡，自己学着搭了个，看在它这么好看的份上，我也会坚持写写写！<br><a id="more"></a></p>
<h1 id="为什么用Github-Pages-Hexo"><a href="#为什么用Github-Pages-Hexo" class="headerlink" title="为什么用Github Pages + Hexo"></a>为什么用Github Pages + Hexo</h1><h2 id="Github-Page优点"><a href="#Github-Page优点" class="headerlink" title="Github Page优点"></a>Github Page优点</h2><ul>
<li>轻量级的博客系统，没有麻烦的配置</li>
<li>使用标记语言，比如<a href="http://markdown.tw/" target="_blank" rel="external">Markdown</a></li>
<li>无需自己搭建服务器</li>
<li>根据Github的限制，对应的每个站有300MB空间</li>
<li>可以绑定自己的域名</li>
</ul>
<p>Github Page有两种page模式，User/Organization Pages（个人或公司站点）和Project Pages（项目站点）。这里我们用的是user/Organization Pages，要求使用自己的用户名，每个用户名下面只能建立一个，资源命名必须符合这样的规则username/username.github.com，主干上内容被用来构建和发布页面</p>
<h2 id="Hexo优点"><a href="#Hexo优点" class="headerlink" title="Hexo优点"></a>Hexo优点</h2><ul>
<li>用于搭建博客网站框架，可以简单实现优美的博客网站;</li>
<li>在本地端搭建，就可脱机查阅;</li>
<li>架构不依托于其他门户网站，不再担心门户网站倒闭，不担心博文丢失或难以导出;</li>
<li>博文为markdown格式，通用，容易上手，便于快速书写;</li>
<li>可部署在github上；</li>
<li>创造者来自中国台湾，所以几乎所有模板都关注到了中文的兼容性，很适合使用汉语的码农。</li>
</ul>
<h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><h2 id="新建github-repository"><a href="#新建github-repository" class="headerlink" title="新建github repository"></a>新建github repository</h2><p>在github上新建repository，name为username.github.io。</p>
<h2 id="Hexo安装"><a href="#Hexo安装" class="headerlink" title="Hexo安装"></a>Hexo安装</h2><p>先安装git和node.js</p>
<pre>brew install git
brew install node</pre>
验证是否安装成功
<pre>node -v
npm -v</pre>
安装Hexo
<pre>npm install -g hexo #-g表示全局安装, npm默认为当前项目安装</pre>

<h2 id="Hexo部署"><a href="#Hexo部署" class="headerlink" title="Hexo部署"></a>Hexo部署</h2><p>新建文件夹并打开，在文件夹内操作。</p>
<pre> hexo init #新建博客目录
 hexo g #根据当前目录下文件生成静态网页
 hexo s #启动服务器</pre>

<p>现在就可以到浏览器输入localhost:4000查看啦。</p>
<p>简单介绍一下文件目录</p>
<ul>
<li>public：执行hexo generate命令，输出的静态网页内容目录</li>
<li>scaffolds：layout模板文件目录，其中的md文件可以添加编辑</li>
<li>scripts：扩展脚本目录，这里可以自定义一些javascript脚本</li>
<li>source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。</li>
<li>drafts：草稿文章</li>
<li>posts：发布文章themes：主题文件目录</li>
<li>config.yml：全局配置文件，大多数的设置都在这里</li>
<li>package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的 关于 按钮</li>
</ul>
<h2 id="Hexo复制主题"><a href="#Hexo复制主题" class="headerlink" title="Hexo复制主题"></a>Hexo复制主题</h2><pre> hexo clean
 hexo g
 hexo s
 git clone https://github.com/cnfeat/cnfeat.git themes/jacman</pre>

<h2 id="启用主题"><a href="#启用主题" class="headerlink" title="启用主题"></a>启用主题</h2><p>修改Hexo目录下的config.yml配置文件中的theme属性，将其设置为jacman。</p>
<pre>theme: jacman #或你的主题名，注意冒号后有一个空格</pre>

<p>注意：Hexo有两个config.yml文件，一个在根目录，一个在theme下，此时修改的是在根目录下的。</p>
<h2 id="更新主题"><a href="#更新主题" class="headerlink" title="更新主题"></a>更新主题</h2><pre> cd themes/jacman
 git pull</pre>

<p>注意：为避免出错，请先备份你的_config.yml 文件后再升级</p>
<h2 id="Hexo本地调试"><a href="#Hexo本地调试" class="headerlink" title="Hexo本地调试"></a>Hexo本地调试</h2><pre> hexo g #生成
 hexo s #启动本地服务，进行文章预览调试
 hexo d -g #或者直接作用组合命令</pre>

<p>浏览器输入localhost:4000，即可查看搭建效果。每次变更config.yml 文件或者上传文件都可以先用此命令调试。</p>
<h2 id="Hexo部署到github"><a href="#Hexo部署到github" class="headerlink" title="Hexo部署到github"></a>Hexo部署到github</h2><pre>npm install hexo-deployer-git --save</pre>
在 Hexo 文件夹下找到 config.yml 文件, 找到其中的 deploy 标签，改成下图所示形式，并保存。注意：冒号后面要加上一个空格，否则会报错
<pre>
deploy:
  type: git
  repo: https://github.com/Shuang0420/Shuang0420.github.io.git</pre>

<p>运行如下命令：</p>
<pre>hexo clean
hexo generate
hexo deploy</pre>


<h2 id="发博文"><a href="#发博文" class="headerlink" title="发博文"></a>发博文</h2><pre>
 hexo new "postname" #然后在posts目录下的postname.md文件中编辑博客
 hexo clean
 hexo generate
 # (若要本地预览就先执行 hexo server)
 hexo deploy</pre>

<h2 id="快捷命令"><a href="#快捷命令" class="headerlink" title="快捷命令"></a>快捷命令</h2><pre> hexo g == hexo generate
 hexo d == hexo deploy
 hexo s == hexo server
 hexo n == hexo new
# 还能组合使用，如：
hexo d -g</pre>

<blockquote>
<p>参考链接：</p>
<blockquote>
<p><a href="http://mozhenhau.com/2015/03/05/%E5%9C%A8Mac%E9%80%9A%E8%BF%87Hexo%E5%9C%A8github%E4%B8%8A%E5%BB%BA%E7%AB%8B%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/" target="_blank" rel="external">http://mozhenhau.com/2015/03/05/%E5%9C%A8Mac%E9%80%9A%E8%BF%87Hexo%E5%9C%A8github%E4%B8%8A%E5%BB%BA%E7%AB%8B%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/</a></p>
<p><a href="http://evakasch.github.io/2016/05/04/hexo-setup/" target="_blank" rel="external">http://evakasch.github.io/2016/05/04/hexo-setup/</a></p>
</blockquote>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[让进程在后台可靠运行的几种方法]]></title>
      <url>http://www.shuang0420.com/2016/04/22/%E8%AE%A9%E8%BF%9B%E7%A8%8B%E5%9C%A8%E5%90%8E%E5%8F%B0%E5%8F%AF%E9%9D%A0%E8%BF%90%E8%A1%8C%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。<br><a id="more"></a></p>
<h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h3><p>只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。同时可在结尾加上”&amp;”来将命令同时放入后台运行，也可用”&gt;filename 2&gt;&amp;1”来更改缺省的重定向文件名。</p>
<pre>
$ nohup ping www.ibm.com &
[1] 6982
$ nohup: appending output to `nohup.out'
$ ps -ef |grep www.ibm.com
  501  6982  5823   0  4:23下午 ttys000    0:00.03 ping www.ibm.com
  501  7120  5823   0  4:26下午 ttys000    0:00.01 grep www.ibm.com
</pre>

<h3 id="setsid"><a href="#setsid" class="headerlink" title="setsid"></a>setsid</h3><p>nohup 能通过忽略 HUP 信号来使我们的进程避免中途被中断，换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。</p>
<pre>
$ setsid ping www.ibm.com
$ ps -ef |grep www.ibm.com
root     31094     1  0 07:28 ?        00:00:00 ping www.ibm.com
root     31102 29217  0 07:29 pts/4    00:00:00 grep www.ibm.com
</pre>

<p>  值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。</p>
<h3 id="disown"><a href="#disown" class="headerlink" title="disown"></a>disown</h3><p>如果未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？</p>
<ul>
<li>用disown -h jobspec来使某个作业忽略HUP信号。</li>
<li>用disown -ah 来使所有的作业都忽略HUP信号。</li>
<li><p>用disown -rh 来使正在运行的作业忽略HUP信号。</p>
<p>需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。<br>但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了”&amp;”来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用 CTRL-z（按住Ctrl键的同时按住z键）了！<br>CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用bg jobspec来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。</p>
<p>disown 示例1（如果提交命令时已经用“&amp;”将命令放入后台运行，则可以直接使用“disown”）</p>
<pre>$ cp -r testLargeFile largeFile &
[1] 4825
$ jobs
[1]+  Running                 cp -i -r testLargeFile largeFile &
$ disown -h %1
$ ps -ef |grep largeFile
root      4825   968  1 09:46 pts/4    00:00:00 cp -i -r testLargeFile largeFile
root      4853   968  0 09:46 pts/4    00:00:00 grep largeFile
$ logout</pre>

<p>disown 示例2（如果提交命令时未使用“&amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）</p>
<pre>$ cp -r testLargeFile largeFile2
[1]+  Stopped                 cp -i -r testLargeFile largeFile2
$ bg %1
[1]+ cp -i -r testLargeFile largeFile2 &
$ jobs
[1]+  Running                 cp -i -r testLargeFile largeFile2 &
$ disown -h %1
$ ps -ef |grep largeFile2
root      5790  5577  1 10:04 pts/3    00:00:00 cp -i -r testLargeFile largeFile2
root      5824  5577  0 10:05 pts/3    00:00:00 grep largeFile2</pre>

</li>
</ul>
<h3 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h3><p>如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？<br>此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen 的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP 信号的影响。<br>使用 screen 很方便，有以下几个常用选项：</p>
<ul>
<li>用screen -dmS session name来建立一个处于断开模式下的会话（并指定其会话名）。</li>
<li>用screen -list 来列出所有会话。</li>
<li>用screen -r session name来重新连接指定会话。</li>
<li><p>用快捷键CTRL-a d 来暂时断开当前会话。</p>
<p>screen 示例</p>
<pre>$ screen -dmS Urumchi
$ screen -list
There is a screen on:
      12842.Urumchi   (Detached)
1 Socket in /tmp/screens/S-root.
$ screen -r Urumchi</pre>

<p>当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。</p>
</li>
</ul>
<ol>
<li><p>未使用 screen 时新进程的进程树</p>
<pre>$ ping www.google.com &
[1] 9499
$ pstree -H 9499
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─2*[sendmail]
  ├─sshd─┬─sshd───bash───pstree
  │       └─sshd───bash───ping</pre>

<p>我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP 信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。</p>
</li>
<li><p>使用了 screen 后新进程的进程树</p>
<pre>$screen -r Urumchi
$ ping www.ibm.com &
[1] 9488
$ pstree -H 9488
init─┬─Xvnc
  ├─acpid
  ├─atd
  ├─screen───bash───ping
  ├─2*[sendmail]</pre>

<p>而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是 init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。</p>
</li>
</ol>
<blockquote>
<p>参考链接<br><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/linux/l-cn-nohup/</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> c </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[python-jieba-分词----官方文档截取]]></title>
      <url>http://www.shuang0420.com/2016/04/01/python-jieba-%E5%88%86%E8%AF%8D----%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E6%88%AA%E5%8F%96/</url>
      <content type="html"><![CDATA[<h1 id="jieba"><a href="#jieba" class="headerlink" title="jieba"></a>jieba</h1><p>“结巴”中文分词：做最好的 Python 中文分词组件</p>
<a id="more"></a>
<h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><ul>
<li>支持三种分词模式：</li>
</ul>
<ol>
<li>精确模式，试图将句子最精确地切开，适合文本分析；</li>
<li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li>
<li>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li>
</ol>
<ul>
<li><p>支持繁体分词</p>
</li>
<li><p>支持自定义词典</p>
</li>
</ul>
<h1 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h1><p>代码对 Python 2/3 均兼容</p>
<ul>
<li>全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba</li>
<li>半自动安装：先下载 <a href="http://pypi.python.org/pypi/jieba/" target="_blank" rel="external">http://pypi.python.org/pypi/jieba/</a> ，解压后运行 python setup.py install</li>
<li>手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录</li>
</ul>
<p>通过 import jieba 来引用</p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ul>
<li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li>
<li>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</li>
</ul>
<h1 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h1><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型<br>jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细<br>待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8<br>jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用 jieba.lcut 以及 jieba.lcut_for_search 直接返回 list<br>jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。</p>
<p>代码示例</p>
<pre>
# encoding=utf-8
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))</pre>

<p>输出:</p>
<p>【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</p>
<p>【精确模式】: 我/ 来到/ 北京/ 清华大学</p>
<p>【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</p>
<p>【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</p>
<h2 id="添加自定义词典"><a href="#添加自定义词典" class="headerlink" title="添加自定义词典"></a>添加自定义词典</h2><p>载入词典</p>
<p>开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率<br>用法：</p>
<pre>jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径</pre>

<p>词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。<br>词频省略时使用自动计算的能保证分出该词的词频。</p>
<p>例如：</p>
<p>创新办 3 i<br>云计算 5<br>凱特琳 nz<br>台中<br>更改分词器（默认为 jieba.dt）的 tmp_dir 和 cache_file 属性，可分别指定缓存文件所在的文件夹及其文件名，用于受限的文件系统。</p>
<p>范例：</p>
<p>自定义词典：<a href="https://github.com/fxsjy/jieba/blob/master/test/userdict.txt" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/userdict.txt</a></p>
<p>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py</a></p>
<p>之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /</p>
<p>加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /</p>
<p>调整词典</p>
<p>使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。<br>使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。</p>
<p>注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。</p>
<p>代码示例：</p>
<pre>
>>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))
如果/放到/post/中将/出错/。
>>> jieba.suggest_freq(('中', '将'), True)
494
>>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))
如果/放到/post/中/将/出错/。
>>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))
「/台/中/」/正确/应该/不会/被/切开
>>> jieba.suggest_freq('台中', True)
69
>>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))
「/台中/」/正确/应该/不会/被/切开
"通过用户自定义词典来增强歧义纠错能力" --- https://github.com/fxsjy/jieba/issues/14
</pre>

<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><p>基于 TF-IDF 算法的关键词抽取</p>
<pre>
import jieba.analyse

jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())</pre>

<p>sentence 为待提取的文本<br>topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20<br>withWeight 为是否一并返回关键词权重值，默认值为 False<br>allowPOS 仅包括指定词性的词，默认值为空，即不筛选<br>jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件</p>
<p>代码示例 （关键词提取）</p>
<p><a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py</a></p>
<p>关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径</p>
<p>用法： <pre>jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径</pre></p>
<p>自定义语料库示例：<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big</a><br>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py</a></p>
<p>关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径</p>
<p>用法： <pre>jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径</pre></p>
<p>自定义语料库示例：<a href="https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt</a><br>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py</a></p>
<p>关键词一并返回关键词权重值示例</p>
<p>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py</a></p>
<p>基于 TextRank 算法的关键词抽取</p>
<pre>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) #直接使用，接口相同，注意默认过滤词性。</pre>
<pre>jieba.analyse.TextRank() #新建自定义 TextRank 实例</pre>

<p>算法论文： TextRank: Bringing Order into Texts</p>
<p><strong>基本思想:</strong></p>
<ul>
<li>将待抽取关键词的文本进行分词</li>
<li>以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图</li>
<li>计算图中节点的PageRank，注意是无向带权图</li>
</ul>
<p>使用示例:</p>
<p>见 test/demo.py</p>
<h2 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h2><p>jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。<br>标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。<br>用法示例</p>
<pre>
>>> import jieba.posseg as pseg
>>> words = pseg.cut("我爱北京天安门")
>>> for word, flag in words:
...    print('%s %s' % (word, flag))
...
我 r
爱 v
北京 ns
天安门 ns
</pre>

<h2 id="并行分词"><a href="#并行分词" class="headerlink" title="并行分词"></a>并行分词</h2><p>原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升<br>基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows<br>用法：</p>
<pre>
jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数
jieba.disable_parallel() # 关闭并行分词模式
</pre>

<p>例子：<a href="https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py</a></p>
<p>实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。</p>
<p>注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。</p>
<h2 id="Tokenize：返回词语在原文的起止位置"><a href="#Tokenize：返回词语在原文的起止位置" class="headerlink" title="Tokenize：返回词语在原文的起止位置"></a>Tokenize：返回词语在原文的起止位置</h2><p>注意，输入参数只接受 unicode</p>
<pre>
# 默认模式
result = jieba.tokenize(u'永和服装饰品有限公司')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))
word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限公司            start: 6                end:10

# 搜索模式
result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))
word 永和                start: 0                end:2
word 服装                start: 2                end:4
word 饰品                start: 4                end:6
word 有限                start: 6                end:8
word 公司                start: 8                end:10
word 有限公司            start: 6                end:10
</pre>


<h2 id="ChineseAnalyzer-for-Whoosh-搜索引擎"><a href="#ChineseAnalyzer-for-Whoosh-搜索引擎" class="headerlink" title="ChineseAnalyzer for Whoosh 搜索引擎"></a>ChineseAnalyzer for Whoosh 搜索引擎</h2><p>引用： from jieba.analyse import ChineseAnalyzer<br>用法示例：<a href="https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py</a></p>
<h2 id="命令行分词"><a href="#命令行分词" class="headerlink" title="命令行分词"></a>命令行分词</h2><p>使用示例：</p>
<pre>python -m jieba news.txt > cut_result.txt</pre>

<p>命令行选项（翻译）：</p>
<p>使用:</p>
<pre>python -m jieba [options] filename</pre>

<p>如果没有指定文件名，则使用标准输入。<br>–help 选项输出：</p>
<pre>
$> python -m jieba --help
Jieba command line interface.

positional arguments:
  filename              input file

optional arguments:
  -h, --help            show this help message and exit
  -d [DELIM], --delimiter [DELIM]
                        use DELIM instead of ' / ' for word delimiter; or a
                        space if it is used without DELIM
  -p [DELIM], --pos [DELIM]
                        enable POS tagging; if DELIM is specified, use DELIM
                        instead of '\_' for POS delimiter
  -D DICT, --dict DICT  use DICT as dictionary
  -u USER_DICT, --user-dict USER_DICT
                        use USER_DICT together with the default dictionary or
                        DICT (if specified)
  -a, --cut-all         full pattern cutting (ignored with POS tagging)
  -n, --no-hmm          don't use the Hidden Markov Model
  -q, --quiet           don't print loading messages to stderr
  -V, --version         show program's version number and exit
</pre>

<p>If no filename specified, use STDIN instead.</p>
<h2 id="延迟加载机制"><a href="#延迟加载机制" class="headerlink" title="延迟加载机制"></a>延迟加载机制</h2><p>jieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。</p>
<p>import jieba<br>jieba.initialize()  # 手动初始化（可选）<br>在 0.28 之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径:</p>
<p>jieba.set_dictionary(‘data/dict.txt.big’)<br>例子： <a href="https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py</a></p>
<h2 id="其他词典"><a href="#其他词典" class="headerlink" title="其他词典"></a>其他词典</h2><p>占用内存较小的词典文件 <a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small" target="_blank" rel="external">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small</a></p>
<p>支持繁体分词更好的词典文件 <a href="https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big" target="_blank" rel="external">https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big</a></p>
<p>下载你所需要的词典，然后覆盖 jieba/dict.txt 即可；或者用 jieba.set_dictionary(‘data/dict.txt.big’)</p>
<h2 id="其他语言实现"><a href="#其他语言实现" class="headerlink" title="其他语言实现"></a>其他语言实现</h2><p>结巴分词 Java 版本</p>
<p>作者：piaolingxue 地址：<a href="https://github.com/huaban/jieba-analysis" target="_blank" rel="external">https://github.com/huaban/jieba-analysis</a></p>
<p>结巴分词 C++ 版本</p>
<p>作者：yanyiwu 地址：<a href="https://github.com/yanyiwu/cppjieba" target="_blank" rel="external">https://github.com/yanyiwu/cppjieba</a></p>
<p>结巴分词 Node.js 版本</p>
<p>作者：yanyiwu 地址：<a href="https://github.com/yanyiwu/nodejieba" target="_blank" rel="external">https://github.com/yanyiwu/nodejieba</a></p>
<p>结巴分词 Erlang 版本</p>
<p>作者：falood 地址：<a href="https://github.com/falood/exjieba" target="_blank" rel="external">https://github.com/falood/exjieba</a></p>
<p>结巴分词 R 版本</p>
<p>作者：qinwf 地址：<a href="https://github.com/qinwf/jiebaR" target="_blank" rel="external">https://github.com/qinwf/jiebaR</a></p>
<p>结巴分词 iOS 版本</p>
<p>作者：yanyiwu 地址：<a href="https://github.com/yanyiwu/iosjieba" target="_blank" rel="external">https://github.com/yanyiwu/iosjieba</a></p>
<p>结巴分词 PHP 版本</p>
<p>作者：fukuball 地址：<a href="https://github.com/fukuball/jieba-php" target="_blank" rel="external">https://github.com/fukuball/jieba-php</a></p>
<p>结巴分词 .NET(C#) 版本</p>
<p>作者：anderscui 地址：<a href="https://github.com/anderscui/jieba.NET/" target="_blank" rel="external">https://github.com/anderscui/jieba.NET/</a></p>
<p>系统集成</p>
<p>Solr: <a href="https://github.com/sing1ee/jieba-solr" target="_blank" rel="external">https://github.com/sing1ee/jieba-solr</a><br>分词速度</p>
<p>1.5 MB / Second in Full Mode<br>400 KB / Second in Default Mode<br>测试环境: Intel(R) Core(TM) i7-2600 CPU @ 3.4GHz；《围城》.txt</p>
<h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><ul>
<li>模型的数据是如何生成的？</li>
</ul>
<p>详见： <a href="https://github.com/fxsjy/jieba/issues/7" target="_blank" rel="external">https://github.com/fxsjy/jieba/issues/7</a></p>
<ul>
<li>“台中”总是被切成“台 中”？（以及类似情况）</li>
</ul>
<p>P(台中) ＜ P(台)×P(中)，“台中”词频不够导致其成词概率较低</p>
<p>解决方法：强制调高词频</p>
<p>jieba.add_word(‘台中’) 或者 jieba.suggest_freq(‘台中’, True)</p>
<ul>
<li>“今天天气 不错”应该被切成“今天 天气 不错”？（以及类似情况）</li>
</ul>
<p>解决方法：强制调低词频</p>
<p>jieba.suggest_freq((‘今天’, ‘天气’), True)</p>
<p>或者直接删除该词 jieba.del_word(‘今天天气’)</p>
<ul>
<li>切出了词典中没有的词语，效果不理想？</li>
</ul>
<p>解决方法：关闭新词发现</p>
<p>jieba.cut(‘丰田太省了’, HMM=False) jieba.cut(‘我们中出了一个叛徒’, HMM=False)</p>
<p>更多问题请点击：<a href="https://github.com/fxsjy/jieba/issues?sort=updated&amp;state=closed" target="_blank" rel="external">https://github.com/fxsjy/jieba/issues?sort=updated&amp;state=closed</a></p>
<p>修订历史</p>
<p><a href="https://github.com/fxsjy/jieba/blob/master/Changelog" target="_blank" rel="external">https://github.com/fxsjy/jieba/blob/master/Changelog</a></p>
<blockquote>
<p>参考链接<br><a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">https://github.com/fxsjy/jieba</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> NLP </category>
            
        </categories>
        
        
        <tags>
            
            <tag> gensim </tag>
            
            <tag> 分词 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[linux-python转码问题]]></title>
      <url>http://www.shuang0420.com/2016/03/22/linux-python%E8%BD%AC%E7%A0%81%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>之前用的一直是utf-8编码，几乎不会出现乱码问题。奈何公司的分词软件支持的输入和输出编码都是gbk，因此必须进行转码，一个非常痛苦的过程，如实记录下遇到的问题，供以后参考</p>
<a id="more"></a>
<h3 id="标准utf8输出"><a href="#标准utf8输出" class="headerlink" title="标准utf8输出"></a>标准utf8输出</h3><pre>
#!/usr/bin/python
# -*- coding: utf8 -*-
#################### deal with base64 file ###################
import gensim, logging
from gensim.models import Doc2Vec
import os
import multiprocessing
import numpy as np
import base64
import codecs
import g_url_text_pb2
import re
import sys
reload(sys)
sys.setdefaultencoding('utf8')
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def newFile():
    global count
    fw = open('baike_url_part_000.chinese','w')
    with open('baike_url_part_000.result') as f:
        for line in f:
            base64Test = base64.b64decode(line)
            model=g_url_text_pb2.TextInfo()
            model.ParseFromString(base64Test)
            doc = model.title.decode('gbk', 'ignore')+' '+(model.content.decode('gbk', 'ignore'))
            doc = "".join(doc.split())#处理/r/t/n等
            fw.write("".join(doc)+"\n")
newFile()
</pre>

<p><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E9%83%A8%E5%88%86%E4%B9%B1%E7%A0%81.png" alt=""></p>
<h3 id="转gbk输出"><a href="#转gbk输出" class="headerlink" title="转gbk输出"></a>转gbk输出</h3><p>先不管那些奇怪的^@^F等字符，转成gbk编码写入文件，加个encode就行啦</p>
<pre>doc = doc.encode('gbk','ignore')
doc = "".join(doc.split())
fw.write("".join(doc)+"\n")
</pre>

<p>打开一看，纳尼，怎么变成了这样！这是什么鬼！<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E4%B9%B1%E7%A0%81.png" alt=""></p>
<p>冷静……查看一下编码格式</p>
<pre>file baike_url_part_000.chinese</pre>

<p>然而……只显示了data……好忧伤……</p>
<pre>baike_url_part_000.chinese: data</pre>

<p>再看一下？好吧……binary。。</p>
<pre>file -i baike_url_part_000.chinese
baike_url_part_000.chinese: application/octet-stream; charset=binary</pre>


<h3 id="iconv-文件编码转换"><a href="#iconv-文件编码转换" class="headerlink" title="iconv 文件编码转换"></a>iconv 文件编码转换</h3><p>iconv [选项…] [文件…]</p>
<p>输入/输出格式规范：<br>-f, –from-code=名称 原始文本编码<br>-t, –to-code=名称 输出编码</p>
<p>信息：<br>-l, –list 列举所有已知的字符集</p>
<p>输出控制：<br>-c 从输出中忽略无效的字符<br>-o, –output=FILE 输出文件<br>-s, –silent 关闭警告<br>–verbose 打印进度信息</p>
<pre># utf 转 gbk
iconv -c -f utf-8 -t gb2312 file</pre>

<h3 id="傻瓜命令行工具enca"><a href="#傻瓜命令行工具enca" class="headerlink" title="傻瓜命令行工具enca"></a>傻瓜命令行工具enca</h3><p>好了，这时候就要用神器啦！傻瓜命令行工具enca – 不但能智能识别文件的编码，而且还支持成批转换！心动了吗？心动不如行动！来！安装！so easy~  　　</p>
<pre>sudo apt-get install enca</pre>

<p>常用的命令格式如下 　　</p>
<pre>#检查文件的编码　
#enca -L 当前语言 -x 目标编码 文件名　
enca -L zh_CN file   　　
#将文件编码转换为"UTF-8"编码　
enca -L zh_CN -x UTF-8 file
#如果不想覆盖原文件
enca -L zh_CN -x UTF-8 < file1 > file2
#把当前目录下的所有文件都转成utf-8  　　
enca -L zh_CN -x utf-8 * </pre>  

<p>这里我们这么用👇</p>
<pre>$ enca -L zh_CN baike_url_part_000.chinese
Simplified Chinese National Standard; GB2312
</pre>

<h3 id="locale"><a href="#locale" class="headerlink" title="locale"></a>locale</h3><p>发现是GB2312,说明不是代码的问题。但是为什么显示出来是乱码呢？那只是因为显示的时候使用的字符编码方式和实际内容的字符编码不一致，所以解决方式当然就是双方都用同一种编码方式喽。简单的命令就可以实现啦</p>
<pre>
export LC_ALL=
</pre>

<p>关于locale，强烈推荐看看<a href="http://www.linuxsky.org/doc/desktop/200704/20.html" target="_blank" rel="external">Locale 详解</a>，然后搞明白以下三个环境变量的优先级：LC<em>ALL&gt;LC</em>*&gt;LANG。locale相关的各个环境变量的作用参见<a href="https://help.ubuntu.com/community/EnvironmentVariables#Locale_setting_variables" target="_blank" rel="external">这里</a>。</p>
<h3 id="处理-F-A等特殊字符"><a href="#处理-F-A等特殊字符" class="headerlink" title="处理^@^F^A等特殊字符"></a>处理^@^F^A等特殊字符</h3><p>虽然可以显示了，但中间还有许多不能识别的字符<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E9%83%A8%E5%88%86%E4%B9%B1%E7%A0%81.png" alt=""></p>
<p>看一下这些字符的ascii对照表<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/ascii.jpg" alt=""></p>
<p>然后一键替换，下面是将^@替换为空格的例子</p>
<pre>sed -i "s/[\x00]/ /g" baike_url_part_000.chinese </pre>

<p>然而不可见字符这么多，总不能一个个替换吧！在python中直接用正则做替换，在split前加上一行代码</p>
<pre>doc = re.sub(r'[\x00-\x0F]+',' ', doc)
doc = "".join(doc.split())
fw.write("".join(doc)+"\n")</pre>

<p>顺便提一下，在python中，字符串前加r代表此字符串为原样显示，不转义。就像字符串’\n’转义是换行，若其前加上字母r,即r’\n’，则不进行转义，结果将原样显示’\n’。</p>
<p>这样，才算真正解决了这里的乱码问题。<br><img src="http://7xu83c.com1.z0.glb.clouddn.com/%E6%AD%A3%E7%A1%AE.png" alt=""></p>
<p>　</p>
]]></content>
      
        <categories>
            
            <category> Others </category>
            
        </categories>
        
        
        <tags>
            
            <tag> python </tag>
            
            <tag> shell </tag>
            
            <tag> 编码 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
